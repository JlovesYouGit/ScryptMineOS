This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
stratum-1.4.0/.github/workflows/auto-rebase.yaml
stratum-1.4.0/.github/workflows/clippy-lint.yaml
stratum-1.4.0/.github/workflows/coverage-protocols.yaml
stratum-1.4.0/.github/workflows/coverage-roles.yaml
stratum-1.4.0/.github/workflows/coverage-utils.yaml
stratum-1.4.0/.github/workflows/docs.yaml
stratum-1.4.0/.github/workflows/fmt.yaml
stratum-1.4.0/.github/workflows/integration-tests.yaml
stratum-1.4.0/.github/workflows/lockfiles.yaml
stratum-1.4.0/.github/workflows/release-libs.yaml
stratum-1.4.0/.github/workflows/rust-msrv.yaml
stratum-1.4.0/.github/workflows/semver-check.yaml
stratum-1.4.0/.github/workflows/test.yaml
stratum-1.4.0/.gitignore
stratum-1.4.0/benches/benches/README.md
stratum-1.4.0/benches/benches/src/sv1/criterion_sv1_benchmark.rs
stratum-1.4.0/benches/benches/src/sv1/iai_sv1_benchmark.rs
stratum-1.4.0/benches/benches/src/sv1/lib/client.rs
stratum-1.4.0/benches/benches/src/sv2/criterion_sv2_benchmark.rs
stratum-1.4.0/benches/benches/src/sv2/iai_sv2_benchmark.rs
stratum-1.4.0/benches/benches/src/sv2/lib/client.rs
stratum-1.4.0/benches/Cargo.toml
stratum-1.4.0/codecov.yaml
stratum-1.4.0/common/Cargo.toml
stratum-1.4.0/common/src/lib.rs
stratum-1.4.0/CONTRIBUTING.md
stratum-1.4.0/examples/interop-cpp-no-cargo/.gitignore
stratum-1.4.0/examples/interop-cpp-no-cargo/README.md
stratum-1.4.0/examples/interop-cpp-no-cargo/run.sh
stratum-1.4.0/examples/interop-cpp-no-cargo/rust-build-script.sh
stratum-1.4.0/examples/interop-cpp/.gitignore
stratum-1.4.0/examples/interop-cpp/Cargo.toml
stratum-1.4.0/examples/interop-cpp/README.md
stratum-1.4.0/examples/interop-cpp/run.sh
stratum-1.4.0/examples/interop-cpp/src/main.rs
stratum-1.4.0/examples/ping-pong-encrypted/Cargo.toml
stratum-1.4.0/examples/ping-pong-encrypted/README.md
stratum-1.4.0/examples/ping-pong-encrypted/src/client.rs
stratum-1.4.0/examples/ping-pong-encrypted/src/error.rs
stratum-1.4.0/examples/ping-pong-encrypted/src/main.rs
stratum-1.4.0/examples/ping-pong-encrypted/src/messages.rs
stratum-1.4.0/examples/ping-pong-encrypted/src/server.rs
stratum-1.4.0/examples/ping-pong/Cargo.toml
stratum-1.4.0/examples/ping-pong/README.md
stratum-1.4.0/examples/ping-pong/src/client.rs
stratum-1.4.0/examples/ping-pong/src/error.rs
stratum-1.4.0/examples/ping-pong/src/main.rs
stratum-1.4.0/examples/ping-pong/src/messages.rs
stratum-1.4.0/examples/ping-pong/src/server.rs
stratum-1.4.0/LICENSE-APACHE
stratum-1.4.0/LICENSE-MIT
stratum-1.4.0/LICENSE.md
stratum-1.4.0/protocols/Cargo.toml
stratum-1.4.0/protocols/fuzz-tests/.gitignore
stratum-1.4.0/protocols/fuzz-tests/Cargo.toml
stratum-1.4.0/protocols/fuzz-tests/src/main.rs
stratum-1.4.0/protocols/tarpaulin.toml
stratum-1.4.0/protocols/v1/Cargo.toml
stratum-1.4.0/protocols/v1/examples/client_and_server.rs
stratum-1.4.0/protocols/v1/README.md
stratum-1.4.0/protocols/v1/src/error.rs
stratum-1.4.0/protocols/v1/src/json_rpc.rs
stratum-1.4.0/protocols/v1/src/lib.rs
stratum-1.4.0/protocols/v1/src/methods/client_to_server.rs
stratum-1.4.0/protocols/v1/src/methods/mod.rs
stratum-1.4.0/protocols/v1/src/methods/server_to_client.rs
stratum-1.4.0/protocols/v1/src/utils.rs
stratum-1.4.0/protocols/v2/binary-sv2/Cargo.toml
stratum-1.4.0/protocols/v2/binary-sv2/codec/.gitignore
stratum-1.4.0/protocols/v2/binary-sv2/codec/Cargo.toml
stratum-1.4.0/protocols/v2/binary-sv2/codec/README.md
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/decodable.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/encodable.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/impls.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/mod.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/copy_data_types.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/mod.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/inner.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/mod.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/seq_inner.rs
stratum-1.4.0/protocols/v2/binary-sv2/codec/src/lib.rs
stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/.gitignore
stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/Cargo.toml
stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/README.md
stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/src/lib.rs
stratum-1.4.0/protocols/v2/binary-sv2/examples/encode_decode.rs
stratum-1.4.0/protocols/v2/binary-sv2/README.md
stratum-1.4.0/protocols/v2/binary-sv2/src/lib.rs
stratum-1.4.0/protocols/v2/codec-sv2/Cargo.toml
stratum-1.4.0/protocols/v2/codec-sv2/examples/encrypted.rs
stratum-1.4.0/protocols/v2/codec-sv2/examples/unencrypted.rs
stratum-1.4.0/protocols/v2/codec-sv2/README.md
stratum-1.4.0/protocols/v2/codec-sv2/src/decoder.rs
stratum-1.4.0/protocols/v2/codec-sv2/src/encoder.rs
stratum-1.4.0/protocols/v2/codec-sv2/src/error.rs
stratum-1.4.0/protocols/v2/codec-sv2/src/lib.rs
stratum-1.4.0/protocols/v2/framing-sv2/Cargo.toml
stratum-1.4.0/protocols/v2/framing-sv2/examples/sv2_frame.rs
stratum-1.4.0/protocols/v2/framing-sv2/README.md
stratum-1.4.0/protocols/v2/framing-sv2/src/error.rs
stratum-1.4.0/protocols/v2/framing-sv2/src/framing.rs
stratum-1.4.0/protocols/v2/framing-sv2/src/header.rs
stratum-1.4.0/protocols/v2/framing-sv2/src/lib.rs
stratum-1.4.0/protocols/v2/noise-sv2/Cargo.toml
stratum-1.4.0/protocols/v2/noise-sv2/examples/handshake.rs
stratum-1.4.0/protocols/v2/noise-sv2/README.md
stratum-1.4.0/protocols/v2/noise-sv2/src/aed_cipher.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/cipher_state.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/error.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/handshake.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/initiator.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/lib.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/responder.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/signature_message.rs
stratum-1.4.0/protocols/v2/noise-sv2/src/test.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/Cargo.toml
stratum-1.4.0/protocols/v2/roles-logic-sv2/README.md
stratum-1.4.0/protocols/v2/roles-logic-sv2/reg-test-block.toml
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channel_logic/channel_factory.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channel_logic/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/chain_tip.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/error.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/extended.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/group.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/share_accounting.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/standard.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/error.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/extended.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/group.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/error.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/extended.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/factory.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/job_store.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/standard.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/share_accounting.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/standard.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/errors.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/common.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/job_declaration.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/mining.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/template_distribution.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/job_creator.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/lib.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/parsers.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/utils.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/classic.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/error.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/mod.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/test/classic.rs
stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/test/mod.rs
stratum-1.4.0/protocols/v2/subprotocols/common-messages/Cargo.toml
stratum-1.4.0/protocols/v2/subprotocols/common-messages/README.md
stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/channel_endpoint_changed.rs
stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/lib.rs
stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/reconnect.rs
stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/setup_connection.rs
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/Cargo.toml
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/README.md
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/allocate_mining_job_token.rs
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/declare_mining_job.rs
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/lib.rs
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/provide_missing_transactions.rs
stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/push_solution.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/Cargo.toml
stratum-1.4.0/protocols/v2/subprotocols/mining/README.md
stratum-1.4.0/protocols/v2/subprotocols/mining/src/close_channel.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/lib.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/new_mining_job.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/open_channel.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_custom_mining_job.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_extranonce_prefix.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_group_channel.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_new_prev_hash.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_target.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/submit_shares.rs
stratum-1.4.0/protocols/v2/subprotocols/mining/src/update_channel.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/Cargo.toml
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/README.md
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/coinbase_output_constraints.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/lib.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/new_template.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/request_transaction_data.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/set_new_prev_hash.rs
stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/submit_solution.rs
stratum-1.4.0/protocols/v2/sv2-ffi/Cargo.toml
stratum-1.4.0/protocols/v2/sv2-ffi/src/lib.rs
stratum-1.4.0/protocols/v2/sv2-ffi/sv2.h
stratum-1.4.0/README.md
stratum-1.4.0/RELEASE.md
stratum-1.4.0/roles/Cargo.toml
stratum-1.4.0/roles/jd-client/Cargo.toml
stratum-1.4.0/roles/jd-client/config-examples/jdc-config-hosted-example.toml
stratum-1.4.0/roles/jd-client/config-examples/jdc-config-local-example.toml
stratum-1.4.0/roles/jd-client/README.md
stratum-1.4.0/roles/jd-client/src/args.rs
stratum-1.4.0/roles/jd-client/src/lib/config.rs
stratum-1.4.0/roles/jd-client/src/lib/downstream.rs
stratum-1.4.0/roles/jd-client/src/lib/error.rs
stratum-1.4.0/roles/jd-client/src/lib/job_declarator/message_handler.rs
stratum-1.4.0/roles/jd-client/src/lib/job_declarator/mod.rs
stratum-1.4.0/roles/jd-client/src/lib/job_declarator/setup_connection.rs
stratum-1.4.0/roles/jd-client/src/lib/mod.rs
stratum-1.4.0/roles/jd-client/src/lib/status.rs
stratum-1.4.0/roles/jd-client/src/lib/template_receiver/message_handler.rs
stratum-1.4.0/roles/jd-client/src/lib/template_receiver/mod.rs
stratum-1.4.0/roles/jd-client/src/lib/template_receiver/setup_connection.rs
stratum-1.4.0/roles/jd-client/src/lib/upstream_sv2/mod.rs
stratum-1.4.0/roles/jd-client/src/lib/upstream_sv2/upstream.rs
stratum-1.4.0/roles/jd-client/src/main.rs
stratum-1.4.0/roles/jd-server/Cargo.toml
stratum-1.4.0/roles/jd-server/config-examples/jds-config-hosted-example.toml
stratum-1.4.0/roles/jd-server/config-examples/jds-config-local-example.toml
stratum-1.4.0/roles/jd-server/src/args.rs
stratum-1.4.0/roles/jd-server/src/lib/config.rs
stratum-1.4.0/roles/jd-server/src/lib/error.rs
stratum-1.4.0/roles/jd-server/src/lib/job_declarator/message_handler.rs
stratum-1.4.0/roles/jd-server/src/lib/job_declarator/mod.rs
stratum-1.4.0/roles/jd-server/src/lib/mempool/error.rs
stratum-1.4.0/roles/jd-server/src/lib/mempool/mod.rs
stratum-1.4.0/roles/jd-server/src/lib/mod.rs
stratum-1.4.0/roles/jd-server/src/lib/status.rs
stratum-1.4.0/roles/jd-server/src/main.rs
stratum-1.4.0/roles/pool/Cargo.toml
stratum-1.4.0/roles/pool/config-examples/pool-config-hosted-tp-example.toml
stratum-1.4.0/roles/pool/config-examples/pool-config-local-tp-example.toml
stratum-1.4.0/roles/pool/README.md
stratum-1.4.0/roles/pool/src/args.rs
stratum-1.4.0/roles/pool/src/lib/config.rs
stratum-1.4.0/roles/pool/src/lib/error.rs
stratum-1.4.0/roles/pool/src/lib/mining_pool/message_handler.rs
stratum-1.4.0/roles/pool/src/lib/mining_pool/mod.rs
stratum-1.4.0/roles/pool/src/lib/mining_pool/setup_connection.rs
stratum-1.4.0/roles/pool/src/lib/mod.rs
stratum-1.4.0/roles/pool/src/lib/status.rs
stratum-1.4.0/roles/pool/src/lib/template_receiver/message_handler.rs
stratum-1.4.0/roles/pool/src/lib/template_receiver/mod.rs
stratum-1.4.0/roles/pool/src/lib/template_receiver/setup_connection.rs
stratum-1.4.0/roles/pool/src/main.rs
stratum-1.4.0/roles/roles-utils/config-helpers/Cargo.toml
stratum-1.4.0/roles/roles-utils/config-helpers/README.md
stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/errors.rs
stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/mod.rs
stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/serde_types.rs
stratum-1.4.0/roles/roles-utils/config-helpers/src/lib.rs
stratum-1.4.0/roles/roles-utils/config-helpers/src/logging.rs
stratum-1.4.0/roles/roles-utils/config-helpers/src/toml.rs
stratum-1.4.0/roles/roles-utils/network-helpers/Cargo.toml
stratum-1.4.0/roles/roles-utils/network-helpers/src/lib.rs
stratum-1.4.0/roles/roles-utils/network-helpers/src/noise_connection.rs
stratum-1.4.0/roles/roles-utils/network-helpers/src/plain_connection.rs
stratum-1.4.0/roles/roles-utils/network-helpers/src/sv1_connection.rs
stratum-1.4.0/roles/roles-utils/rpc/Cargo.toml
stratum-1.4.0/roles/roles-utils/rpc/src/lib.rs
stratum-1.4.0/roles/roles-utils/rpc/src/mini_rpc_client.rs
stratum-1.4.0/roles/tarpaulin.toml
stratum-1.4.0/roles/test-utils/mining-device-sv1/Cargo.toml
stratum-1.4.0/roles/test-utils/mining-device-sv1/src/client.rs
stratum-1.4.0/roles/test-utils/mining-device-sv1/src/job.rs
stratum-1.4.0/roles/test-utils/mining-device-sv1/src/lib.rs
stratum-1.4.0/roles/test-utils/mining-device-sv1/src/main.rs
stratum-1.4.0/roles/test-utils/mining-device-sv1/src/miner.rs
stratum-1.4.0/roles/test-utils/mining-device/Cargo.toml
stratum-1.4.0/roles/test-utils/mining-device/README.md
stratum-1.4.0/roles/test-utils/mining-device/src/lib/mod.rs
stratum-1.4.0/roles/test-utils/mining-device/src/main.rs
stratum-1.4.0/roles/translator/Cargo.toml
stratum-1.4.0/roles/translator/config-examples/tproxy-config-hosted-pool-example.toml
stratum-1.4.0/roles/translator/config-examples/tproxy-config-local-jdc-example.toml
stratum-1.4.0/roles/translator/config-examples/tproxy-config-local-pool-example.toml
stratum-1.4.0/roles/translator/README.md
stratum-1.4.0/roles/translator/src/args.rs
stratum-1.4.0/roles/translator/src/lib/config.rs
stratum-1.4.0/roles/translator/src/lib/downstream_sv1/diff_management.rs
stratum-1.4.0/roles/translator/src/lib/downstream_sv1/downstream.rs
stratum-1.4.0/roles/translator/src/lib/downstream_sv1/mod.rs
stratum-1.4.0/roles/translator/src/lib/error.rs
stratum-1.4.0/roles/translator/src/lib/mod.rs
stratum-1.4.0/roles/translator/src/lib/proxy/bridge.rs
stratum-1.4.0/roles/translator/src/lib/proxy/mod.rs
stratum-1.4.0/roles/translator/src/lib/proxy/next_mining_notify.rs
stratum-1.4.0/roles/translator/src/lib/status.rs
stratum-1.4.0/roles/translator/src/lib/upstream_sv2/diff_management.rs
stratum-1.4.0/roles/translator/src/lib/upstream_sv2/mod.rs
stratum-1.4.0/roles/translator/src/lib/upstream_sv2/upstream_connection.rs
stratum-1.4.0/roles/translator/src/lib/upstream_sv2/upstream.rs
stratum-1.4.0/roles/translator/src/lib/utils.rs
stratum-1.4.0/roles/translator/src/main.rs
stratum-1.4.0/rustfmt.toml
stratum-1.4.0/scripts/build_header.sh
stratum-1.4.0/scripts/build-on-all-workspaces.sh
stratum-1.4.0/scripts/clippy-fmt-and-test.sh
stratum-1.4.0/scripts/coverage-protocols.sh
stratum-1.4.0/scripts/coverage-roles.sh
stratum-1.4.0/scripts/coverage-utils.sh
stratum-1.4.0/scripts/release-libs.sh
stratum-1.4.0/scripts/rust/clippy.sh
stratum-1.4.0/scripts/sv2-publish.sh
stratum-1.4.0/test/integration-tests/.config/nextest.toml
stratum-1.4.0/test/integration-tests/Cargo.toml
stratum-1.4.0/test/integration-tests/lib/interceptor.rs
stratum-1.4.0/test/integration-tests/lib/message_aggregator.rs
stratum-1.4.0/test/integration-tests/lib/mock_roles.rs
stratum-1.4.0/test/integration-tests/lib/mod.rs
stratum-1.4.0/test/integration-tests/lib/sniffer_error.rs
stratum-1.4.0/test/integration-tests/lib/sniffer.rs
stratum-1.4.0/test/integration-tests/lib/sv1_sniffer.rs
stratum-1.4.0/test/integration-tests/lib/template_provider.rs
stratum-1.4.0/test/integration-tests/lib/types.rs
stratum-1.4.0/test/integration-tests/lib/utils.rs
stratum-1.4.0/test/integration-tests/README.md
stratum-1.4.0/test/integration-tests/tests/jd_integration.rs
stratum-1.4.0/test/integration-tests/tests/jd_provide_missing_transaction.rs
stratum-1.4.0/test/integration-tests/tests/jd_tproxy_integration.rs
stratum-1.4.0/test/integration-tests/tests/jdc_block_propogation.rs
stratum-1.4.0/test/integration-tests/tests/jdc_fallback.rs
stratum-1.4.0/test/integration-tests/tests/jdc_receives_submit_shares_success.rs
stratum-1.4.0/test/integration-tests/tests/jds_block_propogation.rs
stratum-1.4.0/test/integration-tests/tests/pool_integration.rs
stratum-1.4.0/test/integration-tests/tests/sniffer_integration.rs
stratum-1.4.0/test/integration-tests/tests/sv1.rs
stratum-1.4.0/test/integration-tests/tests/sv2_mining_device.rs
stratum-1.4.0/test/integration-tests/tests/translator_integration.rs
stratum-1.4.0/test/scale/Cargo.toml
stratum-1.4.0/test/scale/README.md
stratum-1.4.0/test/scale/src/main.rs
stratum-1.4.0/utils/bip32-key-derivation/Cargo.toml
stratum-1.4.0/utils/bip32-key-derivation/README.md
stratum-1.4.0/utils/bip32-key-derivation/src/lib.rs
stratum-1.4.0/utils/bip32-key-derivation/src/main.rs
stratum-1.4.0/utils/buffer/BENCHES.md
stratum-1.4.0/utils/buffer/benches/control_struct.rs
stratum-1.4.0/utils/buffer/benches/pool_benchmark.rs
stratum-1.4.0/utils/buffer/benches/pool_iai.rs
stratum-1.4.0/utils/buffer/Cargo.toml
stratum-1.4.0/utils/buffer/examples/basic_buffer_pool.rs
stratum-1.4.0/utils/buffer/examples/buffer_pool_exhaustion.rs
stratum-1.4.0/utils/buffer/examples/variable_sized_messages.rs
stratum-1.4.0/utils/buffer/fuzz/.gitignore
stratum-1.4.0/utils/buffer/fuzz/Cargo.toml
stratum-1.4.0/utils/buffer/fuzz/fuzz_targets/faster.rs
stratum-1.4.0/utils/buffer/fuzz/fuzz_targets/slower.rs
stratum-1.4.0/utils/buffer/fuzz/run.sh
stratum-1.4.0/utils/buffer/fuzz/rust-toolchain.toml
stratum-1.4.0/utils/buffer/README.md
stratum-1.4.0/utils/buffer/src/buffer_pool/mod.rs
stratum-1.4.0/utils/buffer/src/buffer_pool/pool_back.rs
stratum-1.4.0/utils/buffer/src/buffer.rs
stratum-1.4.0/utils/buffer/src/lib.rs
stratum-1.4.0/utils/buffer/src/slice.rs
stratum-1.4.0/utils/buffer/src/test.rs
stratum-1.4.0/utils/Cargo.toml
stratum-1.4.0/utils/error-handling/Cargo.toml
stratum-1.4.0/utils/error-handling/src/lib.rs
stratum-1.4.0/utils/key-utils/Cargo.toml
stratum-1.4.0/utils/key-utils/README.md
stratum-1.4.0/utils/key-utils/src/lib.rs
stratum-1.4.0/utils/key-utils/src/main.rs
stratum-1.4.0/utils/tarpaulin.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="stratum-1.4.0/.github/workflows/auto-rebase.yaml">
name: Auto Rebase

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  rebase-outdated-prs:
    runs-on: ubuntu-latest
    steps:

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PAT }}
          fetch-depth: 0  # Fetch full history to have the entire commit history

      - name: Fetch open pull requests with label
        run: |
          gh auth setup-git
          gh pr list --state open --label "ready-to-be-merged" --json number,headRepositoryOwner,headRefName --jq '.[] | "\(.number) \(.headRepositoryOwner.login) \(.headRefName)"' > pr_details.txt
        env:
          GITHUB_TOKEN: ${{ secrets.PAT }}

      - name: Rebase pull requests
        run: |
          while read pr_number pr_owner pr_branch; do
            echo "Processing PR #$pr_number"

            # Add the contributor's fork as a remote
            git remote add contributor https://github.com/$pr_owner/$(gh repo view --json name -q '.name').git

            # Fetch the contributor's branches
            git fetch contributor

            # Create a unique branch name for this PR
            unique_branch_name="contributor-branch-$pr_number"

            # Checkout the branch from the contributor's fork
            git checkout -b $unique_branch_name contributor/$pr_branch

            # Set the committer name and email to match the PR author
            PR_AUTHOR_NAME=$(gh pr view $pr_number --json author --jq '.author.login')
            PR_AUTHOR_EMAIL="${PR_AUTHOR_NAME}@users.noreply.github.com"

            git config --global user.name "$PR_AUTHOR_NAME"
            git config --global user.email "$PR_AUTHOR_EMAIL"

            # Rebase the branch on top of the main branch
            git fetch origin main
            if ! git rebase origin/main; then
              echo "Conflict detected. Aborting rebase and continuing."
              git rebase --abort

              # Post a comment on the PR to notify the author about the conflict
              gh pr comment $pr_number --body "Hey @$PR_AUTHOR_NAME, your PR cannot be rebased due to conflicts. Could you resolve them, please?"

              continue
            fi

            # Push the rebased branch back to the contributor's fork
            git push --force-with-lease contributor $unique_branch_name:$pr_branch

            # Remove the remote
            git remote remove contributor

            # Ensure we are not on the branch to be deleted
            git checkout main

            # Delete the local unique branch
            git branch -D $unique_branch_name

          done < pr_details.txt
        env:
          GITHUB_TOKEN: ${{ secrets.PAT }}
</file>

<file path="stratum-1.4.0/.github/workflows/clippy-lint.yaml">
on:
  pull_request:
    branches:
      - main

name: Clippy Lint

jobs:
  clippy-check:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - macos-latest
          - ubuntu-latest
        include:
          - os: macos-latest
            target: x86_64-apple-darwin
          - os: ubuntu-latest
            target: x86_64-unknown-linux-musl

    steps:
      - uses: actions/checkout@v4
      - uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: stable
          override: true
          components: clippy
      - name: Run Clippy on different workspaces and crates
        run: ./scripts/rust/clippy.sh
</file>

<file path="stratum-1.4.0/.github/workflows/coverage-protocols.yaml">
name: Protocol test Coverage

on:
  push:
    branches:
      - main

jobs:
  protocols-coverage:

    name: tarpaulin Test
    runs-on: ubuntu-latest
    container:
      image: xd009642/tarpaulin:0.27.1-nightly
      options: --security-opt seccomp=unconfined
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate code coverage
        run: |
          ./scripts/coverage-protocols.sh

      - name: Upload protocols coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports
          file: ./protocols/target/tarpaulin-reports/cobertura.xml
          flags: protocols
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload binary_codec_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/codec-coverage
          file: ./protocols/target/tarpaulin-reports/codec-coverage/cobertura.xml
          flags: binary_codec_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
      
      - name: Upload binary_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/binary-sv2-coverage
          file: ./protocols/target/tarpaulin-reports/binary-sv2-coverage/cobertura.xml
          flags: binary_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload codec_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/codec-sv2-coverage
          file: ./protocols/target/tarpaulin-reports/codec-sv2-coverage/cobertura.xml
          flags: codec_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload common_messages_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/common-messages-coverage
          file: ./protocols/target/tarpaulin-reports/common-messages-coverage/cobertura.xml
          flags: common_messages_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload framing_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/framing-sv2-coverage
          file: ./protocols/target/tarpaulin-reports/framing-sv2-coverage/cobertura.xml
          flags: framing_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload job_declaration_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/job-declaration-coverage
          file: ./protocols/target/tarpaulin-reports/job-declaration-coverage/cobertura.xml
          flags: job_declaration_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload noise_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/noise-sv2-coverage
          file: ./protocols/target/tarpaulin-reports/noise-sv2-coverage/cobertura.xml
          flags: noise_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload roles_logic_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/roles-logic-sv2-coverage
          file: ./protocols/target/tarpaulin-reports/roles-logic-sv2-coverage/cobertura.xml
          flags: roles_logic_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload v1-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/v1-coverage
          file: ./protocols/target/tarpaulin-reports/v1-coverage/cobertura.xml
          flags: v1-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload sv2_ffi-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/sv2-ffi-coverage
          file: ./protocols/target/tarpaulin-reports/sv2-ffi-coverage/cobertura.xml
          flags: sv2_ffi-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload template_distribution_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/template-distribution-coverage
          file: ./protocols/target/tarpaulin-reports/template-distribution-coverage/cobertura.xml
          flags: template_distribution_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload mining-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./protocols/target/tarpaulin-reports/mining-coverage
          file: ./protocols/target/tarpaulin-reports/mining-coverage/cobertura.xml
          flags: mining-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
</file>

<file path="stratum-1.4.0/.github/workflows/coverage-roles.yaml">
name: Roles test Coverage

on:
  push:
    branches:
      - main

jobs:
  roles-coverage:

    name: tarpaulin Test
    runs-on: ubuntu-latest
    container:
      image: xd009642/tarpaulin:0.27.1-nightly
      options: --security-opt seccomp=unconfined
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate code coverage
        run: |
          ./scripts/coverage-roles.sh

      - name: Upload roles coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports
          file: ./roles/target/tarpaulin-reports/cobertura.xml
          flags: roles
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload jd_client-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/jd-client-coverage
          file: ./roles/target/tarpaulin-reports/jd-client-coverage/cobertura.xml
          flags: jd_client-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
      
      - name: Upload jd_server-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/jd-server-coverage
          file: ./roles/target/tarpaulin-reports/jd-server-coverage/cobertura.xml
          flags: jd_server-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload mining_device-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/mining-device-coverage
          file: ./rroles/target/tarpaulin-reports/mining-device-coverage/cobertura.xml
          flags: mining_device-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload pool_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/pool-coverage
          file: ./roles/target/tarpaulin-reports/pool-coverage/cobertura.xml
          flags: pool_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload sv1-mining-device-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/sv1-mining-device-coverage
          file: ./roles/target/tarpaulin-reports/sv1-mining-device-coverage/cobertura.xml
          flags: sv1-mining-device-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload translator_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./roles/target/tarpaulin-reports/translator-coverage
          file: ./roles/target/tarpaulin-reports/translator-coverage/cobertura.xml
          flags: translator_sv2-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
</file>

<file path="stratum-1.4.0/.github/workflows/coverage-utils.yaml">
name: Util Test Coverage

on:
  push:
    branches:
      - main

jobs:
  utils-coverage:

    name: tarpaulin Test
    runs-on: ubuntu-latest
    container:
      image: xd009642/tarpaulin:0.27.1-nightly
      options: --security-opt seccomp=unconfined
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Generate code coverage
        run: |
          ./scripts/coverage-utils.sh

      - name: Upload utils coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./utils/target/tarpaulin-reports
          file: ./utils/target/tarpaulin-reports/cobertura.xml
          flags: utils
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload bip32_derivation-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./utils/target/tarpaulin-reports/bip32-key-derivation-coverage
          file: ./utils/target/tarpaulin-reports/bip32-key-derivation-coverage/cobertura.xml
          flags: bip32_derivation-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
      
      - name: Upload buffer_sv2-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./utils/target/tarpaulin-reports/buffer-coverage
          file: ./utils/target/tarpaulin-reports/buffer-coverage/cobertura.xml
          flags: buffer_sv2-coverage
          
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload error_handling-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./utils/target/tarpaulin-reports/error-handling-coverage
          file: ./utils/target/tarpaulin-reports/error-handling-coverage/cobertura.xml
          flags: error_handling-coverage
          token: ${{ secrets.CODECOV_TOKEN }}

      - name: Upload key-utils-coverage to codecov.io
        uses: codecov/codecov-action@v4
        with:
          directory: ./utils/target/tarpaulin-reports/key-utils-coverage
          file: ./utils/target/tarpaulin-reports/key-utils-coverage/cobertura.xml
          flags: key-utils-coverage
          token: ${{ secrets.CODECOV_TOKEN }}
</file>

<file path="stratum-1.4.0/.github/workflows/docs.yaml">
# Enforces Rust Docs sanity (protocols crates only)
# If `cargo doc --all-features` fails for one crate, the entire workflow fails

name: Rust Docs

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  cargo-doc-all-features:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: actions/checkout@v4
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true

      - name: Rust Docs crate common
        run: |
          cd common
          cargo doc

      - name: Rust Docs crate buffer_sv2
        run: |
          cd utils/buffer
          cargo doc

      - name: Rust Docs crate binary_sv2 derive_codec
        run: |
          cd protocols/v2/binary-sv2/derive_codec
          cargo doc

      - name: Rust Docs crate binary_sv2 codec
        run: |
          cd protocols/v2/binary-sv2/codec
          cargo doc --features with_buffer_pool

      - name: Rust Docs crate binary_sv2
        run: |
          cd protocols/v2/binary-sv2
          cargo doc --features with_buffer_pool

      - name: Rust Docs crate framing_sv2
        run: |
          cd protocols/v2/framing-sv2
          cargo doc --features with_buffer_pool

      - name: Rust Docs crate noise_sv2
        run: |
          cd protocols/v2/noise-sv2
          cargo doc --features std

      - name: Rust Docs crate codec_sv2
        run: |
          cd protocols/v2/codec-sv2
          cargo doc --features with_buffer_pool,noise_sv2

      - name: Rust Docs crate common_messages
        run: |
          cd protocols/v2/subprotocols/common-messages
          cargo doc

      - name: Rust Docs crate job_declaration
        run: |
          cd protocols/v2/subprotocols/job-declaration
          cargo doc --all-features

      - name: Rust Docs crate mining
        run: |
          cd protocols/v2/subprotocols/mining
          cargo doc --all-features

      - name: Rust Docs crate template_distribution
        run: |
          cd protocols/v2/subprotocols/template-distribution
          cargo doc

      - name: Rust Docs crate sv2_ffi
        run: |
          cd protocols/v2/sv2-ffi
          cargo doc

      - name: Rust Docs crate roles_logic_sv2

        run: |
          cd protocols/v2/roles-logic-sv2
          cargo doc

      - name: Rust Docs crate sv1_api
        run: |
          cd protocols/v1
          cargo doc
</file>

<file path="stratum-1.4.0/.github/workflows/fmt.yaml">
on:
  pull_request:
    branches:
      - main

name: Rustfmt

jobs:
  fmt:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - macos-latest
          - ubuntu-latest
        include:
          - os: macos-latest
            target: x86_64-apple-darwin
          - os: ubuntu-latest
            target: x86_64-unknown-linux-musl

    steps:
      - uses: actions/checkout@v4
      - uses: actions-rs/toolchain@v1
        with:
          profile: minimal
          toolchain: nightly
          override: true
          components: rustfmt
      - name: Run fmt in different workspaces and crates
        run: |
          cargo fmt --all --manifest-path=benches/Cargo.toml -- --check
          cargo fmt --all --manifest-path=common/Cargo.toml -- --check
          cargo fmt --all --manifest-path=protocols/Cargo.toml -- --check
          cargo fmt --all --manifest-path=roles/Cargo.toml -- --check
          cargo fmt --all --manifest-path=utils/Cargo.toml -- --check
          cargo fmt --all --manifest-path=test/integration-tests/Cargo.toml -- --check
</file>

<file path="stratum-1.4.0/.github/workflows/integration-tests.yaml">
on:
  pull_request:
    branches:
      - main

name: Integration Tests

jobs:
  ci:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - ubuntu-latest
        include:
          - os: ubuntu-latest
            target: x86_64-unknown-linux-gnu

    steps:
      - name: Use stable toolchain
        uses: actions/checkout@v4
        with:
          profile: minimal
          toolchain: stable
          override: true

      - name: Install cargo-nextest
        run: cargo install cargo-nextest --locked

      - name: Integration Tests
        run: |
         RUST_BACKTRACE=1 RUST_LOG=debug cargo nextest run --manifest-path=test/integration-tests/Cargo.toml --features sv1 --nocapture
</file>

<file path="stratum-1.4.0/.github/workflows/lockfiles.yaml">
name: Lockfiles

# Trigger the workflow on pull request events for the main branch
on:
  pull_request:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: stable
          override: true

      - name: Build with locked dependencies
        run: |
          cargo build --manifest-path=roles/Cargo.toml --locked
          cargo build --manifest-path=utils/Cargo.toml --locked
          cargo build --manifest-path=test/integration-tests/Cargo.toml --locked
</file>

<file path="stratum-1.4.0/.github/workflows/release-libs.yaml">
# This workflow is used to publish SV2 crates to crates.io
# the workflow tries to publish all the library crates by running scripts/release-libs.sh
# in case the `cargo publish` command fails, the script returns 1 and the entire workflow fails
# the only exception is when the `cargo publish` command fails because the crate has already
# been published, in which case the workflow continues

name: Release Libs

on:
  # Manually run by going to "Actions/Release" in Github and running the workflow
  workflow_dispatch:
  # every time a new release tag is created
  push:
    tags:
      - "v*.*.*"

jobs:
  libs_publish:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: actions/checkout@v4
      - uses: actions-rs/toolchain@v1
        with:
            toolchain: 1.75.0
            override: true
      - name: Login
        run: cargo login ${{ secrets.CRATES_IO_DEPLOY_KEY }}

      # Base dependencies with no local dependencies
      - name: Publish crate buffer_sv2
        run: |
          ./scripts/release-libs.sh utils/buffer

      - name: Publish crate error-handling
        run: |
          ./scripts/release-libs.sh utils/error-handling

      - name: Publish crate key-utils
        run: |
          ./scripts/release-libs.sh utils/key-utils

      - name: Publish crate noise_sv2
        run: |
          ./scripts/release-libs.sh protocols/v2/noise-sv2

      # binary_sv2 (depends on buffer_sv2)
      - name: Publish crate binary_sv2 codec
        run: |
          ./scripts/release-libs.sh protocols/v2/binary-sv2/codec

      - name: Publish crate binary_sv2 derive_codec
        run: |
          ./scripts/release-libs.sh protocols/v2/binary-sv2/derive_codec

      - name: Publish crate binary_sv2
        run: |
          ./scripts/release-libs.sh protocols/v2/binary-sv2

      # framing_sv2(depends on binary_sv2, buffer_sv2, noise_sv2)
      - name: Publish crate framing_sv2
        run: |
          ./scripts/release-libs.sh protocols/v2/framing-sv2

      # codec_sv2 (depends on framing_sv2, noise_sv2, binary_sv2, buffer_sv2, key-utils)
      - name: Publish crate codec_sv2
        run: |
          ./scripts/release-libs.sh protocols/v2/codec-sv2

      # Subprotocols (depend on binary_sv2)
      - name: Publish crate common_messages
        run: |
          ./scripts/release-libs.sh protocols/v2/subprotocols/common-messages

      - name: Publish crate job_declaration
        run: |
          ./scripts/release-libs.sh protocols/v2/subprotocols/job-declaration

      - name: Publish crate mining
        run: |
          ./scripts/release-libs.sh protocols/v2/subprotocols/mining

      - name: Publish crate template_distribution
        run: |
          ./scripts/release-libs.sh protocols/v2/subprotocols/template-distribution

      # sv1_api (depends on binary_sv2)
      - name: Publish crate v1
        run: |
          ./scripts/release-libs.sh protocols/v1

      # sv2_ffi (depends on codec_sv2, binary_sv2, common_messages, template_distribution)
      - name: Publish crate sv2_ffi
        run: |
          ./scripts/release-libs.sh protocols/v2/sv2-ffi

      # Roles logic (depends on codec_sv2 and subprotocols)
      - name: Publish crate roles_logic_sv2
        run: |
          ./scripts/release-libs.sh protocols/v2/roles-logic-sv2

      # Network helpers (depends on codec_sv2, sv1_api)
      - name: Publish crate network_helpers_sv2
        run: |
          ./scripts/release-libs.sh roles/roles-utils/network-helpers

      # Common (depends on roles_logic_sv2 and network_helpers_sv2)
      - name: Publish crate common
        run: |
          ./scripts/release-libs.sh common

      # Utilities that depend on stratum-common
      - name: Publish crate bip32-key-derivation
        run: |
          ./scripts/release-libs.sh utils/bip32-key-derivation

      - name: Publish crate rpc_sv2
        run: |
          ./scripts/release-libs.sh roles/roles-utils/rpc
</file>

<file path="stratum-1.4.0/.github/workflows/rust-msrv.yaml">
on:
  pull_request:
    branches:
      - main

name: MSRV 1.75 Check

jobs:

  build:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        rust:
          - 1.75.0 # MSRV

    steps:
      - uses: actions/checkout@v4
      - uses: Swatinem/rust-cache@v1.2.0
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ matrix.rust }}
          override: true
      - name: Build Benches
        run: cargo build --manifest-path=benches/Cargo.toml
      - name: Build Protocols
        run: cargo build --manifest-path=protocols/Cargo.toml
      - name: Build Roles
        run: cargo build --manifest-path=roles/Cargo.toml
      - name: Build Utils
        run: cargo build --manifest-path=utils/Cargo.toml
      - name: Build Integration Tests
        run: cargo build --manifest-path=test/integration-tests/Cargo.toml
      
      # also check test compilation without running tests
      - name: Check Test Compilation for Benches
        run: cargo test --manifest-path=benches/Cargo.toml --no-run
      - name: Check Test Compilation for Protocols
        run: cargo test --manifest-path=protocols/Cargo.toml --no-run
      - name: Check Test Compilation for Roles
        run: cargo test --manifest-path=roles/Cargo.toml --no-run
      - name: Check Test Compilation for Utils
        run: cargo test --manifest-path=utils/Cargo.toml --no-run
      - name: Check Test Compilation for Integration Tests
        run: cargo test --manifest-path=test/integration-tests/Cargo.toml --no-run
</file>

<file path="stratum-1.4.0/.github/workflows/semver-check.yaml">
name: Semver Check

on:
  pull_request:
    branches:
      - "main"

jobs:
  semver-check:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Rust stable
        uses: actions-rs/toolchain@v1
        with:
          toolchain: 1.85
          override: true

      - name: Cache Cargo registry
        uses: actions/cache@v4
        with:
          path: ~/.cargo/registry
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-registry-

      - name: Cache Cargo index
        uses: actions/cache@v4
        with:
          path: ~/.cargo/git
          key: ${{ runner.os }}-cargo-index-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-index-

      - name: Install dependencies
        run: sudo apt-get update && sudo apt-get install -y cmake

      - name: Install cargo-semver-checks
        run: cargo install cargo-semver-checks --version 0.37.0 --locked

      - name: Run semver checks for common
        working-directory: common
        run: cargo semver-checks

      - name: Run semver checks for utils/buffer
        working-directory: utils/buffer
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/binary-sv2/codec
        working-directory: protocols/v2/binary-sv2/codec
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/binary-sv2
        working-directory: protocols/v2/binary-sv2
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/framing-sv2
        working-directory: protocols/v2/framing-sv2
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/noise-sv2
        working-directory: protocols/v2/noise-sv2
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/codec-sv2
        working-directory: protocols/v2/codec-sv2
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/subprotocols/common-messages
        working-directory: protocols/v2/subprotocols/common-messages
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/subprotocols/job-declaration
        working-directory: protocols/v2/subprotocols/job-declaration
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/subprotocols/mining
        working-directory: protocols/v2/subprotocols/mining
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/subprotocols/template-distribution
        working-directory: protocols/v2/subprotocols/template-distribution
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/sv2-ffi
        working-directory: protocols/v2/sv2-ffi
        run: cargo semver-checks

      - name: Run semver checks for protocols/v2/roles-logic-sv2
        working-directory: protocols/v2/roles-logic-sv2
        run: cargo semver-checks --default-features

      - name: Run semver checks for protocols/v1
        working-directory: protocols/v1
        run: cargo semver-checks

      - name: Run semver checks for utils/bip32-key-derivation
        working-directory: utils/bip32-key-derivation
        run: cargo semver-checks

      - name: Run semver checks for utils/error-handling
        working-directory: utils/error-handling
        run: cargo semver-checks

      - name: Run semver checks for utils/key-utils
        working-directory: utils/key-utils
        run: cargo semver-checks

      - name: Run semver checks for roles/roles-utils/network-helpers
        working-directory: roles/roles-utils/network-helpers
        run: cargo semver-checks

      - name: Run semver checks for roles/roles-utils/rpc
        working-directory: roles/roles-utils/rpc
        run: cargo semver-checks
</file>

<file path="stratum-1.4.0/.github/workflows/test.yaml">
on:
  pull_request:
    branches:
      - main

name: Test, Prop Tests, Example Tests

jobs:
  ci:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os:
          - macos-13
          - ubuntu-latest
        include:
          - os: macos-13
            target: x86_64-apple-darwin
          - os: ubuntu-latest
            target: x86_64-unknown-linux-musl

    steps:
      - name: Install stable toolchain & components
        uses: actions/checkout@v4
        with:
          profile: minimal
          toolchain: nightly
          override: true

      - name: Build
        run: |
          cargo build --manifest-path=benches/Cargo.toml
          cargo build --manifest-path=common/Cargo.toml
          cargo build --manifest-path=protocols/Cargo.toml
          cargo build --manifest-path=roles/Cargo.toml
          cargo build --manifest-path=utils/Cargo.toml
          cargo build --manifest-path=roles/roles-utils/config-helpers/Cargo.toml

      - name: Run sv1-client-and-server example
        run: |
          cargo run --manifest-path=protocols/v1/Cargo.toml --example client_and_server 30

      - name: Run framing-sv2 example
        run: |
          cargo run --manifest-path=protocols/v2/framing-sv2/Cargo.toml --example sv2_frame

      - name: Run codec-sv2 examples
        run: |
          cargo run --manifest-path=protocols/v2/codec-sv2/Cargo.toml --example unencrypted
          cargo run --manifest-path=protocols/v2/codec-sv2/Cargo.toml --example encrypted --features=noise_sv2

      - name: Run binary-sv2 examples
        run: |
          cargo run --manifest-path=protocols/v2/binary-sv2/Cargo.toml --example encode_decode

      - name: Run noise-sv2 examples
        run: |
          cargo run --manifest-path=protocols/v2/noise-sv2/Cargo.toml --example handshake

      - name: fuzz tests
        run: |
          if [ ${{ matrix.os }} == "ubuntu-latest" ]; then
            ./run.sh 30
          else
            echo "Skipping fuzz test on ${{ matrix.os }} - not supported"
          fi
        working-directory: utils/buffer/fuzz

      - name: Test
        run: |
          cargo test --manifest-path=benches/Cargo.toml
          cargo test --manifest-path=common/Cargo.toml
          cargo test --manifest-path=protocols/Cargo.toml
          cargo test --manifest-path=roles/Cargo.toml
          cargo test --manifest-path=utils/Cargo.toml
          cargo test --manifest-path=roles/roles-utils/config-helpers/Cargo.toml
          cargo test --manifest-path=roles/roles-utils/network-helpers/Cargo.toml sv1_connection::tests::test_sv1_connection --features sv1

      - name: Property based testing
        run: |
          cargo test --manifest-path=protocols/Cargo.toml --features prop_test

      - name: Run ping-pong-encrypted example
        run: |
          cargo run --manifest-path=examples/ping-pong-encrypted/Cargo.toml

      - name: Run ping-pong example
        run: |
          cargo run --manifest-path=examples/ping-pong/Cargo.toml
</file>

<file path="stratum-1.4.0/.gitignore">
.idea
*/**/target
/protocols/guix-example/guix-example.h
/protocols/Cargo.lock
/benches/Cargo.lock
/ignore
/vendor/ed25519-dalek/target
/utils/buffer/target
/sv2.h
/test/bitcoin_data/regtest
lcov.info
/target
.vscode
*.py
**/conf/**
cobertura.xml
/roles/*/*-config.toml
/examples/*/Cargo.lock
/scripts/sv2.h
/test/integration-tests/template-provider
**/template-provider
stratum-message-generator
*.log
</file>

<file path="stratum-1.4.0/benches/benches/README.md">
# SV1 and SV2 clients benchmarks
This directory contains code that benchmarks the performance of sv1 and sv2 clients using the `criterion` and `iai` library.

The benchmarking project aims to compare the performance of sv2 against sv1 and demonstrate the improvements of sv2. The benchmarks measure various aspects of the clients' performance, including  subscription latency, share submission time, systems requirements such as RAM access, Instructions per cycle and more..

## Project Structure
The project is structured as follows:

- The benches/ directory contains the main project files.
- The src/ directory contains subdirectories for sv1 and sv2 clients.
- Each client directory contains benchmark files criterion_svX_benchmark.rs and iai_svX_benchmark.rs for sv1 and sv2 respectively.
- The lib/client.rs file within each client directory contains additional code relating to each client implementation.


- criterion_svX_benchmark.rs: Uses the criterion crate for latency benchmarking.
- iai_benchmarks.rs: Uses the iai crate to measure system requirements and performance.

## Running Benchmarks
- To run the benchmarks, follow these steps:

- Install Rust and Cargo if you haven't already.
- Clone this repository
```sh
git clone https://github.com/stratum-mining/stratum
```
- Navigate to the benches directory.
```sh
cd benches
```
- Open a terminal and run the following command to execute the benchmarks:
```sh
cargo bench
```

The benchmark results will be displayed in the terminal. `target/criterion` and `target/iai` will also be created, which contains more detailed results.

## Benchmarking
It is important to note that for now, sv2 does not count encryption and decryption.

The following benchmark functions are available:

### sv1
1. **Subscription Benchmarks**:
   - `client_sv1_get_subscribe`: Measures the latency and system requirements of a subscription request.
   - `client_sv1_subscribe_serialize`: Measures the latency and system requirements it takes to serialize a subscription message.
   - `client_sv1_subscribe_serialize_deserialize`: Measures the latency and system requirements it takes to serialize and then deserialize a subscription message.
   - `client_sv1_subscribe_serialize_deserialize_handle`: Measures the latency and system requirements it takes to serialize, deserialize, and handle a subscription message.

2. **Authorization Benchmarks**:
   - `client_sv1_get_authorize`: Measures the latency and system requirementsit takes to initiate an authorization request.
   - `client_sv1_authorize_serialize`: Measures the latency and system requirements it takes to serialize an authorization message.
   - `client_sv1_authorize_serialize_deserialize`: Measures the latency and system requirements it takes to serialize and then deserialize an authorization message.
   - `client_sv1_authorize_serialize_deserialize_handle`: Measures the latency and system requirements it takes to serialize, deserialize, and handle an authorization message.

3. **Share Submission Benchmarks**:
   - `client_sv1_get_submit`: Measures the latency and system requirements it takes to submit a share.
   - `client_sv1_submit_serialize`: Measures the latency and system requirements it takes to serialize a share submission message.
   - `client_sv1_submit_serialize_deserialize`: Measures the tlatency and system requirementsime it takes to serialize and then deserialize a share submission message.
   - `client_sv1_submit_serialize_deserialize_handle`: Measures the latency and system requirements it takes to serialize, deserialize, and handle a share submission message.

### sv2 Client

1. **Setup Connection Performance**:
   - `client_sv2_setup_connection`: Measures the latency and system requirements it takes to initiate a setup connection.
   - `client_sv2_setup_connection_serialize`: Measures the latency and system requirements to serialize a setup connection message.
   - `client_sv2_setup_connection_serialize_deserialize`: Measures the latency and system requirements to serialize and deserialize a setup connection message.

2. **Open Channel Performance**:
   - `client_sv2_open_channel`: Measures the latency and system requirements it takes to open a channel.
   - `client_sv2_open_channel_serialize`: Measures the latency and system requirements to serialize an open channel message.
   - `client_sv2_open_channel_serialize_deserialize`: Measures the latency and system requirements to serialize and deserialize an open channel message.

3. **Mining Message Submit Performance**:
   - `client_sv2_mining_message_submit_standard`: Measures the latency and system requirements it takes to submit a standard mining message.
   - `client_sv2_mining_message_submit_standard_serialize`: Measures the latency and system requirements to serialize a standard mining submit message process..
   - `client_sv2_mining_message_submit_standard_serialize_deserialize`:  Measures the latency and system requirements to serialize and deserialize standard mining submit message process.

4. **Handling Message Performance**:
   - `client_sv2_handle_message_mining`: Measures the latency and system requirements to handle a mining message.
   - `client_sv2_handle_message_common`: Measures the latency and system requirements to handle a common message.

## Results

After running the benchmarks, the `criterion` crate will generate detailed performance reports. These reports include statistical measurements such as mean, median, standard deviation, and more. These results can provide insights into the performance characteristics of the sv1 protocol under various scenarios.


### Example Benchmark Results

Here are the sample benchmark results for both sv1 and sv2 client functions:

**Combined Table: Benchmark Metrics**


| Function                                               | Latency (s) | Instructions | L1 Accesses | L2 Accesses | RAM Accesses | Estimated Cycles |
|--------------------------------------------------------|------------------|--------------|-------------|-------------|--------------|-----------------|
| client-sv1-get-subscribe                              | 1.2064           | 2918         | 4087        | 12          | 123          | 8452            |
| client-sv1-subscribe-serialize                        | 0.7506           | 4275         | 5957        | 17          | 167          | 11887           |
| client-sv1-subscribe-serialize-deserialize            | 1.1619           | 8321         | 11759       | 41          | 337          | 23759           |
| client-sv1-subscribe-serialize-deserialize-handle     | 1.0221           | 9791         | 13867       | 67          | 408          | 28482           |
| client-sv1-get-authorize                              | 0.9172           | 3871         | 5439        | 8           | 104          | 9119            |
| client-sv1-authorize-serialize                        | 0.7965           | 5454         | 7622        | 11          | 150          | 12927           |
| client-sv1-authorize-serialize-deserialize            | 1.4216           | 10094        | 14301       | 37          | 313          | 25441           |
| client-sv1-authorize-serialize-deserialize-handle     | 1.5415           | 12307        | 17451       | 62          | 383          | 31166           |
| client-sv1-get-submit                                 | 2.3447           | 62046        | 89643       | 50          | 306          | 100603          |
| client-submit-serialize                               | 1.3495           | 64107        | 92513       | 54          | 350          | 105033          |
| client-submit-serialize-deserialize                   | 2.2996           | 70767        | 102142      | 71          | 520          | 120697          |
| client-submit-serialize-deserialize-handle            | 3.0982           | 76044        | 109584      | 111         | 636          | 132399          |
| client_sv2_setup_connection                           | 0.2792           | 1665         | 2537        | 14          | 74           | 5197            |
| client_sv2_setup_connection_serialize                 | 1.5738           | 7619         | 11266       | 57          | 247          | 20196           |
| client_sv2_setup_connection_serialize_deserialize     | 3.7460           | 17478        | 25993       | 106         | 461          | 42658           |
| client_sv2_open_channel                               | 0.6050               | 1518       | 2244      | 9            | 65     | 4564 |
| client_sv2_open_channel_serialize                     | 1.0275               | 5746       | 8470      | 39        | 202       | 15735     |
| client_sv2_open_channel_serialize_deserialize         | 1.5892               | 9102       | 13419     | 84        | 373       | 26894     |
| client_sv2_mining_message_submit_standard             | 0.0621           | 2173         | 3190        | 10          | 98           | 6670            |
| client_sv2_mining_message_submit_standard_serialize   | 0.6413           | 5455         | 8049        | 53          | 226          | 16224           |
| client_sv2_mining_message_submit_standard_serialize_deserialize | 2.3043     | 12059        | 17797       | 94          | 380          | 31567           |
| client_sv2_handle_message_common                             | 0.0468           | 389          | 570         | 4           | 31           | 1675            |
| client_sv2_handle_message_mining                     | 0.0778           | 5396         | 7740        | 58          | 211          | 15415           |


Lets Analyze some these results:


**Table 1: Performance Comparison (Setup Process)**

Function                            | Median Time (s) | Instructions | L1 Accesses | L2 Accesses | RAM Accesses | Estimated Cycles | Performance Index (%)
----------------------------------- | ---------------- | ------------ | ----------- | ----------- | ------------ | ---------------- | -----------------------
client-sv1-get-subscribe            | 1.2064           | 2918         | 4087        | 12          | 123          | 8452             | -
client-sv1-get-authorize            | 0.9172           | 3871         | 5439        | 8           | 104          | 9119             | -
**SV1 Setup Process**               | **2.1236**       | **6798**     | **9526**    | **20**      | **227**      | **17571**        | **-**
client-sv2-setup_connection         | 0.2792           | 1665         | 2537        | 14          | 74           | 5197             | -
client-sv2-open_channel             | 0.6050           | 1518         | 2244        | 9           | 65           | 4564             | -
**SV2 Setup Process**               | **0.8842**       | **3183**     | **4781**    | **23**      | **139**      | **9761**         | **-**

Please note that the "SV1 Setup Process" is the sum of the metrics for `client-sv1-get-subscribe` and `client-sv1-get-authorize`, and the "SV2 Setup Process" is the sum of the metrics for `client-sv2-setup_connection` and `client-sv2-open_channel`.


| Setup Process         | Latency (s) | Instructions | L1 Accesses | L2 Accesses | RAM Accesses | Estimated Cycles | Latency (%) | L1 Access Improvement (%) | L2 Access Improvement (%) | RAM Access Improvement (%) | Estimated Cycle Improvement (%) | Instruction Cycle Improvement (%) |
|-----------------------|------------------|--------------|-------------|-------------|--------------|-----------------|----------------------|---------------------------|--------------------------|---------------------------|--------------------------------|-----------------------------------|
| SV1 Setup Process     | 2.1236           | 6798         | 9526        | 20          | 227          | 17571           | -                    | -                         | -                        | -                         | -                              | -                                 |
| SV2 Setup Process     | 0.8842           | 3183         | 4781        | 23          | 139          | 9761            | 58.36                | 49.81                     | -15                    | 38.77                     | 44.51                          | 53.12                             |


The Performance Index is calculated using the formula:

```markdown
Performance Index = (SV1 Value - SV2 Value) / SV1 Value * 100

```

**Implications**

- **Latency**: The SV2 connection process demonstrates a significant improvement in latency compared to the SV1 connection process, with an approximately 58.36% improvement. This indicates that SV2's connection process is more efficient and faster.

- **Instructions**: SV2's setup process requires far fewer instructions compared to SV1, resulting in a 53.12% improvement. This implies that SV2's setup process is more streamlined and optimized.

- **L1 Accesses**: SV2's setup process significantly reduces L1 cache accesses compared to SV1, with a 49.81% improvement. This suggests that SV2's setup process is more cache-friendly and reduces memory overhead.

- **L2 Accesses:** Both SV1 and SV2 have similar L2 cache accesses, with sv2 being slightly higher by 20.00%. High L2 access can indicate reduced latency between the CPU and memory, faster data retrieval, and improved overall system performance

- **RAM Accesses**: SV2's setup process drastically reduces RAM accesses compared to SV1, showing a 61.23% improvement. This indicates that SV2's process is designed to minimize main memory interactions.

- **Estimated Cycles**: SV2's setup process requires significantly fewer estimated cycles than SV1, with a 44.51% improvement. This suggests that SV2's setup process is more optimized and requires less computational effort.


**Table 2: Performance Comparison (Submission)**

| Submission                | Median Time (s) | Instructions | L1 Accesses | L2 Accesses | RAM Accesses | Estimated Cycles | Time Improvement (%) | L1 Access Improvement (%) | L2 Access Improvement (%) | RAM Access Improvement (%) |
|---------------------------|------------------|--------------|-------------|-------------|--------------|-----------------|----------------------|---------------------------|--------------------------|---------------------------|
| SV1 Submission            | 2.3447           | 62046        | 89643       | 50          | 306          | 100603          | -                    | -                         | -                        | -                         |
| SV2 Submission            | 0.0621           | 2173         | 3190        | 10          | 98           | 6670            | 97.35                | 64.33                     | 80.00                    | 67.32                     |


**Implications**

- **Latency**: The SV2 Mining Message Standard demonstrates a remarkable improvement in latency compared to SV1 Get Submit, with an approximately 97.35% reduction. This implies that SV2's mining message submission process is highly efficient and faster.

- **Instructions**: SV2's mining message submission requires far fewer instructions compared to SV1 Get Submit, resulting in a 96.50% improvement. This suggests that SV2's process is more streamlined and requires less computational effort.

- **L1 Accesses**: SV2's mining message submission process significantly reduces L1 cache accesses compared to SV1 Get Submit, showing a 96.43% improvement. This implies that SV2's process minimizes memory overhead and is more cache-friendly.

- **L2 Accesses**: Both SV1 Get Submit and SV2 Mining Message Standard have minimal L2 cache accesses. The slight difference in favor of SV2 (80.00% reduction) is not as significant as other improvements.

- **RAM Accesses**: SV2's mining message submission process significantly reduces RAM accesses compared to SV1 Get Submit, with a 67.32% improvement. This suggests that SV2's process minimizes main memory interactions.

- **Estimated Cycles**: SV2's mining message submission process requires far fewer estimated cycles than SV1 Get Submit, showing a 93.37% improvement. This indicates that SV2's process is highly optimized and requires less computational effort.



## Conclusion

The sv2 client's benchmark results clearly demonstrate its superiority over the sv1 client in terms of performance. These results pave the way for increased adoption of sv2 and highlight its potential to become the preferred choice for mining communication protocols.

---
</file>

<file path="stratum-1.4.0/benches/benches/src/sv1/criterion_sv1_benchmark.rs">
//! This code uses `criterion` crate to benchmark the performance sv1.
//! It measures connection time, send subscription latency and share submission time.

use criterion::{black_box, Criterion};
use v1::{ClientStatus, IsClient};

#[path = "./lib/client.rs"]
mod client;
use crate::client::{extranonce_from_hex, notify, *};

fn benchmark_get_subscribe(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-get-subscribe");

    group.bench_function("client-sv1-get-subscribe", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let extranonce = Some(extranonce_from_hex("0000"));
            client.subscribe(black_box(10), extranonce).unwrap();
        });
    });
}

fn benchmark_subscribe_serialize(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-subscribe-serialize");

    group.bench_function("client-sv1-subscribe-serialize", |b| {
        client.status = ClientStatus::Configured;
        let extranonce = Some(extranonce_from_hex("0000"));
        let subscribe = client.subscribe(black_box(10), extranonce).unwrap();
        b.iter(|| {
            Client::serialize_message(black_box(subscribe.clone()));
        });
    });
}

fn benchmark_subscribe_serialize_deserialize(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-subscribe-serialize-deserialize");

    group.bench_function("client-sv1-subscribe-serialize-deserialize", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let subscribe = client.subscribe(black_box(10), None).unwrap();
            let serialized = Client::serialize_message(black_box(subscribe));
            Client::parse_message(black_box(serialized));
        });
    });
}

fn benchmark_subscribe_serialize_deserialize_handle(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-subscribe-serialize-deserialize-handle");

    group.bench_function("client-sv1-subscribe-serialize-deserialize-handle", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let subscribe = client.subscribe(black_box(10), None).unwrap();
            let serialized = Client::serialize_message(black_box(subscribe));
            let deserilized = Client::parse_message(black_box(serialized));
            client.handle_message(black_box(deserilized));
        });
    });
}

fn benchmark_get_authorize(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-get-authorize");

    group.bench_function("client-sv1-get-authorize", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            client
                .authorize(
                    black_box(10),
                    black_box("user".to_string()),
                    black_box("passowrd".to_string()),
                )
                .unwrap();
        });
    });
}

fn benchmark_authorize_serialize(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-authorize-serialize");

    group.bench_function("client-sv1-authorize-serialize", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let authorize = client
                .authorize(
                    black_box(10),
                    black_box("user".to_string()),
                    black_box("passowrd".to_string()),
                )
                .unwrap();
            Client::serialize_message(black_box(authorize));
        });
    });
}

fn benchmark_authorize_serialize_deserialize(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-authorize-serialize-deserialize");

    group.bench_function("client-sv1-authorize-serialize-deserialize", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let authorize = client
                .authorize(
                    black_box(10),
                    black_box("user".to_string()),
                    black_box("passowrd".to_string()),
                )
                .unwrap();
            let serialized = Client::serialize_message(black_box(authorize));
            Client::parse_message(black_box(serialized));
        });
    });
}

fn benchmark_authorize_serialize_deserialize_handle(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-sv1-authorize-serialize-deserialize-handle");

    group.bench_function("client-sv1-authorize-serialize-deserialize-handle", |b| {
        b.iter(|| {
            client.status = ClientStatus::Configured;
            let authorize = client
                .authorize(
                    black_box(10),
                    black_box("user".to_string()),
                    black_box("passowrd".to_string()),
                )
                .unwrap();
            let serialized = Client::serialize_message(black_box(authorize));
            let deserilized = Client::parse_message(black_box(serialized));
            client.handle_message(black_box(deserilized));
        });
    });
}

fn benchmark_get_submit(c: &mut Criterion, mut client: Client) {
    c.bench_function("client-sv1-get-submit", |b| {
        b.iter(|| {
            notify(&mut client);
            client.authorize_user_name("user".to_string());
            client
                .submit(
                    0,
                    "user".to_string(),
                    extranonce_from_hex("00"),
                    78,
                    78,
                    None,
                )
                .unwrap();
        });
    });
}

fn benchmark_submit_serialize(c: &mut Criterion, mut client: Client) {
    c.bench_function("client-submit-serialize", |b| {
        b.iter(|| {
            notify(&mut client);
            client.authorize_user_name("user".to_string());
            let submit = client
                .submit(
                    0,
                    "user".to_string(),
                    extranonce_from_hex("00"),
                    78,
                    78,
                    None,
                )
                .unwrap();
            Client::serialize_message(black_box(submit));
        });
    });
}

fn benchmark_submit_serialize_deserialize(c: &mut Criterion, mut client: Client) {
    c.bench_function("client-submit-serialize-deserialize", |b| {
        b.iter(|| {
            notify(&mut client);
            client.authorize_user_name("user".to_string());
            let submit = client
                .submit(
                    0,
                    "user".to_string(),
                    extranonce_from_hex("00"),
                    78,
                    78,
                    None,
                )
                .unwrap();
            let serialized = Client::serialize_message(black_box(submit));
            Client::parse_message(black_box(serialized));
        });
    });
}

fn benchmark_submit_serialize_deserialize_handle(c: &mut Criterion, mut client: Client) {
    let mut group = c.benchmark_group("client-submit-serialize-deserialize-handle");
    group.bench_function("client-submit-serialize-deserialize-handle", |b| {
        b.iter(|| {
            client.status = ClientStatus::Subscribed;
            notify(&mut client);
            client.authorize_user_name("user".to_string());
            let submit = client
                .submit(
                    0,
                    "user".to_string(),
                    extranonce_from_hex("00"),
                    78,
                    78,
                    None,
                )
                .unwrap();
            let serialized = Client::serialize_message(black_box(submit));
            let deserialized = Client::parse_message(serialized);
            client.handle_message(black_box(deserialized));
        });
    });
}

fn main() {
    let mut criterion = Criterion::default()
        .sample_size(100)
        .measurement_time(std::time::Duration::from_secs(5));
    let client = Client::new(90);
    benchmark_get_subscribe(&mut criterion, client.clone());
    benchmark_subscribe_serialize(&mut criterion, client.clone());
    benchmark_subscribe_serialize_deserialize(&mut criterion, client.clone());
    benchmark_subscribe_serialize_deserialize_handle(&mut criterion, client.clone());
    benchmark_get_authorize(&mut criterion, client.clone());
    benchmark_authorize_serialize(&mut criterion, client.clone());
    benchmark_authorize_serialize_deserialize(&mut criterion, client.clone());
    benchmark_authorize_serialize_deserialize_handle(&mut criterion, client.clone());
    benchmark_get_submit(&mut criterion, client.clone());
    benchmark_submit_serialize(&mut criterion, client.clone());
    benchmark_submit_serialize_deserialize(&mut criterion, client.clone());
    benchmark_submit_serialize_deserialize_handle(&mut criterion, client.clone());
    criterion.final_summary();
}
</file>

<file path="stratum-1.4.0/benches/benches/src/sv1/iai_sv1_benchmark.rs">
//! The code uses iai library to measure the system requirements of sv1 client.

use iai::{black_box, main};
use v1::{ClientStatus, IsClient};

#[path = "./lib/client.rs"]
mod client;
use crate::client::{extranonce_from_hex, notify, Client};

fn get_subscribe() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    black_box(client.subscribe(black_box(10), None).unwrap());
}

fn serialize_subscribe() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let subscribe = client.subscribe(black_box(10), None).unwrap();
    black_box(Client::serialize_message(black_box(subscribe)));
}

fn serialize_deserialize_subscribe() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let subscribe = client.subscribe(black_box(10), None).unwrap();
    let serlialized = Client::serialize_message(black_box(subscribe));
    black_box(Client::parse_message(black_box(serlialized)));
}

fn serialize_deserialize_handle_subscribe() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let subscribe = client.subscribe(black_box(10), None).unwrap();
    let serlialized = Client::serialize_message(black_box(subscribe));
    let serialized = Client::parse_message(black_box(serlialized));
    black_box(client.handle_message(black_box(serialized)));
}

fn get_authorize() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    black_box(
        client
            .authorize(
                black_box(10),
                black_box("user".to_string()),
                black_box("passowrd".to_string()),
            )
            .unwrap(),
    );
}

fn serialize_authorize() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let authorize = client
        .authorize(
            black_box(10),
            black_box("user".to_string()),
            black_box("passowrd".to_string()),
        )
        .unwrap();
    black_box(Client::serialize_message(black_box(authorize)));
}

fn serialize_deserialize_authorize() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let authorize = client
        .authorize(
            black_box(10),
            black_box("user".to_string()),
            black_box("passowrd".to_string()),
        )
        .unwrap();
    let serlialized = Client::serialize_message(black_box(authorize));
    black_box(Client::parse_message(black_box(serlialized)));
}

fn serialize_deserialize_handle_authorize() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    let authorize = client
        .authorize(
            black_box(10),
            black_box("user".to_string()),
            black_box("passowrd".to_string()),
        )
        .unwrap();
    let serialized = Client::serialize_message(black_box(authorize));
    let deserialized = Client::parse_message(black_box(serialized));
    black_box(client.handle_message(black_box(deserialized)));
}

fn get_submit() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    notify(&mut client);
    client.authorize_user_name("user".to_string());
    black_box(
        client
            .submit(
                0,
                "user".to_string(),
                extranonce_from_hex("00"),
                78,
                78,
                None,
            )
            .unwrap(),
    );
}

fn serialize_submit() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Subscribed;
    notify(&mut client);
    client.authorize_user_name("user".to_string());
    let submit = client
        .submit(
            0,
            "user".to_string(),
            extranonce_from_hex("00"),
            78,
            78,
            None,
        )
        .unwrap();
    black_box(Client::serialize_message(black_box(submit)));
}

fn serialize_deserialize_submit() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    notify(&mut client);
    client.authorize_user_name("user".to_string());
    let submit = client
        .submit(
            0,
            "user".to_string(),
            extranonce_from_hex("00"),
            78,
            78,
            None,
        )
        .unwrap();
    let serlialized = Client::serialize_message(black_box(submit));
    black_box(Client::parse_message(black_box(serlialized)));
}

fn serialize_deserialize_handle_submit() {
    let mut client = Client::new(0);
    client.status = ClientStatus::Configured;
    notify(&mut client);
    client.authorize_user_name("user".to_string());
    let submit = client
        .submit(
            0,
            "user".to_string(),
            extranonce_from_hex("00"),
            78,
            78,
            None,
        )
        .unwrap();
    let serialized = Client::serialize_message(black_box(submit));
    let deserialized = Client::parse_message(black_box(serialized));
    black_box(client.handle_message(black_box(deserialized)));
}
main!(
    get_subscribe,
    serialize_subscribe,
    serialize_deserialize_subscribe,
    serialize_deserialize_handle_subscribe,
    get_authorize,
    serialize_authorize,
    serialize_deserialize_authorize,
    serialize_deserialize_handle_authorize,
    get_submit,
    serialize_submit,
    serialize_deserialize_submit,
    serialize_deserialize_handle_submit
);
</file>

<file path="stratum-1.4.0/benches/benches/src/sv1/lib/client.rs">
//! The defines a sv1 `Client` struct that handles message exchange with the server.
//! It includes methods for initializing the client, parsing messages, and sending various types of
//! messages. It also provides a trait implementation for handling server messages and managing
//! client state.

use v1::{
    client_to_server,
    error::Error,
    json_rpc, server_to_client,
    utils::{Extranonce, HexU32Be, MerkleNode, PrevHash},
    ClientStatus, IsClient,
};

#[derive(Debug, Clone)]
pub struct Client {
    client_id: u32,
    extranonce1: Extranonce<'static>,
    extranonce2_size: usize,
    version_rolling_mask: Option<HexU32Be>,
    version_rolling_min_bit: Option<HexU32Be>,
    pub status: ClientStatus,
    last_notify: Option<server_to_client::Notify<'static>>,
    sented_authorize_request: Vec<(u64, String)>, // (id, user_name)
    authorized: Vec<String>,
}

impl Client {
    pub fn new(client_id: u32) -> Client {
        Client {
            client_id,
            extranonce1: extranonce_from_hex("00000000"),
            extranonce2_size: 4,
            version_rolling_mask: None,
            version_rolling_min_bit: None,
            status: ClientStatus::Init,
            last_notify: None,
            sented_authorize_request: vec![],
            authorized: vec![],
        }
    }

    // this is what we want to benchmark
    pub fn parse_message(incoming_message: String) -> json_rpc::Message {
        match serde_json::from_str::<json_rpc::Message>(&incoming_message) {
            Ok(message) => message,
            // no need to handle errors in benchmarks
            Err(_err) => panic!(),
        }
    }

    // also this is what we want to benchmark
    pub fn serialize_message(msg: json_rpc::Message) -> String {
        let json_msg = serde_json::to_string(&msg);
        match json_msg {
            Ok(json_str) => json_str,
            // no need to handle errors in benchmarks
            Err(_err) => panic!(),
        }
    }
}

impl IsClient<'static> for Client {
    fn handle_set_difficulty(
        &mut self,
        _conf: &mut server_to_client::SetDifficulty,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }
    fn handle_set_version_mask(
        &mut self,
        _conf: &mut server_to_client::SetVersionMask,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn handle_set_extranonce(
        &mut self,
        _conf: &mut server_to_client::SetExtranonce,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn handle_notify(
        &mut self,
        notify: server_to_client::Notify<'static>,
    ) -> Result<(), Error<'static>> {
        self.last_notify = Some(notify);
        Ok(())
    }

    fn handle_configure(
        &mut self,
        _conf: &mut server_to_client::Configure,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn handle_subscribe(
        &mut self,
        _subscribe: &server_to_client::Subscribe<'static>,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn set_extranonce1(&mut self, extranonce1: Extranonce<'static>) {
        self.extranonce1 = extranonce1;
    }

    fn extranonce1(&self) -> Extranonce<'static> {
        self.extranonce1.clone()
    }

    fn set_extranonce2_size(&mut self, extra_nonce2_size: usize) {
        self.extranonce2_size = extra_nonce2_size;
    }

    fn extranonce2_size(&self) -> usize {
        self.extranonce2_size
    }

    fn version_rolling_mask(&self) -> Option<HexU32Be> {
        self.version_rolling_mask.clone()
    }

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_mask = mask;
    }

    fn set_version_rolling_min_bit(&mut self, min: Option<HexU32Be>) {
        self.version_rolling_min_bit = min;
    }

    fn set_status(&mut self, status: ClientStatus) {
        self.status = status;
    }

    fn signature(&self) -> String {
        format!("{}", self.client_id)
    }

    fn status(&self) -> ClientStatus {
        self.status
    }

    fn version_rolling_min_bit(&mut self) -> Option<HexU32Be> {
        self.version_rolling_min_bit.clone()
    }

    fn id_is_authorize(&mut self, id: &u64) -> Option<String> {
        let req: Vec<&(u64, String)> = self
            .sented_authorize_request
            .iter()
            .filter(|x| x.0 == *id)
            .collect();
        match req.len() {
            0 => None,
            _ => Some(req[0].1.clone()),
        }
    }

    fn id_is_submit(&mut self, _: &u64) -> bool {
        false
    }

    fn authorize_user_name(&mut self, name: String) {
        self.authorized.push(name)
    }

    fn is_authorized(&self, name: &String) -> bool {
        self.authorized.contains(name)
    }

    fn authorize(
        &mut self,
        id: u64,
        name: String,
        password: String,
    ) -> Result<json_rpc::Message, Error> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.authorize".to_string())),
            _ => {
                self.sented_authorize_request.push((id, name.to_string()));
                Ok(client_to_server::Authorize { id, name, password }.into())
            }
        }
    }

    fn last_notify(&self) -> Option<server_to_client::Notify> {
        self.last_notify.clone()
    }

    fn handle_error_message(
        &mut self,
        _message: v1::Message,
    ) -> Result<Option<json_rpc::Message>, Error<'static>> {
        Ok(None)
    }
}
pub fn extranonce_from_hex(hex: &str) -> Extranonce<'static> {
    let data = utils::decode_hex(hex).unwrap();
    Extranonce::try_from(data).expect("Failed to convert hex to U256")
}
pub fn prevhash_from_hex<'a>(hex: &str) -> PrevHash<'a> {
    let data = utils::decode_hex(hex).unwrap();
    let len = data.len();
    if hex.len() >= 64 {
        // panic if hex is larger than 32 bytes
        PrevHash::try_from(hex).expect("Failed to convert hex to U256")
    } else {
        // prepend hex with zeros so that it is 32 bytes
        let mut new_vec = vec![0_u8; 32 - len];
        new_vec.extend(data.iter());
        PrevHash::try_from(utils::encode_hex(&new_vec).as_str())
            .expect("Failed to convert hex to U256")
    }
}

pub fn merklenode_from_hex<'a>(hex: &str) -> MerkleNode<'a> {
    let data = utils::decode_hex(hex).unwrap();
    let len = data.len();
    if hex.len() >= 64 {
        // panic if hex is larger than 32 bytes
        MerkleNode::try_from(hex).expect("Failed to convert hex to U256")
    } else {
        // prepend hex with zeros so that it is 32 bytes
        let mut new_vec = vec![0_u8; 32 - len];
        new_vec.extend(data.iter());
        MerkleNode::try_from(utils::encode_hex(&new_vec).as_str())
            .expect("Failed to convert hex to U256")
    }
}
pub fn notify(client: &mut Client) {
    client.status = ClientStatus::Subscribed;
    let notify = v1::server_to_client::Notify {
        job_id: "ciao".to_string(),
        prev_hash: prevhash_from_hex("00"),
        coin_base1: "00".try_into().unwrap(),
        coin_base2: "00".try_into().unwrap(),
        merkle_branch: vec![merklenode_from_hex("00")],
        version: HexU32Be(5667),
        bits: HexU32Be(5678),
        time: HexU32Be(5609),
        clean_jobs: true,
    };
    Client::handle_notify(client, notify).unwrap();
}

mod utils {

    pub fn decode_hex(s: &str) -> Result<Vec<u8>, core::num::ParseIntError> {
        let s = match s.strip_prefix("0x") {
            Some(s) => s,
            None => s,
        };
        (0..s.len())
            .step_by(2)
            .map(|i| u8::from_str_radix(&s[i..i + 2], 16))
            .collect()
    }

    pub fn encode_hex(bytes: &[u8]) -> String {
        bytes.iter().map(|b| format!("{b:02x}")).collect()
    }
}
</file>

<file path="stratum-1.4.0/benches/benches/src/sv2/criterion_sv2_benchmark.rs">
use codec_sv2::{StandardEitherFrame, StandardSv2Frame};
use criterion::{black_box, Criterion};
use roles_logic_sv2::{
    handlers::{common::ParseCommonMessagesFromUpstream, mining::ParseMiningMessagesFromUpstream},
    parsers::{AnyMessage, Mining, MiningDeviceMessages},
    routing_logic::{CommonRoutingLogic, MiningRoutingLogic},
    utils::Mutex,
};
use std::{
    convert::TryInto,
    net::{IpAddr, Ipv4Addr, SocketAddr},
    sync::Arc,
};

#[path = "./lib/client.rs"]
mod client;
use crate::client::{
    create_client, create_mock_frame, open_channel, Device, SetupConnectionHandler,
};

pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

fn client_sv2_setup_connection(c: &mut Criterion) {
    c.bench_function("client_sv2_setup_connection", |b| {
        let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        b.iter(|| {
            SetupConnectionHandler::get_setup_connection_message(address);
        });
    });
}

fn client_sv2_setup_connection_serialize(c: &mut Criterion) {
    c.bench_function("client_sv2_setup_connection_serialize", |b| {
        let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        let setup_message = SetupConnectionHandler::get_setup_connection_message(address);
        let setup_message: Message = setup_message.into();
        let frame: StdFrame = setup_message.try_into().unwrap();
        let size = frame.encoded_length();
        let mut dst = vec![0; size];
        b.iter(move || black_box(frame.clone()).serialize(&mut dst));
    });
}

fn client_sv2_setup_connection_serialize_deserialize(c: &mut Criterion) {
    c.bench_function("client_sv2_setup_connection_serialize_deserialize", |b| {
        let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        let setup_message = SetupConnectionHandler::get_setup_connection_message(address);
        let setup_message: Message = setup_message.into();
        let frame: StdFrame = setup_message.try_into().unwrap();
        let size = frame.encoded_length();
        let mut dst = vec![0; size];
        let _serialized = frame.serialize(&mut dst);
        b.iter(|| {
            let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
            let type_ = frame.get_header().unwrap().msg_type().clone();
            let payload = frame.payload();
            let _ = AnyMessage::try_from((type_, payload)).unwrap();
        });
    });
}

fn client_sv2_open_channel(c: &mut Criterion) {
    c.bench_function("client_sv2_open_channel", |b| {
        let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        b.iter(|| {
            black_box(MiningDeviceMessages::Mining(
                Mining::OpenStandardMiningChannel(open_channel()),
            ));
        });
    });
}

fn client_sv2_open_channel_serialize(c: &mut Criterion) {
    c.bench_function("client_sv2_open_channel_serialize", |b| {
        let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        let open_channel =
            MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(open_channel()));
        let frame: StdFrame = open_channel.try_into().unwrap();
        let size = frame.encoded_length();
        let mut dst = vec![0; size];
        b.iter(|| black_box(frame.clone().serialize(&mut dst)));
    });
}

fn client_sv2_open_channel_serialize_deserialize(c: &mut Criterion) {
    c.bench_function("client_sv2_open_channel_serialize_deserialize", |b| {
        let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
        let open_channel =
            MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(open_channel()));
        let frame: StdFrame = open_channel.try_into().unwrap();
        let size = frame.encoded_length();
        let mut dst = vec![0; size];
        frame.serialize(&mut dst);
        b.iter(|| {
            let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
            let type_ = frame.get_header().unwrap().msg_type().clone();
            let payload = frame.payload();
            black_box(AnyMessage::try_from((type_, payload)).unwrap());
        });
    });
}

fn client_sv2_mining_message_submit_standard(c: &mut Criterion) {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    c.bench_function("client_sv2_mining_message_submit_standard", |b| {
        b.iter(|| {
            Device::send_mining_message(self_mutex.clone(), nonce, job_id, version, ntime);
        });
    });
}

fn client_sv2_mining_message_submit_standard_serialize(c: &mut Criterion) {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    let submit_share_message =
        Device::send_mining_message(self_mutex.clone(), nonce, job_id, version, ntime);
    let frame: StdFrame = submit_share_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    c.bench_function("client_sv2_mining_message_submit_standard_serialize", |b| {
        b.iter(|| black_box(frame.clone().serialize(&mut dst)));
    });
}

fn client_sv2_mining_message_submit_standard_serialize_deserialize(c: &mut Criterion) {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    let submit_share_message =
        Device::send_mining_message(self_mutex.clone(), nonce, job_id, version, ntime);
    let frame: StdFrame = submit_share_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    frame.serialize(&mut dst);
    c.bench_function(
        "client_sv2_mining_message_submit_standard_serialize_deserialize",
        |b| {
            b.iter(|| {
                let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
                let type_ = frame.get_header().unwrap().msg_type().clone();
                let payload = frame.payload();
                black_box(AnyMessage::try_from((type_, payload)).unwrap());
            });
        },
    );
}

fn client_sv2_handle_message_mining(c: &mut Criterion) {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let _frame = create_mock_frame();
    let message_type = u8::from_str_radix("8", 16).unwrap();
    let payload: u8 = 200;
    let payload: &mut [u8] = &mut [payload];
    c.bench_function("client_sv2_handle_message_mining", |b| {
        b.iter(|| {
            black_box(Device::handle_message_mining(
                self_mutex.clone(),
                message_type,
                payload,
                MiningRoutingLogic::None,
            ))
        });
    });
}

fn client_sv2_handle_message_common(c: &mut Criterion) {
    let self_ = Arc::new(Mutex::new(SetupConnectionHandler {}));
    let message_type = u8::from_str_radix("8", 16).unwrap();
    let payload: u8 = 200;
    let payload: &mut [u8] = &mut [payload];
    c.bench_function("client_sv2_handle_message_common", |b| {
        b.iter(|| {
            black_box(ParseCommonMessagesFromUpstream::handle_message_common(
                self_.clone(),
                message_type,
                payload,
                CommonRoutingLogic::None,
            ))
        });
    });
}

fn main() {
    let mut criterion = Criterion::default()
        .sample_size(100)
        .measurement_time(std::time::Duration::from_secs(5));
    client_sv2_setup_connection(&mut criterion);
    client_sv2_setup_connection_serialize(&mut criterion);
    client_sv2_setup_connection_serialize_deserialize(&mut criterion);
    client_sv2_open_channel(&mut criterion);
    client_sv2_open_channel_serialize(&mut criterion);
    client_sv2_open_channel_serialize_deserialize(&mut criterion);
    client_sv2_mining_message_submit_standard(&mut criterion);
    client_sv2_mining_message_submit_standard_serialize(&mut criterion);
    client_sv2_mining_message_submit_standard_serialize_deserialize(&mut criterion);
    client_sv2_handle_message_common(&mut criterion);
    client_sv2_handle_message_mining(&mut criterion);
    criterion.final_summary();
}
</file>

<file path="stratum-1.4.0/benches/benches/src/sv2/iai_sv2_benchmark.rs">
use codec_sv2::{StandardEitherFrame, StandardSv2Frame};
use iai::{black_box, main};
use roles_logic_sv2::{
    handlers::{
        common::ParseCommonMessagesFromUpstream, mining::ParseMiningMessagesFromUpstream, SendTo_,
    },
    parsers::{AnyMessage, Mining, MiningDeviceMessages},
    routing_logic::{CommonRoutingLogic, MiningRoutingLogic},
    utils::Mutex,
};
use std::{
    convert::TryInto,
    net::{IpAddr, Ipv4Addr, SocketAddr},
    sync::Arc,
};

#[path = "./lib/client.rs"]
mod client;
use crate::client::{create_client, open_channel, Device, SetupConnectionHandler};

pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

fn client_sv2_setup_connection() {
    let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    black_box(SetupConnectionHandler::get_setup_connection_message(
        address,
    ));
}

fn client_sv2_setup_connection_serialize() -> Result<(), framing_sv2::Error> {
    let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    let setup_message = SetupConnectionHandler::get_setup_connection_message(address);
    let setup_message: Message = setup_message.into();
    let frame: StdFrame = setup_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    black_box(frame.clone()).serialize(&mut dst)
}

fn client_sv2_setup_connection_serialize_deserialize() {
    let address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    let setup_message = SetupConnectionHandler::get_setup_connection_message(address);
    let setup_message: Message = setup_message.into();
    let frame: StdFrame = setup_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    frame.serialize(&mut dst);
    let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
    let type_ = frame.get_header().unwrap().msg_type().clone();
    let payload = frame.payload();
    black_box(AnyMessage::try_from((type_, payload)));
}

fn client_sv2_open_channel() {
    let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    black_box(MiningDeviceMessages::Mining(
        Mining::OpenStandardMiningChannel(open_channel()),
    ));
}

fn client_sv2_open_channel_serialize() -> Result<(), framing_sv2::Error> {
    let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    let open_channel =
        MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(open_channel()));
    let frame: StdFrame = open_channel.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    black_box(frame.clone().serialize(&mut dst))
}

fn client_sv2_open_channel_serialize_deserialize() {
    let _address: SocketAddr = SocketAddr::new(IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0)), 34254);
    let open_channel =
        MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(open_channel()));
    let frame: StdFrame = open_channel.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    frame.serialize(&mut dst);
    let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
    let type_ = frame.get_header().unwrap().msg_type().clone();
    let payload = frame.payload();
    black_box(AnyMessage::try_from((type_, payload)));
}

fn client_sv2_mining_message_submit_standard() {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    black_box(Device::send_mining_message(
        self_mutex.clone(),
        nonce,
        job_id,
        version,
        ntime,
    ));
}

fn client_sv2_mining_message_submit_standard_serialize() -> Result<(), framing_sv2::Error> {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    let submit_share_message =
        Device::send_mining_message(self_mutex.clone(), nonce, job_id, version, ntime);
    let frame: StdFrame = submit_share_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    black_box(frame.clone().serialize(&mut dst))
}

fn client_sv2_mining_message_submit_standard_serialize_deserialize() {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let nonce: u32 = 96;
    let job_id: u32 = 1;
    let version = 78;
    let ntime = 2;
    let submit_share_message =
        Device::send_mining_message(self_mutex.clone(), nonce, job_id, version, ntime);
    let frame: StdFrame = submit_share_message.try_into().unwrap();
    let size = frame.encoded_length();
    let mut dst = vec![0; size];
    frame.serialize(&mut dst);
    let mut frame = StdFrame::from_bytes(black_box(dst.clone().into())).unwrap();
    let type_ = frame.get_header().unwrap().msg_type().clone();
    let payload = frame.payload();
    black_box(AnyMessage::try_from((type_, payload)));
}

fn client_sv2_handle_message_mining(
) -> Result<SendTo_<roles_logic_sv2::parsers::Mining<'static>, ()>, roles_logic_sv2::Error> {
    let client = create_client();
    let self_mutex = Arc::new(Mutex::new(client));
    let message_type = u8::from_str_radix("8", 16).unwrap();
    let payload: u8 = 200;
    let payload: &mut [u8] = &mut [payload];
    black_box(Device::handle_message_mining(
        self_mutex.clone(),
        message_type,
        payload,
        MiningRoutingLogic::None,
    ))
}

fn client_sv2_handle_message_common() {
    let self_ = Arc::new(Mutex::new(SetupConnectionHandler {}));
    let message_type = u8::from_str_radix("8", 16).unwrap();
    let payload: u8 = 200;
    let payload: &mut [u8] = &mut [payload];
    black_box(ParseCommonMessagesFromUpstream::handle_message_common(
        self_.clone(),
        message_type,
        payload,
        CommonRoutingLogic::None,
    ));
}

main! {
    client_sv2_setup_connection,
    client_sv2_setup_connection_serialize,
    client_sv2_setup_connection_serialize_deserialize,
    client_sv2_mining_message_submit_standard,
    client_sv2_mining_message_submit_standard_serialize,
    client_sv2_mining_message_submit_standard_serialize_deserialize,
    client_sv2_open_channel,
    client_sv2_open_channel_serialize,
    client_sv2_open_channel_serialize_deserialize,
    client_sv2_handle_message_common,
    client_sv2_handle_message_mining
}
</file>

<file path="stratum-1.4.0/benches/benches/src/sv2/lib/client.rs">
use async_channel::{Receiver, Sender};
use async_std::channel::unbounded;
use binary_sv2::u256_from_int;
use codec_sv2::{StandardEitherFrame, StandardSv2Frame};
use primitive_types::U256;
use roles_logic_sv2::{
    common_messages_sv2::{Protocol, SetupConnection, SetupConnectionSuccess},
    common_properties::{IsMiningUpstream, IsUpstream},
    errors::Error,
    handlers::{
        common::ParseCommonMessagesFromUpstream,
        mining::{ParseMiningMessagesFromUpstream, SendTo, SupportedChannelTypes},
    },
    mining_sv2::*,
    parsers::{Mining, MiningDeviceMessages},
    routing_logic::NoRouting,
    selectors::NullDownstreamMiningSelector,
    utils::{Id, Mutex},
};
use std::{net::SocketAddr, sync::Arc};
use stratum_common::bitcoin::{blockdata::block::Header, hash_types::BlockHash, hashes::Hash};
use tracing::{debug, error, info, trace};
pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

pub fn create_client() -> Device {
    let (sender, receiver) = unbounded();
    let miner = Arc::new(Mutex::new(Miner::new(10)));

    Device {
        channel_opened: false,
        receiver: receiver.clone(),
        sender: sender.clone(),
        miner: miner.clone(),
        jobs: Vec::new(),
        prev_hash: None,
        channel_id: None,
        sequence_numbers: Id::new(),
    }
}

pub fn create_mock_frame() -> StdFrame {
    let _client = create_client();
    let open_channel =
        MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(open_channel()));
    open_channel.try_into().unwrap()
}

pub struct SetupConnectionHandler {}
use std::convert::TryInto;

impl SetupConnectionHandler {
    pub fn get_setup_connection_message(address: SocketAddr) -> SetupConnection<'static> {
        let endpoint_host = address.ip().to_string().into_bytes().try_into().unwrap();
        let vendor = String::new().try_into().unwrap();
        let hardware_version = String::new().try_into().unwrap();
        let firmware = String::new().try_into().unwrap();
        let device_id = String::new().try_into().unwrap();
        SetupConnection {
            protocol: Protocol::MiningProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0b0000_0000_0000_0000_0000_0000_0000_0001,
            endpoint_host,
            endpoint_port: address.port(),
            vendor,
            hardware_version,
            firmware,
            device_id,
        }
    }
}

impl ParseCommonMessagesFromUpstream<NoRouting> for SetupConnectionHandler {
    fn handle_setup_connection_success(
        &mut self,
        _: SetupConnectionSuccess,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        use roles_logic_sv2::handlers::common::SendTo;
        Ok(SendTo::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }
}

#[derive(Debug)]
pub struct Device {
    #[allow(dead_code)]
    pub receiver: Receiver<EitherFrame>,
    pub sender: Sender<EitherFrame>,
    #[allow(dead_code)]
    pub channel_opened: bool,
    pub channel_id: Option<u32>,
    pub miner: Arc<Mutex<Miner>>,
    pub jobs: Vec<NewMiningJob<'static>>,
    pub prev_hash: Option<SetNewPrevHash<'static>>,
    pub sequence_numbers: Id,
}

pub fn open_channel() -> OpenStandardMiningChannel<'static> {
    let user_identity = "ABC".to_string().try_into().unwrap();
    let id: u32 = 10;
    OpenStandardMiningChannel {
        request_id: id.into(),
        user_identity,
        nominal_hash_rate: 1000.0, // use 1000 or 10000 to test group channels
        max_target: u256_from_int(567_u64),
    }
}

impl Device {
    pub fn send_mining_message(
        self_mutex: Arc<Mutex<Self>>,
        nonce: u32,
        job_id: u32,
        version: u32,
        ntime: u32,
    ) -> MiningDeviceMessages<'static> {
        let share: MiningDeviceMessages<'_> =
            MiningDeviceMessages::Mining(Mining::SubmitSharesStandard(SubmitSharesStandard {
                channel_id: self_mutex.safe_lock(|_: &mut Device| 0).unwrap(),
                sequence_number: self_mutex.safe_lock(|s| s.sequence_numbers.next()).unwrap(),
                job_id,
                nonce,
                ntime,
                version,
            }));
        share
    }
}

impl IsUpstream for Device {
    fn get_version(&self) -> u16 {
        todo!()
    }

    fn get_flags(&self) -> u32 {
        todo!()
    }

    fn get_supported_protocols(&self) -> Vec<Protocol> {
        todo!()
    }

    fn get_id(&self) -> u32 {
        todo!()
    }

    fn get_mapper(&mut self) -> Option<&mut roles_logic_sv2::common_properties::RequestIdMapper> {
        todo!()
    }

    fn get_remote_selector(&mut self) -> &mut NullDownstreamMiningSelector {
        todo!()
    }
}

impl IsMiningUpstream for Device {
    fn total_hash_rate(&self) -> u64 {
        todo!()
    }

    fn add_hash_rate(&mut self, _to_add: u64) {
        todo!()
    }
    fn get_opened_channels(
        &mut self,
    ) -> &mut Vec<roles_logic_sv2::common_properties::UpstreamChannel> {
        todo!()
    }

    fn update_channels(&mut self, _: roles_logic_sv2::common_properties::UpstreamChannel) {
        todo!()
    }
}

impl ParseMiningMessagesFromUpstream<(), NullDownstreamMiningSelector, NoRouting> for Device {
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::Standard
    }

    fn is_work_selection_enabled(&self) -> bool {
        false
    }

    fn handle_open_standard_mining_channel_success(
        &mut self,
        m: OpenStandardMiningChannelSuccess,
        _: Option<std::sync::Arc<Mutex<()>>>,
    ) -> Result<SendTo<()>, Error> {
        self.channel_opened = true;
        self.channel_id = Some(m.channel_id);
        let req_id = m.get_request_id_as_u32();
        println!(
            "MINING DEVICE: channel opened with: group id {}, channel id {}, request id {}",
            m.group_channel_id, m.channel_id, req_id
        );
        self.miner
            .safe_lock(|miner| miner.new_target(m.target.to_vec()))
            .unwrap();
        Ok(SendTo::None(None))
    }

    fn handle_open_extended_mining_channel_success(
        &mut self,
        _: OpenExtendedMiningChannelSuccess,
    ) -> Result<SendTo<()>, Error> {
        unreachable!()
    }

    fn handle_open_mining_channel_error(
        &mut self,
        _: OpenMiningChannelError,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_update_channel_error(&mut self, _: UpdateChannelError) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_close_channel(&mut self, _: CloseChannel) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_extranonce_prefix(
        &mut self,
        _: SetExtranoncePrefix,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_submit_shares_success(
        &mut self,
        m: SubmitSharesSuccess,
    ) -> Result<SendTo<()>, Error> {
        info!("Received SubmitSharesSuccess");
        debug!("SubmitSharesSuccess: {:?}", m);
        Ok(SendTo::None(None))
    }

    fn handle_submit_shares_error(&mut self, m: SubmitSharesError) -> Result<SendTo<()>, Error> {
        error!(
            "Received SubmitSharesError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::None(None))
    }

    fn handle_new_mining_job(&mut self, m: NewMiningJob) -> Result<SendTo<()>, Error> {
        info!(
            "Received new mining job for channel id: {} with job id: {} is future: {}",
            m.channel_id,
            m.job_id,
            m.is_future()
        );
        debug!("NewMiningJob: {:?}", m);
        match (m.is_future(), self.prev_hash.as_ref()) {
            (false, Some(p_h)) => {
                self.miner
                    .safe_lock(|miner| miner.new_header(p_h, &m))
                    .unwrap();
                self.jobs = vec![m.as_static()];
            }
            (true, _) => self.jobs.push(m.as_static()),
            (false, None) => {
                panic!()
            }
        }
        Ok(SendTo::None(None))
    }

    fn handle_new_extended_mining_job(
        &mut self,
        _: NewExtendedMiningJob,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo<()>, Error> {
        info!(
            "Received SetNewPrevHash channel id: {}, job id: {}",
            m.channel_id, m.job_id
        );
        debug!("SetNewPrevHash: {:?}", m);
        let jobs: Vec<&NewMiningJob<'static>> = self
            .jobs
            .iter()
            .filter(|j| j.job_id == m.job_id && j.is_future())
            .collect();
        match jobs.len() {
            0 => {
                self.prev_hash = Some(m.as_static());
            }
            1 => {
                self.miner
                    .safe_lock(|miner| miner.new_header(&m, jobs[0]))
                    .unwrap();
                self.jobs = vec![jobs[0].clone()];
                self.prev_hash = Some(m.as_static());
            }
            _ => panic!(),
        }
        Ok(SendTo::None(None))
    }

    fn handle_set_custom_mining_job_success(
        &mut self,
        _: SetCustomMiningJobSuccess,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_custom_mining_job_error(
        &mut self,
        _: SetCustomMiningJobError,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_target(&mut self, _: SetTarget) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_reconnect(&mut self, _: Reconnect) -> Result<SendTo<()>, Error> {
        todo!()
    }
}

#[derive(Debug)]
pub struct Miner {
    header: Option<Header>,
    target: Option<U256>,
    job_id: Option<u32>,
    version: Option<u32>,
    handicap: u32,
}

impl Miner {
    pub fn new(handicap: u32) -> Self {
        Self {
            target: None,
            header: None,
            job_id: None,
            version: None,
            handicap,
        }
    }

    fn new_target(&mut self, mut target: Vec<u8>) {
        // target is sent in LE and comparisons in this file are done in BE
        target.reverse();
        self.target = Some(U256::from_big_endian(target.try_into().unwrap()));
    }

    fn new_header(&mut self, set_new_prev_hash: &SetNewPrevHash, new_job: &NewMiningJob) {
        self.job_id = Some(new_job.job_id);
        self.version = Some(new_job.version);
        let prev_hash: [u8; 32] = set_new_prev_hash.prev_hash.to_vec().try_into().unwrap();
        let prev_hash = Hash::from_inner(prev_hash);
        let merkle_root: [u8; 32] = new_job.merkle_root.to_vec().try_into().unwrap();
        let merkle_root = Hash::from_inner(merkle_root);
        // fields need to be added as BE and the are converted to LE in the background before
        // hashing
        let header = Header {
            version: new_job.version as i32,
            prev_blockhash: BlockHash::from_hash(prev_hash),
            merkle_root,
            time: std::time::SystemTime::now()
                .duration_since(
                    std::time::SystemTime::UNIX_EPOCH - std::time::Duration::from_secs(60),
                )
                .unwrap()
                .as_secs() as u32,
            bits: set_new_prev_hash.nbits,
            nonce: 0,
        };
        self.header = Some(header);
    }
}
</file>

<file path="stratum-1.4.0/benches/Cargo.toml">
[package]
name = "benchmark"
version = "1.0.1"
edition = "2021"

[dependencies]
bitcoin = { version = "0.32.5", optional = true }
async-std={version = "1.10.0", features = ["attributes"]}
criterion = "0.5.1"
async-channel = "1.4.0"
v1 = { path="../protocols/v1", package="sv1_api" }
serde_json = { version = "1.0.64", default-features = false, features = ["alloc"] }
iai="0.1"
mining_sv2 = { path = "../protocols/v2/subprotocols/mining" }
roles_logic_sv2 = { path = "../protocols/v2/roles-logic-sv2" }
framing_sv2 = { path = "../protocols/v2/framing-sv2" }
serde = { version = "1.0.89", default-features = false, features = ["derive", "alloc"] }
num-bigint = "0.4.3"
num-traits = "0.2.15"
codec_sv2 = { path = "../protocols/v2/codec-sv2", features=["noise_sv2"] }
binary_sv2 = { path = "../protocols/v2/binary-sv2" }
network_helpers_sv2 = { path = "../roles/roles-utils/network-helpers" }
rand = "0.8.4"
primitive-types = "0.13.1"
tracing = "0.1"

[[bench]]
name = "criterion_sv1_benchmark"
path="benches/src/sv1/criterion_sv1_benchmark.rs"
harness = false

[[bench]]
name = "iai_sv1_benchmark"
path="benches/src/sv1/iai_sv1_benchmark.rs"
harness = false

[[bench]]
name = "criterion_sv2_benchmark"
path = "benches/src/sv2/criterion_sv2_benchmark.rs"
harness = false

[[bench]]
name = "iai_sv2_benchmark"
path = "benches/src/sv2/iai_sv2_benchmark.rs"
harness = false
</file>

<file path="stratum-1.4.0/codecov.yaml">
coverage:
  status:
    project:
      default:
        target: auto
        threshold: 100%
        base: auto
        informational: false
    patch:
      default:
        target: auto
        threshold: 100%
        base: auto
github_checks:
    annotations: false
</file>

<file path="stratum-1.4.0/common/Cargo.toml">
[package]
name = "stratum-common"
version = "3.0.0"
edition = "2018"
description = "SV2 pool role"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"

[dependencies]
roles_logic_sv2 = { path = "../protocols/v2/roles-logic-sv2", version = "3.0.0" }
network_helpers_sv2 = { path = "../roles/roles-utils/network-helpers", version = "4.0.0", features = ["with_buffer_pool"], optional = true }

[features]
with_network_helpers = ["dep:network_helpers_sv2"]
sv1 = ["network_helpers_sv2/sv1"]
</file>

<file path="stratum-1.4.0/common/src/lib.rs">
//! # Stratum Common Crate
//!
//! `stratum_common` is a utility crate designed to centralize
//! and manage the shared dependencies and utils across stratum crates.

#[cfg(feature = "with_network_helpers")]
pub use network_helpers_sv2;
pub use roles_logic_sv2;
</file>

<file path="stratum-1.4.0/CONTRIBUTING.md">
<!-- omit in toc -->
# Contributing to SRI (Stratum V2 Reference Implementation)

First off, thanks for taking the time to contribute! 

All types of contributions are encouraged and valued. See the [Table of Contents](#table-of-contents) for different ways to help and details about how this project handles them. Please make sure to read the relevant section before making your contribution. It will make it a lot easier for us maintainers and smooth out the experience for all involved. The community looks forward to your contributions. 

> And if you like the project, but just don't have time to contribute, that's fine. There are other easy ways to support the project and show your appreciation, which we would also be very happy about:
> - Star the project
> - Tweet about it
> - Refer this project in your project's readme
> - Mention the project at local meetups and tell your friends/colleagues

<!-- omit in toc -->
## Table of Contents

- [I Have a Question](#i-have-a-question)
- [What Should I Know Before I Get Started](#what-should-i-know-before-i-get-started)
  - [Important Resources About SRI](#important-resources-about-sri)
  - [Project Communications](#project-communications)
- [I Want To Contribute](#i-want-to-contribute)
  - [Project Structure](#project-structure)
  - [Contribution workflow](#contribution-workflow)
  - [Your First Code Contribution](#your-first-code-contribution)
  

## I Have a Question

> If you want to ask a question, we assume that you have read the documentation available at [stratumprotocol.org/docs](https://stratumprotocol.org).

Best way to ask a question is to hop onto our community [Discord](https://discord.com/invite/fsEW23wFYs). Two most suitable places to post a question are:
- #newbies-qs and
- #dev, for technical questions, suitable for developers building on top of SRI, or contributing to it.

If you then still feel the need to ask a question and need clarification, we recommend the following:

- Open an [Issue](https://github.com/stratum-mining/stratum/issues/new).
- Provide as much context as you can about what you're running into.
  
We will then take care of the issue as soon as possible.

## What Should I Know Before I Get Started

### Important Resources About SRI

In order to have a better overview about what SRI covers, have a look at the following resources before getting started with contributions.

  - Stratum V2 Protocol [Specifications](https://github.com/stratum-mining/sv2-spec). 
    - Studying SV2 specs can take some time and requires effort, but it's the best way to properly understand what SV2 is about and how it's composed.
  - SRI [Getting-started](https://stratumprotocol.org/getting-started/) guide.
    - This can be explored in the meantime of SV2 protocol study, so that it can help getting a general overview about SV2. Moreover, it's the best way to really understand how SRI project is built on. 
  - Stratum V2 [Master Degree Thesis](https://github.com/GitGab19/Stratum-V2-Master-Degree-Thesis) (by [@gitgab19](https://github.com/GitGab19/)).
    - This resource can be useful to get some knowledge about Bitcoin mining, pooled mining protocols history, and Stratum V2.  
  - Stratum V2 Explained - [Videos Playlist](https://www.youtube.com/playlist?list=PLZXAi8dsUIn0GmElOcmqUtgA5psfFIZoO) (by [@plebhash](https://github.com/plebhash)). 
    - This is a series of videos explaining Stratum V2 in depth, which cover the aforementioned topics.

### Project Communications

Most project communications happen in our [Discord](https://discord.gg/fsEW23wFYs) server. Communications related to general development typically happen under [dev](https://discord.com/channels/950687892169195530/958814900770205739) channel.

Discussion about specific codebase work happens in GitHub [issues](https://github.com/stratum-mining/stratum/issues/) and on [pull requests](https://github.com/stratum-mining/stratum/pulls/).

Our dev calls are scheduled every Tuesday at 18.00 CET. You can see them in the sidebar under Events on Discord and subscribe to them to be notified.

## I Want To Contribute
> When contributing to this project, you must agree that you have authored 100% of the content, that you have the necessary rights to the content and that the content you contribute may be provided under the project license.

### Project Structure
It's possible to contribute to SRI opening PRs on three different repositories:
  - [Stratum V2 Reference Implementation](https://github.com/stratum-mining/stratum)
    - This repo contains our implementation of the SV2 specs, written in Rust.
  - [Stratum V2 Specifications](https://github.com/stratum-mining/sv2-specs)
    - This repo contains the entire SV2 protocol specifications.
  - [SRI website - stratumprotocol.org](https://github.com/stratum-mining/stratumprotocol.org)
    - This repo manages our website, containing docs, specs, and getting-started guides.

### Contribution workflow

The SRI project follows an open contributor model, where anyone is welcome to contribute through reviews, documentation, testing, and patches. Follow these steps to contribute:

1. **Fork the Repository**

2. **Create a Branch** 

3. **Commit Your Changes**
    
    **Note:** Commits should cover both the issue fixed and the solution's rationale. These [guidelines](https://chris.beams.io/posts/git-commit/) should be kept in mind.

4. **Run Tests, Clippy, and Formatter:** 

    `cargo test`: this command runs the project's test suite. Ensure that all tests pass without errors.

    `cargo clippy`: Clippy is a linter tool for detecting common mistakes and style issues. Address any warnings or errors reported by Clippy.

    `cargo fmt`: this command formats your code according to the project's style guidelines. Make sure to run this command to ensure consistency in code formatting.

5. **Submit a Pull Request:** once you're satisfied with your changes, submit a pull request to the original SRI repository. Provide a clear and concise description of the changes you've made. If your pull request addresses an existing issue, reference the issue number in the description. In order to contribute to the protocol implementation, every PR must be opened against `main` branch.

6. **Review and Iterate** 

7. **Merge and Close:** Once your pull request has been approved and all discussions have been resolved, a project maintainer will merge your changes into the `main` branch. Your contribution will then be officially part of the project. The pull request will be closed, marking the completion of your contribution.

### Your First Code Contribution
>In order to contribute, a basic learning about git and github is needed. If you're not familiar with them, have a look at https://docs.github.com/en/get-started/start-your-journey/git-and-github-learning-resources to dig into and learn how to use them.

Unsure where to begin contributing to SRI? You can start by looking through `good first issue` and `help wanted` issues:

* [Good first issue](https://github.com/stratum-mining/stratum/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) are issues which should only require a few lines of code, and a test or two.
* [Help wanted](https://github.com/stratum-mining/stratum/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22) - issues which should be a bit more involved than `good first issue` issues.

Another way to better understand where to focus your contribution is by looking at our roadmap: https://github.com/orgs/stratum-mining/projects/5
</file>

<file path="stratum-1.4.0/examples/interop-cpp-no-cargo/.gitignore">
/sv2.h
/a.out
/libsv2_ffi.a
/deps
</file>

<file path="stratum-1.4.0/examples/interop-cpp-no-cargo/README.md">
# C++ interop no cargo

An example of how to build the code in ../../interop-cpp/ without using cargo.

`./run.sh` will build and run the example
</file>

<file path="stratum-1.4.0/examples/interop-cpp-no-cargo/run.sh">
#! /bin/sh

touch libsv2_ffi.a
touch a.out

# CLEAN
rm -f libsv2_ffi.a
rm -f a.out
rm -f sv2.h

./rust-build-script.sh ../../protocols/v2/ ../../utils/

g++ -I ../../protocols/v2/sv2-ffi ../interop-cpp/template-provider/template-provider.cpp  libsv2_ffi.a  -lpthread -ldl

./a.out &
provider_pid=$!
sleep 1 # wait for provider to start listening

cargo run --manifest-path ../interop-cpp/Cargo.toml &
run_pid=$!

# If there is a first argument sleep for that long
if [ -n "$1" ]; then
    sleep "$1"

  if ps -p $provider_pid > /dev/null && ps -p $run_pid > /dev/null
  then
      echo "Success!"
      kill $provider_pid
      kill $run_pid
  else
      echo "Failure!!!"
      exit 1
  fi
fi
</file>

<file path="stratum-1.4.0/examples/interop-cpp-no-cargo/rust-build-script.sh">
#! /bin/sh

set -ex

ROOT=$1
UTILS=$2

DEPS="./deps"

rm -rf $DEPS

mkdir $DEPS

rustc \
        --crate-name buffer_sv2 \
        --edition=2018 \
        "$UTILS"/buffer/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi,artifacts \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS \

rustc \
        --crate-name binary_codec_sv2 \
        --edition=2018 \
        $ROOT/binary-sv2/codec/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C embed-bitcode=no \
        -C debug-assertions=off \
        --out-dir $DEPS \
        -L dependency=$DEPS

rustc \
        --crate-name binary_codec_sv2 \
        --edition=2018 \
        $ROOT/binary-sv2/codec/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi,artifacts \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS

rustc \
        --crate-name derive_codec_sv2 \
        --edition=2018 \
        $ROOT/binary-sv2/derive_codec/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type proc-macro \
        --emit=dep-info,link \
        -C prefer-dynamic \
        -C embed-bitcode=no \
        -C debug-assertions=off \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_codec_sv2=$DEPS/libbinary_codec_sv2.rlib \
        --extern proc_macro

rustc \
        --crate-name binary_sv2 \
        --edition=2018 \
        $ROOT/binary-sv2/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi,artifacts \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --cfg 'feature="default"' \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_codec_sv2=$DEPS/libbinary_codec_sv2.rmeta \
        --extern derive_codec_sv2=$DEPS/libderive_codec_sv2.so

rustc \
        --crate-name framing_sv2 \
        --edition=2018 \
        $ROOT/framing-sv2/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi,artifacts \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_sv2=$DEPS/libbinary_sv2.rmeta \
        --extern const_sv2=$DEPS/libconst_sv2.rmeta

rustc \
        --crate-name common_messages_sv2 \
        --edition=2018 \
        $ROOT/subprotocols/common-messages/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_sv2=$DEPS/libbinary_sv2.rmeta \
        --extern const_sv2=$DEPS/libconst_sv2.rmeta

rustc \
        --crate-name template_distribution_sv2 \
        --edition=2018 \
        $ROOT/subprotocols/template-distribution/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_sv2=$DEPS/libbinary_sv2.rmeta \
        --extern const_sv2=$DEPS/libconst_sv2.rmeta

rustc \
        --crate-name codec_sv2 \
        --edition=2018 \
        $ROOT/codec-sv2/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type lib \
        --emit=dep-info,metadata,link \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir $DEPS \
        -L dependency=$DEPS \
        --extern binary_sv2=$DEPS/libbinary_sv2.rmeta \
        --extern const_sv2=$DEPS/libconst_sv2.rmeta \
        --extern framing_sv2=$DEPS/libframing_sv2.rmeta \
        --extern buffer_sv2=$DEPS/libbuffer_sv2.rmeta

rustc \
        --crate-name sv2_ffi \
        --edition=2018 \
        $ROOT/sv2-ffi/src/lib.rs \
        --error-format=json \
        --json=diagnostic-rendered-ansi \
        --crate-type staticlib \
        -C opt-level=3 \
        -C embed-bitcode=no \
        --out-dir ./  \
        -L dependency=$DEPS \
        --extern binary_sv2=$DEPS/libbinary_sv2.rlib \
        --extern codec_sv2=$DEPS/libcodec_sv2.rlib \
        --extern common_messages_sv2=$DEPS/libcommon_messages_sv2.rlib \
        --extern const_sv2=$DEPS/libconst_sv2.rlib \
        --extern template_distribution_sv2=$DEPS/libtemplate_distribution_sv2.rlib
</file>

<file path="stratum-1.4.0/examples/interop-cpp/.gitignore">
/sv2.h
/a.out
/libsv2_ffi.a
</file>

<file path="stratum-1.4.0/examples/interop-cpp/Cargo.toml">
[package]
name = "interop-cpp"
version = "0.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
publish = false

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
codec_sv2 = { path = "../../protocols/v2/codec-sv2" }
stratum_common = { path = "../../common" }
binary_sv2 = { path = "../../protocols/v2/binary-sv2" }
common_messages_sv2 = { path = "../../protocols/v2/subprotocols/common-messages" }
template_distribution_sv2 = { path = "../../protocols/v2/subprotocols/template-distribution" }
</file>

<file path="stratum-1.4.0/examples/interop-cpp/README.md">
# C++ interop

This crate provides an example of how to use the Rust Sv2 `Decoder` and `Encoder` from C++. 

To run the example: `./run.sh`.

The example is composed by a Rust "downstream node" that keep sending a
[`common_messages_sv2::SetupConnection`] message to a C++ "upstream node" that receive the message
and keep answering with a [`common_messages_sv2::SetupConnectionError`].

The Rust codec is exported as a C static library by the crate [sv2-ffi](../../protocols/v2/sv2-ffi).

## Intro

### Header file

The [header file](../../protocols/v2/sv2-ffi/sv2.h) is generated with `cbindgen`.

Rust enums definition are transformed by `cbingen` in:
```c
struct [Rust_enum_name] {
  union class Tag {
    [union_element_1_name]
    [union_element_2_name]
    ...
  }

  struct [union_element_1_name]_Body {
    [inner_union_element_name_if_any_1] _0;
    [inner_union_element_name_if_any_2] _1;
    ...
  }

  struct [union_element_2_name]_Body {
    [inner_union_element_name_if_any_1] _0;
    [inner_union_element_name_if_any_2] _1;
    ...
  }

  ...

  union {
    [union_element_1_name]_Body [union_element_1_name]
    [union_element_2_name]_Body [union_element_2_name]
    ...
  }
  
}
```

For example the below Rust enum:
```Rust
#[repr(C)]
pub enum CResult<T, E> {
    Ok(T),
    Err(E),
}
```

Will be transformed in:
```c
template<typename T, typename E>
struct CResult {
  enum class Tag {
    Ok,
    Err,
  };

  struct Ok_Body {
    T _0;
  };

  struct Err_Body {
    E _0;
  };

  Tag tag;
  union {
    Ok_Body ok;
    Err_Body err;
  };
};
```

### Conventions

#### Memory
All the memory used shared struct/enums (also when borrowed) is allocated by Rust.

When C++ take ownership of a Sv2 message the message must be manually dropped.

#### Enums
In order to pattern match against a Rust defined enum from C++:
```
CResult < CSv2Message, Sv2Error > frame = next_frame(decoder);

switch (frame.tag) {

case CResult < CSv2Message, Sv2Error > ::Tag::Ok:
  on_success(frame.ok._0);
  cout << "\n";
  cout << "START PARSING NEW FRAME";
  cout << "\n";
  send_setup_connection_error(new_socket);
  break;
case CResult < CSv2Message, Sv2Error > ::Tag::Err:
  on_error(frame.err._0);
  break;
};
```

### `CVec`
[`binary_sv2::binary_codec_sv2::CVec`] is used to share bytes buffers between Rust and C++.

A `CVec` can be either "borrowed" or "owned" if is on or the other depend by the method that we
use to construct it.

* (borrowed) [`binary_sv2::binary_codec_sv2::CVec::as_shared_buffer`]: used when we need to fill a Rust
   allocated buffer from C++. This method does not guarantee anything about the pointed memory
   and the user must enforce that the Rust side does not free the pointed memory while the
   C++ part is using it
   A `CVec` constructed with this method must not be freed by C++ (this is enforced by the fact that 
   the function used to free the `CVec` is not exported in the C library)
* (owned) `&[u8].into::<CVec>()`: used to copy the contents of the `&[u8]` to a `CVec`. 
   It must be dropped from C++ via [`sv2_ffi::drop_sv2_message`]
* (owned) [`binary_sv2::binary_codec_sv2::cvec_from_buffer`]: used when a `CVec` must be created in C++,
   is used to construct a [`sv2_ffi::CSv2Message`] that will be dropped as usual with
   [`sv2_ffi::drop_sv2_message`]
* (owned) `CVec2.into::<Vec<CVec>>()`, see `CVec2` section
* (owned) `Inner.into::<CVec>()`: same as `&[u8].into::<CVec>()`

### `CVec2`
A `CVec2` is a vector of `CVec`'s. It is always allocated in Rust, is used only as field of Sv2 messages, and is
dropped when the Sv2 message gets dropped.

## Memory management

### Decoder

[`sv2_ffi::DecoderWrapper`] is instantiated in C++ via [`sv2_ffi::new_decoder`].
There is no need to drop it as it will live for the entire life of the program.

[`sv2_ffi::get_writable`] returns a `CVec` and is Rust allocated memory that C++ can fill with the
socket content. The `CVec` is "borrowed" (`&[u8].into::<CVec>()`) so it will be automatically
dropped by Rust.

[`sv2_ffi::next_frame`] is used if a complete Sv2 frame is available, it returns a [`sv2_ffi::CSv2Message`].
The message can contain one or more "owned" `CVec`'s, so it must be manually dropped via
[`sv2_ffi::drop_sv2_message`]. 


### Encoder

[`sv2_ffi::EncoderWrapper`] is instantiated in C++ via [`sv2_ffi::new_encoder`].
There is no need to drop it as it will live for the entire life of the program.

A [`sv2_ffi::CSv2Message`] can be constructed in C++ ([here is an example](template-provider/template-provider.cpp#67))
if the message contains one or more `CVec`'s, then the content of the `CVec` must be copied in a Rust allocated
`CVec` with [`binary_sv2::binary_codec_sv2::cvec_from_buffer`]. The message must be dropped with 
[`sv2_ffi::drop_sv2_message`].

[`sv2_ffi::encode`] encodes a [`sv2_ffi::CSv2Message`] as an encoded Sv2 frame in a buffer internal
to [`sv2_ffi::EncoderWrapper`]. The buffer contents are returned as a "borrowed" `CVec`. After
that, C++ has copied it and it must free the encoder with [`sv2_ffi::flush_encoder`].
This is necessary because the encoder will reuse the internal buffer to encode the next message with
[`sv2_ffi::flush_encoder`]. We let the encoder know that the content of the internal buffer has been copied
and can be overwritten. 


## Decode Sv2 messages in C++

1. Instantiate a decoder with [`sv2_ffi::new_decoder`]
2. Fill the decoder, copying the input bytes in the buffer returned by [`sv2_ffi::get_writable`]
3. If the above buffer is full, call [`sv2_ffi::next_frame`]. If the decoder has enough bytes to
   decode an Sv2 frame it will return a `CSv2Message`, otherwise it returns 2.

## Encode Sv2 messages in C++

1. Instantiate an encoder with [`sv2_ffi::new_encoder`]
2. Call [`sv2_ffi::encode`] with a valid `CSv2Message`
3. Copy the returned encoded frame where needed
4. Call [`sv2_ffi::flush_encoder`] to let the encoder know that the encoded frame has been copied
</file>

<file path="stratum-1.4.0/examples/interop-cpp/run.sh">
#! /bin/sh

touch libsv2_ffi.a
touch a.out

# CLEAN
rm -f libsv2_ffi.a
rm -f a.out
rm -f sv2.h

cargo build \
    --manifest-path=../../protocols/Cargo.toml \
    --release \
    -p sv2_ffi && \
    cp ../../protocols/target/release/libsv2_ffi.a ./

../../scripts/build_header.sh ../../protocols && mv ../../scripts/sv2.h .

g++ -I ./ ./template-provider/template-provider.cpp  libsv2_ffi.a  -lpthread -ldl

./a.out &
provider_pid=$!
sleep 1 # wait for provider to start listening
cargo run &
run_pid=$!

# If there is a first argument sleep for that long
if [ -n "$1" ]; then
    sleep "$1"

  if ps -p $provider_pid > /dev/null && ps -p $run_pid > /dev/null
  then
      echo "Success!"
      kill $provider_pid
      kill $run_pid
  else
      echo "Failure!!!"
      exit 1
  fi
fi
</file>

<file path="stratum-1.4.0/examples/interop-cpp/src/main.rs">
fn main() -> Result<(), std::io::Error> {
    use main_::main;
    main()
}

mod main_ {
    use codec_sv2::{Encoder, StandardDecoder, StandardSv2Frame};
    use common_messages_sv2::{Protocol, SetupConnection, SetupConnectionError};
    use std::{
        convert::{TryFrom, TryInto},
        io::{Read, Write},
        net::TcpStream,
    };
    use stratum_common::{
        CHANNEL_BIT_SETUP_CONNECTION, MESSAGE_TYPE_SETUP_CONNECTION,
        MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
    };

    use binary_sv2::{
        decodable::{DecodableField, FieldMarker},
        encodable::EncodableField,
        from_bytes, Deserialize, Error,
    };

    #[derive(Clone, Debug)]
    pub enum Sv2Message<'a> {
        SetupConnection(SetupConnection<'a>),
        SetupConnectionError(SetupConnectionError<'a>),
    }

    impl binary_sv2::GetSize for Sv2Message<'_> {
        fn get_size(&self) -> usize {
            match self {
                Sv2Message::SetupConnection(a) => a.get_size(),
                Sv2Message::SetupConnectionError(a) => a.get_size(),
            }
        }
    }

    impl<'decoder> Deserialize<'decoder> for Sv2Message<'decoder> {
        fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
            unimplemented!()
        }
        fn from_decoded_fields(
            _v: Vec<DecodableField<'decoder>>,
        ) -> std::result::Result<Self, binary_sv2::Error> {
            unimplemented!()
        }
    }

    impl<'a> TryFrom<(u8, &'a mut [u8])> for Sv2Message<'a> {
        type Error = Error;

        fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
            let msg_type = v.0;
            match msg_type {
                MESSAGE_TYPE_SETUP_CONNECTION => {
                    let message: SetupConnection<'a> = from_bytes(v.1)?;
                    Ok(Sv2Message::SetupConnection(message))
                }
                MESSAGE_TYPE_SETUP_CONNECTION_ERROR => {
                    let message: SetupConnectionError<'a> = from_bytes(v.1)?;
                    Ok(Sv2Message::SetupConnectionError(message))
                }
                _ => panic!(),
            }
        }
    }

    impl<'decoder> From<Sv2Message<'decoder>> for EncodableField<'decoder> {
        fn from(m: Sv2Message<'decoder>) -> Self {
            match m {
                Sv2Message::SetupConnection(a) => a.into(),
                Sv2Message::SetupConnectionError(a) => a.into(),
            }
        }
    }

    pub fn main() -> Result<(), std::io::Error> {
        let mut encoder = Encoder::<SetupConnection>::new();

        let setup_connection = SetupConnection {
            protocol: Protocol::TemplateDistributionProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0,
            endpoint_host: "0.0.0.0".to_string().into_bytes().try_into().unwrap(),
            endpoint_port: 8081,
            vendor: "Bitmain".to_string().into_bytes().try_into().unwrap(),
            hardware_version: "901".to_string().into_bytes().try_into().unwrap(),
            firmware: "abcX".to_string().into_bytes().try_into().unwrap(),
            device_id: "89567".to_string().into_bytes().try_into().unwrap(),
        };

        let setup_connection = StandardSv2Frame::from_message(
            setup_connection,
            MESSAGE_TYPE_SETUP_CONNECTION,
            0,
            CHANNEL_BIT_SETUP_CONNECTION,
        )
        .unwrap();
        let setup_connection = encoder.encode(setup_connection).unwrap();

        #[allow(deprecated)]
        std::thread::sleep_ms(2000);

        let mut stream = TcpStream::connect("0.0.0.0:8080")?;

        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        loop {
            #[allow(deprecated)]
            std::thread::sleep_ms(500);

            stream.write_all(setup_connection)?;

            loop {
                let buffer = decoder.writable();
                stream.read_exact(buffer).unwrap();
                if let Ok(mut f) = decoder.next_frame() {
                    let msg_type = f.get_header().unwrap().msg_type();
                    let payload = f.payload();
                    let message: Sv2Message = (msg_type, payload).try_into().unwrap();
                    match message {
                        Sv2Message::SetupConnection(_) => panic!(),
                        Sv2Message::SetupConnectionError(m) => {
                            println!("RUST MESSAGE RECEIVED");
                            println!("  {}", std::str::from_utf8(m.error_code.as_ref()).unwrap());
                        }
                    }
                    break;
                }
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/Cargo.toml">
[package]
name = "ping-pong-encrypted"
version = "0.1.0"
edition = "2021"
authors = [ "SRI Community" ]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../protocols/v2/binary-sv2" }
codec_sv2 = { path = "../../protocols/v2/codec-sv2", features = [ "noise_sv2" ] }
noise_sv2 = { path = "../../protocols/v2/noise-sv2" }
key-utils = { version = "^1.0.0", path = "../../utils/key-utils" }
network_helpers_sv2 = { version = "4.0.0", path = "../../roles/roles-utils/network-helpers" }
rand = "0.8"
tokio = { version = "1.44.1", features = [ "full" ] }
async-channel = "1.5.1"
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/README.md">
`ping-pong-encrypted` is an example of how to encode and decode SV2 binary frames (without any encryption layer) while leveraging the following crates:
- [`binary_sv2`](http://docs.rs/binary_sv2)
- [`codec_sv2`](http://docs.rs/codec_sv2)
- [`framing_sv2`](http://docs.rs/framing_sv2) (which is actually just re-exported by `codec_sv2`)
- [`noise_sv2`](http://docs.rs/noise_sv2)

We establish a simple `Ping`-`Pong` protocol with a server and a client communicating over a TCP socket.

The server expects to receive a `Ping` message encoded as a SV2 binary frame.
The `Ping` message contains a `nonce`, which is a `u8` generated randomly by the client.

The client expects to get a `Pong` message in response, also encoded as a SV2 binary frame, with the same `nonce`.

The messages are assigned arbitrary values for binary encoding:
```rust
pub const PING_MSG_TYPE: u8 = 0xfe;
pub const PONG_MSG_TYPE: u8 = 0xff;
``` 

All communication is encrypted with [SV2 Noise Protocol](https://stratumprotocol.org/specification/04-Protocol-Security/).
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/src/client.rs">
use crate::messages::{Message, Ping, Pong, PING_MSG_TYPE, PONG_MSG_TYPE};
use codec_sv2::{Frame, HandshakeRole, Initiator, StandardSv2Frame};
use key_utils::Secp256k1PublicKey;
use network_helpers_sv2::noise_connection::Connection;
use tokio::net::TcpStream;

use crate::error::Error;

pub async fn start_client(address: &str, k_pub: String) -> Result<(), Error> {
    let stream = TcpStream::connect(address).await?;

    println!("CLIENT: Connected to server on {}", address);

    // parse server pubkey
    let k_pub: Secp256k1PublicKey = k_pub.try_into()?;

    // noise handshake initiator
    let initiator = Initiator::from_raw_k(k_pub.into_bytes())?;

    // channels for encrypted connection
    let (receiver, sender) = Connection::new(stream, HandshakeRole::Initiator(initiator)).await?;

    // create Ping message
    let ping = Ping::new()?;
    let ping_nonce = ping.get_nonce();
    let message = Message::Ping(ping);

    // create Ping frame
    let ping_frame =
        StandardSv2Frame::<Message>::from_message(message.clone(), PING_MSG_TYPE, 0, false)
            .ok_or(Error::FrameFromMessage)?;

    // send Ping frame (sender takes care of encryption)
    println!(
        "CLIENT: Sending encrypted Ping to server with nonce: {}",
        ping_nonce
    );
    sender
        .send(ping_frame.into())
        .await
        .map_err(|_| Error::Sender)?;

    // ok, we have successfully sent the ping message
    // now it's time to receive and verify the pong response
    // receiver already took care of decryption
    let mut frame: StandardSv2Frame<Message> = match receiver.recv().await {
        Ok(f) => f.try_into()?,
        Err(_) => return Err(Error::Receiver),
    };

    let frame_header = frame.get_header().ok_or(Error::FrameHeader)?;

    // check message type on header
    if frame_header.msg_type() != PONG_MSG_TYPE {
        return Err(Error::FrameHeader);
    }

    // decode frame payload
    let decoded_payload: Pong = match binary_sv2::from_bytes(frame.payload()) {
        Ok(pong) => pong,
        Err(e) => return Err(Error::BinarySv2(e)),
    };

    // check if nonce is the same as ping
    let pong_nonce = decoded_payload.get_nonce();
    if ping_nonce == pong_nonce {
        println!(
            "CLIENT: Received encrypted Pong with identical nonce as Ping: {}",
            pong_nonce
        );
    } else {
        return Err(Error::Nonce);
    }

    Ok(())
}
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/src/error.rs">
#[derive(std::fmt::Debug)]
pub enum Error {
    Io(std::io::Error),
    CodecSv2(codec_sv2::Error),
    FramingSv2(codec_sv2::framing_sv2::Error),
    BinarySv2(binary_sv2::Error),
    NoiseSv2(noise_sv2::Error),
    NetworkHelpersSv2(network_helpers_sv2::Error),
    KeyUtils(key_utils::Error),
    Receiver,
    Sender,
    FrameHeader,
    FrameFromMessage,
    Nonce,
    WrongMessage,
    Tcp(std::io::Error),
}

impl From<std::io::Error> for Error {
    fn from(e: std::io::Error) -> Error {
        Error::Io(e)
    }
}

impl From<codec_sv2::Error> for Error {
    fn from(e: codec_sv2::Error) -> Error {
        Error::CodecSv2(e)
    }
}

impl From<network_helpers_sv2::Error> for Error {
    fn from(e: network_helpers_sv2::Error) -> Error {
        Error::NetworkHelpersSv2(e)
    }
}

impl From<binary_sv2::Error> for Error {
    fn from(e: binary_sv2::Error) -> Error {
        Error::BinarySv2(e)
    }
}

impl From<noise_sv2::Error> for Error {
    fn from(e: noise_sv2::Error) -> Error {
        Error::NoiseSv2(e)
    }
}

impl From<key_utils::Error> for Error {
    fn from(e: key_utils::Error) -> Error {
        Error::KeyUtils(e)
    }
}

impl From<codec_sv2::framing_sv2::Error> for Error {
    fn from(e: codec_sv2::framing_sv2::Error) -> Error {
        Error::FramingSv2(e)
    }
}
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/src/main.rs">
mod client;
mod error;
mod messages;
mod server;

const ADDR: &str = "127.0.0.1:3333";
const SERVER_PUBLIC_K: &str = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72";
const SERVER_PRIVATE_K: &str = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n";
const SERVER_CERT_VALIDITY: std::time::Duration = std::time::Duration::from_secs(3600);

#[tokio::main]
async fn main() {
    // start the server in a separate thread
    tokio::spawn(async {
        server::start_server(
            ADDR,
            SERVER_PUBLIC_K.to_string(),
            SERVER_PRIVATE_K.to_string(),
            SERVER_CERT_VALIDITY,
        )
        .await
        .expect("Server failed");
    });

    // give the server a moment to start up
    std::thread::sleep(std::time::Duration::from_secs(1));

    // start the client
    // note: it only knows the server's pubkey!
    client::start_client(ADDR, SERVER_PUBLIC_K.to_string())
        .await
        .expect("Client failed");
}
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/src/messages.rs">
use crate::error::Error;
use binary_sv2::{
    binary_codec_sv2,
    decodable::{DecodableField, FieldMarker},
    Deserialize, Serialize,
};

use rand::Rng;

pub const PING_MSG_TYPE: u8 = 0xfe;
pub const PONG_MSG_TYPE: u8 = 0xff;

// we derive binary_sv2::{Serialize, Deserialize}
// to allow for binary encoding
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Ping {
    nonce: u8,
}

impl Ping {
    pub fn new() -> Result<Self, Error> {
        let mut rng = rand::thread_rng();
        let random: u8 = rng.gen();
        Ok(Self { nonce: random })
    }

    pub fn get_nonce(&self) -> u8 {
        self.nonce
    }
}

// we derive binary_sv2::{Serialize, Deserialize}
// to allow for binary encoding
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Pong {
    nonce: u8,
}

impl<'decoder> Pong {
    pub fn new(nonce: u8) -> Result<Self, Error> {
        Ok(Self { nonce })
    }

    pub fn get_nonce(&self) -> u8 {
        self.nonce
    }
}

// unifies message types for noise_connection_tokio::Connection
#[derive(Clone)]
pub enum Message {
    Ping(Ping),
    Pong(Pong),
}

impl binary_sv2::GetSize for Message {
    fn get_size(&self) -> usize {
        match self {
            Self::Ping(ping) => ping.get_size(),
            Self::Pong(pong) => pong.get_size(),
        }
    }
}

impl From<Message> for binary_sv2::encodable::EncodableField<'_> {
    fn from(m: Message) -> Self {
        match m {
            Message::Ping(p) => p.into(),
            Message::Pong(p) => p.into(),
        }
    }
}

impl Deserialize<'_> for Message {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}
</file>

<file path="stratum-1.4.0/examples/ping-pong-encrypted/src/server.rs">
use crate::{
    error::Error,
    messages::{Message, Ping, Pong, PING_MSG_TYPE, PONG_MSG_TYPE},
};
use codec_sv2::{Frame, StandardEitherFrame, StandardSv2Frame};

use codec_sv2::{HandshakeRole, Responder};
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use network_helpers_sv2::noise_connection::Connection;

use async_channel::{Receiver, Sender};
use tokio::net::TcpListener;

pub async fn start_server(
    address: &str,
    k_pub: String,
    k_priv: String,
    cert_validity: std::time::Duration,
) -> Result<(), Error> {
    let listener = TcpListener::bind(address).await?;

    // parse keys
    let k_pub: Secp256k1PublicKey = k_pub.to_string().try_into()?;
    let k_priv: Secp256k1SecretKey = k_priv.to_string().try_into()?;

    println!("SERVER: Listening on {}", address);

    loop {
        let (stream, _) = listener.accept().await?;
        tokio::spawn(async move {
            // noise handshake responder
            let responder = Responder::from_authority_kp(
                &k_pub.into_bytes(),
                &k_priv.into_bytes(),
                cert_validity,
            )?;

            // channels for encrypted connection
            let (receiver, sender) =
                Connection::new(stream, HandshakeRole::Responder(responder)).await?;

            // handle encrypted connection
            handle_connection(receiver, sender).await?;
            Ok::<(), Error>(())
        });
    }
}

async fn handle_connection(
    receiver: Receiver<StandardEitherFrame<Message>>,
    sender: Sender<StandardEitherFrame<Message>>,
) -> Result<(), Error> {
    // first, we need to read the ping frame
    // receiver already took care of decryption
    let mut frame: StandardSv2Frame<Message> = match receiver.recv().await {
        Ok(f) => f.try_into()?,
        Err(_) => return Err(Error::Receiver),
    };

    let frame_header = frame.get_header().ok_or(Error::FrameHeader)?;

    // check message type on header
    if frame_header.msg_type() != PING_MSG_TYPE {
        return Err(Error::WrongMessage);
    }

    // decode frame payload
    let decoded_payload: Ping = match binary_sv2::from_bytes(frame.payload()) {
        Ok(ping) => ping,
        Err(e) => return Err(Error::BinarySv2(e)),
    };

    // ok, we have successfully received the ping message
    // now it's time to send the pong response

    // we need the ping nonce to create our pong response
    let ping_nonce = decoded_payload.get_nonce();

    println!("SERVER: Received encrypted Ping with nonce: {}", ping_nonce);

    // create Pong message
    let pong = Pong::new(ping_nonce)?;
    let message = Message::Pong(pong.clone());

    // create Pong frame
    let pong_frame =
        StandardSv2Frame::<Message>::from_message(message.clone(), PONG_MSG_TYPE, 0, false)
            .ok_or(Error::FrameFromMessage)?;

    // respond Pong (sender takes care of encryption)
    println!(
        "SERVER: Sending encrypted Pong to client with nonce: {}",
        pong.get_nonce()
    );
    sender
        .send(pong_frame.into())
        .await
        .map_err(|_| Error::Sender)?;

    Ok(())
}
</file>

<file path="stratum-1.4.0/examples/ping-pong/Cargo.toml">
[package]
name = "ping-pong"
version = "0.1.0"
edition = "2021"
authors = [ "SRI Community" ]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../protocols/v2/binary-sv2" }
codec_sv2 = { path = "../../protocols/v2/codec-sv2" }

rand = "0.8"
</file>

<file path="stratum-1.4.0/examples/ping-pong/README.md">
`ping-pong` is an example of how to encode and decode SV2 binary frames (without any encryption layer) while leveraging the following crates:
- [`binary_sv2`](http://docs.rs/binary_sv2)
- [`codec_sv2`](http://docs.rs/codec_sv2)
- [`framing_sv2`](http://docs.rs/framing_sv2) (which is actually just re-exported by `codec_sv2`)

We establish a simple `Ping`-`Pong` protocol with a server and a client communicating over a TCP socket.

The server expects to receive a `Ping` message encoded as a SV2 binary frame.
The `Ping` message contains a `nonce`, which is a `u8` generated randomly by the client.

The client expects to get a `Pong` message in response, also encoded as a SV2 binary frame, with the same `nonce`.

The messages are assigned arbitrary values for binary encoding:
```rust
pub const PING_MSG_TYPE: u8 = 0xfe;
pub const PONG_MSG_TYPE: u8 = 0xff;
```
</file>

<file path="stratum-1.4.0/examples/ping-pong/src/client.rs">
use crate::messages::{Ping, Pong, PING_MSG_TYPE, PONG_MSG_TYPE};
use codec_sv2::{Frame, StandardDecoder, StandardSv2Frame};
use std::{
    io::{Read, Write},
    net::TcpStream,
};

use crate::error::Error;

pub fn start_client(address: &str) -> Result<(), Error> {
    let mut stream = TcpStream::connect(address)?;

    println!("CLIENT: Connected to server on {}", address);

    // create Ping message
    let ping_message = Ping::new()?;
    let ping_nonce = ping_message.get_nonce();

    // create Ping frame
    let ping_frame =
        StandardSv2Frame::<Ping>::from_message(ping_message.clone(), PING_MSG_TYPE, 0, false)
            .ok_or(Error::FrameFromMessage)?;

    // encode Ping frame
    let mut encoder = codec_sv2::Encoder::<Ping>::new();
    let ping_encoded = encoder.encode(ping_frame)?;

    println!("CLIENT: Sending Ping to server with nonce: {}", ping_nonce);
    stream.write_all(ping_encoded)?;

    // ok, we have successfully sent the ping message
    // now it's time to receive and verify the pong response

    // initialize decoder
    let mut decoder = StandardDecoder::<Pong>::new();

    // right now, the decoder buffer can only read a frame header
    // because decoder.missing_b is initialized with a header size
    let decoder_buf = decoder.writable();

    // read frame header into decoder_buf
    stream.read_exact(decoder_buf)?;

    // this returns an error (MissingBytes), because it only read the header, and there's no payload
    // in memory yet therefore, we safely ignore the error
    // the important thing here is that we loaded decoder.missing_b with the expected frame payload
    // size
    let _ = decoder.next_frame();

    // now, the decoder buffer has the expected size of the frame payload
    let decoder_buf = decoder.writable();

    // read the payload into the decoder_buf
    stream.read_exact(decoder_buf)?;

    // finally read the frame
    let mut frame = decoder.next_frame()?;
    let frame_header = frame.get_header().ok_or(Error::FrameHeader)?;

    // check message type on header
    if frame_header.msg_type() != PONG_MSG_TYPE {
        return Err(Error::FrameHeader);
    }

    // decode frame payload
    let decoded_payload: Pong = match binary_sv2::from_bytes(frame.payload()) {
        Ok(pong) => pong,
        Err(e) => return Err(Error::BinarySv2(e)),
    };

    // check if nonce is the same as ping
    let pong_nonce = decoded_payload.get_nonce();
    if ping_nonce == pong_nonce {
        println!(
            "CLIENT: Received Pong with identical nonce as Ping: {}",
            pong_nonce
        );
    } else {
        return Err(Error::Nonce);
    }

    Ok(())
}
</file>

<file path="stratum-1.4.0/examples/ping-pong/src/error.rs">
#[derive(std::fmt::Debug)]
pub enum Error {
    Io(std::io::Error),
    Codec(codec_sv2::Error),
    BinarySv2(binary_sv2::Error),
    FrameHeader,
    FrameFromMessage,
    Nonce,
    WrongMessage,
    Tcp(std::io::Error),
}

impl From<std::io::Error> for Error {
    fn from(e: std::io::Error) -> Error {
        Error::Io(e)
    }
}

impl From<codec_sv2::Error> for Error {
    fn from(e: codec_sv2::Error) -> Error {
        Error::Codec(e)
    }
}

impl From<binary_sv2::Error> for Error {
    fn from(e: binary_sv2::Error) -> Error {
        Error::BinarySv2(e)
    }
}
</file>

<file path="stratum-1.4.0/examples/ping-pong/src/main.rs">
mod client;
mod error;
mod messages;
mod server;

const ADDR: &str = "127.0.0.1:3333";

fn main() {
    // Start the server in a separate thread
    std::thread::spawn(|| {
        server::start_server(ADDR).expect("Server failed");
    });

    // Give the server a moment to start up
    std::thread::sleep(std::time::Duration::from_secs(1));

    // Start the client
    client::start_client(ADDR).expect("Client failed");
}
</file>

<file path="stratum-1.4.0/examples/ping-pong/src/messages.rs">
use crate::error::Error;
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};

use rand::Rng;

pub const PING_MSG_TYPE: u8 = 0xfe;
pub const PONG_MSG_TYPE: u8 = 0xff;

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Ping {
    nonce: u8,
}

impl Ping {
    pub fn new() -> Result<Self, Error> {
        let mut rng = rand::thread_rng();
        let random: u8 = rng.gen();
        Ok(Self { nonce: random })
    }

    pub fn get_nonce(&self) -> u8 {
        self.nonce
    }
}

#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct Pong {
    nonce: u8,
}

impl<'decoder> Pong {
    pub fn new(nonce: u8) -> Result<Self, Error> {
        Ok(Self { nonce })
    }

    pub fn get_nonce(&self) -> u8 {
        self.nonce
    }
}
</file>

<file path="stratum-1.4.0/examples/ping-pong/src/server.rs">
use crate::{
    error::Error,
    messages::{Ping, Pong, PING_MSG_TYPE, PONG_MSG_TYPE},
};
use codec_sv2::{Frame, StandardDecoder, StandardSv2Frame};
use std::{
    io::{Read, Write},
    net::{TcpListener, TcpStream},
    thread,
};

use codec_sv2::framing_sv2::header::Header as StandardSv2Header;

pub fn start_server(address: &str) -> Result<(), Error> {
    let listener = TcpListener::bind(address)?;

    println!("SERVER: Listening on {}", address);

    for stream in listener.incoming() {
        match stream {
            Ok(stream) => {
                thread::spawn(|| {
                    handle_connection(stream)?;
                    Ok::<(), Error>(())
                });
            }
            Err(e) => return Err(Error::Tcp(e)),
        }
    }

    Ok(())
}

fn handle_connection(mut stream: TcpStream) -> Result<(), Error> {
    // first, we need to read the ping message

    // initialize decoder
    let mut decoder = StandardDecoder::<Ping>::new();

    // right now, the decoder buffer can only read a frame header
    // because decoder.missing_b is initialized with a header size
    let decoder_buf = decoder.writable();

    // read frame header into decoder_buf
    stream.read_exact(decoder_buf)?;

    // this returns an error (MissingBytes), because it only read the header, and there's no payload
    // in memory yet therefore, we safely ignore the error
    // the important thing here is that we loaded decoder.missing_b with the expected frame payload
    // size
    let _ = decoder.next_frame();

    // now, the decoder buffer has the expected size of the frame payload
    let decoder_buf = decoder.writable();

    // read from stream into decoder_buf again, loading the payload into memory
    stream.read_exact(decoder_buf)?;

    // parse into a Sv2Frame
    let mut frame: StandardSv2Frame<Ping> = decoder.next_frame()?;
    let frame_header: StandardSv2Header = frame.get_header().ok_or(Error::FrameHeader)?;

    // check message type on header
    if frame_header.msg_type() != PING_MSG_TYPE {
        return Err(Error::WrongMessage);
    }

    // decode frame payload
    let decoded_payload: Ping = match binary_sv2::from_bytes(frame.payload()) {
        Ok(ping) => ping,
        Err(e) => return Err(Error::BinarySv2(e)),
    };

    // ok, we have successfully received the ping message
    // now it's time to send the pong response

    // we need the ping nonce to create our pong response
    let ping_nonce = decoded_payload.get_nonce();

    println!("SERVER: Received Ping message with nonce: {}", ping_nonce);

    // create Pong message
    let pong_message = Pong::new(ping_nonce)?;

    // create Pong frame
    let pong_frame =
        StandardSv2Frame::<Pong>::from_message(pong_message.clone(), PONG_MSG_TYPE, 0, false)
            .ok_or(Error::FrameFromMessage)?;

    // encode Pong frame
    let mut encoder = codec_sv2::Encoder::<Pong>::new();
    let pong_encoded = encoder.encode(pong_frame)?;

    println!(
        "SERVER: Sending Pong to client with nonce: {}",
        pong_message.get_nonce()
    );
    stream.write_all(pong_encoded)?;

    Ok(())
}
</file>

<file path="stratum-1.4.0/LICENSE-APACHE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="stratum-1.4.0/LICENSE-MIT">
Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
the Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
</file>

<file path="stratum-1.4.0/LICENSE.md">
This software is licensed under [Apache 2.0](LICENSE-APACHE) or
[MIT](LICENSE-MIT), at your option.

Some files retain their own copyright notice, however, for full authorship
information, see version control history.

Except as otherwise noted in individual files, all files in this repository are
licensed under the Apache License, Version 2.0 <LICENSE-APACHE or
http://www.apache.org/licenses/LICENSE-2.0> or the MIT license <LICENSE-MIT or
http://opensource.org/licenses/MIT>, at your option.

You may not use, copy, modify, merge, publish, distribute, sublicense, and/or
sell copies of this software or any files in this repository except in
accordance with one or both of these licenses.
</file>

<file path="stratum-1.4.0/protocols/Cargo.toml">
[workspace]

resolver="2"

members = [
    "v1",
    "v2/binary-sv2/codec",
    "v2/binary-sv2/derive_codec",
    "v2/binary-sv2",
    "v2/noise-sv2",
    "v2/framing-sv2",
    "v2/codec-sv2",
    "v2/subprotocols/common-messages",
    "v2/subprotocols/template-distribution",
    "v2/subprotocols/mining",
    "v2/subprotocols/job-declaration",
    "v2/sv2-ffi",
    "v2/roles-logic-sv2",
]

[profile.dev]
# Required by super_safe_lock
opt-level = 1

[profile.test]
# Required by super_safe_lock
opt-level = 1
</file>

<file path="stratum-1.4.0/protocols/fuzz-tests/.gitignore">
/target
</file>

<file path="stratum-1.4.0/protocols/fuzz-tests/Cargo.toml">
[package]
name = "fuzz-tests"
version = "1.0.1"
edition = "2021"
authors = ["Automatically generated"]
publish = false
documentation = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[package.metadata]
cargo-fuzz = true

[dependencies]
libfuzzer-sys = { version = "0.4.0", features = ["arbitrary-derive"] }
arbitrary = { version = "1", features = ["derive"] }
rand = "0.8.3"
binary_codec_sv2 = { path = "../v2/binary-sv2/codec"}
codec_sv2 = { path = "../v2/codec-sv2", features = ["noise_sv2"]}
roles_logic_sv2 = { path = "../v2/roles-logic-sv2"}
affinity = "0.1.1"
threadpool = "1.8.1"
lazy_static = "1.4.0"

# Prevent this from interfering with workspaces
[workspace]
members = ["."]
</file>

<file path="stratum-1.4.0/protocols/fuzz-tests/src/main.rs">
#![no_main]
use libfuzzer_sys::fuzz_target;
use binary_codec_sv2::{Seq064K,U256,B0255,Seq0255};
use binary_codec_sv2::from_bytes;
use codec_sv2::{StandardDecoder,Sv2Frame};
use roles_logic_sv2::parsers::AnyMessage;

type F = Sv2Frame<AnyMessage<'static>,Vec<u8>>;

fuzz_target!(|data: Vec<u8>| {
    let mut data = data;
    let mut decoder = StandardDecoder::<AnyMessage>::new();
    let _: Result<Seq064K<bool>,_> = from_bytes(&mut data);
    let _: Result<Seq064K<u64>,_> = from_bytes(&mut data);
    let _: Result<U256,_> = from_bytes(&mut data);
    let _: Result<B0255,_> = from_bytes(&mut data);
    let _: Result<Seq064K<B0255>,_> = from_bytes(&mut data);
    let _: Result<Seq0255<B0255>,_> = from_bytes(&mut data);
    let _: Result<Seq0255<U256>,_> = from_bytes(&mut data);
    let _: Result<F,_> = Sv2Frame::from_bytes(data.clone());

    let mut data_iter = data.clone().into_iter();
    loop {
        let writable = decoder.writable();
        for i in 0..writable.len() {
            match data_iter.next() {
                Some(x) => writable[i] = x,
                None => break
            }
        };
        let _ = decoder.next_frame();
        break;
    }
});
</file>

<file path="stratum-1.4.0/protocols/tarpaulin.toml">
[default]
features = "disable_nopanic prop_test noise_sv2 with_buffer_pool derive_codec_sv2 binary_codec_sv2 default core"
run-types = [ "Lib" ]
timeout = "120s"
fail-under = 0

[report]
out = ["Xml"]
</file>

<file path="stratum-1.4.0/protocols/v1/Cargo.toml">
[package]
name = "sv1_api"
version = "1.0.1"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "API for bridging SV1 miners to SV2 pools"
documentation = "https://docs.rs/sv1_api"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
bitcoin_hashes = "0.3.2"
byteorder = "1.2.7"
hex = "0.4.3"
serde = { version = "1.0.89", default-features = false, features = ["derive", "alloc"] }
serde_json = { version = "1.0.64", default-features = false, features = ["alloc"] }
tracing = {version = "0.1"}
binary_sv2 = { path = "../v2/binary-sv2", version = "^3.0.0" }

[dev-dependencies]
quickcheck = "1"
quickcheck_macros = "1"
</file>

<file path="stratum-1.4.0/protocols/v1/examples/client_and_server.rs">
use std::{
    convert::{TryFrom, TryInto},
    io::{BufRead, BufReader, Write},
    net::{SocketAddr, TcpListener, TcpStream},
    process::exit,
    sync::{mpsc, Arc, Mutex},
    thread,
    time::{Duration, SystemTime},
};

const ADDR: &str = "127.0.0.1:0";
const TEST_DURATION: i32 = 30;

type Receiver<T> = mpsc::Receiver<T>;
type Sender<T> = mpsc::Sender<T>;

use sv1_api::{
    client_to_server,
    error::Error,
    json_rpc, server_to_client,
    utils::{Extranonce, HexU32Be, MerkleNode, PrevHash},
    ClientStatus, IsClient, IsServer, Message,
};

fn new_extranonce<'a>() -> Extranonce<'a> {
    extranonce_from_hex("08000002")
}

fn extranonce_from_hex<'a>(hex: &str) -> Extranonce<'a> {
    let data = utils::decode_hex(hex).unwrap();
    Extranonce::try_from(data).expect("Failed to convert hex to U256")
}

fn merklenode_from_hex<'a>(hex: &str) -> MerkleNode<'a> {
    let data = utils::decode_hex(hex).unwrap();
    let len = data.len();
    if hex.len() >= 64 {
        // panic if hex is larger than 32 bytes
        MerkleNode::try_from(hex).expect("Failed to convert hex to U256")
    } else {
        // prepend hex with zeros so that it is 32 bytes
        let mut new_vec = vec![0_u8; 32 - len];
        new_vec.extend(data.iter());
        MerkleNode::try_from(utils::encode_hex(&new_vec).as_str())
            .expect("Failed to convert hex to U256")
    }
}

fn prevhash_from_hex<'a>(hex: &str) -> PrevHash<'a> {
    let data = utils::decode_hex(hex).unwrap();
    let len = data.len();
    if hex.len() >= 64 {
        // panic if hex is larger than 32 bytes
        PrevHash::try_from(hex).expect("Failed to convert hex to U256")
    } else {
        // prepend hex with zeros so that it is 32 bytes
        let mut new_vec = vec![0_u8; 32 - len];
        new_vec.extend(data.iter());
        PrevHash::try_from(utils::encode_hex(&new_vec).as_str())
            .expect("Failed to convert hex to U256")
    }
}

fn new_extranonce2_size() -> usize {
    4
}

fn new_version_rolling_mask() -> HexU32Be {
    HexU32Be(0xffffffff)
}
fn new_version_rolling_min() -> HexU32Be {
    HexU32Be(0x00000000)
}

struct Server<'a> {
    authorized_names: Vec<String>,
    extranonce1: Extranonce<'a>,
    extranonce2_size: usize,
    version_rolling_mask: Option<HexU32Be>,
    version_rolling_min_bit: Option<HexU32Be>,
    receiver_incoming: Receiver<String>,
    sender_outgoing: Sender<String>,
}

fn server_pool_listen(listener: TcpListener) {
    loop {
        match listener.accept() {
            Ok((stream, addr)) => {
                println!("SERVER - Accepting from: {addr}");
                let server = Server::new(stream);
                let _ = Arc::new(Mutex::new(server));
            }
            Err(e) => {
                eprintln!("SERVER - Accept error: {e}");
                break;
            }
        }
    }
}

impl Server<'_> {
    pub fn new(stream: TcpStream) -> Arc<Mutex<Server<'static>>> {
        let (sender_incoming, receiver_incoming) = mpsc::channel::<String>();
        let (sender_outgoing, receiver_outgoing) = mpsc::channel::<String>();

        let reader_stream = stream.try_clone().expect("Failed to clone stream (read)");
        let mut writer_stream = stream;

        // read thread
        thread::spawn(move || {
            let reader = BufReader::new(reader_stream);
            for line in reader.lines().map_while(Result::ok) {
                if sender_incoming.send(line).is_err() {
                    break;
                }
            }
        });

        thread::spawn(move || {
            for msg in receiver_outgoing {
                let _ = writer_stream.write_all(msg.as_bytes());
            }
        });

        let server = Server {
            authorized_names: vec![],
            extranonce1: extranonce_from_hex("00000000"),
            extranonce2_size: 2,
            version_rolling_mask: None,
            version_rolling_min_bit: None,
            receiver_incoming,
            sender_outgoing,
        };

        let server_arc = Arc::new(Mutex::new(server));

        {
            let cloned = Arc::clone(&server_arc);
            thread::spawn(move || loop {
                if let Ok(mut self_) = cloned.try_lock() {
                    if let Ok(line) = self_.receiver_incoming.try_recv() {
                        println!("SERVER - message: {line}");
                        let message: Result<json_rpc::Message, _> = serde_json::from_str(&line);
                        if let Ok(message) = message {
                            if let Ok(Some(resp)) = self_.handle_message(message) {
                                Self::send_message(
                                    &self_.sender_outgoing,
                                    json_rpc::Message::OkResponse(resp),
                                );
                            }
                        }
                    }
                }
                thread::sleep(Duration::from_millis(100));
            });
        }

        {
            let cloned = Arc::clone(&server_arc);
            thread::spawn(move || {
                let mut run_time = TEST_DURATION;
                loop {
                    let notify_time = 5;
                    if let Ok(mut self_) = cloned.try_lock() {
                        let sender = self_.sender_outgoing.clone();
                        if let Ok(notify_msg) = self_.notify() {
                            Server::send_message(&sender, notify_msg);
                        }
                    }
                    thread::sleep(Duration::from_secs(notify_time));
                    run_time -= notify_time as i32;

                    if run_time <= 0 {
                        println!("Test Success - ran for {TEST_DURATION} seconds");
                        exit(0)
                    }
                }
            });
        }

        server_arc
    }

    fn handle_message(
        &mut self,
        _message: json_rpc::Message,
    ) -> Result<Option<json_rpc::Response>, Error<'static>> {
        Ok(None)
    }

    fn send_message(sender_outgoing: &Sender<String>, msg: json_rpc::Message) {
        let msg = format!("{}\n", serde_json::to_string(&msg).unwrap());
        let _ = sender_outgoing.send(msg);
    }
}

impl<'a> IsServer<'a> for Server<'a> {
    fn handle_configure(
        &mut self,
        _request: &client_to_server::Configure,
    ) -> (Option<server_to_client::VersionRollingParams>, Option<bool>) {
        self.version_rolling_mask
            .get_or_insert_with(new_version_rolling_mask);
        self.version_rolling_min_bit
            .get_or_insert_with(new_version_rolling_min);

        let mask = self.version_rolling_mask.as_ref().unwrap().clone();
        let min_bit = self.version_rolling_min_bit.as_ref().unwrap().clone();

        (
            Some(server_to_client::VersionRollingParams::new(mask, min_bit).unwrap()),
            Some(false),
        )
    }

    fn handle_subscribe(&self, _request: &client_to_server::Subscribe) -> Vec<(String, String)> {
        vec![]
    }

    fn handle_authorize(&self, _request: &client_to_server::Authorize) -> bool {
        true
    }

    fn handle_submit(&self, _request: &client_to_server::Submit) -> bool {
        true
    }

    /// Indicates to the server that the client supports the mining.set_extranonce method.
    fn handle_extranonce_subscribe(&self) {}

    fn is_authorized(&self, _name: &str) -> bool {
        true
    }

    fn authorize(&mut self, name: &str) {
        self.authorized_names.push(name.to_string())
    }

    /// Set extranonce1 to extranonce1 if provided. If not create a new one and set it.
    fn set_extranonce1(&mut self, extranonce1: Option<Extranonce<'a>>) -> Extranonce<'a> {
        self.extranonce1 = extranonce1.unwrap_or_else(new_extranonce);
        self.extranonce1.clone()
    }

    fn extranonce1(&self) -> Extranonce<'a> {
        self.extranonce1.clone()
    }

    /// Set extranonce2_size to extranonce2_size if provided. If not create a new one and set it.
    fn set_extranonce2_size(&mut self, extra_nonce2_size: Option<usize>) -> usize {
        self.extranonce2_size = extra_nonce2_size.unwrap_or_else(new_extranonce2_size);
        self.extranonce2_size
    }

    fn extranonce2_size(&self) -> usize {
        self.extranonce2_size
    }

    fn version_rolling_mask(&self) -> Option<HexU32Be> {
        self.version_rolling_mask.clone()
    }

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_mask = mask;
    }

    fn set_version_rolling_min_bit(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_min_bit = mask
    }

    fn notify(&mut self) -> Result<json_rpc::Message, Error<'a>> {
        let hex = "ffff";
        Ok(server_to_client::Notify {
            job_id: "ciao".to_string(),
            prev_hash: prevhash_from_hex(hex),
            coin_base1: hex.try_into()?,
            coin_base2: hex.try_into()?,
            merkle_branch: vec![merklenode_from_hex(hex)],
            version: HexU32Be(5667),
            bits: HexU32Be(5678),
            time: HexU32Be(5609),
            clean_jobs: true,
        }
        .into())
    }
}

struct Client<'a> {
    client_id: u32,
    extranonce1: Extranonce<'a>,
    extranonce2_size: usize,
    version_rolling_mask: Option<HexU32Be>,
    version_rolling_min_bit: Option<HexU32Be>,
    status: ClientStatus,
    last_notify: Option<server_to_client::Notify<'a>>,
    sented_authorize_request: Vec<(u64, String)>, // (id, user_name)
    authorized: Vec<String>,
    receiver_incoming: Receiver<String>,
    sender_outgoing: Sender<String>,
}

impl Client<'static> {
    pub fn new(client_id: u32, socket: SocketAddr) -> Arc<Mutex<Client<'static>>> {
        loop {
            thread::sleep(Duration::from_secs(1));
            match TcpStream::connect(socket) {
                Ok(st) => {
                    println!("CLIENT - connected to server at {socket}");
                    let (sender_incoming, receiver_incoming) = mpsc::channel::<String>();
                    let (sender_outgoing, receiver_outgoing) = mpsc::channel::<String>();

                    let reader_stream = st.try_clone().unwrap();
                    let mut writer_stream = st;

                    thread::spawn(move || {
                        let reader = BufReader::new(reader_stream);
                        for line in reader.lines().map_while(Result::ok) {
                            if sender_incoming.send(line).is_err() {
                                break;
                            }
                        }
                    });

                    thread::spawn(move || {
                        for msg in receiver_outgoing {
                            let _ = writer_stream.write_all(msg.as_bytes());
                        }
                    });

                    let client = Client {
                        client_id,
                        extranonce1: extranonce_from_hex("00000000"),
                        extranonce2_size: 2,
                        version_rolling_mask: None,
                        version_rolling_min_bit: None,
                        status: ClientStatus::Init,
                        last_notify: None,
                        sented_authorize_request: vec![],
                        authorized: vec![],
                        receiver_incoming,
                        sender_outgoing,
                    };

                    let arc_client = Arc::new(Mutex::new(client));

                    {
                        let cloned = Arc::clone(&arc_client);
                        thread::spawn(move || loop {
                            if let Ok(mut this) = cloned.try_lock() {
                                if let Ok(line) = this.receiver_incoming.try_recv() {
                                    println!("CLIENT {} - message: {}", this.client_id, line);
                                    if let Ok(msg) =
                                        serde_json::from_str::<json_rpc::Message>(&line)
                                    {
                                        this.handle_message(msg).ok();
                                    }
                                }
                            }
                            thread::sleep(Duration::from_millis(100));
                        });
                    }

                    return arc_client;
                }
                Err(_) => {
                    println!("Server not ready... retry");
                    continue;
                }
            }
        }
    }

    fn send_message(sender_outgoing: &Sender<String>, msg: json_rpc::Message) {
        let s = format!("{}\n", serde_json::to_string(&msg).unwrap());
        let _ = sender_outgoing.send(s);
    }

    pub fn send_subscribe(&mut self) {
        while let ClientStatus::Init = self.status {
            thread::sleep(Duration::from_millis(100));
        }
        let id = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        if let Ok(subscribe) = self.subscribe(id, None) {
            Self::send_message(&self.sender_outgoing, subscribe);
        }
    }

    pub fn send_authorize(&mut self) {
        let id = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        if let Ok(authorize) = self.authorize(id, "user".to_string(), "user".to_string()) {
            Self::send_message(&self.sender_outgoing, authorize);
        }
    }

    pub fn send_submit(&mut self) {
        let id = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let extranonce2 = extranonce_from_hex("00");
        let nonce = 78;
        let version_bits = None;
        if let Ok(submit) = self.submit(
            id,
            "user".to_string(),
            extranonce2,
            nonce,
            nonce,
            version_bits,
        ) {
            Self::send_message(&self.sender_outgoing, submit);
        }
    }

    pub fn send_configure(&mut self) {
        let id = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let configure = self.configure(id);
        Self::send_message(&self.sender_outgoing, configure);
    }
}

impl<'a> IsClient<'a> for Client<'a> {
    fn handle_set_difficulty(
        &mut self,
        _conf: &mut server_to_client::SetDifficulty,
    ) -> Result<(), Error<'a>> {
        Ok(())
    }

    fn handle_set_extranonce(
        &mut self,
        _conf: &mut server_to_client::SetExtranonce,
    ) -> Result<(), Error<'a>> {
        Ok(())
    }

    fn handle_set_version_mask(
        &mut self,
        _conf: &mut server_to_client::SetVersionMask,
    ) -> Result<(), Error<'a>> {
        Ok(())
    }

    fn handle_notify(&mut self, notify: server_to_client::Notify<'a>) -> Result<(), Error<'a>> {
        self.last_notify = Some(notify);
        Ok(())
    }

    fn handle_configure(
        &mut self,
        _conf: &mut server_to_client::Configure,
    ) -> Result<(), Error<'a>> {
        Ok(())
    }

    fn handle_subscribe(
        &mut self,
        _subscribe: &server_to_client::Subscribe<'a>,
    ) -> Result<(), Error<'a>> {
        Ok(())
    }

    fn set_extranonce1(&mut self, extranonce1: Extranonce<'a>) {
        self.extranonce1 = extranonce1;
    }

    fn extranonce1(&self) -> Extranonce<'a> {
        self.extranonce1.clone()
    }

    fn set_extranonce2_size(&mut self, extra_nonce2_size: usize) {
        self.extranonce2_size = extra_nonce2_size;
    }

    fn extranonce2_size(&self) -> usize {
        self.extranonce2_size
    }

    fn version_rolling_mask(&self) -> Option<HexU32Be> {
        self.version_rolling_mask.clone()
    }

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_mask = mask;
    }

    fn set_version_rolling_min_bit(&mut self, min: Option<HexU32Be>) {
        self.version_rolling_min_bit = min;
    }

    fn set_status(&mut self, status: ClientStatus) {
        self.status = status;
    }

    fn signature(&self) -> String {
        format!("{}", self.client_id)
    }

    fn status(&self) -> ClientStatus {
        self.status
    }

    fn version_rolling_min_bit(&mut self) -> Option<HexU32Be> {
        self.version_rolling_min_bit.clone()
    }

    fn id_is_authorize(&mut self, id: &u64) -> Option<String> {
        let req: Vec<&(u64, String)> = self
            .sented_authorize_request
            .iter()
            .filter(|x| x.0 == *id)
            .collect();
        match req.len() {
            0 => None,
            _ => Some(req[0].1.clone()),
        }
    }

    fn id_is_submit(&mut self, _: &u64) -> bool {
        false
    }

    fn authorize_user_name(&mut self, name: String) {
        self.authorized.push(name)
    }

    fn is_authorized(&self, name: &String) -> bool {
        self.authorized.contains(name)
    }

    fn authorize(
        &mut self,
        id: u64,
        name: String,
        password: String,
    ) -> Result<json_rpc::Message, Error> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.authorize".to_string())),
            _ => {
                self.sented_authorize_request.push((id, "user".to_string()));
                Ok(client_to_server::Authorize { id, name, password }.into())
            }
        }
    }

    fn last_notify(&self) -> Option<server_to_client::Notify> {
        self.last_notify.clone()
    }

    fn handle_error_message(
        &mut self,
        message: Message,
    ) -> Result<Option<json_rpc::Message>, Error<'a>> {
        println!("{message:?}");
        Ok(None)
    }
}

fn initialize_client(client: Arc<Mutex<Client<'static>>>) {
    loop {
        {
            let mut client_ = client.lock().unwrap();
            match client_.status {
                ClientStatus::Init => client_.send_configure(),
                ClientStatus::Configured => client_.send_subscribe(),
                ClientStatus::Subscribed => {
                    client_.send_authorize();
                    break;
                }
            }
        }
        thread::sleep(Duration::from_millis(1000));
    }

    thread::sleep(Duration::from_millis(2000));
    loop {
        {
            let mut client_ = client.lock().unwrap();
            client_.send_submit();
        }
        thread::sleep(Duration::from_millis(2000));
    }
}

fn main() {
    let listener = TcpListener::bind(ADDR).unwrap();
    println!("Server listening on: {}", listener.local_addr().unwrap());
    let socket = listener.local_addr().unwrap();

    thread::spawn(move || {
        server_pool_listen(listener);
    });

    let client = Client::new(80, socket);
    initialize_client(client);
}

mod utils {
    use std::fmt::Write;

    pub fn decode_hex(s: &str) -> Result<Vec<u8>, core::num::ParseIntError> {
        let s = match s.strip_prefix("0x") {
            Some(s) => s,
            None => s,
        };
        (0..s.len())
            .step_by(2)
            .map(|i| u8::from_str_radix(&s[i..i + 2], 16))
            .collect()
    }

    pub fn encode_hex(bytes: &[u8]) -> String {
        let mut s = String::with_capacity(bytes.len() * 2);
        for &b in bytes {
            write!(&mut s, "{b:02x}").unwrap();
        }
        s
    }

    #[cfg(test)]
    #[test]
    fn test() {
        let test_vec = vec![222, 173, 190, 239];
        let hex_og = "deadbeef";
        let hex_og_w_prefix = "0xdeadbeef";
        // decode hex strings
        let decoded = decode_hex(hex_og).unwrap();
        let decoded_w_prefix = decode_hex(hex_og_w_prefix).unwrap();

        assert_eq!(&decoded, &test_vec, "Hex not decoded correctly");
        assert_eq!(
            &decoded_w_prefix, &test_vec,
            "Hex w/ prefix not decoded correctly"
        );

        // reencode
        let reencoded = encode_hex(&decoded);
        let reencoded_prefix = encode_hex(&decoded);

        assert_eq!(&reencoded, hex_og, "Hex not encoded correctly");
        assert_eq!(
            &reencoded_prefix, &hex_og,
            "Hex w/ prefix not encoded correctly"
        );
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/README.md">
# Stratum V1
</file>

<file path="stratum-1.4.0/protocols/v1/src/error.rs">
use crate::{
    methods::{Method, MethodError},
    utils::HexU32Be,
};

#[derive(Debug)]
#[non_exhaustive]
pub enum Error<'a> {
    BadBytesConvert(binary_sv2::Error),
    BTCHashError(bitcoin_hashes::Error),
    /// Errors on bad hex decode/encode.
    HexError(hex::FromHexError),
    /// Errors if `ClientStatus` is in an unexpected state when a message is received. For example,
    /// if a `mining.subscribed` is received when the `ClientStatus` is in the `Init` state.
    IncorrectClientStatus(String),
    Infallible(std::convert::Infallible),
    /// Errors if server receives a `json_rpc` request as the server should only receive responses.
    /// TODO: Should update to accommodate miner requesting a difficulty change
    InvalidJsonRpcMessageKind,
    /// Errors if the client receives an invalid message that was intended to be sent from the
    /// client to the server, NOT from the server to the client.
    #[allow(clippy::upper_case_acronyms)]
    InvalidReceiver(Box<Method<'a>>),
    /// Errors if server receives and invalid `mining.submit` from the client.
    InvalidSubmission,
    /// Errors encountered during conversion between valid `json_rpc` messages and SV1 messages.
    Method(Box<MethodError<'a>>),
    /// Errors if action is attempted that requires the client to be authorized, but it is
    /// unauthorized. The client username is given in the error message.
    UnauthorizedClient(String),
    /// Errors if server does not recognize the client's `id`.
    UnknownID(u64),
    InvalidVersionMask(HexU32Be),
}

impl std::fmt::Display for Error<'_> {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        match self {
            Error::BadBytesConvert(ref e) => write!(
                f,
                "Bad U256 or B032 conversion (U256 length must be exactly 32 bytes; B032 length must be <= 32 bytes): {e:?}"
            ),
            Error::BTCHashError(ref e) => write!(f, "Bitcoin Hashes Error: `{e:?}`"),
            Error::HexError(ref e) => write!(f, "Bad hex encode/decode: `{e:?}`"),
            Error::IncorrectClientStatus(s) => {
                write!(f, "Client status is incompatible with message: `{s}`")
            }
            Error::Infallible(ref e) => write!(f, "Infallible error{e:?}"),
            Error::InvalidJsonRpcMessageKind => write!(
                f,
                "Server received a `json_rpc` response when it should only receive requests"
            ),
            Error::InvalidReceiver(ref e) => write!(
                f,
                "Client received an invalid message that was intended to be sent from the
            client to the server, NOT from the server to the client. Invalid message: `{e:?}`"
            ),
            Error::InvalidSubmission => {
                write!(f, "Server received an invalid `mining.submit` message.")
            }
            Error::Method(ref e) => {
                write!(
                    f,
                    "Error converting valid `json_rpc` SV1 message: `{e:?}`"
                )
            }
            Error::UnauthorizedClient(id) => write!(
                f,
                "Client with id `{id}` expected to be authorized but is unauthorized."
            ),
            Error::UnknownID(e) => write!(f, "Server did not recognize the client id: `{e}`."),
            Error::InvalidVersionMask(e) => write!(f, "First 3 bits of version rolling mask must be 0 and last 13 bits of version rolling mask must be 0. Version rolling mask is: `{:b}`.", e.0),
        }
    }
}

impl From<bitcoin_hashes::Error> for Error<'_> {
    fn from(e: bitcoin_hashes::Error) -> Self {
        Error::BTCHashError(e)
    }
}

impl From<hex::FromHexError> for Error<'_> {
    fn from(e: hex::FromHexError) -> Self {
        Error::HexError(e)
    }
}

impl From<std::convert::Infallible> for Error<'_> {
    fn from(e: std::convert::Infallible) -> Self {
        Error::Infallible(e)
    }
}

impl<'a> From<MethodError<'a>> for Error<'a> {
    fn from(inner: MethodError<'a>) -> Self {
        Error::Method(Box::new(inner))
    }
}

impl From<binary_sv2::Error> for Error<'_> {
    fn from(inner: binary_sv2::Error) -> Self {
        Error::BadBytesConvert(inner)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/json_rpc.rs">
//! https://www.jsonrpc.org/specification#response_object
use serde::{Deserialize, Serialize};
use std::{fmt, fmt::Display};

#[derive(Clone, Serialize, Deserialize, Debug)]
#[serde(untagged)]
pub enum Message {
    StandardRequest(StandardRequest),
    Notification(Notification),
    OkResponse(Response),
    ErrorResponse(Response),
}

impl Message {
    // TODO: Remove this. Keeping this in to avoid upgrading the major version of the crate.
    pub fn is_response(&self) -> bool {
        match self {
            Message::StandardRequest(_) => false,
            Message::Notification(_) => false,
            Message::OkResponse(_) => true,
            Message::ErrorResponse(_) => true,
        }
    }
}

impl Display for Message {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Message::StandardRequest(sr) => write!(f, "{:?}", sr.method),
            Message::Notification(n) => write!(f, "{:?}", n.method),
            Message::OkResponse(_) => write!(f, "\"result\": true"),
            Message::ErrorResponse(r) => write!(
                f,
                "\"result\": false, \"error\": {:?}",
                r.error.as_ref().unwrap().message
            ),
        }
    }
}

#[derive(Clone, Serialize, Deserialize, Debug, PartialEq, Eq)]
pub struct StandardRequest {
    pub id: u64,
    pub method: String,
    pub params: serde_json::Value,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct Notification {
    pub method: String,
    pub params: serde_json::Value,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct Response {
    pub id: u64,
    pub error: Option<JsonRpcError>,
    pub result: serde_json::Value,
}

#[derive(Clone, Serialize, Deserialize, Debug)]
pub struct JsonRpcError {
    pub code: i32, // json do not specify precision which one should be used?
    pub message: String,
    pub data: Option<serde_json::Value>,
}

impl From<Response> for Message {
    fn from(res: Response) -> Self {
        if res.error.is_some() {
            Message::ErrorResponse(res)
        } else {
            Message::OkResponse(res)
        }
    }
}

impl From<StandardRequest> for Message {
    fn from(sr: StandardRequest) -> Self {
        Message::StandardRequest(sr)
    }
}

impl From<Notification> for Message {
    fn from(n: Notification) -> Self {
        Message::Notification(n)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/lib.rs">
#![allow(clippy::result_unit_err)]
//! Startum V1 application protocol:
//!
//! json-rpc has two types of messages: **request** and **response**.
//! A request message can be either a **notification** or a **standard message**.
//! Standard messages expect a response, notifications do not. A typical example of a notification
//! is the broadcasting of a new block.
//!
//! Every RPC request contains three parts:
//! * message ID: integer or string
//! * remote method: unicode string
//! * parameters: list of parameters
//!
//! ## Standard requests
//! Message ID must be an unique identifier of request during current transport session. It may be
//! integer or some unique string, like UUID. ID must be unique only from one side (it means, both
//! server and clients can initiate request with id 1). Client or server can choose string/UUID
//! identifier for example in the case when standard atomic counter isnt available.
//!
//! ## Notifications
//! Notifications are like Request, but it does not expect any response and message ID is always
//! null:
//! * message ID: null
//! * remote method: unicode string
//! * parameters: list of parameters
//!
//! ## Responses
//! Every response contains the following parts
//! * message ID: same ID as in request, for pairing request-response together
//! * result: any json-encoded result object (number, string, list, array, )
//! * error: null or list (error code, error message)
//!
//! References:
//! [https://docs.google.com/document/d/17zHy1SUlhgtCMbypO8cHgpWH73V5iUQKk_0rWvMqSNs/edit?hl=en_US#]
//! [https://braiins.com/stratum-v1/docs]
//! [https://en.bitcoin.it/wiki/Stratum_mining_protocol]
//! [https://en.bitcoin.it/wiki/BIP_0310]
//! [https://docs.google.com/spreadsheets/d/1z8a3S9gFkS8NGhBCxOMUDqs7h9SQltz8-VX3KPHk7Jw/edit#gid=0]

pub mod error;
pub mod json_rpc;
pub mod methods;
pub mod utils;

use std::convert::{TryFrom, TryInto};
use tracing::debug;

// use error::Result;
use error::Error;
pub use json_rpc::Message;
pub use methods::{client_to_server, server_to_client, Method, MethodError, ParsingMethodError};
use utils::{Extranonce, HexU32Be};

/// json_rpc Response are not handled because stratum v1 does not have any request from a server to
/// a client
/// TODO: Should update to accommodate miner requesting a difficulty change
///
/// A stratum v1 server represent a single connection with a client
pub trait IsServer<'a> {
    /// handle the received message and return a response if the message is a request or
    /// notification.
    fn handle_message(
        &mut self,
        msg: json_rpc::Message,
    ) -> Result<Option<json_rpc::Response>, Error<'a>>
    where
        Self: std::marker::Sized,
    {
        match msg {
            Message::StandardRequest(_) => {
                // handle valid standard request
                self.handle_request(msg)
            }
            Message::Notification(_) => {
                // handle valid server notification
                self.handle_request(msg)
            }
            _ => {
                // Server shouldn't receive json_rpc responses
                Err(Error::InvalidJsonRpcMessageKind)
            }
        }
    }

    /// Call the right handler according with the called method
    fn handle_request(
        &mut self,
        msg: json_rpc::Message,
    ) -> Result<Option<json_rpc::Response>, Error<'a>>
    where
        Self: std::marker::Sized,
    {
        let request = msg.try_into()?;

        match request {
            // TODO: Handle suggested difficulty
            methods::Client2Server::SuggestDifficulty() => Ok(None),
            methods::Client2Server::Authorize(authorize) => {
                let authorized = self.handle_authorize(&authorize);
                if authorized {
                    self.authorize(&authorize.name);
                }
                Ok(Some(authorize.respond(authorized)))
            }
            methods::Client2Server::Configure(configure) => {
                debug!("{:?}", configure);
                self.set_version_rolling_mask(configure.version_rolling_mask());
                self.set_version_rolling_min_bit(configure.version_rolling_min_bit_count());
                let (version_rolling, min_diff) = self.handle_configure(&configure);
                Ok(Some(configure.respond(version_rolling, min_diff)))
            }
            methods::Client2Server::ExtranonceSubscribe(_) => {
                self.handle_extranonce_subscribe();
                Ok(None)
            }
            methods::Client2Server::Submit(submit) => {
                let has_valid_version_bits = match &submit.version_bits {
                    Some(a) => {
                        if let Some(version_rolling_mask) = self.version_rolling_mask() {
                            version_rolling_mask.check_mask(a)
                        } else {
                            false
                        }
                    }
                    None => self.version_rolling_mask().is_none(),
                };

                let is_valid_submission = self.is_authorized(&submit.user_name)
                    && self.extranonce2_size() == submit.extra_nonce2.len()
                    && has_valid_version_bits;

                if is_valid_submission {
                    let accepted = self.handle_submit(&submit);
                    Ok(Some(submit.respond(accepted)))
                } else {
                    Err(Error::InvalidSubmission)
                }
            }
            methods::Client2Server::Subscribe(subscribe) => {
                let subscriptions = self.handle_subscribe(&subscribe);
                let extra_n1 = self.set_extranonce1(None);
                let extra_n2_size = self.set_extranonce2_size(None);
                Ok(Some(subscribe.respond(
                    subscriptions,
                    extra_n1,
                    extra_n2_size,
                )))
            }
        }
    }

    /// This message (JSON RPC Request) SHOULD be the first message sent by the miner after the
    /// connection with the server is established.
    fn handle_configure(
        &mut self,
        request: &client_to_server::Configure,
    ) -> (Option<server_to_client::VersionRollingParams>, Option<bool>);

    /// On the beginning of the session, client subscribes current connection for receiving mining
    /// jobs.
    ///
    /// The client can specify [mining.notify][a] job_id the client wishes to resume working with
    ///
    /// The result contains three items:
    /// * Subscriptions details: 2-tuple with name of subscribed notification and subscription ID.
    ///   Teoretically it may be used for unsubscribing, but obviously miners won't use it.
    /// * Extranonce1 - Hex-encoded, per-connection unique string which will be used for coinbase
    ///   serialization later. Keep it safe!
    /// * Extranonce2_size - Represents expected length of extranonce2 which will be generated by
    ///   the miner.
    ///
    /// Almost instantly after the subscription server start to send [jobs][a]
    ///
    /// This function return the first item of the result (2 tuple with name of subscibed ...)
    ///
    /// [a]: crate::methods::server_to_client::Notify
    fn handle_subscribe(&self, request: &client_to_server::Subscribe) -> Vec<(String, String)>;

    /// You can authorize as many workers as you wish and at any
    /// time during the session. In this way, you can handle big basement of independent mining rigs
    /// just by one Stratum connection.
    ///
    /// https://bitcoin.stackexchange.com/questions/29416/how-do-pool-servers-handle-multiple-workers-sharing-one-connection-with-stratum
    fn handle_authorize(&self, request: &client_to_server::Authorize) -> bool;

    /// When miner find the job which meets requested difficulty, it can submit share to the server.
    /// Only [Submit](client_to_server::Submit) requests for authorized user names can be submitted.
    fn handle_submit(&self, request: &client_to_server::Submit<'a>) -> bool;

    /// Indicates to the server that the client supports the mining.set_extranonce method.
    fn handle_extranonce_subscribe(&self);

    fn is_authorized(&self, name: &str) -> bool;

    fn authorize(&mut self, name: &str);

    /// Set extranonce1 to extranonce1 if provided. If not create a new one and set it.
    fn set_extranonce1(&mut self, extranonce1: Option<Extranonce<'a>>) -> Extranonce<'a>;

    fn extranonce1(&self) -> Extranonce<'a>;

    /// Set extranonce2_size to extranonce2_size if provided. If not create a new one and set it.
    fn set_extranonce2_size(&mut self, extra_nonce2_size: Option<usize>) -> usize;

    fn extranonce2_size(&self) -> usize;

    fn version_rolling_mask(&self) -> Option<HexU32Be>;

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>);

    fn set_version_rolling_min_bit(&mut self, mask: Option<HexU32Be>);

    fn update_extranonce(
        &mut self,
        extra_nonce1: Extranonce<'a>,
        extra_nonce2_size: usize,
    ) -> Result<json_rpc::Message, Error<'a>> {
        self.set_extranonce1(Some(extra_nonce1.clone()));
        self.set_extranonce2_size(Some(extra_nonce2_size));

        Ok(server_to_client::SetExtranonce {
            extra_nonce1,
            extra_nonce2_size,
        }
        .into())
    }
    // {"params":["00003000"], "id":null, "method": "mining.set_version_mask"}
    // fn update_version_rolling_mask

    fn notify(&mut self) -> Result<json_rpc::Message, Error>;

    fn handle_set_difficulty(&mut self, value: f64) -> Result<json_rpc::Message, Error> {
        let set_difficulty = server_to_client::SetDifficulty { value };
        Ok(set_difficulty.into())
    }
}

pub trait IsClient<'a> {
    /// Deserialize a [raw json_rpc message][a] into a [stratum v1 message][b] and handle the
    /// result.
    ///
    /// [a]: crate::...
    /// [b]:
    fn handle_message(
        &mut self,
        msg: json_rpc::Message,
    ) -> Result<Option<json_rpc::Message>, Error<'a>>
    where
        Self: std::marker::Sized,
    {
        let method: Result<Method<'a>, MethodError<'a>> = msg.try_into();

        match method {
            Ok(m) => match m {
                Method::Server2ClientResponse(response) => {
                    let response = self.update_response(response)?;
                    self.handle_response(response)
                }
                Method::Server2Client(request) => self.handle_request(request),
                Method::Client2Server(_) => Err(Error::InvalidReceiver(m.into())),
                Method::ErrorMessage(msg) => self.handle_error_message(msg),
            },
            Err(e) => Err(e.into()),
        }
    }

    fn update_response(
        &mut self,
        response: methods::Server2ClientResponse<'a>,
    ) -> Result<methods::Server2ClientResponse<'a>, Error<'a>> {
        match &response {
            methods::Server2ClientResponse::GeneralResponse(general) => {
                let is_authorize = self.id_is_authorize(&general.id);
                let is_submit = self.id_is_submit(&general.id);
                match (is_authorize, is_submit) {
                    (Some(prev_name), false) => {
                        let authorize = general.clone().into_authorize(prev_name);
                        Ok(methods::Server2ClientResponse::Authorize(authorize))
                    }
                    (None, false) => Ok(methods::Server2ClientResponse::Submit(
                        general.clone().into_submit(),
                    )),
                    _ => Err(Error::UnknownID(general.id)),
                }
            }
            _ => Ok(response),
        }
    }

    /// Call the right handler according with the called method
    fn handle_request(
        &mut self,
        request: methods::Server2Client<'a>,
    ) -> Result<Option<json_rpc::Message>, Error<'a>>
    where
        Self: std::marker::Sized,
    {
        match request {
            methods::Server2Client::Notify(notify) => {
                self.handle_notify(notify)?;
                Ok(None)
            }
            methods::Server2Client::SetDifficulty(mut set_diff) => {
                self.handle_set_difficulty(&mut set_diff)?;
                Ok(None)
            }
            methods::Server2Client::SetExtranonce(mut set_extra_nonce) => {
                self.handle_set_extranonce(&mut set_extra_nonce)?;
                Ok(None)
            }
            methods::Server2Client::SetVersionMask(mut set_version_mask) => {
                self.handle_set_version_mask(&mut set_version_mask)?;
                Ok(None)
            }
        }
    }

    fn handle_response(
        &mut self,
        response: methods::Server2ClientResponse<'a>,
    ) -> Result<Option<json_rpc::Message>, Error<'a>>
    where
        Self: std::marker::Sized,
    {
        match response {
            methods::Server2ClientResponse::Configure(mut configure) => {
                self.handle_configure(&mut configure)?;
                self.set_version_rolling_mask(configure.version_rolling_mask());
                self.set_version_rolling_min_bit(configure.version_rolling_min_bit());
                self.set_status(ClientStatus::Configured);

                //in sv1 the mining.configure message should be the first message to come in before
                // the subscribe - the subscribe response is where the server hands out the
                // extranonce so it doesnt really matter what the server sets the
                // extranonce to in the mining.configure handler
                debug!("NOTICE: Subscribe extranonce is hardcoded by server");
                let subscribe = self
                    .subscribe(
                        configure.id,
                        Some(Extranonce::try_from(hex::decode("08000002")?)?),
                    )
                    .ok();
                Ok(subscribe)
            }
            methods::Server2ClientResponse::Subscribe(subscribe) => {
                self.handle_subscribe(&subscribe)?;
                self.set_extranonce1(subscribe.extra_nonce1);
                self.set_extranonce2_size(subscribe.extra_nonce2_size);
                self.set_status(ClientStatus::Subscribed);
                Ok(None)
            }
            methods::Server2ClientResponse::Authorize(authorize) => {
                if authorize.is_ok() {
                    self.authorize_user_name(authorize.user_name());
                };
                Ok(None)
            }
            methods::Server2ClientResponse::Submit(_) => Ok(None),
            // impossible state
            methods::Server2ClientResponse::GeneralResponse(_) => panic!(),
            methods::Server2ClientResponse::SetDifficulty(_) => Ok(None),
        }
    }

    fn handle_error_message(
        &mut self,
        message: Message,
    ) -> Result<Option<json_rpc::Message>, Error<'a>>;

    /// Check if the client sent an Authorize request with the given id, if so it return the
    /// authorized name
    fn id_is_authorize(&mut self, id: &u64) -> Option<String>;

    /// Check if the client sent a Submit request with the given id
    fn id_is_submit(&mut self, id: &u64) -> bool;

    fn handle_notify(&mut self, notify: server_to_client::Notify<'a>) -> Result<(), Error<'a>>;

    fn handle_configure(&mut self, conf: &mut server_to_client::Configure)
        -> Result<(), Error<'a>>;

    fn handle_set_difficulty(
        &mut self,
        m: &mut server_to_client::SetDifficulty,
    ) -> Result<(), Error<'a>>;

    fn handle_set_extranonce(
        &mut self,
        m: &mut server_to_client::SetExtranonce,
    ) -> Result<(), Error<'a>>;

    fn handle_set_version_mask(
        &mut self,
        m: &mut server_to_client::SetVersionMask,
    ) -> Result<(), Error<'a>>;

    fn handle_subscribe(
        &mut self,
        subscribe: &server_to_client::Subscribe<'a>,
    ) -> Result<(), Error<'a>>;

    fn set_extranonce1(&mut self, extranonce1: Extranonce<'a>);

    fn extranonce1(&self) -> Extranonce<'a>;

    fn set_extranonce2_size(&mut self, extra_nonce2_size: usize);

    fn extranonce2_size(&self) -> usize;

    fn version_rolling_mask(&self) -> Option<HexU32Be>;

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>);

    fn set_version_rolling_min_bit(&mut self, min: Option<HexU32Be>);

    fn version_rolling_min_bit(&mut self) -> Option<HexU32Be>;

    fn set_status(&mut self, status: ClientStatus);

    fn signature(&self) -> String;

    fn status(&self) -> ClientStatus;

    fn last_notify(&self) -> Option<server_to_client::Notify>;

    /// Check if the given user_name has been authorized by the server
    #[allow(clippy::ptr_arg)]
    fn is_authorized(&self, name: &String) -> bool;

    /// Register the given user_name has authorized by the server
    fn authorize_user_name(&mut self, name: String);

    fn configure(&mut self, id: u64) -> json_rpc::Message {
        if self.version_rolling_min_bit().is_none() && self.version_rolling_mask().is_none() {
            client_to_server::Configure::void(id).into()
        } else {
            client_to_server::Configure::new(
                id,
                self.version_rolling_mask(),
                self.version_rolling_min_bit(),
            )
            .into()
        }
    }

    fn subscribe(
        &mut self,
        id: u64,
        extranonce1: Option<Extranonce<'a>>,
    ) -> Result<json_rpc::Message, Error<'a>> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.subscribe".to_string())),
            _ => Ok(client_to_server::Subscribe {
                id,
                agent_signature: self.signature(),
                extranonce1,
            }
            .try_into()?),
        }
    }

    fn authorize(
        &mut self,
        id: u64,
        name: String,
        password: String,
    ) -> Result<json_rpc::Message, Error> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.authorize".to_string())),
            _ => Ok(client_to_server::Authorize { id, name, password }.into()),
        }
    }

    fn submit(
        &mut self,
        id: u64,
        user_name: String,
        extra_nonce2: Extranonce<'a>,
        time: i64,
        nonce: i64,
        version_bits: Option<HexU32Be>,
    ) -> Result<json_rpc::Message, Error<'a>> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.submit".to_string())),
            _ => {
                if let Some(notify) = self.last_notify() {
                    if !self.is_authorized(&user_name) {
                        return Err(Error::UnauthorizedClient(user_name));
                    }
                    Ok(client_to_server::Submit {
                        job_id: notify.job_id,
                        user_name,
                        extra_nonce2,
                        time: HexU32Be(time as u32),
                        nonce: HexU32Be(nonce as u32),
                        version_bits,
                        id,
                    }
                    .into())
                } else {
                    Err(Error::IncorrectClientStatus(
                        "No Notify instance found".to_string(),
                    ))
                }
            }
        }
    }
}

#[derive(Debug, Copy, Clone, PartialEq, Eq)]
pub enum ClientStatus {
    Init,
    Configured,
    Subscribed,
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::collections::HashSet;

    // A minimal implementation of IsServer trait for testing
    struct TestServer<'a> {
        authorized_users: HashSet<String>,
        extranonce1: Extranonce<'a>,
        extranonce2_size: usize,
        version_rolling_mask: Option<HexU32Be>,
        version_rolling_min_bit: Option<HexU32Be>,
    }

    impl<'a> TestServer<'a> {
        fn new(extranonce1: Extranonce<'a>, extranonce2_size: usize) -> Self {
            Self {
                authorized_users: HashSet::new(),
                extranonce1,
                extranonce2_size,
                version_rolling_mask: None,
                version_rolling_min_bit: None,
            }
        }
    }

    impl<'a> IsServer<'a> for TestServer<'a> {
        fn handle_configure(
            &mut self,
            _request: &client_to_server::Configure,
        ) -> (Option<server_to_client::VersionRollingParams>, Option<bool>) {
            (None, None)
        }

        fn handle_subscribe(
            &self,
            _request: &client_to_server::Subscribe,
        ) -> Vec<(String, String)> {
            vec![("mining.notify".to_string(), "1".to_string())]
        }

        fn handle_authorize(&self, _request: &client_to_server::Authorize) -> bool {
            true
        }

        fn notify(&mut self) -> Result<json_rpc::Message, Error> {
            Ok(json_rpc::Message::StandardRequest(
                json_rpc::StandardRequest {
                    id: 1,
                    method: "mining.notify".to_string(),
                    params: serde_json::json!([]),
                },
            ))
        }

        fn handle_submit(&self, _request: &client_to_server::Submit<'a>) -> bool {
            true
        }

        fn handle_extranonce_subscribe(&self) {}

        fn is_authorized(&self, name: &str) -> bool {
            self.authorized_users.contains(name)
        }

        fn authorize(&mut self, name: &str) {
            self.authorized_users.insert(name.to_string());
        }

        fn set_extranonce1(&mut self, extranonce1: Option<Extranonce<'a>>) -> Extranonce<'a> {
            if let Some(extranonce1) = extranonce1 {
                self.extranonce1 = extranonce1;
            }
            self.extranonce1.clone()
        }

        fn extranonce1(&self) -> Extranonce<'a> {
            self.extranonce1.clone()
        }

        fn set_extranonce2_size(&mut self, extra_nonce2_size: Option<usize>) -> usize {
            if let Some(extra_nonce2_size) = extra_nonce2_size {
                self.extranonce2_size = extra_nonce2_size;
            }
            self.extranonce2_size
        }

        fn extranonce2_size(&self) -> usize {
            self.extranonce2_size
        }

        fn version_rolling_mask(&self) -> Option<HexU32Be> {
            None
        }

        fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
            self.version_rolling_mask = mask;
        }

        fn set_version_rolling_min_bit(&mut self, mask: Option<HexU32Be>) {
            self.version_rolling_min_bit = mask;
        }
    }

    #[test]
    fn test_server_handle_invalid_message() {
        let extranonce1 = Extranonce::try_from(hex::decode("08000002").unwrap()).unwrap();
        let mut server = TestServer::new(extranonce1, 4);

        // Create an invalid message (response)
        let request_message = json_rpc::Message::StandardRequest(json_rpc::StandardRequest {
            id: 42,
            method: "mining.subscribe_bad".to_string(),
            params: serde_json::json!([]),
        });

        let result = server.handle_message(request_message);

        assert!(result.is_err());
        match result.unwrap_err() {
            Error::Method(inner) => match *inner {
                MethodError::MethodNotFound(_) => {}
                other => panic!("Expected MethodNotFound error, got {:?}", other),
            },
            other => panic!("Expected Error::Method, got {:?}", other),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/methods/client_to_server.rs">
use bitcoin_hashes::hex::ToHex;
use serde_json::{
    Value,
    Value::{Array as JArrary, Null, Number as JNumber, String as JString},
};
use std::convert::{TryFrom, TryInto};

use crate::{
    error::Error,
    json_rpc::{Message, Response, StandardRequest},
    methods::ParsingMethodError,
    utils::{Extranonce, HexU32Be},
};

#[cfg(test)]
use quickcheck::{Arbitrary, Gen};

#[cfg(test)]
use quickcheck_macros;

/// _mining.authorize("username", "password")_
///
/// The result from an authorize request is usually true (successful), or false.
/// The password may be omitted if the server does not require passwords.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Authorize {
    pub id: u64,
    pub name: String,
    pub password: String,
}

impl Authorize {
    pub fn respond(self, is_ok: bool) -> Response {
        // infallible
        let result = serde_json::to_value(is_ok).unwrap();
        Response {
            id: self.id,
            result,
            error: None,
        }
    }
}

impl From<Authorize> for Message {
    fn from(auth: Authorize) -> Self {
        Message::StandardRequest(StandardRequest {
            id: auth.id,
            method: "mining.authorize".into(),
            params: (&[auth.name, auth.password][..]).into(),
        })
    }
}

impl TryFrom<StandardRequest> for Authorize {
    type Error = ParsingMethodError;

    fn try_from(msg: StandardRequest) -> Result<Self, Self::Error> {
        match msg.params.as_array() {
            Some(params) => {
                let (name, password) = match &params[..] {
                    [JString(a), JString(b)] => (a.into(), b.into()),
                    _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
                };
                let id = msg.id;
                Ok(Self { id, name, password })
            }
            None => Err(ParsingMethodError::not_array_from_value(msg.params)),
        }
    }
}

#[cfg(test)]
impl Arbitrary for Authorize {
    fn arbitrary(g: &mut Gen) -> Self {
        Authorize {
            name: String::arbitrary(g),
            password: String::arbitrary(g),
            id: u64::arbitrary(g),
        }
    }
}

#[cfg(test)]
#[quickcheck_macros::quickcheck]
fn from_to_json_rpc(auth: Authorize) -> bool {
    let message = Into::<Message>::into(auth.clone());
    let request = match message {
        Message::StandardRequest(s) => s,
        _ => panic!(),
    };
    auth == TryInto::<Authorize>::try_into(request).unwrap()
}

// mining.capabilities (DRAFT) (incompatible with mining.configure)

/// _mining.extranonce.subscribe()_
/// Indicates to the server that the client supports the mining.set_extranonce method.
/// https://en.bitcoin.it/wiki/BIP_0310
#[derive(Debug, Clone, Copy)]
pub struct ExtranonceSubscribe();

// mining.get_transactions

/// _mining.submit("username", "job id", "ExtraNonce2", "nTime", "nOnce")_
///
/// Miners submit shares using the method "mining.submit". Client submissions contain:
///
/// * Worker Name.
/// * Job ID.
/// * ExtraNonce2.
/// * nTime.
/// * nOnce.
/// * version_bits (used by version-rolling extension)
///
/// Server response is result: true for accepted, false for rejected (or you may get an error with
/// more details).
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Submit<'a> {
    pub user_name: String,            // root
    pub job_id: String,               // 6
    pub extra_nonce2: Extranonce<'a>, // "8a.."
    pub time: HexU32Be,               //string
    pub nonce: HexU32Be,
    pub version_bits: Option<HexU32Be>,
    pub id: u64,
}
//"{"params": ["spotbtc1.m30s40x16", "2", "147a3f0000000000", "6436eddf", "41d5deb0", "00000000"],
//"{"params": "id": 2196, "method": "mining.submit"}"

impl Submit<'_> {
    pub fn respond(self, is_ok: bool) -> Response {
        // infallibel
        let result = serde_json::to_value(is_ok).unwrap();
        Response {
            id: self.id,
            result,
            error: None,
        }
    }
}

impl From<Submit<'_>> for Message {
    fn from(submit: Submit) -> Self {
        let ex: String = submit.extra_nonce2.0.inner_as_ref().to_hex();
        let mut params: Vec<Value> = vec![
            submit.user_name.into(),
            submit.job_id.into(),
            ex.into(),
            submit.time.into(),
            submit.nonce.into(),
        ];
        if let Some(a) = submit.version_bits {
            let a: String = a.into();
            params.push(a.into());
        };
        Message::StandardRequest(StandardRequest {
            id: submit.id,
            method: "mining.submit".into(),
            params: params.into(),
        })
    }
}

impl TryFrom<StandardRequest> for Submit<'_> {
    type Error = ParsingMethodError;

    #[allow(clippy::many_single_char_names)]
    fn try_from(msg: StandardRequest) -> Result<Self, Self::Error> {
        match msg.params.as_array() {
            Some(params) => {
                let (user_name, job_id, extra_nonce2, time, nonce, version_bits) = match &params[..]
                {
                    [JString(a), JString(b), JString(c), JNumber(d), JNumber(e), JString(f)] => (
                        a.into(),
                        b.into(),
                        Extranonce::try_from(hex::decode(c)?)?,
                        HexU32Be(d.as_u64().unwrap() as u32),
                        HexU32Be(e.as_u64().unwrap() as u32),
                        Some((f.as_str()).try_into()?),
                    ),
                    [JString(a), JString(b), JString(c), JString(d), JString(e), JString(f)] => (
                        a.into(),
                        b.into(),
                        Extranonce::try_from(hex::decode(c)?)?,
                        (d.as_str()).try_into()?,
                        (e.as_str()).try_into()?,
                        Some((f.as_str()).try_into()?),
                    ),
                    [JString(a), JString(b), JString(c), JNumber(d), JNumber(e)] => (
                        a.into(),
                        b.into(),
                        Extranonce::try_from(hex::decode(c)?)?,
                        HexU32Be(d.as_u64().unwrap() as u32),
                        HexU32Be(e.as_u64().unwrap() as u32),
                        None,
                    ),
                    [JString(a), JString(b), JString(c), JString(d), JString(e)] => (
                        a.into(),
                        b.into(),
                        Extranonce::try_from(hex::decode(c)?)?,
                        (d.as_str()).try_into()?,
                        (e.as_str()).try_into()?,
                        None,
                    ),
                    _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
                };
                let id = msg.id;
                let res = crate::client_to_server::Submit {
                    user_name,
                    job_id,
                    extra_nonce2,
                    time,
                    nonce,
                    version_bits,
                    id,
                };
                Ok(res)
            }
            None => Err(ParsingMethodError::not_array_from_value(msg.params)),
        }
    }
}

#[cfg(test)]
impl Arbitrary for Submit<'static> {
    fn arbitrary(g: &mut Gen) -> Self {
        let mut extra = Vec::<u8>::arbitrary(g);
        extra.resize(32, 0);
        println!("\nEXTRA: {extra:?}\n");
        let bits = Option::<u32>::arbitrary(g);
        println!("\nBITS: {bits:?}\n");
        let extra: Extranonce = extra.try_into().unwrap();
        let bits = bits.map(HexU32Be);
        println!("\nBITS: {bits:?}\n");
        Submit {
            user_name: String::arbitrary(g),
            job_id: String::arbitrary(g),
            extra_nonce2: extra,
            time: HexU32Be(u32::arbitrary(g)),
            nonce: HexU32Be(u32::arbitrary(g)),
            version_bits: bits,
            id: u64::arbitrary(g),
        }
    }
}

#[cfg(test)]
#[quickcheck_macros::quickcheck]
fn submit_from_to_json_rpc(submit: Submit<'static>) -> bool {
    let message = Into::<Message>::into(submit.clone());
    println!("\nMESSAGE: {message:?}\n");
    let request = match message {
        Message::StandardRequest(s) => s,
        _ => panic!(),
    };
    println!("\nREQUEST: {request:?}\n");
    submit == TryInto::<Submit>::try_into(request).unwrap()
}

/// _mining.subscribe("user agent/version", "extranonce1")_
///
/// extranonce1 specifies a [mining.notify][a] extranonce1 the client wishes to
/// resume working with (possibly due to a dropped connection). If provided, a server MAY (at its
/// option) issue the connection the same extranonce1. Note that the extranonce1 may be the same
/// (allowing a resumed connection) even if the subscription id is changed!
///
/// [a]: crate::methods::server_to_client::Notify
#[derive(Debug, Clone)]
pub struct Subscribe<'a> {
    pub id: u64,
    pub agent_signature: String,
    pub extranonce1: Option<Extranonce<'a>>,
}

impl<'a> Subscribe<'a> {
    pub fn respond(
        self,
        subscriptions: Vec<(String, String)>,
        extra_nonce1: Extranonce<'a>,
        extra_nonce2_size: usize,
    ) -> Response {
        let response = crate::server_to_client::Subscribe {
            subscriptions,
            extra_nonce1,
            extra_nonce2_size,
            id: self.id,
        };
        match Message::from(response) {
            Message::OkResponse(r) => r,
            _ => unreachable!(),
        }
    }
}

impl<'a> TryFrom<Subscribe<'a>> for Message {
    type Error = Error<'a>;

    fn try_from(subscribe: Subscribe) -> Result<Self, Error> {
        let params = match (subscribe.agent_signature, subscribe.extranonce1) {
            (a, Some(b)) => vec![a, b.0.inner_as_ref().to_hex()],
            (a, None) => vec![a],
        };
        Ok(Message::StandardRequest(StandardRequest {
            id: subscribe.id,
            method: "mining.subscribe".into(),
            params: (&params[..]).into(),
        }))
    }
}

impl TryFrom<StandardRequest> for Subscribe<'_> {
    type Error = ParsingMethodError;

    fn try_from(msg: StandardRequest) -> Result<Self, Self::Error> {
        match msg.params.as_array() {
            Some(params) => {
                let (agent_signature, extranonce1) = match &params[..] {
                    // bosminer subscribe message
                    [JString(a), Null, JString(_), Null] => (a.into(), None),
                    // bosminer subscribe message
                    [JString(a), Null] => (a.into(), None),
                    [JString(a), JString(b)] => {
                        (a.into(), Some(Extranonce::try_from(hex::decode(b)?)?))
                    }
                    [JString(a)] => (a.into(), None),
                    [] => ("".to_string(), None),
                    _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
                };
                let id = msg.id;
                let res = Subscribe {
                    id,
                    agent_signature,
                    extranonce1,
                };
                Ok(res)
            }
            None => Err(ParsingMethodError::not_array_from_value(msg.params)),
        }
    }
}

#[derive(Debug, Clone)]
pub struct Configure {
    extensions: Vec<ConfigureExtension>,
    id: u64,
}

impl Configure {
    pub fn new(id: u64, mask: Option<HexU32Be>, min_bit_count: Option<HexU32Be>) -> Self {
        let extension = ConfigureExtension::VersionRolling(VersionRollingParams {
            mask,
            min_bit_count,
        });
        Configure {
            extensions: vec![extension],
            id,
        }
    }

    pub fn void(id: u64) -> Self {
        Configure {
            extensions: vec![],
            id,
        }
    }

    pub fn respond(
        self,
        version_rolling: Option<crate::server_to_client::VersionRollingParams>,
        minimum_difficulty: Option<bool>,
    ) -> Response {
        let response = crate::server_to_client::Configure {
            id: self.id,
            version_rolling,
            minimum_difficulty,
        };
        match Message::from(response) {
            Message::OkResponse(r) => r,
            _ => unreachable!(),
        }
    }

    pub fn version_rolling_mask(&self) -> Option<HexU32Be> {
        let mut res = None;
        for ext in &self.extensions {
            if let ConfigureExtension::VersionRolling(p) = ext {
                res = Some(p.mask.clone().unwrap_or(HexU32Be(0x1FFFE000)));
            };
        }
        res
    }

    pub fn version_rolling_min_bit_count(&self) -> Option<HexU32Be> {
        let mut res = None;
        for ext in &self.extensions {
            if let ConfigureExtension::VersionRolling(p) = ext {
                // TODO check if 0 is the right default value
                res = Some(p.min_bit_count.clone().unwrap_or(HexU32Be(0)));
            };
        }
        res
    }
}

impl From<Configure> for Message {
    fn from(conf: Configure) -> Self {
        let mut params = serde_json::Map::new();
        let extension_names: Vec<Value> = conf
            .extensions
            .iter()
            .map(|x| x.get_extension_name())
            .collect();
        for parameter in conf.extensions {
            let mut parameter: serde_json::Map<String, Value> = parameter.into();
            params.append(&mut parameter);
        }
        Message::StandardRequest(StandardRequest {
            id: conf.id,
            method: "mining.configure".into(),
            params: vec![JArrary(extension_names), params.into()].into(),
        })
    }
}

impl TryFrom<StandardRequest> for Configure {
    type Error = ParsingMethodError;

    fn try_from(msg: StandardRequest) -> Result<Self, Self::Error> {
        let extensions = ConfigureExtension::from_value(&msg.params)?;
        let id = msg.id;
        Ok(Self { extensions, id })
    }
}

#[derive(Debug, Clone)]
pub enum ConfigureExtension {
    VersionRolling(VersionRollingParams),
    MinimumDifficulty(u64),
    SubcribeExtraNonce,
    Info(InfoParams),
}

#[allow(clippy::unnecessary_unwrap)]
impl ConfigureExtension {
    pub fn from_value(val: &Value) -> Result<Vec<ConfigureExtension>, ParsingMethodError> {
        let mut res = vec![];
        let root = val
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(val.clone()))?;
        if root.is_empty() {
            return Err(ParsingMethodError::Todo);
        };

        let version_rolling_mask = val.pointer("/1/version-rolling.mask");
        let version_rolling_min_bit = val.pointer("/1/version-rolling.min-bit-count");
        let info_connection_url = val.pointer("/1/info.connection-url");
        let info_hw_version = val.pointer("/1/info.hw-version");
        let info_sw_version = val.pointer("/1/info.sw-version");
        let info_hw_id = val.pointer("/1/info.hw-id");
        let minimum_difficulty_value = val.pointer("/1/minimum-difficulty.value");

        if root[0]
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(root[0].clone()))?
            .contains(&JString("subscribe-extranonce".to_string()))
        {
            res.push(ConfigureExtension::SubcribeExtraNonce)
        }
        let (mask, min_bit_count) = match (version_rolling_mask, version_rolling_min_bit) {
            (None, None) => (None, None),
            // WhatsMiner sent mask without min bit count
            (Some(JString(mask)), None) => {
                let mask: HexU32Be = mask.as_str().try_into()?;
                (Some(mask), None)
            }
            // Min bit can be a string cpuminer
            (Some(JString(mask)), Some(JString(min_bit))) => {
                let mask: HexU32Be = mask.as_str().try_into()?;
                let min_bit: HexU32Be = min_bit.as_str().try_into()?;
                (Some(mask), Some(min_bit))
            }
            // Min bit can be a number s9, s19
            (Some(JString(mask)), Some(JNumber(min_bit))) => {
                let mask: HexU32Be = mask.as_str().try_into()?;
                // min_bit is a json number checked above so as_u64 can not fail
                let min_bit: HexU32Be = HexU32Be(min_bit.as_u64().unwrap() as u32);
                (Some(mask), Some(min_bit))
            }
            // We can not have min bit count without a mask
            (None, Some(_)) => return Err(ParsingMethodError::Todo),
            // Mask need to be a JString
            (Some(_), None) => return Err(ParsingMethodError::Todo),
            // Min bit need to be a string or a number
            (Some(_), Some(_)) => return Err(ParsingMethodError::Todo),
        };
        if mask.is_some() || min_bit_count.is_some() {
            let params = VersionRollingParams {
                mask,
                min_bit_count,
            };
            res.push(ConfigureExtension::VersionRolling(params));
        }

        if let Some(minimum_difficulty_value) = minimum_difficulty_value {
            let min_diff = match minimum_difficulty_value {
                JNumber(a) => a
                    .as_u64()
                    .ok_or_else(|| ParsingMethodError::not_unsigned_from_value(a.clone()))?,
                _ => {
                    return Err(ParsingMethodError::unexpected_value_from_value(
                        minimum_difficulty_value.clone(),
                    ))
                }
            };

            res.push(ConfigureExtension::MinimumDifficulty(min_diff));
        };

        if info_connection_url.is_some()
            || info_hw_id.is_some()
            || info_hw_version.is_some()
            || info_sw_version.is_some()
        {
            let connection_url = if info_connection_url.is_some()
                // infallible
                && info_connection_url.unwrap().as_str().is_some()
            {
                // infallible
                Some(info_connection_url.unwrap().as_str().unwrap().to_string())
            } else if info_connection_url.is_some() {
                return Err(ParsingMethodError::Todo);
            } else {
                None
            };
            // infallible
            let hw_id = if info_hw_id.is_some() && info_hw_id.unwrap().as_str().is_some() {
                // infallible
                Some(info_hw_id.unwrap().as_str().unwrap().to_string())
            } else if info_hw_id.is_some() {
                return Err(ParsingMethodError::Todo);
            } else {
                None
            };
            // infallible
            let hw_version =
                if info_hw_version.is_some() && info_hw_version.unwrap().as_str().is_some() {
                    // infallible
                    Some(info_hw_version.unwrap().as_str().unwrap().to_string())
                } else if info_hw_version.is_some() {
                    return Err(ParsingMethodError::Todo);
                } else {
                    None
                };
            let sw_version =
                // infallible
                if info_sw_version.is_some() && info_sw_version.unwrap().as_str().is_some() {
                    // infallible
                    Some(info_sw_version.unwrap().as_str().unwrap().to_string())
                } else if info_sw_version.is_some() {
                    return Err(ParsingMethodError::Todo);
                } else {
                    None
                };
            let params = InfoParams {
                connection_url,
                hw_id,
                hw_version,
                sw_version,
            };
            res.push(ConfigureExtension::Info(params));
        };
        Ok(res)
    }
}

impl ConfigureExtension {
    pub fn get_extension_name(&self) -> Value {
        match self {
            ConfigureExtension::VersionRolling(_) => "version-rolling".into(),
            ConfigureExtension::MinimumDifficulty(_) => "minimum-difficulty".into(),
            ConfigureExtension::SubcribeExtraNonce => "subscribe-extranonce".into(),
            ConfigureExtension::Info(_) => "info".into(),
        }
    }
}

impl From<ConfigureExtension> for serde_json::Map<String, Value> {
    fn from(conf: ConfigureExtension) -> Self {
        match conf {
            ConfigureExtension::VersionRolling(a) => a.into(),
            ConfigureExtension::SubcribeExtraNonce => serde_json::Map::new(),
            ConfigureExtension::Info(a) => a.into(),
            ConfigureExtension::MinimumDifficulty(a) => {
                let mut map = serde_json::Map::new();
                map.insert("minimum-difficulty".to_string(), a.into());
                map
            }
        }
    }
}

#[derive(Debug, Clone)]
pub struct VersionRollingParams {
    mask: Option<HexU32Be>,
    min_bit_count: Option<HexU32Be>,
}

impl From<VersionRollingParams> for serde_json::Map<String, Value> {
    fn from(conf: VersionRollingParams) -> Self {
        let mut params = serde_json::Map::new();
        match (conf.mask, conf.min_bit_count) {
            (Some(mask), Some(min)) => {
                let mask: String = mask.into();
                let min: String = min.into();
                params.insert("version-rolling.mask".to_string(), mask.into());
                params.insert("version-rolling.min-bit-count".to_string(), min.into());
            }
            (Some(mask), None) => {
                let mask: String = mask.into();
                params.insert("version-rolling.mask".to_string(), mask.into());
            }
            (None, Some(min)) => {
                let min: String = min.into();
                params.insert("version-rolling.min-bit-count".to_string(), min.into());
            }
            (None, None) => (),
        };
        params
    }
}

#[derive(Debug, Clone)]
pub struct InfoParams {
    connection_url: Option<String>,
    #[allow(dead_code)]
    hw_id: Option<String>,
    #[allow(dead_code)]
    hw_version: Option<String>,
    #[allow(dead_code)]
    sw_version: Option<String>,
}

impl From<InfoParams> for serde_json::Map<String, Value> {
    fn from(info: InfoParams) -> Self {
        let mut params = serde_json::Map::new();
        if info.connection_url.is_some() {
            params.insert(
                "info.connection-url".to_string(),
                // infallible
                info.connection_url.unwrap().into(),
            );
        }
        params
    }
}

// mining.suggest_difficulty

// mining.suggest_target

// mining.minimum_difficulty (extension)
#[test]
fn test_version_extension_with_broken_bit_count() {
    let client_message = r#"{"id":0,
            "method": "mining.configure",
            "params":[
                ["version-rolling"],
                {"version-rolling.mask":"1fffe000",
                "version-rolling.min-bit-count":"16"}
            ]
        }"#;
    let client_message: StandardRequest = serde_json::from_str(client_message).unwrap();
    let server_configure = Configure::try_from(client_message).unwrap();
    match &server_configure.extensions[0] {
        ConfigureExtension::VersionRolling(params) => {
            assert!(params.min_bit_count.as_ref().unwrap().0 == 0x16)
        }
        _ => panic!(),
    };
}
#[test]
fn test_version_extension_with_non_string_bit_count() {
    let client_message = r#"{"id":0,
            "method": "mining.configure",
            "params":[
                ["version-rolling"],
                {"version-rolling.mask":"1fffe000",
                "version-rolling.min-bit-count":16}
            ]
        }"#;
    let client_message: StandardRequest = serde_json::from_str(client_message).unwrap();
    let server_configure = Configure::try_from(client_message).unwrap();
    match &server_configure.extensions[0] {
        ConfigureExtension::VersionRolling(params) => {
            assert!(params.min_bit_count.as_ref().unwrap().0 == 16)
        }
        _ => panic!(),
    };
}

#[test]
fn test_version_extension_with_no_bit_count() {
    let client_message = r#"{"id":0,
            "method": "mining.configure",
            "params":[
                ["version-rolling"],
                {"version-rolling.mask":"ffffffff"}
            ]
        }"#;
    let client_message: StandardRequest = serde_json::from_str(client_message).unwrap();
    let server_configure = Configure::try_from(client_message).unwrap();
    match &server_configure.extensions[0] {
        ConfigureExtension::VersionRolling(params) => {
            assert!(params.min_bit_count.as_ref().is_none());
        }
        _ => panic!(),
    };
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/methods/mod.rs">
use crate::error::Error;
use bitcoin_hashes::Error as BTCHashError;
use hex::FromHexError;
use std::convert::{TryFrom, TryInto};

pub mod client_to_server;
pub mod server_to_client;

use crate::json_rpc::{Message, Response};

/// Errors encountered during conversion between valid json_rpc messages and Sv1 messages.
#[derive(Debug, Clone)]
pub enum MethodError<'a> {
    /// If the json_rpc message call a method not defined by Sv1. It contains the called method
    MethodNotFound(String),
    /// If the json_rpc Response co"ntain an error in this case the error should just be reported
    ResponseIsAnError(Box<crate::json_rpc::Response>),
    /// Method can not be parsed
    ParsingMethodError((ParsingMethodError, Message)),
    // Method can not be serialized
    // SerializeError(Box<Method>),
    UnexpectedMethod(Method<'a>),
    // json_rpc message is not a request
    NotARequest,
}

//~:wimplimpl From<ParsingMethodError> for MethodError {
//~:wimpl    fn from(pars_err: ParsingMethodError) -> Self {
//~:wimpl        MethodError::ParsingMethodError(pars_err)
//~:wimpl    }
//~:wimpl}

impl From<FromHexError> for ParsingMethodError {
    fn from(hex_err: FromHexError) -> Self {
        ParsingMethodError::HexError(Box::new(hex_err))
    }
}

impl From<BTCHashError> for ParsingMethodError {
    fn from(btc_err: BTCHashError) -> Self {
        ParsingMethodError::BTCHashError(Box::new(btc_err))
    }
}

impl From<binary_sv2::Error> for ParsingMethodError {
    fn from(u256_err: binary_sv2::Error) -> Self {
        ParsingMethodError::BadU256Convert(Box::new(u256_err))
    }
}

#[derive(Debug, Clone)]
pub enum ParsingMethodError {
    BadU256Convert(Box<binary_sv2::Error>),
    HexError(Box<FromHexError>),
    #[allow(clippy::upper_case_acronyms)]
    BTCHashError(Box<BTCHashError>),
    ValueNotAnArray(Box<serde_json::Value>),
    WrongArgs(Box<serde_json::Value>),
    ValueNotAString(Box<serde_json::Value>),
    ValueNotAFloat(Box<serde_json::Value>),
    ValueNotAnUnsigned(Box<serde_json::value::Number>),
    ValueNotAnInt(Box<serde_json::value::Number>),
    UnexpectedValue(Box<serde_json::Value>),
    ImpossibleToParseResultField(Box<Response>),
    ImpossibleToParseAsU64(Box<serde_json::Number>),
    UnexpectedArrayParams(Vec<serde_json::Value>),
    UnexpectedObjectParams(serde_json::Map<String, serde_json::Value>),
    MultipleError(Vec<ParsingMethodError>),
    Todo,
}

impl From<Error<'_>> for ParsingMethodError {
    fn from(inner: Error) -> Self {
        match inner {
            Error::HexError(e) => ParsingMethodError::HexError(Box::new(e)),
            Error::BTCHashError(e) => ParsingMethodError::BTCHashError(Box::new(e)),
            _ => panic!("v1 Error does not implement this ParsingMethodError, but probably should"),
        }
    }
}

impl ParsingMethodError {
    pub fn as_method_error(self, msg: Message) -> MethodError<'static> {
        MethodError::ParsingMethodError((self, msg))
    }
}

impl ParsingMethodError {
    pub fn not_array_from_value(v: serde_json::Value) -> Self {
        ParsingMethodError::ValueNotAnArray(Box::new(v))
    }

    pub fn not_string_from_value(v: serde_json::Value) -> Self {
        ParsingMethodError::ValueNotAString(Box::new(v))
    }

    pub fn not_float_from_value(v: serde_json::Value) -> Self {
        ParsingMethodError::ValueNotAFloat(Box::new(v))
    }

    pub fn not_unsigned_from_value(v: serde_json::value::Number) -> Self {
        ParsingMethodError::ValueNotAnUnsigned(Box::new(v))
    }

    pub fn not_int_from_value(v: serde_json::value::Number) -> Self {
        ParsingMethodError::ValueNotAnInt(Box::new(v))
    }

    pub fn wrong_args_from_value(v: serde_json::Value) -> Self {
        ParsingMethodError::WrongArgs(Box::new(v))
    }

    pub fn unexpected_value_from_value(v: serde_json::Value) -> Self {
        ParsingMethodError::UnexpectedValue(Box::new(v))
    }
}

#[derive(Debug, Clone)]
pub enum Method<'a> {
    Client2Server(Client2Server<'a>),
    Server2Client(Server2Client<'a>),
    Server2ClientResponse(Server2ClientResponse<'a>),
    ErrorMessage(Message),
}

#[derive(Debug, Clone)]
pub enum Client2Server<'a> {
    SuggestDifficulty(),
    Subscribe(client_to_server::Subscribe<'a>),
    Authorize(client_to_server::Authorize),
    ExtranonceSubscribe(client_to_server::ExtranonceSubscribe),
    Submit(client_to_server::Submit<'a>),
    Configure(client_to_server::Configure),
}

impl<'a> From<Client2Server<'a>> for Method<'a> {
    fn from(a: Client2Server<'a>) -> Self {
        Method::Client2Server(a)
    }
}

impl<'a> TryFrom<Message> for Client2Server<'a> {
    type Error = MethodError<'a>;

    fn try_from(msg: Message) -> Result<Self, Self::Error> {
        let method: Method = msg.try_into()?;
        match method {
            Method::Client2Server(client_to_server) => Ok(client_to_server),
            Method::ErrorMessage(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::Server2Client(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::Server2ClientResponse(_) => Err(MethodError::UnexpectedMethod(method)),
        }
    }
}

#[derive(Debug, Clone)]
pub enum Server2Client<'a> {
    Notify(server_to_client::Notify<'a>),
    SetDifficulty(server_to_client::SetDifficulty),
    SetExtranonce(server_to_client::SetExtranonce<'a>),
    SetVersionMask(server_to_client::SetVersionMask),
}

impl<'a> From<Server2Client<'a>> for Method<'a> {
    fn from(a: Server2Client<'a>) -> Self {
        Method::Server2Client(a)
    }
}

impl<'a> TryFrom<Message> for Server2Client<'a> {
    type Error = MethodError<'a>;

    fn try_from(msg: Message) -> Result<Self, Self::Error> {
        let method: Method = msg.try_into()?;
        match method {
            Method::Server2Client(client_to_server) => Ok(client_to_server),
            Method::ErrorMessage(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::Client2Server(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::Server2ClientResponse(_) => Err(MethodError::UnexpectedMethod(method)),
        }
    }
}

#[derive(Debug, Clone)]
pub enum Server2ClientResponse<'a> {
    Configure(server_to_client::Configure),
    Subscribe(server_to_client::Subscribe<'a>),
    GeneralResponse(server_to_client::GeneralResponse),
    Authorize(server_to_client::Authorize),
    Submit(server_to_client::Submit),
    SetDifficulty(server_to_client::SetDifficulty),
}

impl<'a> From<Server2ClientResponse<'a>> for Method<'a> {
    fn from(a: Server2ClientResponse<'a>) -> Self {
        Method::Server2ClientResponse(a)
    }
}

impl<'a> TryFrom<Message> for Server2ClientResponse<'a> {
    type Error = MethodError<'a>;

    fn try_from(msg: Message) -> Result<Self, Self::Error> {
        let method: Method = msg.try_into()?;
        match method {
            Method::Server2ClientResponse(server_to_client) => Ok(server_to_client),
            Method::Client2Server(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::Server2Client(_) => Err(MethodError::UnexpectedMethod(method)),
            Method::ErrorMessage(_) => Err(MethodError::UnexpectedMethod(method)),
        }
    }
}

impl<'a> TryFrom<Message> for Method<'a> {
    type Error = MethodError<'a>;

    fn try_from(msg: Message) -> Result<Self, MethodError<'a>> {
        match &msg {
            Message::StandardRequest(request) => match &request.method[..] {
                "mining.suggest_difficulty" => {
                    Ok(Method::Client2Server(Client2Server::SuggestDifficulty()))
                }
                "mining.subscribe" => {
                    let method = request
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Client2Server(Client2Server::Subscribe(method)))
                }
                "mining.authorize" => {
                    let method = request
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Client2Server(Client2Server::Authorize(method)))
                }
                "mining.extranonce.subscribe" => Ok(Method::Client2Server(
                    Client2Server::ExtranonceSubscribe(client_to_server::ExtranonceSubscribe()),
                )),
                "mining.submit" => {
                    let method = request
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Client2Server(Client2Server::Submit(method)))
                }
                "mining.configure" => {
                    let method = request
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Client2Server(Client2Server::Configure(method)))
                }
                _ => Err(MethodError::MethodNotFound(request.clone().method)),
            },
            Message::Notification(notification) => match &notification.method[..] {
                "mining.notify" => {
                    let method = notification
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Server2Client(Server2Client::Notify(method)))
                }
                "mining.set_version_mask" => {
                    let method = notification
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Server2Client(Server2Client::SetVersionMask(method)))
                }
                "mining.set_difficulty" => {
                    let method = notification
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Server2Client(Server2Client::SetDifficulty(method)))
                }
                "mining.set_extranonce" => {
                    let method = notification
                        .clone()
                        .try_into()
                        .map_err(|e: ParsingMethodError| e.as_method_error(msg))?;
                    Ok(Method::Server2Client(Server2Client::SetExtranonce(method)))
                }
                _ => Err(MethodError::MethodNotFound(notification.clone().method)),
            },
            Message::OkResponse(response) => response
                .clone()
                .try_into()
                .map(Method::Server2ClientResponse)
                .map_err(|e| e.as_method_error(msg)),
            Message::ErrorResponse(_) => Ok(Method::ErrorMessage(msg)),
        }
    }
}

impl TryFrom<crate::json_rpc::Response> for Server2ClientResponse<'_> {
    type Error = ParsingMethodError;

    fn try_from(msg: Response) -> Result<Self, Self::Error> {
        let subscribe: Result<server_to_client::Subscribe, ParsingMethodError> = (&msg).try_into();
        let configure: Result<server_to_client::Configure, ParsingMethodError> = (&msg).try_into();
        let general_response: Result<server_to_client::GeneralResponse, ParsingMethodError> =
            (&msg).try_into();
        match (subscribe, configure, general_response) {
            (Ok(a), Err(_), Err(_)) => Ok(Server2ClientResponse::Subscribe(a)),
            (Err(_), Ok(a), Err(_)) => Ok(Server2ClientResponse::Configure(a)),
            (Err(_), Err(_), Ok(a)) => Ok(Server2ClientResponse::GeneralResponse(a)),
            (Err(e), Err(ee), Err(eee)) => Err(ParsingMethodError::MultipleError(vec![e, ee, eee])),
            // Impossible state a message can not be more than one method
            _ => panic!(),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/methods/server_to_client.rs">
use serde_json::{
    Value,
    Value::{Array as JArrary, Bool as JBool, Number as JNumber, String as JString},
};
use std::convert::{TryFrom, TryInto};

use crate::{
    error::Error,
    json_rpc::{Message, Notification, Response},
    methods::ParsingMethodError,
    utils::{Extranonce, HexBytes, HexU32Be, MerkleNode, PrevHash},
};

// client.get_version()

// client.reconnect

// client.show_message

/// Fields in order:
///
/// * Job ID: This is included when miners submit a results so work can be matched with proper
///   transactions.
/// * Hash of previous block: Used to build the header.
/// * Generation transaction (part 1): The miner inserts ExtraNonce1 and ExtraNonce2 after this
///   section of the transaction data.
/// * Generation transaction (part 2): The miner appends this after the first part of the
///   transaction data and the two ExtraNonce values.
/// * List of merkle branches: The generation transaction is hashed against the merkle branches to
///   build the final merkle root.
/// * Bitcoin block version: Used in the block header.
///     * nBits: The encoded network difficulty. Used in the block header.
///     * nTime: The current time. nTime rolling should be supported, but should not increase faster
///       than actual time.
/// * Clean Jobs: If true, miners should abort their current work and immediately use the new job.
///   If false, they can still use the current job, but should move to the new one after exhausting
///   the current nonce range.
#[derive(Debug, Clone)]
pub struct Notify<'a> {
    pub job_id: String,
    pub prev_hash: PrevHash<'a>,
    pub coin_base1: HexBytes,
    pub coin_base2: HexBytes,
    pub merkle_branch: Vec<MerkleNode<'a>>,
    pub version: HexU32Be,
    pub bits: HexU32Be,
    pub time: HexU32Be,
    pub clean_jobs: bool,
}

impl From<Notify<'_>> for Message {
    fn from(notify: Notify) -> Self {
        let prev_hash: Value = notify.prev_hash.into();
        let coin_base1: Value = notify.coin_base1.into();
        let coin_base2: Value = notify.coin_base2.into();
        let mut merkle_branch: Vec<Value> = vec![];
        for mb in notify.merkle_branch {
            let mb: Value = mb.into();
            merkle_branch.push(mb);
        }
        let merkle_branch = JArrary(merkle_branch);
        let version: Value = notify.version.into();
        let bits: Value = notify.bits.into();
        let time: Value = notify.time.into();
        Message::Notification(Notification {
            method: "mining.notify".to_string(),
            params: (&[
                notify.job_id.into(),
                prev_hash,
                coin_base1,
                coin_base2,
                merkle_branch,
                version,
                bits,
                time,
                notify.clean_jobs.into(),
            ][..])
                .into(),
        })
    }
}

impl TryFrom<Notification> for Notify<'_> {
    type Error = ParsingMethodError;

    #[allow(clippy::many_single_char_names)]
    fn try_from(msg: Notification) -> Result<Self, Self::Error> {
        let params = msg
            .params
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(msg.params.clone()))?;
        let (
            job_id,
            prev_hash,
            coin_base1,
            coin_base2,
            merkle_branch_,
            version,
            bits,
            time,
            clean_jobs,
        ) = match &params[..] {
            [JString(a), JString(b), JString(c), JString(d), JArrary(e), JString(f), JString(g), JString(h), JBool(i)] => {
                (
                    a.into(),
                    b.as_str().try_into()?,
                    c.as_str().try_into()?,
                    d.as_str().try_into()?,
                    e,
                    f.as_str().try_into()?,
                    g.as_str().try_into()?,
                    h.as_str().try_into()?,
                    *i,
                )
            }
            _ => {
                return Err(ParsingMethodError::wrong_args_from_value(
                    msg.params.clone(),
                ))
            }
        };
        let mut merkle_branch = vec![];
        for h in merkle_branch_ {
            let h: MerkleNode = h
                .as_str()
                .ok_or_else(|| ParsingMethodError::not_string_from_value(h.clone()))?
                .try_into()?;

            merkle_branch.push(h);
        }
        let notify = Notify {
            job_id,
            prev_hash,
            coin_base1,
            coin_base2,
            merkle_branch,
            version,
            bits,
            time,
            clean_jobs,
        };
        Ok(notify.clone())
    }
}

/// mining.set_difficulty(difficulty)
///
/// The server can adjust the difficulty required for miner shares with the "mining.set_difficulty"
/// method. The miner should begin enforcing the new difficulty on the next job received. Some pools
/// may force a new job out when set_difficulty is sent, using clean_jobs to force the miner to
/// begin using the new difficulty immediately.
#[derive(Debug, Clone)]
pub struct SetDifficulty {
    pub value: f64,
}

impl From<SetDifficulty> for Message {
    fn from(sd: SetDifficulty) -> Self {
        let value: Value = sd.value.into();
        Message::Notification(Notification {
            method: "mining.set_difficulty".to_string(),
            params: (&[value][..]).into(),
        })
    }
}

impl TryFrom<Notification> for SetDifficulty {
    type Error = ParsingMethodError;

    fn try_from(msg: Notification) -> Result<Self, Self::Error> {
        let params = msg
            .params
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(msg.params.clone()))?;
        let (value,) = match &params[..] {
            [a] => (a
                .as_f64()
                .ok_or_else(|| ParsingMethodError::not_float_from_value(a.clone()))?,),
            _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
        };
        Ok(SetDifficulty { value })
    }
}

/// SetExtranonce message (sent if we subscribed with `ExtranonceSubscribe`)
///
/// mining.set_extranonce("extranonce1", extranonce2_size)
///
/// These values, when provided, replace the initial subscription values beginning with the next
/// mining.notify job.
///
/// check if it is a Notification or a StandardRequest this implementation assume that it is a
/// Notification
#[derive(Debug, Clone)]
pub struct SetExtranonce<'a> {
    pub extra_nonce1: Extranonce<'a>,
    pub extra_nonce2_size: usize,
}

impl From<SetExtranonce<'_>> for Message {
    fn from(se: SetExtranonce) -> Self {
        let extra_nonce1: Value = se.extra_nonce1.into();
        let extra_nonce2_size: Value = se.extra_nonce2_size.into();
        Message::Notification(Notification {
            method: "mining.set_extranonce".to_string(),
            params: (&[extra_nonce1, extra_nonce2_size][..]).into(),
        })
    }
}

impl TryFrom<Notification> for SetExtranonce<'_> {
    type Error = ParsingMethodError;

    fn try_from(msg: Notification) -> Result<Self, Self::Error> {
        let params = msg
            .params
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(msg.params.clone()))?;
        let (extra_nonce1, extra_nonce2_size) = match &params[..] {
            [JString(a), JNumber(b)] => (
                Extranonce::try_from(hex::decode(a)?)?,
                b.as_u64()
                    .ok_or_else(|| ParsingMethodError::not_unsigned_from_value(b.clone()))?
                    as usize,
            ),
            _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
        };
        Ok(SetExtranonce {
            extra_nonce1,
            extra_nonce2_size,
        })
    }
}

#[derive(Debug, Clone)]
/// Server may arbitrarily adjust version mask
pub struct SetVersionMask {
    version_mask: HexU32Be,
}

impl From<SetVersionMask> for Message {
    fn from(sv: SetVersionMask) -> Self {
        let version_mask: Value = sv.version_mask.into();
        Message::Notification(Notification {
            method: "mining.set_version".to_string(),
            params: (&[version_mask][..]).into(),
        })
    }
}

impl TryFrom<Notification> for SetVersionMask {
    type Error = ParsingMethodError;

    fn try_from(msg: Notification) -> Result<Self, Self::Error> {
        let params = msg
            .params
            .as_array()
            .ok_or_else(|| ParsingMethodError::not_array_from_value(msg.params.clone()))?;
        let version_mask = match &params[..] {
            [JString(a)] => a.as_str().try_into()?,
            _ => return Err(ParsingMethodError::wrong_args_from_value(msg.params)),
        };
        Ok(SetVersionMask { version_mask })
    }
}

//pub struct Authorize(pub crate::json_rpc::Response, pub String);

/// Authorize and Submit responsed are identical
#[derive(Debug, Clone)]
pub struct GeneralResponse {
    pub id: u64,
    result: bool,
}

impl GeneralResponse {
    pub fn into_authorize(self, prev_request_name: String) -> Authorize {
        Authorize {
            id: self.id,
            authorized: self.result,
            prev_request_name,
        }
    }
    pub fn into_submit(self) -> Submit {
        Submit {
            id: self.id,
            is_ok: self.result,
        }
    }
}

impl TryFrom<&Response> for GeneralResponse {
    type Error = ParsingMethodError;

    fn try_from(msg: &Response) -> Result<Self, Self::Error> {
        let id = msg.id;
        let result = msg.result.as_bool().ok_or_else(|| {
            ParsingMethodError::ImpossibleToParseResultField(Box::new(msg.clone()))
        })?;
        Ok(GeneralResponse { id, result })
    }
}

#[derive(Debug, Clone)]
pub struct Authorize {
    pub id: u64,
    authorized: bool,
    pub prev_request_name: String,
}

impl Authorize {
    pub fn is_ok(&self) -> bool {
        self.authorized
    }

    pub fn user_name(self) -> String {
        self.prev_request_name
    }
}

#[derive(Debug, Clone)]
pub struct Submit {
    pub id: u64,
    is_ok: bool,
}

impl Submit {
    pub fn is_ok(&self) -> bool {
        self.is_ok
    }
}

/// mining.subscribe
/// mining.subscribe("user agent/version", "extranonce1")
/// The optional second parameter specifies a mining.notify subscription id the client wishes to
/// resume working with (possibly due to a dropped connection). If provided, a server MAY (at its
/// option) issue the connection the same extranonce1. Note that the extranonce1 may be the same
/// (allowing a resumed connection) even if the subscription id is changed!
///
/// The client receives a result:
///
///
/// The result contains three items:
///
///    Subscriptions. - An array of 2-item tuples, each with a subscription type and id.
///
///    ExtraNonce1. - Hex-encoded, per-connection unique string which will be used for creating
///    generation transactions later.
///
///    ExtraNonce2_size. - The number of bytes that the miner users for its ExtraNonce2 counter.
#[derive(Debug, Clone)]
pub struct Subscribe<'a> {
    pub id: u64,
    pub extra_nonce1: Extranonce<'a>,
    pub extra_nonce2_size: usize,
    pub subscriptions: Vec<(String, String)>,
}

impl From<Subscribe<'_>> for Message {
    fn from(su: Subscribe) -> Self {
        let extra_nonce1: Value = su.extra_nonce1.into();
        let extra_nonce2_size: Value = su.extra_nonce2_size.into();
        let subscriptions: Vec<Value> = su
            .subscriptions
            .iter()
            .map(|x| JArrary(vec![JString(x.0.clone()), JString(x.1.clone())]))
            .collect();
        let subscriptions: Value = subscriptions.into();
        Message::OkResponse(Response {
            id: su.id,
            error: None,
            result: (&[subscriptions, extra_nonce1, extra_nonce2_size][..]).into(),
        })
    }
}

impl TryFrom<&Response> for Subscribe<'_> {
    type Error = ParsingMethodError;

    fn try_from(msg: &Response) -> Result<Self, Self::Error> {
        let id = msg.id;
        let params = msg.result.as_array().ok_or_else(|| {
            ParsingMethodError::ImpossibleToParseResultField(Box::new(msg.clone()))
        })?;
        let (subscriptions_, extra_nonce1, extra_nonce2_size) = match &params[..] {
            [JArrary(a), JString(b), JNumber(c)] => (
                a,
                // infallible
                b.as_str().try_into()?,
                c.as_u64().ok_or_else(|| {
                    ParsingMethodError::ImpossibleToParseAsU64(Box::new(c.clone()))
                })? as usize,
            ),
            _ => return Err(ParsingMethodError::UnexpectedArrayParams(params.clone())),
        };
        let mut subscriptions: Vec<(String, String)> = vec![];
        for s in subscriptions_ {
            // we already checked that subscriptions_ is an array
            let s = s.as_array().unwrap();
            if s.len() != 2 {
                return Err(ParsingMethodError::UnexpectedArrayParams(params.clone()));
            };
            let s = (
                s[0].as_str()
                    .ok_or_else(|| ParsingMethodError::UnexpectedArrayParams(params.clone()))?
                    .to_string(),
                s[1].as_str()
                    .ok_or_else(|| ParsingMethodError::UnexpectedArrayParams(params.clone()))?
                    .to_string(),
            );
            subscriptions.push(s);
        }
        Ok(Subscribe {
            id,
            extra_nonce1,
            extra_nonce2_size,
            subscriptions,
        })
    }
}

#[derive(Debug, Clone)]
pub struct Configure {
    pub id: u64,
    pub version_rolling: Option<VersionRollingParams>,
    pub minimum_difficulty: Option<bool>,
}

impl Configure {
    pub fn version_rolling_mask(&self) -> Option<HexU32Be> {
        match &self.version_rolling {
            None => None,
            Some(a) => {
                if a.version_rolling {
                    Some(a.version_rolling_mask.clone())
                } else {
                    None
                }
            }
        }
    }
    pub fn version_rolling_min_bit(&self) -> Option<HexU32Be> {
        match &self.version_rolling {
            None => None,
            Some(a) => {
                if a.version_rolling {
                    Some(a.version_rolling_min_bit_count.clone())
                } else {
                    None
                }
            }
        }
    }
}

impl From<Configure> for Message {
    fn from(co: Configure) -> Self {
        let mut params = serde_json::Map::new();
        if let Some(version_rolling_) = co.version_rolling {
            let mut version_rolling: serde_json::Map<String, Value> = version_rolling_.into();
            params.append(&mut version_rolling);
        };
        if let Some(min_diff) = co.minimum_difficulty {
            let minimum_difficulty: Value = min_diff.into();
            params.insert("minimum-difficulty".to_string(), minimum_difficulty);
        };
        Message::OkResponse(Response {
            id: co.id,
            error: None,
            result: serde_json::Value::Object(params),
        })
    }
}

impl TryFrom<&Response> for Configure {
    type Error = ParsingMethodError;

    fn try_from(msg: &Response) -> Result<Self, ParsingMethodError> {
        let id = msg.id;
        let params = msg.result.as_object().ok_or_else(|| {
            ParsingMethodError::ImpossibleToParseResultField(Box::new(msg.clone()))
        })?;

        let version_rolling_ = params.get("version-rolling");
        let version_rolling_mask = params.get("version-rolling.mask");
        let version_rolling_min_bit_count = params.get("version-rolling.min-bit-count");
        let minimum_difficulty = params.get("minimum-difficulty");

        // Deserialize version-rolling response.
        // Composed by 3 fields:
        //   version-rolling (required),
        //   version-rolling.mask (required)
        //   version-rolling.min-bit-count (optional)
        let version_rolling: Option<VersionRollingParams>;
        if version_rolling_.is_some() && version_rolling_mask.is_some() {
            let vr: bool = version_rolling_
                .unwrap()
                .as_bool()
                .ok_or_else(|| ParsingMethodError::UnexpectedObjectParams(params.clone()))?;

            let version_rolling_mask: HexU32Be = version_rolling_mask
                .unwrap()
                .as_str()
                .ok_or_else(|| ParsingMethodError::UnexpectedObjectParams(params.clone()))?
                .try_into()?;

            // version-rolling.min-bit-count is often not returned by stratum servers,
            // but min-bit-count should be taken into consideration in the returned mask
            let version_rolling_min_bit_count: HexU32Be = match version_rolling_min_bit_count {
                Some(version_rolling_min_bit_count) => version_rolling_min_bit_count
                    .as_str()
                    .ok_or_else(|| ParsingMethodError::UnexpectedObjectParams(params.clone()))?
                    .try_into()?,
                None => HexU32Be(0),
            };

            version_rolling = Some(VersionRollingParams {
                version_rolling: vr,
                version_rolling_mask,
                version_rolling_min_bit_count,
            });
        } else if version_rolling_.is_none()
            && version_rolling_mask.is_none()
            && version_rolling_min_bit_count.is_none()
        {
            version_rolling = None;
        } else {
            return Err(ParsingMethodError::UnexpectedObjectParams(params.clone()));
        };

        let minimum_difficulty = match minimum_difficulty {
            Some(a) => Some(
                a.as_bool()
                    .ok_or_else(|| ParsingMethodError::UnexpectedObjectParams(params.clone()))?,
            ),
            None => None,
        };

        Ok(Configure {
            id,
            version_rolling,
            minimum_difficulty,
        })
    }
}

#[derive(Debug, Clone)]
pub struct VersionRollingParams {
    pub version_rolling: bool,
    pub version_rolling_mask: HexU32Be,
    pub version_rolling_min_bit_count: HexU32Be,
}

#[test]
fn configure_response_parsing_all_fields() {
    let client_response_str = r#"{"id":0,
            "result":{
                "version-rolling":true,
                "version-rolling.mask":"1fffe000",
                "version-rolling.min-bit-count":"00000005",
                "minimum-difficulty":false
            }
        }"#;
    let client_response = serde_json::from_str(client_response_str).unwrap();
    let server_configure = Configure::try_from(&client_response).unwrap();
    println!("{server_configure:?}");

    let version_rolling = server_configure.version_rolling.unwrap();
    assert!(version_rolling.version_rolling);
    assert_eq!(version_rolling.version_rolling_mask, HexU32Be(0x1fffe000));
    assert_eq!(version_rolling.version_rolling_min_bit_count, HexU32Be(5));

    assert_eq!(server_configure.minimum_difficulty, Some(false));
}

#[test]
fn configure_response_parsing_no_vr_min_bit_count() {
    let client_response_str = r#"{"id":0,
            "result":{
                "version-rolling":true,
                "version-rolling.mask":"1fffe000",
                "minimum-difficulty":false
            }
        }"#;
    let client_response = serde_json::from_str(client_response_str).unwrap();
    let server_configure = Configure::try_from(&client_response).unwrap();
    println!("{server_configure:?}");

    let version_rolling = server_configure.version_rolling.unwrap();
    assert!(version_rolling.version_rolling);
    assert_eq!(version_rolling.version_rolling_mask, HexU32Be(0x1fffe000));
    assert_eq!(version_rolling.version_rolling_min_bit_count, HexU32Be(0));

    assert_eq!(server_configure.minimum_difficulty, Some(false));
}

impl VersionRollingParams {
    pub fn new(
        version_rolling_mask: HexU32Be,
        version_rolling_min_bit_count: HexU32Be,
    ) -> Result<Self, Error<'static>> {
        // 0x1FFFE000 should be configured
        let negotiated_mask = HexU32Be(version_rolling_mask.clone() & 0x1FFFE000);

        let version_head_ok = negotiated_mask.0 >> 29 == 0;
        let version_tail_ok = negotiated_mask.0 << 19 == 0;
        if version_head_ok && version_tail_ok {
            Ok(VersionRollingParams {
                version_rolling: true,
                version_rolling_mask: negotiated_mask,
                version_rolling_min_bit_count,
            })
        } else {
            Err(Error::InvalidVersionMask(version_rolling_mask))
        }
    }
}

impl From<VersionRollingParams> for serde_json::Map<String, Value> {
    fn from(vp: VersionRollingParams) -> Self {
        let version_rolling: Value = vp.version_rolling.into();
        let version_rolling_mask: Value = vp.version_rolling_mask.into();
        let version_rolling_min_bit_count: Value = vp.version_rolling_min_bit_count.into();
        let mut params = serde_json::Map::new();
        params.insert("version-rolling".to_string(), version_rolling);
        params.insert("version-rolling.mask".to_string(), version_rolling_mask);
        params.insert(
            "version-rolling.min-bit-count".to_string(),
            version_rolling_min_bit_count,
        );
        params
    }
}
</file>

<file path="stratum-1.4.0/protocols/v1/src/utils.rs">
use crate::error::{self, Error};
use binary_sv2::{B032, U256};
use bitcoin_hashes::hex::{FromHex, ToHex};
use byteorder::{BigEndian, ByteOrder, LittleEndian, WriteBytesExt};
use serde::{Deserialize, Serialize};
use serde_json::Value;
use std::{convert::TryFrom, mem::size_of, ops::BitAnd};

/// Helper type that allows simple serialization and deserialization of byte vectors
/// that are represented as hex strings in JSON.
/// Extranonce must be less than or equal to 32 bytes.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct Extranonce<'a>(pub B032<'a>);

impl Extranonce<'_> {
    pub fn len(&self) -> usize {
        self.0.inner_as_ref().len()
    }
    pub fn is_empty(&self) -> bool {
        self.0.inner_as_ref().is_empty()
    }
}

impl<'a> TryFrom<Vec<u8>> for Extranonce<'a> {
    type Error = Error<'a>;
    fn try_from(value: Vec<u8>) -> Result<Self, Error<'a>> {
        Ok(Extranonce(B032::try_from(value)?))
    }
}

impl<'a> From<Extranonce<'a>> for Vec<u8> {
    fn from(v: Extranonce<'a>) -> Self {
        v.0.to_vec()
    }
}

impl<'a> From<Extranonce<'a>> for Value {
    fn from(eb: Extranonce<'a>) -> Self {
        Into::<String>::into(eb).into()
    }
}

/// fix for error on odd-length hex sequences
/// FIXME: find a nicer solution
fn hex_decode(s: &str) -> Result<Vec<u8>, Error<'static>> {
    if s.len() % 2 != 0 {
        Ok(hex::decode(format!("0{s}"))?)
    } else {
        Ok(hex::decode(s)?)
    }
}

impl<'a> TryFrom<&str> for Extranonce<'a> {
    type Error = error::Error<'a>;

    fn try_from(value: &str) -> Result<Self, Error<'a>> {
        Ok(Extranonce(B032::try_from(hex_decode(value)?)?))
    }
}

impl<'a> From<Extranonce<'a>> for String {
    fn from(bytes: Extranonce<'a>) -> String {
        hex::encode(bytes.0)
    }
}

impl BitAnd<u32> for HexU32Be {
    type Output = u32;

    fn bitand(self, rhs: u32) -> Self::Output {
        self.0 & rhs
    }
}

impl<'a> From<B032<'a>> for Extranonce<'a> {
    fn from(b: B032<'a>) -> Self {
        Extranonce(b)
    }
}

/// Big-endian alternative of the HexU32
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct HexU32Be(pub u32);

impl HexU32Be {
    pub fn check_mask(&self, mask: &HexU32Be) -> bool {
        ((!self.0) & mask.0) == 0
    }
}

impl From<HexU32Be> for Value {
    fn from(eu: HexU32Be) -> Self {
        Into::<String>::into(eu).into()
    }
}

impl TryFrom<&str> for HexU32Be {
    type Error = Error<'static>;

    fn try_from(value: &str) -> Result<Self, Error<'static>> {
        let expected_len = 8;
        let delta_len = expected_len - value.len();
        let mut prefix = "".to_string();
        for _ in 0..delta_len {
            prefix.push('0');
        }
        prefix.push_str(value);
        let parsed_bytes: [u8; 4] = FromHex::from_hex(&prefix)?;
        Ok(HexU32Be(u32::from_be_bytes(parsed_bytes)))
    }
}

/// Helper Serializer
impl From<HexU32Be> for String {
    fn from(v: HexU32Be) -> Self {
        v.0.to_be_bytes().to_hex()
    }
}

impl From<u32> for HexU32Be {
    fn from(a: u32) -> Self {
        HexU32Be(a)
    }
}

//Example of serialization for testing purpose
impl Serialize for HexU32Be {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        let serialized_string = self.0.to_be_bytes().to_hex();
        serializer.serialize_str(&serialized_string)
    }
}

//Example of deserialization for testing purpose
impl<'de> Deserialize<'de> for HexU32Be {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        let hex_string: String = Deserialize::deserialize(deserializer)?;

        match u32::from_str_radix(&hex_string, 16) {
            Ok(value) => Ok(HexU32Be(value)),
            Err(_) => Err(serde::de::Error::custom("Invalid hex value")),
        }
    }
}

/// PrevHash in Stratum V1 has brain-damaged serialization as it swaps bytes of every u32 word
/// into big endian. Therefore, we need a special type for it
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct PrevHash<'a>(pub U256<'a>);

impl<'a> From<PrevHash<'a>> for Vec<u8> {
    fn from(p_hash: PrevHash<'a>) -> Self {
        p_hash.0.to_vec()
    }
}

impl<'a> TryFrom<&str> for PrevHash<'a> {
    type Error = Error<'a>;

    fn try_from(value: &str) -> Result<Self, Error<'a>> {
        // Reorder PrevHash will be stored via this cursor
        // let mut prev_hash_cursor = std::io::Cursor::new([0_u8; 32]);
        let mut prev_hash_arr = [0_u8; 32];

        // Decode the plain byte array and sanity check
        let prev_hash_stratum_order = hex_decode(value)?;

        match prev_hash_stratum_order.len() {
            32 => {
                // Swap every u32 from big endian to little endian byte order
                for (chunk, mut arr_chunks) in prev_hash_stratum_order
                    .chunks(size_of::<u32>())
                    .zip(prev_hash_arr.chunks_mut(size_of::<u32>()))
                {
                    let prev_hash_word = BigEndian::read_u32(chunk);
                    arr_chunks
                        .write_u32::<LittleEndian>(prev_hash_word)
                        .expect("Internal error: Could not write buffer");
                }
                Ok(PrevHash(prev_hash_arr.into()))
            }
            _ => Err(error::Error::BadBytesConvert(
                binary_sv2::Error::InvalidU256(prev_hash_stratum_order.len()),
            )),
        }
    }
}

impl From<PrevHash<'_>> for Value {
    fn from(ph: PrevHash) -> Self {
        Into::<String>::into(ph).into()
    }
}

/// Helper Serializer that peforms the reverse process of converting the prev hash into stratum V1
/// ordering
impl From<PrevHash<'_>> for String {
    fn from(v: PrevHash) -> Self {
        let mut prev_hash_stratum_cursor = std::io::Cursor::new(Vec::new());
        // swap every u32 from little endian to big endian
        for chunk in v.0.inner_as_ref().chunks(size_of::<u32>()) {
            let prev_hash_word = LittleEndian::read_u32(chunk);
            prev_hash_stratum_cursor
                .write_u32::<BigEndian>(prev_hash_word)
                .expect("Internal error: Could not write buffer");
        }
        hex::encode(prev_hash_stratum_cursor.into_inner())
    }
}

// / Referencing the internal part of hex bytes
impl AsRef<[u8]> for PrevHash<'_> {
    fn as_ref(&self) -> &[u8] {
        self.0.inner_as_ref()
    }
}

/// Referencing the internal part of hex bytes
impl<'a> AsRef<U256<'a>> for PrevHash<'a> {
    fn as_ref(&self) -> &U256<'a> {
        &self.0
    }
}

/// Referencing the internal part of hex bytes
impl AsRef<[u8]> for Extranonce<'_> {
    fn as_ref(&self) -> &[u8] {
        self.0.inner_as_ref()
    }
}

#[derive(Clone, Debug, PartialEq, Eq)]
pub struct MerkleNode<'a>(pub U256<'a>);

impl MerkleNode<'_> {
    pub fn is_empty(&self) -> bool {
        self.0.inner_as_ref().is_empty()
    }
}

impl<'a> TryFrom<Vec<u8>> for MerkleNode<'a> {
    type Error = Error<'a>;

    fn try_from(value: Vec<u8>) -> Result<Self, Error<'a>> {
        //TODO handle error
        Ok(MerkleNode(U256::try_from(value).unwrap()))
    }
}

impl<'a> From<MerkleNode<'a>> for Vec<u8> {
    fn from(v: MerkleNode<'a>) -> Self {
        v.0.to_vec()
    }
}

impl<'a> From<MerkleNode<'a>> for Value {
    fn from(eb: MerkleNode<'a>) -> Self {
        Into::<String>::into(eb).into()
    }
}

/// Referencing the internal part of hex bytes
impl AsRef<[u8]> for MerkleNode<'_> {
    fn as_ref(&self) -> &[u8] {
        self.0.inner_as_ref()
    }
}

impl<'a> TryFrom<&str> for MerkleNode<'a> {
    type Error = Error<'a>;

    fn try_from(value: &str) -> Result<Self, Error<'a>> {
        //TODO handle error
        Ok(MerkleNode(U256::try_from(hex_decode(value)?).unwrap()))
    }
}

impl<'a> From<MerkleNode<'a>> for String {
    fn from(bytes: MerkleNode<'a>) -> String {
        hex::encode(bytes.0)
    }
}

/// Helper type that allows simple serialization and deserialization of byte vectors
/// that are represented as hex strings in JSON.
/// HexBytes must be less than or equal to 32 bytes.
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct HexBytes(Vec<u8>);

impl HexBytes {
    pub fn len(&self) -> usize {
        self.0.len()
    }
    pub fn is_empty(&self) -> bool {
        self.0.is_empty()
    }
}

impl From<Vec<u8>> for HexBytes {
    fn from(value: Vec<u8>) -> Self {
        HexBytes(value)
    }
}

impl From<HexBytes> for Vec<u8> {
    fn from(v: HexBytes) -> Self {
        v.0
    }
}

impl From<HexBytes> for Value {
    fn from(eb: HexBytes) -> Self {
        Into::<String>::into(eb).into()
    }
}

/// Referencing the internal part of hex bytes
impl AsRef<Vec<u8>> for HexBytes {
    fn as_ref(&self) -> &Vec<u8> {
        &self.0
    }
}

impl TryFrom<&str> for HexBytes {
    type Error = Error<'static>;

    fn try_from(value: &str) -> Result<Self, Error<'static>> {
        Ok(HexBytes(hex_decode(value)?))
    }
}

impl From<HexBytes> for String {
    fn from(bytes: HexBytes) -> String {
        hex::encode(bytes.0)
    }
}

//Example of serialization for testing purpose
impl Serialize for HexBytes {
    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: serde::Serializer,
    {
        let bytes = self.0.as_ref();
        let serialized_string = String::from_utf8_lossy(bytes);
        serializer.serialize_str(&serialized_string)
    }
}

//Example of deserialization for testing purpose
impl<'de> Deserialize<'de> for HexBytes {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        let hex_string: String = Deserialize::deserialize(deserializer)?;
        let bytes = hex_string.as_bytes().to_vec();
        Ok(HexBytes(bytes))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[quickcheck_macros::quickcheck]
    fn test_prev_hash(mut bytes: Vec<u8>) -> bool {
        bytes.resize(32, 0);
        let be_hex = bytes.to_hex();
        let me = PrevHash::try_from(be_hex.clone().as_str()).unwrap();
        let back_to_hex = String::from(me.clone());
        let back_to_hex_value = Value::from(me.clone());
        let value_to_string = back_to_hex_value.as_str().unwrap();

        let chunk_size: usize = size_of::<u32>();
        let me_chunks = me.clone().0.to_vec();
        let me_chunks = me_chunks.chunks(chunk_size);
        for (be_chunk, le_chunk) in bytes.clone().chunks(chunk_size).zip(me_chunks) {
            let le_chunk = [le_chunk[0], le_chunk[1], le_chunk[2], le_chunk[3]];
            let be_chunk = [be_chunk[0], be_chunk[1], be_chunk[2], be_chunk[3]];
            let le_u32 = u32::from_le_bytes(le_chunk);
            let be_u32 = u32::from_be_bytes(be_chunk);

            if le_u32 != be_u32 {
                return false;
            }
        }

        be_hex == back_to_hex && be_hex == value_to_string
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/Cargo.toml">
[package]
name = "binary_sv2"
version = "3.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 data format"
documentation = "https://docs.rs/binary_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_codec_sv2 = { path = "codec", version = "^2.0.0" }
derive_codec_sv2 = { path = "derive_codec", version = "^1.0.0" }

[features]
prop_test = ["binary_codec_sv2/prop_test"]
with_buffer_pool = ["binary_codec_sv2/with_buffer_pool"]

[package.metadata.docs.rs]
features = ["with_buffer_pool"]
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/.gitignore">
/target
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/Cargo.toml">
[package]
name = "binary_codec_sv2"
version = "2.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 data format"
documentation = "https://docs.rs/binary_codec_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
quickcheck = { version = "1.0.0", optional = true }
buffer_sv2 = { path = "../../../../utils/buffer", optional=true, version = "^2.0.0" }

[features]
no_std = []
default = ["no_std"]
prop_test = ["quickcheck"]
with_buffer_pool = ["buffer_sv2"]

[package.metadata.docs.rs]
features = ["with_buffer_pool"]
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/README.md">
# binary_codec_sv2

[![crates.io](https://img.shields.io/crates/v/binary_codec_sv2.svg)](https://crates.io/crates/binary_codec_sv2)  
[![docs.rs](https://docs.rs/binary_codec_sv2/badge.svg)](https://docs.rs/binary_codec_sv2)  
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)  
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=binary_codec_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)  

`binary_codec_sv2` is a `no_std` Rust crate that helps serialize and de-serialize binary data into and from Stratum V2 types.

## Key Features

- **Comprehensive Encoding and Decoding**: Provides traits (`Encodable`, `Decodable`) for converting between Rust and SV2 data types/structures.  
- **Support for Complex Data Structures**: Handles primitives, nested structures, and protocol-specific types like `U24`, `U256`,`Str0255` and rest.  
- **Error Handling**: Robust mechanisms for managing encoding/decoding failures, including size mismatches and invalid data.  
- **Cross-Language Compatibility**: Utilities like `CVec` and `CError` ensure smooth integration with other programming languages.  
- **`no_std` Compatibility**: Fully supports constrained environments without the Rust standard library.  

## Sv2 Type Mapping

The crate supports the following mappings between Rust and SV2 types

| Rust Type   | Sv2 Type       |  
|-------------|----------------|  
| `bool`      | `BOOL`         |  
| `u8`        | `U8`           |  
| `u16`       | `U16`          |  
| `U24`       | `U24`          |  
| `u32`       | `U32`          |  
| `u64`       | `U64`          |  
| `f32`       | `F32`          |  
| `Str0255`   | `STRO_255`     |  
| `Signature` | `SIGNATURE`    |  
| `[u8]`      | `BYTES`        |  
| `Seq0255`   | `SEQ0_255[T]`  |  
| `Seq064K`   | `SEQ0_64K[T]`  | 

## Installation

Add `binary_codec_sv2` to your project by running:

```sh
cargo add binary_codec_sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/decodable.rs">
use crate::{
    codec::{GetSize, SizeHint},
    datatypes::{Signature, Sv2DataType, U32AsRef, B016M, B0255, B032, B064K, U24, U256},
    Error,
};
use alloc::vec::Vec;
use core::convert::TryFrom;
#[cfg(not(feature = "no_std"))]
use std::io::{Cursor, Read};

/// Custom deserialization of types from binary data.
///
/// Defines the process of reconstructing a type from a sequence of bytes. It handles both simple
/// and nested or complex data structures.
pub trait Decodable<'a>: Sized {
    /// Defines the expected structure of a type based on binary data.
    ///
    /// Returns a vector of [`FieldMarker`]s, each representing a component of the structure.
    /// Useful for guiding the decoding process.
    fn get_structure(data: &[u8]) -> Result<Vec<FieldMarker>, Error>;

    /// Constructs the type from a vector of decoded fields.
    ///
    /// After the data has been split into fields, this method combines those fields
    /// back into the original type, handling nested structures or composite fields.
    fn from_decoded_fields(data: Vec<DecodableField<'a>>) -> Result<Self, Error>;

    /// Decodes the type from raw bytes.
    ///
    /// Orchestrates the decoding process, calling `get_structure` to break down
    /// the raw data, decoding each field, and then using `from_decoded_fields` to reassemble
    /// the fields into the original type.
    fn from_bytes(data: &'a mut [u8]) -> Result<Self, Error> {
        let structure = Self::get_structure(data)?;
        let mut fields = Vec::new();
        let mut tail = data;

        for field in structure {
            let field_size = field.size_hint_(tail, 0)?;
            if field_size > tail.len() {
                return Err(Error::DecodableConversionError);
            }
            let (head, t) = tail.split_at_mut(field_size);
            tail = t;
            fields.push(field.decode(head)?);
        }
        Self::from_decoded_fields(fields)
    }

    /// Converts a readable input to self representation.
    ///
    /// Reads data from an input which implements [`std::ioRead`] and constructs the original struct
    /// out of it.
    #[cfg(not(feature = "no_std"))]
    fn from_reader(reader: &mut impl Read) -> Result<Self, Error> {
        let mut data = Vec::new();
        reader.read_to_end(&mut data)?;

        let structure = Self::get_structure(&data[..])?;

        let mut fields = Vec::new();
        let mut reader = Cursor::new(data);

        for field in structure {
            fields.push(field.from_reader(&mut reader)?);
        }
        Self::from_decoded_fields(fields)
    }
}

// Primitive data marker.
//
// Fundamental data types that can be passed to a decoder to define the structure of the type to be
// decoded in a standardized way.
#[derive(Debug, Clone, Copy)]
pub enum PrimitiveMarker {
    U8,
    U16,
    Bool,
    U24,
    U256,
    Signature,
    U32,
    U32AsRef,
    F32,
    U64,
    B032,
    B0255,
    B064K,
    B016M,
}

/// Recursive enum representing data structure fields.
///
/// A `FieldMarker` can either be a primitive or a nested structure. The marker helps the decoder
/// understand the layout and type of each field in the data, guiding the decoding process.
#[derive(Debug, Clone)]
pub enum FieldMarker {
    /// A primitive data type.
    Primitive(PrimitiveMarker),

    /// A structured type composed of multiple fields, allowing for nested data.
    Struct(Vec<FieldMarker>),
}

/// Trait for retrieving the [`FieldMarker`] associated with a type.
///
/// Provides a standardized way to retrieve a `FieldMarker` for a type, allowing the protocol to
/// identify the structure and layout of data fields during decoding.
pub trait GetMarker {
    /// Defines the structure of a type for decoding purposes, supporting both primitive and
    /// structured types. It helps getting a marker for a type.
    fn get_marker() -> FieldMarker;
}

// Represents a list of decode-able primitive data types.
//
#[derive(Debug)]
pub enum DecodablePrimitive<'a> {
    U8(u8),
    U16(u16),
    Bool(bool),
    U24(U24),
    U256(U256<'a>),
    Signature(Signature<'a>),
    U32(u32),
    U32AsRef(U32AsRef<'a>),
    F32(f32),
    U64(u64),
    B032(B032<'a>),
    B0255(B0255<'a>),
    B064K(B064K<'a>),
    B016M(B016M<'a>),
}

/// Recursive enum representing a Decode-able field.
///
/// May be primitive or a nested struct.
///
/// Once the raw data is decoded, it is either classified as a primitive (e.g., integer, Boolean)
/// or a struct, which may itself contain multiple decoded fields. This type encapsulates that
/// distinction.
#[derive(Debug)]
pub enum DecodableField<'a> {
    /// Primitive field.
    Primitive(DecodablePrimitive<'a>),

    /// Structured field, allowing for nested data structures.
    Struct(Vec<DecodableField<'a>>),
}

impl SizeHint for PrimitiveMarker {
    // PrimitiveMarker needs introspection to return a size hint. This method is not implementable.
    fn size_hint(_data: &[u8], _offset: usize) -> Result<usize, Error> {
        unimplemented!()
    }

    fn size_hint_(&self, data: &[u8], offset: usize) -> Result<usize, Error> {
        match self {
            Self::U8 => u8::size_hint(data, offset),
            Self::U16 => u16::size_hint(data, offset),
            Self::Bool => bool::size_hint(data, offset),
            Self::U24 => U24::size_hint(data, offset),
            Self::U256 => U256::size_hint(data, offset),
            Self::Signature => Signature::size_hint(data, offset),
            Self::U32 => u32::size_hint(data, offset),
            Self::U32AsRef => U32AsRef::size_hint(data, offset),
            Self::F32 => f32::size_hint(data, offset),
            Self::U64 => u64::size_hint(data, offset),
            Self::B032 => B032::size_hint(data, offset),
            Self::B0255 => B0255::size_hint(data, offset),
            Self::B064K => B064K::size_hint(data, offset),
            Self::B016M => B016M::size_hint(data, offset),
        }
    }
}

impl SizeHint for FieldMarker {
    // FieldMarker need introspection to return a size hint. This method is not implementeable
    fn size_hint(_data: &[u8], _offset: usize) -> Result<usize, Error> {
        unimplemented!()
    }

    fn size_hint_(&self, data: &[u8], offset: usize) -> Result<usize, Error> {
        match self {
            Self::Primitive(p) => p.size_hint_(data, offset),
            Self::Struct(ps) => {
                let mut size = 0;
                for p in ps {
                    size += p.size_hint_(data, offset + size)?;
                }
                Ok(size)
            }
        }
    }
}

impl SizeHint for Vec<FieldMarker> {
    // FieldMarker need introspection to return a size hint. This method is not implementeable
    fn size_hint(_data: &[u8], _offset: usize) -> Result<usize, Error> {
        unimplemented!()
    }

    fn size_hint_(&self, data: &[u8], offset: usize) -> Result<usize, Error> {
        let mut size = 0;
        for field in self {
            let field_size = field.size_hint_(data, offset + size)?;
            size += field_size;
        }
        Ok(size)
    }
}

impl From<PrimitiveMarker> for FieldMarker {
    fn from(v: PrimitiveMarker) -> Self {
        FieldMarker::Primitive(v)
    }
}

impl TryFrom<Vec<FieldMarker>> for FieldMarker {
    type Error = crate::Error;

    fn try_from(mut v: Vec<FieldMarker>) -> Result<Self, crate::Error> {
        match v.len() {
            // It shouldn't be possible to call this function with a void Vec but for safety
            // reasons it is implemented with TryFrom and not From if needed should be possible
            // to use From and just panic
            0 => Err(crate::Error::VoidFieldMarker),
            // This is always safe: if v.len is 1 pop can not fail
            1 => Ok(v.pop().unwrap()),
            _ => Ok(FieldMarker::Struct(v)),
        }
    }
}

impl<'a> From<DecodableField<'a>> for Vec<DecodableField<'a>> {
    fn from(v: DecodableField<'a>) -> Self {
        match v {
            DecodableField::Primitive(p) => vec![DecodableField::Primitive(p)],
            DecodableField::Struct(ps) => ps,
        }
    }
}

impl PrimitiveMarker {
    // Decodes a primitive value from a byte slice at the given offset, returning the corresponding
    // `DecodablePrimitive`. The specific decoding logic depends on the type of the primitive (e.g.,
    // `u8`, `u16`, etc.).
    fn decode<'a>(&self, data: &'a mut [u8], offset: usize) -> DecodablePrimitive<'a> {
        match self {
            Self::U8 => DecodablePrimitive::U8(u8::from_bytes_unchecked(&mut data[offset..])),
            Self::U16 => DecodablePrimitive::U16(u16::from_bytes_unchecked(&mut data[offset..])),
            Self::Bool => DecodablePrimitive::Bool(bool::from_bytes_unchecked(&mut data[offset..])),
            Self::U24 => DecodablePrimitive::U24(U24::from_bytes_unchecked(&mut data[offset..])),
            Self::U256 => DecodablePrimitive::U256(U256::from_bytes_unchecked(&mut data[offset..])),
            Self::Signature => {
                DecodablePrimitive::Signature(Signature::from_bytes_unchecked(&mut data[offset..]))
            }
            Self::U32 => DecodablePrimitive::U32(u32::from_bytes_unchecked(&mut data[offset..])),
            Self::U32AsRef => {
                DecodablePrimitive::U32AsRef(U32AsRef::from_bytes_unchecked(&mut data[offset..]))
            }
            Self::F32 => DecodablePrimitive::F32(f32::from_bytes_unchecked(&mut data[offset..])),
            Self::U64 => DecodablePrimitive::U64(u64::from_bytes_unchecked(&mut data[offset..])),
            Self::B032 => DecodablePrimitive::B032(B032::from_bytes_unchecked(&mut data[offset..])),
            Self::B0255 => {
                DecodablePrimitive::B0255(B0255::from_bytes_unchecked(&mut data[offset..]))
            }
            Self::B064K => {
                DecodablePrimitive::B064K(B064K::from_bytes_unchecked(&mut data[offset..]))
            }
            Self::B016M => {
                DecodablePrimitive::B016M(B016M::from_bytes_unchecked(&mut data[offset..]))
            }
        }
    }

    // Decodes a primitive value from a reader stream, returning the corresponding
    // `DecodablePrimitive`. This is useful when reading data from a file or network socket,
    // where the data is not immediately available as a slice but must be read incrementally.
    #[allow(clippy::wrong_self_convention)]
    #[cfg(not(feature = "no_std"))]
    fn from_reader<'a>(&self, reader: &mut impl Read) -> Result<DecodablePrimitive<'a>, Error> {
        match self {
            Self::U8 => Ok(DecodablePrimitive::U8(u8::from_reader_(reader)?)),
            Self::U16 => Ok(DecodablePrimitive::U16(u16::from_reader_(reader)?)),
            Self::Bool => Ok(DecodablePrimitive::Bool(bool::from_reader_(reader)?)),
            Self::U24 => Ok(DecodablePrimitive::U24(U24::from_reader_(reader)?)),
            Self::U256 => Ok(DecodablePrimitive::U256(U256::from_reader_(reader)?)),
            Self::Signature => Ok(DecodablePrimitive::Signature(Signature::from_reader_(
                reader,
            )?)),
            Self::U32 => Ok(DecodablePrimitive::U32(u32::from_reader_(reader)?)),
            Self::U32AsRef => Ok(DecodablePrimitive::U32AsRef(U32AsRef::from_reader_(
                reader,
            )?)),
            Self::F32 => Ok(DecodablePrimitive::F32(f32::from_reader_(reader)?)),
            Self::U64 => Ok(DecodablePrimitive::U64(u64::from_reader_(reader)?)),
            Self::B032 => Ok(DecodablePrimitive::B032(B032::from_reader_(reader)?)),
            Self::B0255 => Ok(DecodablePrimitive::B0255(B0255::from_reader_(reader)?)),
            Self::B064K => Ok(DecodablePrimitive::B064K(B064K::from_reader_(reader)?)),
            Self::B016M => Ok(DecodablePrimitive::B016M(B016M::from_reader_(reader)?)),
        }
    }
}

impl GetSize for DecodablePrimitive<'_> {
    fn get_size(&self) -> usize {
        match self {
            DecodablePrimitive::U8(v) => v.get_size(),
            DecodablePrimitive::U16(v) => v.get_size(),
            DecodablePrimitive::Bool(v) => v.get_size(),
            DecodablePrimitive::U24(v) => v.get_size(),
            DecodablePrimitive::U256(v) => v.get_size(),
            DecodablePrimitive::Signature(v) => v.get_size(),
            DecodablePrimitive::U32(v) => v.get_size(),
            DecodablePrimitive::U32AsRef(v) => v.get_size(),
            DecodablePrimitive::F32(v) => v.get_size(),
            DecodablePrimitive::U64(v) => v.get_size(),
            DecodablePrimitive::B032(v) => v.get_size(),
            DecodablePrimitive::B0255(v) => v.get_size(),
            DecodablePrimitive::B064K(v) => v.get_size(),
            DecodablePrimitive::B016M(v) => v.get_size(),
        }
    }
}

impl FieldMarker {
    // Implements the decoding functionality for a `FieldMarker`.
    // Depending on whether the field is primitive or structured, this method decodes the
    // corresponding data. If the field is a structure, it recursively decodes each nested field
    // and returns the resulting `DecodableField`.
    pub(crate) fn decode<'a>(&self, data: &'a mut [u8]) -> Result<DecodableField<'a>, Error> {
        match self {
            Self::Primitive(p) => Ok(DecodableField::Primitive(p.decode(data, 0))),
            Self::Struct(ps) => {
                let mut decodeds = Vec::new();
                let mut tail = data;
                for p in ps {
                    let field_size = p.size_hint_(tail, 0)?;
                    let (head, t) = tail.split_at_mut(field_size);
                    tail = t;
                    decodeds.push(p.decode(head)?);
                }
                Ok(DecodableField::Struct(decodeds))
            }
        }
    }

    #[allow(clippy::wrong_self_convention)]
    #[cfg(not(feature = "no_std"))]
    #[allow(clippy::wrong_self_convention)]
    pub(crate) fn from_reader<'a>(
        &self,
        reader: &mut impl Read,
    ) -> Result<DecodableField<'a>, Error> {
        match self {
            Self::Primitive(p) => Ok(DecodableField::Primitive(p.from_reader(reader)?)),
            Self::Struct(ps) => {
                let mut decodeds = Vec::new();
                for p in ps {
                    decodeds.push(p.from_reader(reader)?);
                }
                Ok(DecodableField::Struct(decodeds))
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/encodable.rs">
use crate::{
    codec::GetSize,
    datatypes::{Signature, Sv2DataType, U32AsRef, B016M, B0255, B032, B064K, U24, U256},
    Error,
};
use alloc::vec::Vec;
#[cfg(not(feature = "no_std"))]
use std::io::{Error as E, Write};

/// The `Encodable` trait defines the interface for encoding a type into bytes.
///
/// The trait provides methods for serializing an instance of a type into a byte
/// array or writing it directly into an output writer. The trait is flexible,
/// allowing various types, including primitives, structures, and collections,
/// to implement custom serialization logic.
///
/// The trait offers two key methods for encoding:
///
/// - The first, `to_bytes`, takes a mutable byte slice as a destination buffer. This method encodes
///   the object directly into the provided buffer, returning the number of bytes written or an
///   error if the encoding process fails.
/// - The second, `to_writer`, (only available when not compiling for `no-std`) accepts a writer as
///   a destination for the encoded bytes, allowing the serialized data to be written to any
///   implementor of the `Write` trait.
///
/// Implementing types can define custom encoding logic, and this trait is
/// especially useful when dealing with different data structures that need
/// to be serialized for transmission.
pub trait Encodable {
    /// Encodes the object into the provided byte slice.
    ///
    /// The method uses the destination buffer `dst` to write the serialized
    /// bytes. It returns the number of bytes written on success or an `Error`
    /// if encoding fails.
    #[allow(clippy::wrong_self_convention)]
    fn to_bytes(self, dst: &mut [u8]) -> Result<usize, Error>;

    /// Write the encoded object into the provided writer.
    ///
    /// Serializes the object and writes it directly
    /// to the `dst` writer. It is only available in environments
    /// where `std` is available. If the encoding fails, error is
    /// returned.
    #[cfg(not(feature = "no_std"))]
    #[allow(clippy::wrong_self_convention)]
    fn to_writer(self, dst: &mut impl Write) -> Result<(), E>;
}

impl<'a, T: Into<EncodableField<'a>>> Encodable for T {
    #[allow(clippy::wrong_self_convention)]
    fn to_bytes(self, dst: &mut [u8]) -> Result<usize, Error> {
        let encoded_field = self.into();
        encoded_field.encode(dst, 0)
    }

    #[cfg(not(feature = "no_std"))]
    #[allow(clippy::wrong_self_convention, unconditional_recursion)]
    fn to_writer(self, dst: &mut impl Write) -> Result<(), E> {
        let encoded_field = self.into();
        encoded_field.to_writer(dst)
    }
}

/// The `EncodablePrimitive` enum defines primitive types  that can be encoded.
///
/// The enum represents various data types, such a integers, bool, and byte array
/// that can be encoded into a byte representation. Each variant holds a specific
/// type, and encoding logic is provided through the `encode` method.
#[derive(Debug)]
pub enum EncodablePrimitive<'a> {
    /// U8 Primitive, representing a byte
    U8(u8),
    /// Owned U8 Primitive, representing an owned byte
    OwnedU8(u8),
    /// U16 Primitive, representing a u16 type
    U16(u16),
    /// Bool Primitive, representing a bool type
    Bool(bool),
    /// U24 Primitive, representing a U24 type
    U24(U24),
    /// U256 Primitive, representing a U256 type
    U256(U256<'a>),
    /// Signature Primitive, representing a Signature type
    Signature(Signature<'a>),
    /// U32 Primitive, representing a u32 type
    U32(u32),
    /// U32AsRef Primitive, representing a U32AsRef type
    U32AsRef(U32AsRef<'a>),
    /// F32 Primitive, representing a f32 type
    F32(f32),
    /// U64 Primitive, representing a u64 type
    U64(u64),
    /// B032 Primitive, representing a B032 type
    B032(B032<'a>),
    /// B0255 Primitive, representing a B0255 type
    B0255(B0255<'a>),
    /// B064K Primitive, representing a B064K type
    B064K(B064K<'a>),
    /// B016M Primitive, representing a B016M type
    B016M(B016M<'a>),
}

impl EncodablePrimitive<'_> {
    // Provides the encoding logic for each primitive type.
    //
    // The `encode` method takes the `EncodablePrimitive` variant and serializes it
    // into the destination buffer `dst`. The method returns the number of bytes written
    // . If the buffer is too small or encoding fails, it returns an error.
    fn encode(&self, dst: &mut [u8]) -> Result<usize, Error> {
        match self {
            Self::U8(v) => v.to_slice(dst),
            Self::OwnedU8(v) => v.to_slice(dst),
            Self::U16(v) => v.to_slice(dst),
            Self::Bool(v) => v.to_slice(dst),
            Self::U24(v) => v.to_slice(dst),
            Self::U256(v) => v.to_slice(dst),
            Self::Signature(v) => v.to_slice(dst),
            Self::U32(v) => v.to_slice(dst),
            Self::U32AsRef(v) => v.to_slice(dst),
            Self::F32(v) => v.to_slice(dst),
            Self::U64(v) => v.to_slice(dst),
            Self::B032(v) => v.to_slice(dst),
            Self::B0255(v) => v.to_slice(dst),
            Self::B064K(v) => v.to_slice(dst),
            Self::B016M(v) => v.to_slice(dst),
        }
    }

    // Write the encoded object into the provided writer.
    //
    // Serializes the object and writes it directly to the
    // provided writer. It is only available in environments where `std`
    // is available.
    #[cfg(not(feature = "no_std"))]
    pub fn write(&self, writer: &mut impl Write) -> Result<(), E> {
        match self {
            Self::U8(v) => v.to_writer_(writer),
            Self::OwnedU8(v) => v.to_writer_(writer),
            Self::U16(v) => v.to_writer_(writer),
            Self::Bool(v) => v.to_writer_(writer),
            Self::U24(v) => v.to_writer_(writer),
            Self::U256(v) => v.to_writer_(writer),
            Self::Signature(v) => v.to_writer_(writer),
            Self::U32(v) => v.to_writer_(writer),
            Self::U32AsRef(v) => v.to_writer_(writer),
            Self::F32(v) => v.to_writer_(writer),
            Self::U64(v) => v.to_writer_(writer),
            Self::B032(v) => v.to_writer_(writer),
            Self::B0255(v) => v.to_writer_(writer),
            Self::B064K(v) => v.to_writer_(writer),
            Self::B016M(v) => v.to_writer_(writer),
        }
    }
}

// Provides the logic for calculating the size of the encodable field.
impl GetSize for EncodablePrimitive<'_> {
    fn get_size(&self) -> usize {
        match self {
            Self::U8(v) => v.get_size(),
            Self::OwnedU8(v) => v.get_size(),
            Self::U16(v) => v.get_size(),
            Self::Bool(v) => v.get_size(),
            Self::U24(v) => v.get_size(),
            Self::U256(v) => v.get_size(),
            Self::Signature(v) => v.get_size(),
            Self::U32(v) => v.get_size(),
            Self::U32AsRef(v) => v.get_size(),
            Self::F32(v) => v.get_size(),
            Self::U64(v) => v.get_size(),
            Self::B032(v) => v.get_size(),
            Self::B0255(v) => v.get_size(),
            Self::B064K(v) => v.get_size(),
            Self::B016M(v) => v.get_size(),
        }
    }
}

/// The [`EncodableField`] enum defines encodable fields, which may be a primitive or struct.
///
/// Each [`EncodableField`] represents either a primitive value or a collection of values
/// (a struct). The encoding process for [`EncodableField`] supports nesting, allowing
/// for complex hierarchical data structures to be serialized.
#[derive(Debug)]
pub enum EncodableField<'a> {
    /// Represents a primitive value
    ///
    /// For the full supported list please see [`EncodablePrimitive`]
    Primitive(EncodablePrimitive<'a>),
    /// Represents a struct like field structure.
    ///
    /// Note that this is a recursive enum type.
    Struct(Vec<EncodableField<'a>>),
}

impl EncodableField<'_> {
    /// The `encode` method serializes a field into the destination buffer `dst`, starting
    /// at the provided `offset`. If the field is a structure, it recursively encodes
    /// each contained field. If the buffer is too small or encoding fails, the method
    /// returns an error.
    pub fn encode(&self, dst: &mut [u8], mut offset: usize) -> Result<usize, Error> {
        match (self, dst.len() >= offset) {
            (Self::Primitive(p), true) => p.encode(&mut dst[offset..]),
            (Self::Struct(ps), true) => {
                let mut result = 0;
                for p in ps {
                    let encoded_bytes = p.encode(dst, offset)?;
                    offset += encoded_bytes;
                    result += encoded_bytes;
                }
                Ok(result)
            }
            (_, false) => Err(Error::WriteError(offset, dst.len())),
        }
    }

    #[cfg(not(feature = "no_std"))]
    pub fn to_writer(&self, writer: &mut impl Write) -> Result<(), E> {
        match self {
            Self::Primitive(p) => p.write(writer),
            Self::Struct(ps) => {
                for p in ps {
                    p.to_writer(writer)?;
                }
                Ok(())
            }
        }
    }
}

impl GetSize for EncodableField<'_> {
    fn get_size(&self) -> usize {
        match self {
            Self::Primitive(p) => p.get_size(),
            Self::Struct(ps) => {
                let mut size = 0;
                for p in ps {
                    size += p.get_size();
                }
                size
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/impls.rs">
use crate::{
    codec::{
        decodable::{
            Decodable, DecodableField, DecodablePrimitive, FieldMarker, GetMarker, PrimitiveMarker,
        },
        encodable::{EncodableField, EncodablePrimitive},
    },
    datatypes::*,
    Error,
};
use alloc::vec::Vec;
use core::convert::{TryFrom, TryInto};

// IMPL GET MARKER FOR PRIMITIVES
impl GetMarker for bool {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::Bool)
    }
}
impl GetMarker for u8 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U8)
    }
}
impl GetMarker for u16 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U16)
    }
}
impl GetMarker for U24 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U24)
    }
}
impl GetMarker for u32 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U32)
    }
}
impl GetMarker for f32 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::F32)
    }
}
impl GetMarker for u64 {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U64)
    }
}
impl GetMarker for U256<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U256)
    }
}
impl GetMarker for Signature<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::Signature)
    }
}
impl GetMarker for B032<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::B032)
    }
}
impl GetMarker for B0255<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::B0255)
    }
}
impl GetMarker for B064K<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::B064K)
    }
}
impl GetMarker for B016M<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::B016M)
    }
}
impl GetMarker for U32AsRef<'_> {
    fn get_marker() -> FieldMarker {
        FieldMarker::Primitive(PrimitiveMarker::U32AsRef)
    }
}

// IMPL DECODABLE FOR PRIMITIVES

impl<'a> Decodable<'a> for u8 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U8.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for u16 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U16.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for u32 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U32.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for f32 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::F32.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for u64 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U64.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for bool {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::Bool.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for U24 {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U24.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for U256<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U256.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for Signature<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::Signature.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for B032<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::B032.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for B0255<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::B0255.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for B064K<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::B064K.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}
impl<'a> Decodable<'a> for B016M<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::B016M.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}

impl<'a> Decodable<'a> for U32AsRef<'a> {
    fn get_structure(_: &[u8]) -> Result<Vec<FieldMarker>, Error> {
        Ok(vec![PrimitiveMarker::U32AsRef.into()])
    }

    fn from_decoded_fields(mut data: Vec<DecodableField<'a>>) -> Result<Self, Error> {
        data.pop().ok_or(Error::NoDecodableFieldPassed)?.try_into()
    }
}

// IMPL TRY_FROM PRIMITIVE FOR PRIMITIVEs

impl<'a> TryFrom<DecodablePrimitive<'a>> for u8 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U8(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for u16 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U16(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for u32 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U32(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for f32 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::F32(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for u64 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U64(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for bool {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::Bool(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}

impl<'a> TryFrom<DecodablePrimitive<'a>> for U24 {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U24(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for U256<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U256(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for Signature<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::Signature(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for B032<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::B032(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for B0255<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::B0255(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for B064K<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::B064K(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for B016M<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::B016M(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}
impl<'a> TryFrom<DecodablePrimitive<'a>> for U32AsRef<'a> {
    type Error = Error;

    fn try_from(value: DecodablePrimitive<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodablePrimitive::U32AsRef(val) => Ok(val),
            _ => Err(Error::PrimitiveConversionError),
        }
    }
}

// IMPL TRY_FROM DECODEC FIELD FOR PRIMITIVES

impl<'a> TryFrom<DecodableField<'a>> for u8 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for u16 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for u32 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for f32 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for u64 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for bool {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for U24 {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for U256<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for Signature<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for B032<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for B0255<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for B064K<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for B016M<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}
impl<'a> TryFrom<DecodableField<'a>> for U32AsRef<'a> {
    type Error = Error;

    fn try_from(value: DecodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            DecodableField::Primitive(p) => p.try_into(),
            _ => Err(Error::DecodableConversionError),
        }
    }
}

// IMPL FROM PRIMITIVES FOR ENCODED FIELD

impl From<bool> for EncodableField<'_> {
    fn from(v: bool) -> Self {
        EncodableField::Primitive(EncodablePrimitive::Bool(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for bool {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::Bool(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<u8> for EncodableField<'_> {
    fn from(v: u8) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U8(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for u8 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U8(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<u16> for EncodableField<'_> {
    fn from(v: u16) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U16(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for u16 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U16(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<U24> for EncodableField<'_> {
    fn from(v: U24) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U24(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for U24 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U24(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<u32> for EncodableField<'_> {
    fn from(v: u32) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U32(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for u32 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U32(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<f32> for EncodableField<'_> {
    fn from(v: f32) -> Self {
        EncodableField::Primitive(EncodablePrimitive::F32(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for f32 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::F32(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl From<u64> for EncodableField<'_> {
    fn from(v: u64) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U64(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for u64 {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U64(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<U256<'a>> for EncodableField<'a> {
    fn from(v: U256<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U256(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for U256<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U256(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<Signature<'a>> for EncodableField<'a> {
    fn from(v: Signature<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::Signature(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for Signature<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::Signature(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<B032<'a>> for EncodableField<'a> {
    fn from(v: B032<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::B032(v))
    }
}
impl<'a> From<B0255<'a>> for EncodableField<'a> {
    fn from(v: B0255<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::B0255(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for B032<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::B032(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> TryFrom<EncodableField<'a>> for B0255<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::B0255(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<B064K<'a>> for EncodableField<'a> {
    fn from(v: B064K<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::B064K(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for B064K<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::B064K(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<B016M<'a>> for EncodableField<'a> {
    fn from(v: B016M<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::B016M(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for B016M<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::B016M(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}
impl<'a> From<U32AsRef<'a>> for EncodableField<'a> {
    fn from(v: U32AsRef<'a>) -> Self {
        EncodableField::Primitive(EncodablePrimitive::U32AsRef(v))
    }
}
impl<'a> TryFrom<EncodableField<'a>> for U32AsRef<'a> {
    type Error = Error;

    fn try_from(value: EncodableField<'a>) -> Result<Self, Self::Error> {
        match value {
            EncodableField::Primitive(EncodablePrimitive::U32AsRef(v)) => Ok(v),
            _ => Err(Error::NonPrimitiveTypeCannotBeEncoded),
        }
    }
}

// IMPL INTO FIELD MARKER FOR PRIMITIVES
impl From<bool> for FieldMarker {
    fn from(_: bool) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::Bool)
    }
}
impl From<u8> for FieldMarker {
    fn from(_: u8) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U8)
    }
}

impl From<u16> for FieldMarker {
    fn from(_: u16) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U16)
    }
}

impl From<u32> for FieldMarker {
    fn from(_: u32) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U32)
    }
}

impl From<f32> for FieldMarker {
    fn from(_: f32) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::F32)
    }
}

impl From<u64> for FieldMarker {
    fn from(_: u64) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U64)
    }
}

impl From<U24> for FieldMarker {
    fn from(_: U24) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U24)
    }
}

impl<'a> From<Inner<'a, true, 32, 0, 0>> for FieldMarker {
    fn from(_: Inner<'a, true, 32, 0, 0>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U256)
    }
}

impl<'a> From<Inner<'a, true, 64, 0, 0>> for FieldMarker {
    fn from(_: Inner<'a, true, 64, 0, 0>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::Signature)
    }
}

impl<'a> From<B032<'a>> for FieldMarker {
    fn from(_: B032<'a>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::B032)
    }
}

impl<'a> From<Inner<'a, false, 1, 1, 255>> for FieldMarker {
    fn from(_: Inner<'a, false, 1, 1, 255>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::B0255)
    }
}

impl<'a> From<Inner<'a, false, 1, 2, { 2_usize.pow(16) - 1 }>> for FieldMarker {
    fn from(_: Inner<'a, false, 1, 2, { 2_usize.pow(16) - 1 }>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::B064K)
    }
}

impl<'a> From<Inner<'a, false, 1, 3, { 2_usize.pow(24) - 1 }>> for FieldMarker {
    fn from(_: Inner<'a, false, 1, 3, { 2_usize.pow(24) - 1 }>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::B016M)
    }
}
impl<'a> From<U32AsRef<'a>> for FieldMarker {
    fn from(_: U32AsRef<'a>) -> Self {
        FieldMarker::Primitive(PrimitiveMarker::U32AsRef)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/codec/mod.rs">
// Provides traits and utilities for encoding and decoding data at a low level,
// prioritizing efficiency and memory safety by operating directly on slices rather than
// relying on `Read` or `Write` streams.
//
// ## Overview
//
// Optimized for performance, this module directly manipulates byte slices, avoiding
// the overhead of stream-based I/O. It uses memory-efficient techniques like obtaining
// pointers to original data instead of copying it. Enums are avoided for decoding, as
// each message type can be identified by its numeric identifier, streamlining the process.
//
// ### Key Components
//
// - **Traits**: Defines core traits (`SizeHint`, `GetSize`, `Fixed`, `Variable`) that establish a
//   consistent interface for encoding and decoding operations.
// - **Buffer Management**: With the `with_buffer_pool` feature enabled, the `Slice` type from
//   `buffer_sv2` is included, supporting memory pooling and efficient slice handling for
//   high-performance buffer management scenarios.
//
// ### Traits Overview
//
// - **`SizeHint`**: Estimates the size of a decodable type, useful for variable-length data where
//   the size must be determined dynamically.
// - **`GetSize`**: Provides the exact size of an encodable type in bytes, crucial for buffer
//   allocation.
// - **`Fixed`**: Marks types with a compile-time constant size, simplifying encoding and decoding.
// - **`Variable`**: For types with dynamic sizes, manages size variability and calculates inner
//   sizes.
//
// ## Build Options
//
// - **`no_std` Compatibility**: This module can be compiled without the standard library for
//   constrained environments. Some methods and traits are conditionally available when `std` is
//   included.
// - **`with_buffer_pool`**: When enabled, includes the `Slice` type for managing pooled memory
//   slices, improving memory handling and efficiency in high-performance scenarios.
//
// ## Detailed Trait Descriptions
//
// ### `SizeHint`
// Defines methods to calculate the size of encoded data for types with variable sizes.
// - **`size_hint`**: Returns the total size of the encoded data for raw data and an offset.
// - **`size_hint_`**: Returns the size for a specific instance, offering flexibility.
//
// ### `GetSize`
// Provides a `get_size` method that returns the exact size in bytes of an encodable type.
//
// ### `Fixed`
// For types with a fixed size, this trait defines a constant `SIZE`, simplifying work with
// fixed-size types.
//
// ### `Variable`
// Types with variable sizes implement this trait, providing constants (`HEADER_SIZE`, `MAX_SIZE`)
// and methods for size management and inner size calculation.
//
// ## Summary
//
// This module supports efficient, low-level encoding and decoding by operating directly on slices,
// avoiding excess data copying. It offers capabilities for both fixed and variable-sized data,
// making it versatile for a wide range of encoding tasks.
use crate::Error;
pub mod decodable;
pub mod encodable;
mod impls;
#[cfg(feature = "with_buffer_pool")]
use buffer_sv2::Slice;

use alloc::vec::Vec;

/// The `SizeHint` trait provides a mechanism to return the encoded bytes size of a decodable type.
///
/// It defines two methods for retrieving the size of an encoded message:
///
///
/// These methods are crucial in decoding scenarios where the full size of the message
/// is not immediately available, helping to determine how many bytes need to be read.
pub trait SizeHint {
    /// `size_hint` is a static method that takes the raw data and an offset and returns the total
    /// size of the encoded message. This is particularly useful for types where the encoded size
    /// may vary based on the contents of the data, and we need to calculate how much space is
    /// required for decoding.
    fn size_hint(data: &[u8], offset: usize) -> Result<usize, Error>;
    /// `size_hint_` is an instance method that performs the same function but allows the size to be
    /// be determined with respect to the current instance of the type.
    fn size_hint_(&self, data: &[u8], offset: usize) -> Result<usize, Error>;
}

/// [`GetSize`] is a trait defining a single function that  calculates the total size of an
/// encodable type.
pub trait GetSize {
    /// `get_size` returns total size of the type in bytes.
    fn get_size(&self) -> usize;
}

#[cfg(feature = "with_buffer_pool")]
impl GetSize for Slice {
    fn get_size(&self) -> usize {
        self.len()
    }
}
/// [`Fixed`] is a trait is defining a single element representing a size of a constant.
///
/// This is useful for primitives with a constant fixed size
///
/// Types implementing this trait must define the constant `SIZE`, representing the
/// fixed number of bytes needed to encode or decode the type. This trait is used for
/// types that have a know size at compile time , such as integers, fixed-size arrays, etc.
pub trait Fixed {
    ///the constant `SIZE`, represents the fixed number of bytes needed to encode or decode the
    /// type.
    const SIZE: usize;
}

// Not used and will be removed during refactoring
#[allow(dead_code)]
pub trait Variable {
    const HEADER_SIZE: usize;
    //const ELEMENT_SIZE: usize;
    const MAX_SIZE: usize;

    fn inner_size(&self) -> usize;

    // Retrieves the header as a byte vector. This header typically contains information
    // about the size or type of the data that follows.
    fn get_header(&self) -> Vec<u8>;
}

impl<T: Fixed> SizeHint for T {
    fn size_hint(_data: &[u8], _offset: usize) -> Result<usize, Error> {
        Ok(Self::SIZE)
    }

    fn size_hint_(&self, _: &[u8], _offset: usize) -> Result<usize, Error> {
        Ok(Self::SIZE)
    }
}

impl<T: Fixed> GetSize for T {
    fn get_size(&self) -> usize {
        Self::SIZE
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/copy_data_types.rs">
// Provides implementations for encoding and decoding copy data types as required by the SV2
// protocol. Facilitates byte-level serialization and deserialization, particularly for types
// without dynamically-sized data.
//
// ## Traits and Implementations
//
// ### `Fixed`
// The `Fixed` trait is implemented for various data types to specify a fixed size for each,
// enabling consistent memory allocation during serialization. The `SIZE` constant for each type
// defines its byte size, with implementations provided for `bool`, unsigned integers (e.g., `u8`,
// `u16`, `u32`, `u64`), and custom types like `U24`.
//
// ### `Sv2DataType`
// The `Sv2DataType` trait is implemented for these data types, providing methods for encoding and
// decoding operations such as `from_bytes_unchecked`, `from_vec_`, `from_reader_` (if `std` is
// available), and `to_slice_unchecked`. The methods use little-endian byte order for consistency
// across platforms.
//
// ## Special Types
//
// ### `U24`
// A custom 24-bit unsigned integer, represented as a `U24` struct, handles 3-byte data often used
// in SV2 protocols for memory-efficient encoding. Provides conversion methods to and from `u32`,
// with `TryFrom<u32>` ensuring values stay within the 24-bit range (0 to 16,777,215).
//
// ## Macros
// The `impl_sv2_for_unsigned` macro streamlines the implementation of the `Sv2DataType` trait for
// unsigned integer types, ensuring little-endian byte ordering for serialization and handling both
// in-memory buffers and `std::io::Read`/`Write` interfaces when `std` is available.
use crate::{codec::Fixed, datatypes::Sv2DataType, Error};

use alloc::vec::Vec;
use core::convert::{TryFrom, TryInto};

#[cfg(not(feature = "no_std"))]
use std::io::{Error as E, Read, Write};

// Impl bool as a primitive

impl Fixed for bool {
    const SIZE: usize = 1;
}

impl<'a> Sv2DataType<'a> for bool {
    fn from_bytes_unchecked(data: &'a mut [u8]) -> Self {
        match data
            .first()
            .map(|x: &u8| x << 7)
            .map(|x: u8| x >> 7)
            // This is an unchecked function is fine to panic
            .expect("Try to decode a bool from a buffer of len 0")
        {
            0 => false,
            1 => true,
            // Below panic is impossible value is either 0 or 1
            _ => panic!(),
        }
    }

    fn from_vec_(mut data: Vec<u8>) -> Result<Self, Error> {
        Self::from_bytes_(&mut data)
    }

    fn from_vec_unchecked(mut data: Vec<u8>) -> Self {
        Self::from_bytes_unchecked(&mut data)
    }

    #[cfg(not(feature = "no_std"))]
    fn from_reader_(reader: &mut impl Read) -> Result<Self, Error> {
        let mut dst = [0_u8; Self::SIZE];
        reader.read_exact(&mut dst)?;
        Self::from_bytes_(&mut dst)
    }

    fn to_slice_unchecked(&'a self, dst: &mut [u8]) {
        match self {
            true => dst[0] = 1,
            false => dst[0] = 0,
        };
    }

    #[cfg(not(feature = "no_std"))]
    fn to_writer_(&self, writer: &mut impl Write) -> Result<(), E> {
        match self {
            true => writer.write_all(&[1]),
            false => writer.write_all(&[0]),
        }
    }
}

// Impl unsigned as a primitives

impl Fixed for u8 {
    const SIZE: usize = 1;
}

impl Fixed for u16 {
    const SIZE: usize = 2;
}

impl Fixed for u32 {
    const SIZE: usize = 4;
}

impl Fixed for u64 {
    const SIZE: usize = 8;
}

/// Macro to implement the `Sv2DataType` trait for unsigned integer types.
///
/// Simplifies encoding and decoding for various unsigned integer types, making them
/// compatible with the SV2 protocol. Each implementation uses the little-endian byte order for
/// serialization and deserialization, ensuring consistency across platforms.
macro_rules! impl_sv2_for_unsigned {
    ($a:ty) => {
        impl<'a> Sv2DataType<'a> for $a {
            fn from_bytes_unchecked(data: &'a mut [u8]) -> Self {
                // unchecked function is fine to panic
                let a: &[u8; Self::SIZE] = data[0..Self::SIZE].try_into().expect(
                    "Try to decode a copy data type from a buffer that do not have enough bytes",
                );
                Self::from_le_bytes(*a)
            }

            fn from_vec_(mut data: Vec<u8>) -> Result<Self, Error> {
                Self::from_bytes_(&mut data)
            }

            fn from_vec_unchecked(mut data: Vec<u8>) -> Self {
                Self::from_bytes_unchecked(&mut data)
            }

            #[cfg(not(feature = "no_std"))]
            fn from_reader_(reader: &mut impl Read) -> Result<Self, Error> {
                let mut dst = [0_u8; Self::SIZE];
                reader.read_exact(&mut dst)?;
                Ok(Self::from_bytes_unchecked(&mut dst))
            }

            fn to_slice_unchecked(&'a self, dst: &mut [u8]) {
                let dst = &mut dst[0..Self::SIZE];
                let src = self.to_le_bytes();
                dst.copy_from_slice(&src);
            }

            #[cfg(not(feature = "no_std"))]
            fn to_writer_(&self, writer: &mut impl Write) -> Result<(), E> {
                let bytes = self.to_le_bytes();
                writer.write_all(&bytes)
            }
        }
    };
}
impl_sv2_for_unsigned!(u8);
impl_sv2_for_unsigned!(u16);
impl_sv2_for_unsigned!(u32);
impl_sv2_for_unsigned!(u64);

impl Fixed for f32 {
    const SIZE: usize = 4;
}

impl_sv2_for_unsigned!(f32);

/// Represents a 24-bit unsigned integer (`U24`), supporting SV2 serialization and deserialization.
/// Only first 3 bytes of a u32 is considered to get the SV2 value, and rest are ignored (in little
/// endian).
#[repr(C)]
#[derive(Debug, Clone, Copy, Eq, PartialEq)]
pub struct U24(pub(crate) u32);

impl Fixed for U24 {
    const SIZE: usize = 3;
}

impl U24 {
    fn from_le_bytes(b: [u8; Self::SIZE]) -> Self {
        let inner = u32::from_le_bytes([b[0], b[1], b[2], 0]);
        Self(inner)
    }

    fn to_le_bytes(self) -> [u8; Self::SIZE] {
        let b = self.0.to_le_bytes();
        [b[0], b[1], b[2]]
    }
}

impl_sv2_for_unsigned!(U24);

impl TryFrom<u32> for U24 {
    type Error = Error;

    fn try_from(value: u32) -> Result<Self, Self::Error> {
        if value <= 16777215 {
            Ok(Self(value))
        } else {
            Err(Error::InvalidU24(value))
        }
    }
}

impl From<U24> for u32 {
    fn from(v: U24) -> Self {
        v.0
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/mod.rs">
// Provides implementations for encoding and decoding data types in the SV2 protocol,
// supporting both fixed-size and dynamically-sized types. Defines the `Sv2DataType` trait,
// which standardizes serialization and deserialization across various types, including
// those with custom requirements like byte padding and dynamic sizing.
//
// ## Structure and Contents
//
// ### `Sv2DataType` Trait
// The `Sv2DataType` trait offers methods to:
// - **Deserialize**: Convert byte slices or reader sources into Rust types.
// - **Serialize**: Encode Rust types into byte slices or write them to I/O streams.
//
// Supports both **checked** and **unchecked** variants for serialization and deserialization.
// Checked functions validate data lengths, while unchecked versions assume size correctness for
// optimized performance.
//
// ### Modules
// - **`copy_data_types`**: Defines fixed-size types directly copied into or from byte slices, such
//   as `U24` (24-bit unsigned integer).
// - **`non_copy_data_types`**: Manages dynamically-sized types, like sequences, public keys, and
//   strings, requiring size handling logic for SV2 compatibility.
//
// ### Re-exports
// Re-exports common data types used in SV2 serialization, such as `PubKey`, `Signature`, `Seq0255`,
// and others, simplifying protocol data handling with concrete implementations of `Sv2DataType`.
//
// The `Sv2DataType` trait and its implementations enable seamless conversion between in-memory
// representations and serialized forms, ensuring efficient protocol communication and
// interoperability.

use crate::{
    codec::{GetSize, SizeHint},
    Error,
};
mod non_copy_data_types;

mod copy_data_types;
use crate::codec::decodable::FieldMarker;
pub use copy_data_types::U24;
pub use non_copy_data_types::{
    Inner, PubKey, Seq0255, Seq064K, Signature, Str0255, Sv2Option, U32AsRef, B016M, B0255, B032,
    B064K, U256,
};

use alloc::vec::Vec;
use core::convert::TryInto;
#[cfg(not(feature = "no_std"))]
use std::io::{Error as E, Read, Write};

/// `Sv2DataType` is a trait that defines methods for encoding and decoding Stratum V2 data.
/// It is used for serializing and deserializing both fixed-size and dynamically-sized types.
///
/// Key Responsibilities:
/// - Serialization: Converting data from in-memory representations to byte slices or streams.
/// - Deserialization: Converting byte slices or streams back into the in-memory representation of
///   the data.
///
/// This trait includes functions for both checked and unchecked conversions, providing flexibility
/// in situations where error handling can be safely ignored.
pub trait Sv2DataType<'a>: Sized + SizeHint + GetSize + TryInto<FieldMarker> {
    /// Creates an instance of the type from a mutable byte slice, checking for size constraints.
    ///
    /// This function verifies that the provided byte slice has the correct size according to the
    /// type's size hint.
    fn from_bytes_(data: &'a mut [u8]) -> Result<Self, Error> {
        Self::size_hint(data, 0)?;
        Ok(Self::from_bytes_unchecked(data))
    }

    /// Constructs an instance from a mutable byte slice without verifying size constraints.
    fn from_bytes_unchecked(data: &'a mut [u8]) -> Self;

    /// Constructs an instance from a vector, checking for the correct size.
    fn from_vec_(data: Vec<u8>) -> Result<Self, Error>;

    /// Constructs an instance from a vector without validating its size.
    fn from_vec_unchecked(data: Vec<u8>) -> Self;

    // Constructs an instance from a reader source, checking for size constraints.
    #[cfg(not(feature = "no_std"))]
    fn from_reader_(reader: &mut impl Read) -> Result<Self, Error>;

    /// Serializes the instance to a mutable slice, checking the destination size.
    fn to_slice(&'a self, dst: &mut [u8]) -> Result<usize, Error> {
        if dst.len() >= self.get_size() {
            self.to_slice_unchecked(dst);
            Ok(self.get_size())
        } else {
            Err(Error::WriteError(self.get_size(), dst.len()))
        }
    }

    /// Serializes the instance to a mutable slice without checking the destination size.
    fn to_slice_unchecked(&'a self, dst: &mut [u8]);

    // Serializes the instance to a writer destination, checking for I/O errors.
    #[cfg(not(feature = "no_std"))]
    fn to_writer_(&self, writer: &mut impl Write) -> Result<(), E>;
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/inner.rs">
// Provides a flexible container for managing either owned or mutable references to byte arrays.
//
// # Overview
// Defines the `Inner` enum to manage both mutable references to byte slices and owned vectors
// (`Vec<u8>`). Accommodates both fixed-size and variable-size data using const generics, offering
// control over size and header length constraints.
//
// # `Inner` Enum
// The `Inner` enum has two variants for data management:
// - `Ref(&'a mut [u8])`: A mutable reference to a byte slice, allowing in-place data modification.
// - `Owned(Vec<u8>)`: An owned byte vector, providing full control over data and supporting move
//   semantics.
//
// ## Const Parameters
// Configured using const generics for the following constraints:
// - `ISFIXED`: Indicates whether the data has a fixed size.
// - `SIZE`: Specifies the size when `ISFIXED` is true.
// - `HEADERSIZE`: Defines the size of the header, useful for variable-size data with a prefix
//   length.
// - `MAXSIZE`: Limits the maximum allowable size of the data.
//
// # Usage
// `Inner` offers several methods for data manipulation, including:
// - `to_vec()`: Returns a `Vec<u8>`, cloning the slice or owned data.
// - `inner_as_ref()` and `inner_as_mut()`: Provide immutable or mutable access to the data.
// - `expected_length(data: &[u8])`: Computes the expected length, validating it against
//   constraints.
// - `get_header()`: Returns the data's header based on `HEADERSIZE`.
//
// # Implementations
// The `Inner` enum implements `PartialEq`, `Eq`, `GetSize`, `SizeHint`, and `Sv2DataType` traits,
// enabling buffer size calculations, reading, and writing to byte slices.
//
// # Error Handling
// Methods return `Error` types when data exceeds size limits or deviates from the configuration,
// ensuring compliance with defined constraints.
use super::IntoOwned;
use crate::{
    codec::{GetSize, SizeHint},
    datatypes::Sv2DataType,
    Error,
};

use alloc::vec::Vec;
use core::convert::{TryFrom, TryInto};
#[cfg(not(feature = "no_std"))]
use std::io::{Error as E, Read, Write};

// The `Inner` enum represents a flexible container for managing both reference to mutable
// slices and owned bytes arrays (`Vec<u8>`). This design allows the container to either own
// its data or simply reference existing mutable data. It uses const generics to differentiate
// between fixed-size and variable-size data, as well as to specify key size-related parameters.
//
// It has two variants:
// - `Ref(&'a mut [u8])`: A mutable reference to an external byte slice.
// - `Owned (Vec<u8>)`: A vector that owns its data, enabling dynamic ownership.
//
// The const parameters that govern the behavior of this enum are:
//  - `ISFIXED`: A boolean indicating whether the data has a fixed size.
//  - `SIZE`: The size of the data if `ISFIXED` is true.
//  - `HEADERSIZE`: The size of the header, which is used for types that require a prefix to
//    describe the content's length.
//  - `MAXSIZE`: The maximum allowable size for the data.
#[repr(C)]
#[derive(Debug)]
pub enum Inner<
    'a,
    const ISFIXED: bool,
    const SIZE: usize,
    const HEADERSIZE: usize,
    const MAXSIZE: usize,
> {
    Ref(&'a mut [u8]),
    Owned(Vec<u8>),
}

impl<const SIZE: usize> Inner<'_, true, SIZE, 0, 0> {
    // Converts the inner data to a vector, either by cloning the referenced slice or
    // returning a clone of the owned vector.
    pub fn to_vec(&self) -> Vec<u8> {
        match self {
            Inner::Ref(ref_) => ref_.to_vec(),
            Inner::Owned(v) => v.clone(),
        }
    }
    // Returns an immutable reference to the inner data, whether it's a reference or
    // an owned vector.
    pub fn inner_as_ref(&self) -> &[u8] {
        match self {
            Inner::Ref(ref_) => ref_,
            Inner::Owned(v) => v,
        }
    }
    // Provides a mutable reference to the inner data, allowing modification if the
    // data is being referenced.
    pub fn inner_as_mut(&mut self) -> &mut [u8] {
        match self {
            Inner::Ref(ref_) => ref_,
            Inner::Owned(v) => v,
        }
    }
}

impl<const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Inner<'_, false, SIZE, HEADERSIZE, MAXSIZE>
{
    // Similar to the fixed-size variant, this method converts the inner data into a vector.
    // The data is either cloned from the referenced slice or returned as a clone of the
    // owned vector.
    pub fn to_vec(&self) -> Vec<u8> {
        match self {
            Inner::Ref(ref_) => ref_[..].to_vec(),
            Inner::Owned(v) => v[..].to_vec(),
        }
    }
    // Returns an immutable reference to the inner data for variable-size types, either
    // referencing a slice or an owned vector.
    pub fn inner_as_ref(&self) -> &[u8] {
        match self {
            Inner::Ref(ref_) => &ref_[..],
            Inner::Owned(v) => &v[..],
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    PartialEq for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    // Provides equality comparison between two `Inner` instances by checking the equality
    // of their data, regardless of whether they are references or owned vectors.
    fn eq(&self, other: &Self) -> bool {
        match (self, other) {
            (Inner::Ref(b), Inner::Owned(a)) => *b == &a[..],
            (Inner::Owned(b), Inner::Ref(a)) => *a == &b[..],
            (Inner::Owned(b), Inner::Owned(a)) => b == a,
            (Inner::Ref(b), Inner::Ref(a)) => b == a,
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize> Eq
    for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    // Calculates the expected length of the data based on the type's parameters (fixed-size
    // or variable-size). It checks if the length conforms to the specified constraints like
    // `SIZE`, `MAXSIZE`, and `HEADERSIZE`, returning the length or an error if the data
    // exceeds the limits.
    fn expected_length(data: &[u8]) -> Result<usize, Error> {
        let expected_length = match ISFIXED {
            true => Self::expected_length_fixed(),
            false => Self::expected_length_variable(data)?,
        };
        if ISFIXED || expected_length <= (MAXSIZE + HEADERSIZE) {
            Ok(expected_length)
        } else {
            Err(Error::ReadError(data.len(), MAXSIZE))
        }
    }

    // For fixed-size data, the expected length is always `SIZE`.
    fn expected_length_fixed() -> usize {
        SIZE
    }

    // For variable-size data, this method calculates the size based on the header.
    // The header describes the length of the data, and this method ensures the data
    // is correctly sized relative to the header information.
    fn expected_length_variable(data: &[u8]) -> Result<usize, Error> {
        if data.len() >= HEADERSIZE {
            let size = match HEADERSIZE {
                1 => Ok(data[0] as usize),
                2 => Ok(u16::from_le_bytes([data[0], data[1]]) as usize),
                3 => Ok(u32::from_le_bytes([data[0], data[1], data[2], 0]) as usize),
                // HEADERSIZE for Sv2 datatypes is at maximum 3 bytes
                // When HEADERSIZE is 0 datatypes ISFIXED only exception is Bytes datatypes but is
                // not used
                _ => unreachable!(),
            };
            size.map(|x| x + HEADERSIZE)
        } else {
            Err(Error::ReadError(data.len(), HEADERSIZE))
        }
    }

    // Similar to the above but operates on a reader instead of a byte slice, reading
    // the header from the input and calculating the expected length of the data to be read.
    #[cfg(not(feature = "no_std"))]
    fn expected_length_for_reader(mut reader: impl Read) -> Result<usize, Error> {
        if ISFIXED {
            Ok(SIZE)
        } else {
            let mut header = [0_u8; HEADERSIZE];
            reader.read_exact(&mut header)?;
            let expected_length = match HEADERSIZE {
                1 => header[0] as usize,
                2 => u16::from_le_bytes([header[0], header[1]]) as usize,
                3 => u32::from_le_bytes([header[0], header[1], header[2], 0]) as usize,
                // HEADERSIZE for Sv2 datatypes is at maximum 3 bytes
                // When HEADERSIZE is 0 datatypes ISFIXED only exception is Bytes datatypes but is
                // not used
                _ => unreachable!(),
            };
            if expected_length <= (MAXSIZE + HEADERSIZE) {
                Ok(expected_length)
            } else {
                Err(Error::ReadError(expected_length, MAXSIZE))
            }
        }
    }

    /// Returns the length of the data, either from the reference or the owned vector,
    /// or the fixed size if `ISFIXED` is true.
    pub fn len(&self) -> usize {
        match (self, ISFIXED) {
            (Inner::Ref(data), false) => data.len(),
            (Inner::Owned(data), false) => data.len(),
            (_, true) => 1,
        }
    }

    // Retrieves the header as a byte vector. If `HEADERSIZE` is zero, an empty vector is
    // returned. Otherwise, the header is constructed from the length of the data.
    fn get_header(&self) -> Vec<u8> {
        if HEADERSIZE == 0 {
            Vec::new()
        } else {
            let len = self.len();
            len.to_le_bytes().into()
        }
    }
}

impl<'a, const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    TryFrom<&'a mut [u8]> for Inner<'a, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    type Error = Error;

    fn try_from(value: &'a mut [u8]) -> Result<Self, Self::Error> {
        if ISFIXED && value.len() == SIZE {
            Ok(Self::Ref(value))
        } else if ISFIXED {
            Err(Error::ValueExceedsMaxSize(
                ISFIXED,
                SIZE,
                HEADERSIZE,
                MAXSIZE,
                value.to_vec(),
                value.len(),
            ))
        } else if value.len() <= MAXSIZE {
            Ok(Self::Ref(value))
        } else {
            Err(Error::ValueExceedsMaxSize(
                ISFIXED,
                SIZE,
                HEADERSIZE,
                MAXSIZE,
                value.to_vec(),
                value.len(),
            ))
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    TryFrom<Vec<u8>> for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    type Error = Error;

    fn try_from(value: Vec<u8>) -> Result<Self, Self::Error> {
        if ISFIXED && value.len() == SIZE {
            Ok(Self::Owned(value))
        } else if ISFIXED {
            Err(Error::ValueExceedsMaxSize(
                ISFIXED,
                SIZE,
                HEADERSIZE,
                MAXSIZE,
                value.to_vec(),
                value.len(),
            ))
        } else if value.len() <= MAXSIZE {
            Ok(Self::Owned(value))
        } else {
            Err(Error::ValueExceedsMaxSize(
                ISFIXED,
                SIZE,
                HEADERSIZE,
                MAXSIZE,
                value.to_vec(),
                value.len(),
            ))
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize> GetSize
    for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    fn get_size(&self) -> usize {
        match self {
            Inner::Ref(data) => data.len() + HEADERSIZE,
            Inner::Owned(data) => data.len() + HEADERSIZE,
        }
    }
}

impl<const ISFIXED: bool, const HEADERSIZE: usize, const SIZE: usize, const MAXSIZE: usize> SizeHint
    for Inner<'_, ISFIXED, HEADERSIZE, SIZE, MAXSIZE>
{
    fn size_hint(data: &[u8], offset: usize) -> Result<usize, Error> {
        if offset >= data.len() {
            return Err(Error::ReadError(data.len(), offset));
        }
        Self::expected_length(&data[offset..])
    }

    fn size_hint_(&self, data: &[u8], offset: usize) -> Result<usize, Error> {
        if offset >= data.len() {
            return Err(Error::ReadError(data.len(), offset));
        }
        Self::expected_length(&data[offset..])
    }
}
use crate::codec::decodable::FieldMarker;

impl<'a, const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Sv2DataType<'a> for Inner<'a, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
where
    Self: TryInto<FieldMarker>,
{
    fn from_bytes_unchecked(data: &'a mut [u8]) -> Self {
        if ISFIXED {
            Self::Ref(data)
        } else {
            Self::Ref(&mut data[HEADERSIZE..])
        }
    }

    fn from_vec_(data: Vec<u8>) -> Result<Self, Error> {
        Self::size_hint(&data, 0)?;
        Ok(Self::Owned(data))
    }

    fn from_vec_unchecked(data: Vec<u8>) -> Self {
        Self::Owned(data)
    }

    #[cfg(not(feature = "no_std"))]
    fn from_reader_(mut reader: &mut impl Read) -> Result<Self, Error> {
        let size = Self::expected_length_for_reader(&mut reader)?;

        let mut dst = vec![0; size];

        reader.read_exact(&mut dst)?;
        Ok(Self::from_vec_unchecked(dst))
    }

    fn to_slice_unchecked(&'a self, dst: &mut [u8]) {
        let size = self.get_size();
        let header = self.get_header();
        dst[0..HEADERSIZE].copy_from_slice(&header[..HEADERSIZE]);
        match self {
            Inner::Ref(data) => {
                let dst = &mut dst[0..size];
                dst[HEADERSIZE..].copy_from_slice(data);
            }
            Inner::Owned(data) => {
                let dst = &mut dst[0..size];
                dst[HEADERSIZE..].copy_from_slice(data);
            }
        }
    }

    #[cfg(not(feature = "no_std"))]
    fn to_writer_(&self, writer: &mut impl Write) -> Result<(), E> {
        match self {
            Inner::Ref(data) => {
                writer.write_all(data)?;
            }
            Inner::Owned(data) => {
                writer.write_all(data)?;
            }
        };
        Ok(())
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    IntoOwned for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    fn into_owned(self) -> Self {
        match self {
            Inner::Ref(data) => {
                let v: Vec<u8> = data.into();
                Self::Owned(v)
            }
            Inner::Owned(_) => self,
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    pub fn into_static(self) -> Inner<'static, ISFIXED, SIZE, HEADERSIZE, MAXSIZE> {
        match self {
            Inner::Ref(data) => {
                let mut v = Vec::with_capacity(data.len());
                v.extend_from_slice(data);
                Inner::Owned(v)
            }
            Inner::Owned(data) => Inner::Owned(data),
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize> Clone
    for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    fn clone(&self) -> Inner<'static, ISFIXED, SIZE, HEADERSIZE, MAXSIZE> {
        match self {
            Inner::Ref(data) => {
                let mut v = Vec::with_capacity(data.len());
                v.extend_from_slice(data);
                Inner::Owned(v)
            }
            Inner::Owned(data) => Inner::Owned(data.clone()),
        }
    }
}

impl<const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    AsRef<[u8]> for Inner<'_, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>
{
    fn as_ref(&self) -> &[u8] {
        match self {
            Inner::Ref(r) => &r[..],
            Inner::Owned(r) => &r[..],
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/mod.rs">
// Provides a flexible, low-level interface for representing fixed-size and variable-size byte
// arrays, simplifying serialization and deserialization of cryptographic and protocol data.
//
// The core component is the [`Inner`] type, a wrapper for managing both fixed and variable-length
// data slices or owned values. It offers aliases for commonly used data types like 32-byte hashes
// (`U256`), public keys (`PubKey`), cryptographic signatures (`Signature`), and dynamically-sized
// arrays (`B0255`, `B064K`).

// # Features
// - **Fixed-size Aliases**: Types like [`U32AsRef`], [`U256`], [`PubKey`], and [`Signature`]
//   represent specific byte sizes, often used in cryptographic contexts or protocol identifiers.
// - **Variable-size Aliases**: Types like [`B032`], [`B0255`], [`Str0255`], [`B064K`], and
//   [`B016M`] handle data with bounded sizes, providing flexibility for dynamic data.
// - **Traits and Conversions**: Implements traits like `From`, `TryFrom`, and [`IntoOwned`] for
//   seamless transformations between owned and reference-based values.
// - **Property Testing** (optional, requires the `prop_test` feature): Supports generating
//   arbitrary test data for property-based testing.

// # Type Aliases
// - **[`U32AsRef`]**: 4-byte representation for small identifiers or integer values.
// - **[`U256`]**: 32-byte cryptographic hash (e.g., SHA-256 or protocol IDs).
// - **[`PubKey`]**: 32-byte public key (e.g., Ed25519).
// - **[`Signature`]**: 64-byte cryptographic signature.
// - **[`B032`], [`B0255`], [`Str0255`]**: Variable-size representations for optional fields or
//   protocol data.

// # Feature Flags
// - **`prop_test`**: Enables property-based testing with the `quickcheck` crate. When enabled,
//   types like `U256` and `B016M` gain methods to generate arbitrary test data for testing
//   serialization and deserialization.
#[cfg(feature = "prop_test")]
use quickcheck::{Arbitrary, Gen};

#[cfg(feature = "prop_test")]
use alloc::vec::Vec;
use alloc::{borrow::ToOwned, fmt, string::String};

mod inner;
mod seq_inner;

#[allow(dead_code)]
trait IntoOwned {
    fn into_owned(self) -> Self;
}

pub use inner::Inner;
pub use seq_inner::{Seq0255, Seq064K, Sv2Option};

/// Type alias for a 4-byte slice or owned data represented using the `Inner`
/// type with fixed-size configuration.
pub type U32AsRef<'a> = Inner<'a, true, 4, 0, 0>;
/// Type alias for a 32-byte slice or owned data (commonly used for cryptographic
/// hashes or IDs) represented using the `Inner` type with fixed-size configuration.
pub type U256<'a> = Inner<'a, true, 32, 0, 0>;
/// Type alias for a 32-byte public key represented using the `Inner` type
/// with fixed-size configuration.
pub type PubKey<'a> = Inner<'a, true, 32, 0, 0>;
/// Type alias for a 64-byte cryptographic signature represented using the
/// `Inner` type with fixed-size configuration.
pub type Signature<'a> = Inner<'a, true, 64, 0, 0>;
/// Type alias for a variable-sized byte array with a maximum size of 32 bytes,
/// represented using the `Inner` type with a 1-byte header.
pub type B032<'a> = Inner<'a, false, 1, 1, 32>;
/// Type alias for a variable-sized byte array with a maximum size of 255 bytes,
/// represented using the `Inner` type with a 1-byte header.
pub type B0255<'a> = Inner<'a, false, 1, 1, 255>;
/// Type alias for a variable-sized string with a maximum size of 255 bytes,
/// represented using the `Inner` type with a 1-byte header.
pub type Str0255<'a> = Inner<'a, false, 1, 1, 255>;
/// Type alias for a variable-sized byte array with a maximum size of 64 KB,
/// represented using the `Inner` type with a 2-byte header.
pub type B064K<'a> = Inner<'a, false, 1, 2, { u16::MAX as usize }>;
/// Type alias for a variable-sized byte array with a maximum size of ~16 MB,
/// represented using the `Inner` type with a 3-byte header.
pub type B016M<'a> = Inner<'a, false, 1, 3, { 2_usize.pow(24) - 1 }>;

impl fmt::Display for U32AsRef<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let inner = self.inner_as_ref();
        write!(
            f,
            "U32AsRef({})",
            u32::from_le_bytes([inner[0], inner[1], inner[2], inner[3]])
        )
    }
}

impl fmt::Display for B0255<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let inner = self
            .inner_as_ref()
            .iter()
            .map(|byte| format!("{byte:02x}"))
            .collect::<String>();
        write!(f, "B0255({inner})")
    }
}

impl fmt::Display for Sv2Option<'_, u32> {
    // internally Sv2Option is pub struct Sv2Option<'a, T>(pub Vec<T>, PhantomData<&'a T>);
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let inner = self.to_owned().into_inner();
        match inner {
            Some(value) => write!(f, "Sv2Option({value})"),
            None => write!(f, "Sv2Option(None)"),
        }
    }
}

impl Str0255<'_> {
    /// Returns the value as a UTF-8 string if possible, otherwise as a hex string prefixed with 0x.
    pub fn as_utf8_or_hex(&self) -> String {
        match core::str::from_utf8(self.inner_as_ref()) {
            Ok(s) => alloc::string::String::from(s),
            Err(_) => format!(
                "0x{}",
                self.inner_as_ref()
                    .iter()
                    .map(|b| format!("{b:02x}"))
                    .collect::<String>()
            ),
        }
    }
}

impl fmt::Display for B064K<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let inner = self
            .inner_as_ref()
            .iter()
            .map(|byte| format!("{byte:02x}"))
            .collect::<String>();
        write!(f, "B064K({inner})")
    }
}

impl fmt::Display for U256<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let inner = self
            .inner_as_ref()
            .iter()
            .rev()
            .map(|byte| format!("{byte:02x}"))
            .collect::<String>();
        write!(f, "U256({inner})")
    }
}

impl fmt::Display for Seq0255<'_, U256<'_>> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let len = self.0.len();
        let as_hex = |item: &U256<'_>| {
            item.inner_as_ref()
                .iter()
                .rev()
                .map(|byte| format!("{byte:02x}"))
                .collect::<String>()
        };
        write!(f, "Seq0255<len={len}: ")?;
        match len {
            0 => write!(f, "[]"),
            1 => write!(f, "{}]", as_hex(&self.0[0])),
            2 => write!(f, "{}, {}]", as_hex(&self.0[0]), as_hex(&self.0[1])),
            3 => write!(
                f,
                "[{}, {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[2])
            ),
            _ => write!(
                f,
                "[{}, {}, ... , {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[len - 2]),
                as_hex(&self.0[len - 1])
            ),
        }
    }
}

impl fmt::Display for Seq064K<'_, B016M<'_>> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let len = self.0.len();
        let as_hex = |item: &B016M<'_>| {
            item.inner_as_ref()
                .iter()
                .map(|byte| format!("{byte:02x}"))
                .collect::<String>()
        };
        write!(f, "Seq064K<len={len}: ")?;
        match len {
            0 => write!(f, "[]"),
            1 => write!(f, "[{}]", as_hex(&self.0[0])),
            2 => write!(f, "[{}, {}]", as_hex(&self.0[0]), as_hex(&self.0[1])),
            3 => write!(
                f,
                "[{}, {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[2])
            ),
            _ => write!(
                f,
                "[{}, {}, ... , {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[len - 2]),
                as_hex(&self.0[len - 1])
            ),
        }
    }
}

impl fmt::Display for Seq064K<'_, U256<'_>> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let len = self.0.len();
        let as_hex = |item: &U256<'_>| {
            item.inner_as_ref()
                .iter()
                .map(|byte| format!("{byte:02x}"))
                .collect::<String>()
        };
        write!(f, "Seq064K<len={len}: ")?;
        match len {
            0 => write!(f, "[]"),
            1 => write!(f, "[{}]", as_hex(&self.0[0])),
            2 => write!(f, "[{}, {}]", as_hex(&self.0[0]), as_hex(&self.0[1])),
            3 => write!(
                f,
                "[{}, {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[2])
            ),
            _ => write!(
                f,
                "[{}, {}, ... , {}, {}]",
                as_hex(&self.0[0]),
                as_hex(&self.0[1]),
                as_hex(&self.0[len - 2]),
                as_hex(&self.0[len - 1])
            ),
        }
    }
}

impl fmt::Display for Seq064K<'_, u16> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let len = self.0.len();
        write!(f, "Seq064K<len={len}: ")?;
        match len {
            0 => write!(f, "[]"),
            1 => write!(f, "[{}]", self.0[0]),
            2 => write!(f, "[{}, {}]", self.0[0], self.0[1]),
            3 => write!(f, "[{}, {}, {}]", self.0[0], self.0[1], self.0[2]),
            _ => write!(
                f,
                "[{}, {}, ... , {}, {}]",
                self.0[0],
                self.0[1],
                self.0[len - 2],
                self.0[len - 1]
            ),
        }
    }
}
impl fmt::Display for Seq064K<'_, u32> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let len = self.0.len();
        write!(f, "Seq064K<len={len}: ")?;
        match len {
            0 => write!(f, "[]"),
            1 => write!(f, "[{}]", self.0[0]),
            2 => write!(f, "[{}, {}]", self.0[0], self.0[1]),
            3 => write!(f, "[{}, {}, {}]", self.0[0], self.0[1], self.0[2]),
            _ => write!(
                f,
                "[{}, {}, ... , {}, {}]",
                self.0[0],
                self.0[1],
                self.0[len - 2],
                self.0[len - 1]
            ),
        }
    }
}

impl fmt::Display for B032<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let item = self
            .inner_as_ref()
            .iter()
            .map(|byte| format!("{byte:02x}"))
            .collect::<String>();
        write!(f, "B032({item})")
    }
}

impl From<[u8; 32]> for U256<'_> {
    fn from(v: [u8; 32]) -> Self {
        Inner::Owned(v.into())
    }
}

#[cfg(feature = "prop_test")]
impl<'a> U256<'a> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let mut inner = Vec::<u8>::arbitrary(g);
        inner.resize(32, 0);
        // 32 Bytes arrays are always converted into U256 unwrap never panic
        let inner: [u8; 32] = inner.try_into().unwrap();
        inner.into()
    }
}

#[cfg(feature = "prop_test")]
impl<'a> B016M<'a> {
    pub fn from_gen(g: &mut Gen) -> Self {
        // This can fail but is used only for tests purposes
        Vec::<u8>::arbitrary(g).try_into().unwrap()
    }
}

use core::convert::{TryFrom, TryInto};

// Attempts to convert a `String` into a `Str0255<'a>`.
impl TryFrom<String> for Str0255<'_> {
    type Error = crate::Error;

    fn try_from(value: String) -> Result<Self, Self::Error> {
        value.into_bytes().try_into()
    }
}

/// Represents a reference to a 32-bit unsigned integer (`u32`),
/// providing methods for convenient conversions.
impl U32AsRef<'_> {
    /// Returns the `u32` value represented by this reference.
    pub fn as_u32(&self) -> u32 {
        let inner = self.inner_as_ref();
        u32::from_le_bytes([inner[0], inner[1], inner[2], inner[3]])
    }
}

impl From<u32> for U32AsRef<'_> {
    fn from(v: u32) -> Self {
        let bytes = v.to_le_bytes();
        let inner = vec![bytes[0], bytes[1], bytes[2], bytes[3]];
        U32AsRef::Owned(inner)
    }
}

impl<'a> From<&'a U32AsRef<'a>> for u32 {
    fn from(v: &'a U32AsRef<'a>) -> Self {
        let b = v.inner_as_ref();
        u32::from_le_bytes([b[0], b[1], b[2], b[3]])
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/datatypes/non_copy_data_types/seq_inner.rs">
// # Sequence and Optional Data Structures
//
// Provides specialized implementations of sequences and optional data types, primarily
// designed to handle serialized data with fixed size constraints. These structures are particularly
// suited for encoding and decoding variable-length and optional data fields within serialized
// formats.
//
// ## Provided Types
//
// ### `Seq0255`
// - Represents a sequence of up to 255 elements.
// - Includes utility methods such as:
//   - `to_vec()`: Converts each element into its byte vector representation.
//   - `inner_as_ref()`: Provides references to the inner data for each element.
//   - `new()`: Creates a `Seq0255` instance, enforcing the maximum length constraint.
// - Implements the `Decodable` trait for seamless deserialization, and `GetSize` to calculate the
//   encoded size, ensuring compatibility with various serialization formats.
//
// ### `Seq064K`
// - Represents a sequence of up to 65535 elements.
// - Similar to `Seq0255`, it provides:
//   - `to_vec()` and `inner_as_ref()` methods to convert or reference each element.
//   - `new()` enforces the maximum size limit, preventing excess memory usage.
// - Like `Seq0255`, `Seq064K` is `Decodable` and implements `GetSize`, making it versatile for
//   serialization scenarios.
//
// ### `Sv2Option`
// - Represents an optional data type, encoding a single or absent element.
// - Provides `to_option()` to convert to a standard `Option<Vec<u8>>`.
// - `new()` and `into_inner()` enable flexible conversions between `Option` and `Sv2Option`.
//
// ## Utility Macros
//
// - `impl_codec_for_sequence!`: Implements the `Decodable` trait for a sequence type, allowing for
//   a custom deserialization process that interprets field markers.
// - `impl_into_encodable_field_for_seq!`: Implements conversions to `EncodableField` for a
//   sequence, adapting the sequence for inclusion in serialized structures.
//
// ## Build Options
//
// - `prop_test`: Enables property-based testing compatibility by implementing `TryFrom` for `Vec`
//   conversions.
// - `no_std`: Allows the module to be used in `no_std` environments by disabling `std::io::Read`
//   dependencies.

use crate::{
    codec::{
        decodable::{Decodable, DecodableField, FieldMarker, GetMarker, PrimitiveMarker},
        encodable::{EncodableField, EncodablePrimitive},
        Fixed, GetSize,
    },
    datatypes::{Sv2DataType, *},
    Error,
};
use core::marker::PhantomData;

// TODO add test for that
impl<'a, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Seq0255<'a, super::inner::Inner<'a, false, SIZE, HEADERSIZE, MAXSIZE>>
{
    /// Converts the inner types to owned vector, and collects.
    pub fn to_vec(&self) -> Vec<Vec<u8>> {
        self.0.iter().map(|x| x.to_vec()).collect()
    }
    /// Converts the inner types to shared reference, and collects.
    pub fn inner_as_ref(&self) -> Vec<&[u8]> {
        self.0.iter().map(|x| x.inner_as_ref()).collect()
    }
}

// TODO add test for that
impl<'a, const SIZE: usize> Seq0255<'a, super::inner::Inner<'a, true, SIZE, 0, 0>> {
    /// Converts the inner types to owned vector, and collects.
    pub fn to_vec(&self) -> Vec<Vec<u8>> {
        self.0.iter().map(|x| x.to_vec()).collect()
    }

    /// Converts the inner types to shared reference, and collects.
    pub fn inner_as_ref(&self) -> Vec<&[u8]> {
        self.0.iter().map(|x| x.inner_as_ref()).collect()
    }
}
// TODO add test for that
impl<'a, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Seq064K<'a, super::inner::Inner<'a, false, SIZE, HEADERSIZE, MAXSIZE>>
{
    /// Converts the inner types to owned vector, and collects.
    pub fn to_vec(&self) -> Vec<Vec<u8>> {
        self.0.iter().map(|x| x.to_vec()).collect()
    }

    /// Converts the inner types to shared reference, and collects.
    pub fn inner_as_ref(&self) -> Vec<&[u8]> {
        self.0.iter().map(|x| x.inner_as_ref()).collect()
    }
}

// TODO add test for that
impl<'a, const SIZE: usize> Seq064K<'a, super::inner::Inner<'a, true, SIZE, 0, 0>> {
    /// Converts the inner types to owned vector, and collects.
    pub fn to_vec(&self) -> Vec<Vec<u8>> {
        self.0.iter().map(|x| x.to_vec()).collect()
    }

    /// Converts the inner types to shared reference, and collects.
    pub fn inner_as_ref(&self) -> Vec<&[u8]> {
        self.0.iter().map(|x| x.inner_as_ref()).collect()
    }
}

#[cfg(not(feature = "no_std"))]
use std::io::Read;

/// [`Seq0255`] represents a sequence with a maximum length of 255 elements.
/// This structure uses a generic type `T` and a lifetime parameter `'a`.
#[repr(C)]
#[derive(Debug, Clone, Eq, PartialEq)]
pub struct Seq0255<'a, T>(pub Vec<T>, PhantomData<&'a T>);

impl<'a, T: 'a> Seq0255<'a, T> {
    const HEADERSIZE: usize = 1;

    // Determines the expected length of the sequence by examining the first byte of `data`.
    fn expected_len(data: &[u8]) -> Result<usize, Error> {
        if data.len() >= Self::HEADERSIZE {
            Ok(data[0] as usize)
        } else {
            Err(Error::ReadError(data.len(), Self::HEADERSIZE))
        }
    }

    /// Creates a new `Seq0255` instance with the given inner vector.
    pub fn new(inner: Vec<T>) -> Result<Self, Error> {
        if inner.len() <= 255 {
            Ok(Self(inner, PhantomData))
        } else {
            Err(Error::SeqExceedsMaxSize)
        }
    }

    /// Consumes the `Seq0255` and returns the inner vector of elements.
    pub fn into_inner(self) -> Vec<T> {
        self.0
    }
}

impl<T: GetSize> GetSize for Seq0255<'_, T> {
    // Calculates the total size of the sequence in bytes.
    fn get_size(&self) -> usize {
        let mut size = Self::HEADERSIZE;
        for with_size in &self.0 {
            size += with_size.get_size()
        }
        size
    }
}

/// [`Seq064K`] represents a sequence with a maximum length of 65535 elements.
/// This structure uses a generic type `T` and a lifetime parameter `'a`.
#[derive(Debug, Clone, Eq, PartialEq)]
pub struct Seq064K<'a, T>(pub(crate) Vec<T>, PhantomData<&'a T>);

impl<'a, T: 'a> Seq064K<'a, T> {
    const HEADERSIZE: usize = 2;

    // Determines the expected length of the sequence by examining the first two bytes of `data`.
    fn expected_len(data: &[u8]) -> Result<usize, Error> {
        if data.len() >= Self::HEADERSIZE {
            Ok(u16::from_le_bytes([data[0], data[1]]) as usize)
        } else {
            Err(Error::ReadError(data.len(), Self::HEADERSIZE))
        }
    }

    /// Creates a new `Seq064K` instance with the given inner vector.
    pub fn new(inner: Vec<T>) -> Result<Self, Error> {
        if inner.len() <= 65535 {
            Ok(Self(inner, PhantomData))
        } else {
            Err(Error::SeqExceedsMaxSize)
        }
    }

    /// Consumes the `Seq064K` and returns the inner vector of elements.
    pub fn into_inner(self) -> Vec<T> {
        self.0
    }
}

impl<T: GetSize> GetSize for Seq064K<'_, T> {
    fn get_size(&self) -> usize {
        let mut size = Self::HEADERSIZE;
        for with_size in &self.0 {
            size += with_size.get_size()
        }
        size
    }
}

/// Macro to implement encoding and decoding traits for sequence types (`Seq0255`, `Seq064K`, and
/// `Sv2Option`).
macro_rules! impl_codec_for_sequence {
    ($a:ty) => {
        impl<'a, T: 'a + Sv2DataType<'a> + GetMarker + GetSize + Decodable<'a>> Decodable<'a>
            for $a
        {
            fn get_structure(
                data: &[u8],
            ) -> Result<Vec<crate::codec::decodable::FieldMarker>, Error> {
                let len = Self::expected_len(data)?;
                let mut inner = Vec::with_capacity(len + Self::HEADERSIZE);
                for _ in 0..Self::HEADERSIZE {
                    inner.push(FieldMarker::Primitive(PrimitiveMarker::U8));
                }
                let inner_type = T::get_marker();
                inner.resize(len + Self::HEADERSIZE, inner_type);
                Ok(inner)
            }

            fn from_decoded_fields(
                data: Vec<crate::codec::decodable::DecodableField<'a>>,
            ) -> Result<Self, Error> {
                let mut inner: Vec<T> = Vec::with_capacity(data.len());
                let mut i = 0;
                for element in data {
                    if i >= Self::HEADERSIZE {
                        match element {
                            DecodableField::Primitive(p) => {
                                let element =
                                    T::from_decoded_fields(vec![DecodableField::Primitive(p)]);
                                inner.push(element?)
                            }
                            // A struct always recursivly call decode until it reach a primitive
                            DecodableField::Struct(_) => unreachable!(),
                        }
                    }
                    i += 1;
                }
                Ok(Self(inner, PhantomData))
            }

            fn from_bytes(data: &'a mut [u8]) -> Result<Self, Error> {
                let len = Self::expected_len(data)?;

                let mut inner = Vec::new();
                let mut tail = &mut data[Self::HEADERSIZE..];

                for _ in 0..len {
                    let element_size = T::size_hint(tail, 0)?;
                    if element_size > tail.len() {
                        return Err(Error::OutOfBound);
                    }
                    let (head, t) = tail.split_at_mut(element_size);
                    tail = t;
                    inner.push(T::from_bytes_unchecked(head));
                }
                Ok(Self(inner, PhantomData))
            }

            #[cfg(not(feature = "no_std"))]
            fn from_reader(reader: &mut impl Read) -> Result<Self, Error> {
                let mut header = vec![0; Self::HEADERSIZE];
                reader.read_exact(&mut header)?;

                let len = Self::expected_len(&header)?;

                let mut inner = Vec::new();

                for _ in 0..len {
                    inner.push(T::from_reader_(reader)?);
                }
                Ok(Self(inner, PhantomData))
            }
        }
    };
}

// Implementations for encoding/decoding
impl_codec_for_sequence!(Seq0255<'a, T>);
impl_codec_for_sequence!(Seq064K<'a, T>);
impl_codec_for_sequence!(Sv2Option<'a, T>);

/// The `impl_into_encodable_field_for_seq` macro provides implementations of the `From` trait
/// to convert `Seq0255`, `Seq064K`, and `Sv2Option` types into `EncodableField`, making these
/// sequence types compatible with encoding.
macro_rules! impl_into_encodable_field_for_seq {
    ($a:ty) => {
        impl<'a> From<Seq064K<'a, $a>> for EncodableField<'a> {
            fn from(v: Seq064K<'a, $a>) -> Self {
                let inner_len = v.0.len() as u16;
                let mut as_encodable: Vec<EncodableField> =
                    Vec::with_capacity((inner_len + 2) as usize);
                as_encodable.push(EncodableField::Primitive(EncodablePrimitive::OwnedU8(
                    inner_len.to_le_bytes()[0],
                )));
                as_encodable.push(EncodableField::Primitive(EncodablePrimitive::OwnedU8(
                    inner_len.to_le_bytes()[1],
                )));
                for element in v.0 {
                    as_encodable.push(element.into());
                }
                EncodableField::Struct(as_encodable)
            }
        }

        impl<'a> From<Seq0255<'a, $a>> for EncodableField<'a> {
            fn from(v: Seq0255<$a>) -> Self {
                let inner_len = v.0.len() as u8;
                let mut as_encodable: Vec<EncodableField> =
                    Vec::with_capacity((inner_len + 1) as usize);
                as_encodable.push(EncodableField::Primitive(EncodablePrimitive::OwnedU8(
                    inner_len,
                )));
                for element in v.0 {
                    as_encodable.push(element.into());
                }
                EncodableField::Struct(as_encodable)
            }
        }

        impl<'a> From<Sv2Option<'a, $a>> for EncodableField<'a> {
            fn from(v: Sv2Option<$a>) -> Self {
                let inner_len = v.0.len() as u8;
                let mut as_encodable: Vec<EncodableField> =
                    Vec::with_capacity((inner_len + 1) as usize);
                as_encodable.push(EncodableField::Primitive(EncodablePrimitive::OwnedU8(
                    inner_len,
                )));
                for element in v.0 {
                    as_encodable.push(element.into());
                }
                EncodableField::Struct(as_encodable)
            }
        }
    };
}

impl_into_encodable_field_for_seq!(bool);
impl_into_encodable_field_for_seq!(u8);
impl_into_encodable_field_for_seq!(u16);
impl_into_encodable_field_for_seq!(U24);
impl_into_encodable_field_for_seq!(u32);
impl_into_encodable_field_for_seq!(u64);
impl_into_encodable_field_for_seq!(U256<'a>);
impl_into_encodable_field_for_seq!(Signature<'a>);
impl_into_encodable_field_for_seq!(B0255<'a>);
impl_into_encodable_field_for_seq!(B064K<'a>);
impl_into_encodable_field_for_seq!(B016M<'a>);

#[cfg(feature = "prop_test")]
impl<'a, T> core::convert::TryFrom<Seq0255<'a, T>> for Vec<T> {
    type Error = &'static str;
    fn try_from(v: Seq0255<'a, T>) -> Result<Self, Self::Error> {
        if v.0.len() > 255 {
            Ok(v.0)
        } else {
            Err("Incorrect length, expected 225")
        }
    }
}

#[cfg(feature = "prop_test")]
impl<'a, T> core::convert::TryFrom<Seq064K<'a, T>> for Vec<T> {
    type Error = &'static str;
    fn try_from(v: Seq064K<'a, T>) -> Result<Self, Self::Error> {
        if v.0.len() > 64 {
            Ok(v.0)
        } else {
            Err("Incorrect length, expected 64")
        }
    }
}

impl<T> From<Vec<T>> for Seq0255<'_, T> {
    fn from(v: Vec<T>) -> Self {
        Seq0255(v, PhantomData)
    }
}

impl<T> From<Vec<T>> for Seq064K<'_, T> {
    fn from(v: Vec<T>) -> Self {
        Seq064K(v, PhantomData)
    }
}

impl<T: Fixed> Seq0255<'_, T> {
    /// converts the lifetime to static
    pub fn into_static(self) -> Seq0255<'static, T> {
        // Safe unwrap cause the initial value is a valid Seq0255
        Seq0255::new(self.0).unwrap()
    }
}
impl<T: Fixed> Sv2Option<'_, T> {
    /// converts the lifetime to static
    pub fn into_static(self) -> Sv2Option<'static, T> {
        Sv2Option::new(self.into_inner())
    }
}

impl<'a, const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Seq0255<'a, Inner<'a, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>>
{
    /// converts the lifetime to static
    pub fn into_static(
        self,
    ) -> Seq0255<'static, Inner<'static, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>> {
        let seq = self.0;
        let static_seq = seq.into_iter().map(|x| x.into_static()).collect();
        // Safe unwrap cause the initial value is a valid Seq0255
        Seq0255::new(static_seq).unwrap()
    }
}

impl<'a, const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Sv2Option<'a, Inner<'a, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>>
{
    /// converts the lifetime to static
    pub fn into_static(
        self,
    ) -> Sv2Option<'static, Inner<'static, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>> {
        let inner = self.into_inner();
        let static_inner = inner.map(|x| x.into_static());
        Sv2Option::new(static_inner)
    }
}

impl<T: Fixed> Seq064K<'_, T> {
    /// converts the lifetime to static
    pub fn into_static(self) -> Seq064K<'static, T> {
        // Safe unwrap cause the initial value is a valid Seq064K
        Seq064K::new(self.0).unwrap()
    }
}

impl<'a, const ISFIXED: bool, const SIZE: usize, const HEADERSIZE: usize, const MAXSIZE: usize>
    Seq064K<'a, Inner<'a, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>>
{
    /// converts the lifetime to static
    pub fn into_static(
        self,
    ) -> Seq064K<'static, Inner<'static, ISFIXED, SIZE, HEADERSIZE, MAXSIZE>> {
        let seq = self.0;
        let static_seq = seq.into_iter().map(|x| x.into_static()).collect();
        // Safe unwrap cause the initial value is a valid Seq064K
        Seq064K::new(static_seq).unwrap()
    }
}

/// The lifetime 'a is defined.
#[repr(C)]
#[derive(Debug, Clone, Eq, PartialEq)]
pub struct Sv2Option<'a, T>(pub Vec<T>, PhantomData<&'a T>);

// TODO add test for that
impl<'a, const SIZE: usize> Sv2Option<'a, super::inner::Inner<'a, true, SIZE, 0, 0>> {
    /// Gets the owned first element of the sequence, if present
    pub fn to_option(&self) -> Option<Vec<u8>> {
        let v: Vec<Vec<u8>> = self.0.iter().map(|x| x.to_vec()).collect();
        match v.len() {
            0 => None,
            1 => Some(v[0].clone()),
            // is impossible to deserialize Sv2Options with len bigger than 1
            _ => unreachable!(),
        }
    }
    /// Gets the reference to first element of the sequence, if present
    pub fn inner_as_ref(&self) -> Option<&[u8]> {
        let v: Vec<&[u8]> = self.0.iter().map(|x| x.inner_as_ref()).collect();
        match v.len() {
            0 => None,
            1 => Some(v[0]),
            // is impossible to deserialize Sv2Options with len bigger than 1
            _ => unreachable!(),
        }
    }
}

impl<'a, T: 'a> Sv2Option<'a, T> {
    const HEADERSIZE: usize = 1;

    /// Return the len of the inner vector
    fn expected_len(data: &[u8]) -> Result<usize, Error> {
        if data.len() >= Self::HEADERSIZE {
            match data[0] {
                0 => Ok(0),
                1 => Ok(1),
                _ => Err(Error::Sv2OptionHaveMoreThenOneElement(data[0])),
            }
        } else {
            Err(Error::ReadError(data.len(), Self::HEADERSIZE))
        }
    }

    /// Initializes a new option type
    pub fn new(inner: Option<T>) -> Self {
        match inner {
            Some(x) => Self(vec![x], PhantomData),
            None => Self(vec![], PhantomData),
        }
    }

    /// Gets the inner value of Sv2Option
    pub fn into_inner(mut self) -> Option<T> {
        let len = self.0.len();
        match len {
            0 => None,
            // safe unwrap we already checked the len
            1 => Some(self.0.pop().unwrap()),
            // is impossible to deserialize Sv2Options with len bigger than 1
            _ => unreachable!(),
        }
    }
}

impl<T: GetSize> GetSize for Sv2Option<'_, T> {
    fn get_size(&self) -> usize {
        let mut size = Self::HEADERSIZE;
        for with_size in &self.0 {
            size += with_size.get_size()
        }
        size
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/codec/src/lib.rs">
//! Defines types, encodings, and conversions between custom datatype and standard Rust type,
//! providing abstractions for encoding, decoding, and error handling of SV2 data types.
//!
//! # Overview
//!
//! Enables conversion between various Rust types and SV2-specific data formats for efficient
//! network communication. Provides utilities to encode and decode data types according to the SV2
//! specifications.
//!
//! ## Type Mappings
//! The following table illustrates how standard Rust types map to their SV2 counterparts:
//!
//! ```txt
//! bool     <-> BOOL
//! u8       <-> U8
//! u16      <-> U16
//! U24      <-> U24
//! u32      <-> U32
//! f32      <-> F32     // Not in the spec, but used
//! u64      <-> U64     
//! U256     <-> U256
//! Str0255  <-> STRO_255
//! Signature<-> SIGNATURE
//! B032     <-> B0_32   
//! B0255    <-> B0_255
//! B064K    <-> B0_64K
//! B016M    <-> B0_16M
//! [u8]     <-> BYTES
//! Pubkey   <-> PUBKEY
//! Seq0255  <-> SEQ0_255[T]
//! Seq064K  <-> SEQ0_64K[T]
//! ```
//!
//! # Encoding & Decoding
//!
//! Enables conversion between various Rust types and SV2-specific data formats for efficient
//! network communication. Provides utilities to encode and decode data types according to the SV2
//! specifications.
//!
//! - **to_bytes**: Encodes an SV2 data type into a byte vector.
//! - **to_writer**: Encodes an SV2 data type into a byte slice.
//! - **from_bytes**: Decodes an SV2-encoded byte slice into the specified data type.
//!
//! # Error Handling
//!
//! Defines an `Error` enum for handling failure conditions during encoding, decoding, and data
//! manipulation. Common errors include:
//! - Out-of-bounds accesses
//! - Size mismatches during encoding/decoding
//! - Invalid data representations, such as non-boolean values interpreted as booleans.
//!
//! # Cross-Language Interoperability
//!
//! To support foreign function interface (FFI) use cases, the module includes `CError` and `CVec`
//! types that represent SV2 data and errors in a format suitable for cross-language compatibility.
//!
//! # Build Options
//!
//! Supports optional features like `no_std` for environments without standard library support.
//! Error types are conditionally compiled to work with or without `std`.
//!
//! ## Conditional Compilation
//! - With the `no_std` feature enabled, I/O-related errors use a simplified `IoError`
//!   representation.
//! - Standard I/O errors (`std::io::Error`) are used when `no_std` is disabled.
//!
//! # FFI Interoperability
//!
//! Provides utilities for FFI (Foreign Function Interface) to enable data passing between Rust and
//! other languages. Includes:
//! - `CVec`: Represents a byte vector for safe passing between C and Rust.
//! - `CError`: A C-compatible error type.
//! - `CVec2`: Manages collections of `CVec` objects across FFI boundaries.
//!
//! Facilitates integration of SV2 functionality into cross-language projects.

#![cfg_attr(feature = "no_std", no_std)]

#[cfg(not(feature = "no_std"))]
use std::io::{Error as E, ErrorKind};

mod codec;
mod datatypes;
pub use datatypes::{
    PubKey, Seq0255, Seq064K, Signature, Str0255, Sv2DataType, Sv2Option, U32AsRef, B016M, B0255,
    B032, B064K, U24, U256,
};

pub use crate::codec::{
    decodable::{Decodable, GetMarker},
    encodable::{Encodable, EncodableField},
    Fixed, GetSize, SizeHint,
};

use alloc::vec::Vec;

/// Converts the provided SV2 data type to a byte vector based on the SV2 encoding format.
#[allow(clippy::wrong_self_convention)]
pub fn to_bytes<T: Encodable + GetSize>(src: T) -> Result<Vec<u8>, Error> {
    let mut result = vec![0_u8; src.get_size()];
    src.to_bytes(&mut result)?;
    Ok(result)
}

/// Encodes the SV2 data type to the provided byte slice.
#[allow(clippy::wrong_self_convention)]
pub fn to_writer<T: Encodable>(src: T, dst: &mut [u8]) -> Result<(), Error> {
    src.to_bytes(dst)?;
    Ok(())
}

/// Decodes an SV2-encoded byte slice into the specified data type.
pub fn from_bytes<'a, T: Decodable<'a>>(data: &'a mut [u8]) -> Result<T, Error> {
    T::from_bytes(data)
}

/// Provides an interface and implementation details for decoding complex data structures
/// from raw bytes or I/O streams. Handles deserialization of nested and primitive data
/// structures through traits, enums, and helper functions for managing the decoding process.
///
/// # Overview
/// The [`Decodable`] trait serves as the core component, offering methods to define a type's
/// structure, decode raw byte data, and construct instances from decoded fields. It supports both
/// in-memory byte slices and I/O streams for flexibility across deserialization use cases.
///
/// # Key Concepts and Types
/// - **[`Decodable`] Trait**: Defines methods to decode types from byte data, process individual
///   fields, and construct complete types.
/// - **[`FieldMarker`] and `PrimitiveMarker`**: Enums that represent data types or structures,
///   guiding the decoding process by defining field structures and types.
/// - **[`DecodableField`] and `DecodablePrimitive`**: Represent decoded fields as either primitives
///   or nested structures, forming the building blocks for complex data types.
///
/// # Error Handling
/// Custom error types manage issues during decoding, such as insufficient data or unsupported
/// types. Errors are surfaced through `Result` types to ensure reliability in data parsing tasks.
///
/// # `no_std` Support
/// Compatible with `no_std` environments through conditional compilation. Omits I/O-dependent
/// methods like `from_reader` when `no_std` is enabled, ensuring lightweight builds for constrained
/// environments.
pub mod decodable {
    pub use crate::codec::decodable::{Decodable, DecodableField, FieldMarker};
    //pub use crate::codec::decodable::PrimitiveMarker;
}

/// Provides an encoding framework for serializing various data types into bytes.
///
/// The [`Encodable`] trait is the core of this framework, enabling types to define
/// how they serialize data into bytes. This is essential for transmitting data
/// between components or systems in a consistent, byte-oriented format.
///
/// ## Overview
///
/// Supports a wide variety of data types, including basic types (e.g., integers,
/// booleans, and byte arrays) and complex structures. Each types encoding logic is
/// encapsulated in enums like [`EncodablePrimitive`] and [`EncodableField`], enabling
/// structured and hierarchical data serialization.
///
/// ### Key Types
///
/// - **[`Encodable`]**: Defines methods for converting an object into a byte array or writing it
///   directly to an output stream. It supports both primitive types and complex structures.
/// - **[`EncodablePrimitive`]**: Represents basic types that can be serialized directly. Includes
///   data types like integers, booleans, and byte arrays.
/// - **[`EncodableField`]**: Extends [`EncodablePrimitive`] to support structured and nested data,
///   enabling recursive encoding of complex structures.
///
/// ### `no_std` Compatibility
///
/// When compiled with the `no_std` feature enabled, this module omits the `to_writer` method
/// to support environments without the standard library. Only buffer-based encoding
/// (`to_bytes`) is available in this mode.
///
/// ## Error Handling
///
/// Errors during encoding are handled through the [`Error`] type. Common failure scenarios include
/// buffer overflows and type-specific serialization errors. Each encoding method returns an
/// appropriate error if encoding fails, supporting comprehensive error management.
///
/// ## Trait Details
///
/// ### [`Encodable`]
/// - **`to_bytes`**: Encodes the instance into a byte slice, returning the number of bytes written
///   or an error if encoding fails.
/// - **`to_writer`** (requires `std`): Encodes the instance into any [`Write`] implementor, such as
///   a file or network stream.
///
/// ### Additional Enums and Methods
///
/// Includes utility types and methods for calculating sizes, encoding hierarchical data,
/// and supporting both owned and reference-based data variants.
///
/// - **[`EncodablePrimitive`]**: Handles encoding logic for primitive types, addressing
///   serialization requirements specific to each type.
/// - **[`EncodableField`]**: Extends encoding to support composite types and structured data,
///   enabling recursive encoding of nested structures.
///
/// ## Summary
///
/// Designed for flexibility and extensibility, this module supports a wide range of data
/// serialization needs through customizable encoding strategies. Implementing the
/// [`Encodable`] trait for custom types ensures efficient and consistent data serialization
/// across applications.
pub mod encodable {
    pub use crate::codec::encodable::{Encodable, EncodableField, EncodablePrimitive};
}

#[macro_use]
extern crate alloc;

/// Error types used within the protocol library to indicate various failure conditions.
#[derive(Debug, PartialEq, Eq, Clone)]
pub enum Error {
    /// Indicates an attempt to read beyond a valid range.
    OutOfBound,

    /// Raised when a non-binary value is interpreted as a boolean.
    NotABool(u8),

    /// Occurs when an unexpected size mismatch arises during a write operation, specifying
    /// expected and actual sizes.
    WriteError(usize, usize),

    /// Signifies an overflow condition where a `u32` exceeds the maximum allowable `u24` value.
    U24TooBig(u32),

    /// Reports a size mismatch for a signature, such as when it does not match the expected size.
    InvalidSignatureSize(usize),

    /// Raised when a `u256` value is invalid, typically due to size discrepancies.
    InvalidU256(usize),

    /// Indicates an invalid `u24` representation.
    InvalidU24(u32),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B0255`.
    InvalidB0255Size(usize),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B064K`.
    InvalidB064KSize(usize),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B016M`.
    InvalidB016MSize(usize),

    /// Raised when a sequence size exceeds `0255`.
    InvalidSeq0255Size(usize),

    /// Error when trying to encode a non-primitive data type.
    NonPrimitiveTypeCannotBeEncoded,

    /// Generic conversion error related to primitive types.
    PrimitiveConversionError,

    /// Error occurring during decoding due to conversion issues.
    DecodableConversionError,

    /// Error triggered when a decoder is used without initialization.
    UnInitializedDecoder,

    #[cfg(not(feature = "no_std"))]
    /// Represents I/O-related errors, compatible with `no_std` mode where specific error types may
    /// vary.
    IoError(E),

    #[cfg(feature = "no_std")]
    /// Represents I/O-related errors, compatible with `no_std` mode.
    IoError,

    /// Raised when an unexpected mismatch occurs during read operations, specifying expected and
    /// actual read sizes.
    ReadError(usize, usize),

    /// Used as a marker error for fields that should remain void or empty.
    VoidFieldMarker,

    /// Signifies a value overflow based on protocol restrictions, containing details about
    /// fixed/variable size, maximum size allowed, and the offending value details.
    ValueExceedsMaxSize(bool, usize, usize, usize, Vec<u8>, usize),

    /// Triggered when a sequence type (`Seq0255`, `Seq064K`) exceeds its maximum allowable size.
    SeqExceedsMaxSize,

    /// Raised when no valid decodable field is provided during decoding.
    NoDecodableFieldPassed,

    /// Error for protocol-specific invalid values.
    ValueIsNotAValidProtocol(u8),

    /// Raised when an unsupported or unknown message type is encountered.
    UnknownMessageType(u8),

    /// Indicates a protocol constraint violation where `Sv2Option` unexpectedly contains multiple
    /// elements.
    Sv2OptionHaveMoreThenOneElement(u8),
}

#[cfg(not(feature = "no_std"))]
impl From<E> for Error {
    fn from(v: E) -> Self {
        match v.kind() {
            ErrorKind::UnexpectedEof => Error::OutOfBound,
            _ => Error::IoError(v),
        }
    }
}

/// `CError` is a foreign function interface (FFI)-compatible version of the `Error` enum to
/// facilitate cross-language compatibility.
#[repr(C)]
#[derive(Debug)]
pub enum CError {
    /// Indicates an attempt to read beyond a valid range.
    OutOfBound,

    /// Raised when a non-binary value is interpreted as a boolean.
    NotABool(u8),

    /// Occurs when an unexpected size mismatch arises during a write operation, specifying
    /// expected and actual sizes.
    WriteError(usize, usize),

    /// Signifies an overflow condition where a `u32` exceeds the maximum allowable `u24` value.
    U24TooBig(u32),

    /// Reports a size mismatch for a signature, such as when it does not match the expected size.
    InvalidSignatureSize(usize),

    /// Raised when a `u256` value is invalid, typically due to size discrepancies.
    InvalidU256(usize),

    /// Indicates an invalid `u24` representation.
    InvalidU24(u32),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B0255`.
    InvalidB0255Size(usize),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B064K`.
    InvalidB064KSize(usize),

    /// Error indicating that a byte array exceeds the maximum allowed size for `B016M`.
    InvalidB016MSize(usize),

    /// Raised when a sequence size exceeds `0255`.
    InvalidSeq0255Size(usize),

    /// Error when trying to encode a non-primitive data type.
    NonPrimitiveTypeCannotBeEncoded,

    /// Generic conversion error related to primitive types.
    PrimitiveConversionError,

    /// Error occurring during decoding due to conversion issues.
    DecodableConversionError,

    /// Error triggered when a decoder is used without initialization.
    UnInitializedDecoder,

    #[cfg(not(feature = "no_std"))]
    /// Represents I/O-related errors, compatible with `no_std` mode where specific error types may
    /// vary.
    IoError(E),

    #[cfg(feature = "no_std")]
    /// Represents I/O-related errors, compatible with `no_std` mode.
    IoError,

    /// Raised when an unexpected mismatch occurs during read operations, specifying expected and
    /// actual read sizes.
    ReadError(usize, usize),

    /// Used as a marker error for fields that should remain void or empty.
    VoidFieldMarker,

    /// Signifies a value overflow based on protocol restrictions, containing details about
    /// fixed/variable size, maximum size allowed, and the offending value details.
    ValueExceedsMaxSize(bool, usize, usize, usize, CVec, usize),

    /// Triggered when a sequence type (`Seq0255`, `Seq064K`) exceeds its maximum allowable size.
    SeqExceedsMaxSize,

    /// Raised when no valid decodable field is provided during decoding.
    NoDecodableFieldPassed,

    /// Error for protocol-specific invalid values.
    ValueIsNotAValidProtocol(u8),

    /// Raised when an unsupported or unknown message type is encountered.
    UnknownMessageType(u8),

    /// Indicates a protocol constraint violation where `Sv2Option` unexpectedly contains multiple
    /// elements.
    Sv2OptionHaveMoreThenOneElement(u8),
}

impl From<Error> for CError {
    fn from(e: Error) -> CError {
        match e {
            Error::OutOfBound => CError::OutOfBound,
            Error::NotABool(u) => CError::NotABool(u),
            Error::WriteError(u1, u2) => CError::WriteError(u1, u2),
            Error::U24TooBig(u) => CError::U24TooBig(u),
            Error::InvalidSignatureSize(u) => CError::InvalidSignatureSize(u),
            Error::InvalidU256(u) => CError::InvalidU256(u),
            Error::InvalidU24(u) => CError::InvalidU24(u),
            Error::InvalidB0255Size(u) => CError::InvalidB0255Size(u),
            Error::InvalidB064KSize(u) => CError::InvalidB064KSize(u),
            Error::InvalidB016MSize(u) => CError::InvalidB016MSize(u),
            Error::InvalidSeq0255Size(u) => CError::InvalidSeq0255Size(u),
            Error::NonPrimitiveTypeCannotBeEncoded => CError::NonPrimitiveTypeCannotBeEncoded,
            Error::PrimitiveConversionError => CError::PrimitiveConversionError,
            Error::DecodableConversionError => CError::DecodableConversionError,
            Error::UnInitializedDecoder => CError::UnInitializedDecoder,
            #[cfg(not(feature = "no_std"))]
            Error::IoError(e) => CError::IoError(e),
            #[cfg(feature = "no_std")]
            Error::IoError => CError::IoError,
            Error::ReadError(u1, u2) => CError::ReadError(u1, u2),
            Error::VoidFieldMarker => CError::VoidFieldMarker,
            Error::ValueExceedsMaxSize(isfixed, size, headersize, maxsize, bad_value, bad_len) => {
                let bv1: &[u8] = bad_value.as_ref();
                let bv: CVec = bv1.into();
                CError::ValueExceedsMaxSize(isfixed, size, headersize, maxsize, bv, bad_len)
            }
            Error::SeqExceedsMaxSize => CError::SeqExceedsMaxSize,
            Error::NoDecodableFieldPassed => CError::NoDecodableFieldPassed,
            Error::ValueIsNotAValidProtocol(u) => CError::ValueIsNotAValidProtocol(u),
            Error::UnknownMessageType(u) => CError::UnknownMessageType(u),
            Error::Sv2OptionHaveMoreThenOneElement(u) => CError::Sv2OptionHaveMoreThenOneElement(u),
        }
    }
}

impl Drop for CError {
    fn drop(&mut self) {
        match self {
            Self::OutOfBound => (),
            Self::NotABool(_) => (),
            Self::WriteError(_, _) => (),
            Self::U24TooBig(_) => (),
            Self::InvalidSignatureSize(_) => (),
            Self::InvalidU256(_) => (),
            Self::InvalidU24(_) => (),
            Self::InvalidB0255Size(_) => (),
            Self::InvalidB064KSize(_) => (),
            Self::InvalidB016MSize(_) => (),
            Self::InvalidSeq0255Size(_) => (),
            Self::NonPrimitiveTypeCannotBeEncoded => (),
            Self::PrimitiveConversionError => (),
            Self::DecodableConversionError => (),
            Self::UnInitializedDecoder => (),
            #[cfg(not(feature = "no_std"))]
            Self::IoError(_) => (),
            #[cfg(feature = "no_std")]
            Self::IoError => (),
            Self::ReadError(_, _) => (),
            Self::VoidFieldMarker => (),
            Self::ValueExceedsMaxSize(_, _, _, _, cvec, _) => free_vec(cvec),
            Self::SeqExceedsMaxSize => (),
            Self::NoDecodableFieldPassed => (),
            Self::ValueIsNotAValidProtocol(_) => (),
            Self::UnknownMessageType(_) => (),
            Self::Sv2OptionHaveMoreThenOneElement(_) => (),
        };
    }
}

/// Vec<u8> is used as the Sv2 type Bytes
impl GetSize for Vec<u8> {
    fn get_size(&self) -> usize {
        self.len()
    }
}

// Only needed for implement encodable for Frame never called
impl From<Vec<u8>> for EncodableField<'_> {
    fn from(_v: Vec<u8>) -> Self {
        unreachable!()
    }
}

#[cfg(feature = "with_buffer_pool")]
impl From<buffer_sv2::Slice> for EncodableField<'_> {
    fn from(_v: buffer_sv2::Slice) -> Self {
        unreachable!()
    }
}

/// A struct to facilitate transferring a `Vec<u8>` across FFI boundaries.
#[repr(C)]
#[derive(Debug, Clone, Copy)]
pub struct CVec {
    data: *mut u8,
    len: usize,
    capacity: usize,
}

impl CVec {
    /// Returns a mutable slice of the contained data.
    ///
    /// # Safety
    ///
    /// The caller must ensure that the data pointed to by `self.data`
    /// remains valid for the duration of the returned slice.
    pub fn as_mut_slice(&mut self) -> &mut [u8] {
        unsafe { core::slice::from_raw_parts_mut(self.data, self.len) }
    }

    /// Fills a buffer allocated in Rust from C.
    ///
    /// # Safety
    ///
    /// Constructs a `CVec` without taking ownership of the pointed buffer. If the owner drops the
    /// buffer, the `CVec` will point to invalid memory.
    #[allow(clippy::wrong_self_convention)]
    pub fn as_shared_buffer(v: &mut [u8]) -> Self {
        let (data, len) = (v.as_mut_ptr(), v.len());
        Self {
            data,
            len,
            capacity: len,
        }
    }
}

impl From<&[u8]> for CVec {
    fn from(v: &[u8]) -> Self {
        let mut buffer: Vec<u8> = vec![0; v.len()];
        buffer.copy_from_slice(v);

        // Get the length, first, then the pointer (doing it the other way around **currently**
        // doesn't cause UB, but it may be unsound due to unclear (to me, at least) guarantees of
        // the std lib)
        let len = buffer.len();
        let ptr = buffer.as_mut_ptr();
        core::mem::forget(buffer);

        CVec {
            data: ptr,
            len,
            capacity: len,
        }
    }
}

/// Creates a `CVec` from a buffer that was allocated in C.
///
/// # Safety
/// The caller must ensure that the buffer is valid and that
/// the data length does not exceed the allocated size.
#[no_mangle]
pub unsafe extern "C" fn cvec_from_buffer(data: *const u8, len: usize) -> CVec {
    let input = core::slice::from_raw_parts(data, len);

    let mut buffer: Vec<u8> = vec![0; len];
    buffer.copy_from_slice(input);

    // Get the length, first, then the pointer (doing it the other way around **currently** doesn't
    // cause UB, but it may be unsound due to unclear (to me, at least) guarantees of the std lib)
    let len = buffer.len();
    let ptr = buffer.as_mut_ptr();
    core::mem::forget(buffer);

    CVec {
        data: ptr,
        len,
        capacity: len,
    }
}

/// A struct to manage a collection of `CVec` objects across FFI boundaries.
#[repr(C)]
#[derive(Debug, Clone, Copy)]
pub struct CVec2 {
    data: *mut CVec,
    len: usize,
    capacity: usize,
}

impl CVec2 {
    /// `as_mut_slice`: helps to get a mutable slice
    pub fn as_mut_slice(&mut self) -> &mut [CVec] {
        unsafe { core::slice::from_raw_parts_mut(self.data, self.len) }
    }
}
impl From<CVec2> for Vec<CVec> {
    fn from(v: CVec2) -> Self {
        unsafe { Vec::from_raw_parts(v.data, v.len, v.capacity) }
    }
}

/// Frees the underlying memory of a `CVec`.
pub fn free_vec(buf: &mut CVec) {
    let _: Vec<u8> = unsafe { Vec::from_raw_parts(buf.data, buf.len, buf.capacity) };
}

/// Frees the underlying memory of a `CVec2` and all its elements.
pub fn free_vec_2(buf: &mut CVec2) {
    let vs: Vec<CVec> = unsafe { Vec::from_raw_parts(buf.data, buf.len, buf.capacity) };
    for mut s in vs {
        free_vec(&mut s)
    }
}

impl<'a, const A: bool, const B: usize, const C: usize, const D: usize>
    From<datatypes::Inner<'a, A, B, C, D>> for CVec
{
    fn from(v: datatypes::Inner<'a, A, B, C, D>) -> Self {
        let (ptr, len, cap): (*mut u8, usize, usize) = match v {
            datatypes::Inner::Ref(inner) => {
                // Data is copied in a vector that then will be forgetted from the allocator,
                // cause the owner of the data is going to be dropped by rust
                let mut inner: Vec<u8> = inner.into();

                // Get the length, first, then the pointer (doing it the other way around
                // **currently** doesn't cause UB, but it may be unsound due to unclear (to me, at
                // least) guarantees of the std lib)
                let len = inner.len();
                let cap = inner.capacity();
                let ptr = inner.as_mut_ptr();
                core::mem::forget(inner);

                (ptr, len, cap)
            }
            datatypes::Inner::Owned(mut inner) => {
                // Get the length, first, then the pointer (doing it the other way around
                // **currently** doesn't cause UB, but it may be unsound due to unclear (to me, at
                // least) guarantees of the std lib)
                let len = inner.len();
                let cap = inner.capacity();
                let ptr = inner.as_mut_ptr();
                core::mem::forget(inner);

                (ptr, len, cap)
            }
        };
        Self {
            data: ptr,
            len,
            capacity: cap,
        }
    }
}

/// Initializes an empty `CVec2`.
///
/// # Safety
/// The caller is responsible for freeing the `CVec2` when it is no longer needed.
#[no_mangle]
pub unsafe extern "C" fn init_cvec2() -> CVec2 {
    let mut buffer = Vec::<CVec>::new();

    // Get the length, first, then the pointer (doing it the other way around **currently** doesn't
    // cause UB, but it may be unsound due to unclear (to me, at least) guarantees of the std lib)
    let len = buffer.len();
    let ptr = buffer.as_mut_ptr();
    core::mem::forget(buffer);

    CVec2 {
        data: ptr,
        len,
        capacity: len,
    }
}

/// Adds a `CVec` to a `CVec2`.
///
/// # Safety
/// The caller must ensure no duplicate `CVec`s are added, as duplicates may
/// lead to double-free errors when the message is dropped.
#[no_mangle]
pub unsafe extern "C" fn cvec2_push(cvec2: &mut CVec2, cvec: CVec) {
    let mut buffer: Vec<CVec> = Vec::from_raw_parts(cvec2.data, cvec2.len, cvec2.capacity);
    buffer.push(cvec);

    let len = buffer.len();
    let ptr = buffer.as_mut_ptr();
    core::mem::forget(buffer);

    cvec2.data = ptr;
    cvec2.len = len;
    cvec2.capacity = len;
}

impl<'a, T: Into<CVec>> From<Seq0255<'a, T>> for CVec2 {
    fn from(v: Seq0255<'a, T>) -> Self {
        let mut v: Vec<CVec> = v.0.into_iter().map(|x| x.into()).collect();
        // Get the length, first, then the pointer (doing it the other way around **currently**
        // doesn't cause UB, but it may be unsound due to unclear (to me, at least) guarantees of
        // the std lib)
        let len = v.len();
        let capacity = v.capacity();
        let data = v.as_mut_ptr();
        core::mem::forget(v);
        Self {
            data,
            len,
            capacity,
        }
    }
}
impl<'a, T: Into<CVec>> From<Seq064K<'a, T>> for CVec2 {
    fn from(v: Seq064K<'a, T>) -> Self {
        let mut v: Vec<CVec> = v.0.into_iter().map(|x| x.into()).collect();
        // Get the length, first, then the pointer (doing it the other way around **currently**
        // doesn't cause UB, but it may be unsound due to unclear (to me, at least) guarantees of
        // the std lib)
        let len = v.len();
        let capacity = v.capacity();
        let data = v.as_mut_ptr();
        core::mem::forget(v);
        Self {
            data,
            len,
            capacity,
        }
    }
}

/// Exported FFI functions for interoperability with C code for u24
#[no_mangle]
pub extern "C" fn _c_export_u24(_a: U24) {}
/// Exported FFI functions for interoperability with C code for CVec
#[no_mangle]
pub extern "C" fn _c_export_cvec(_a: CVec) {}
/// Exported FFI functions for interoperability with C code for CVec2
#[no_mangle]
pub extern "C" fn _c_export_cvec2(_a: CVec2) {}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/.gitignore">
/target
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/Cargo.toml">
[package]
name = "derive_codec_sv2"
version = "1.1.1"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Derive macro for Sv2 binary format serializer and deserializer"
documentation = "https://docs.rs/derive_codec_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_codec_sv2 = { path="../codec", version = "^2.0.0" }

[lib]
proc-macro = true
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/README.md">
# derive_codec_sv2

[![crates.io](https://img.shields.io/crates/v/derive-codec-sv2.svg)](https://crates.io/crates/derive-codec-sv2)
[![docs.rs](https://docs.rs/derive-codec-sv2/badge.svg)](https://docs.rs/derive-codec-sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=binary_codec_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`derive-codec-sv2` is a no-std Rust crate offering procedural macros for automating serialization and deserialization of structs used within the Sv2 (Stratum V2) protocol. This crate provides `Encodable` and `Decodable` macros to streamline binary data handling, especially useful for protocol-level implementations where efficient encoding and decoding are essential.

## Key Capabilities

- **Automatic Encoding and Decoding**: Derives methods for converting structs to and from binary format, reducing boilerplate code for data    structures used in Sv2.
- **Attribute-Based Configuration**: Supports `#[already_sized]` attribute for marking fixed-size structs, enabling optimizations in binary handling.
- **Flexible Field Parsing**: Allows parsing of fields with lifetimes, generics, and static references, enhancing compatibility with various protocol requirements.
- **Custom Size Calculation**: Provides field-specific size calculation through the derived `GetSize` trait, helpful for dynamic protocol message framing.

## Usage

To include this crate in your project, run:

```sh
cargo add derive-codec-sv2
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/derive_codec/src/lib.rs">
//! # Procedural Macros for Automatic Serialization and Deserialization
//!
//! Provides procedural macros for deriving serialization and deserialization
//! traits on structs used in binary protocol communication. The macros `Encodable` and `Decodable`
//! generate implementations of encoding and decoding behaviors, making it simpler to work with
//! binary data by automatically handling field parsing, sizing, and transformation.
//!
//! ## Overview
//!
//! These macros parse struct definitions to produce code that supports efficient and type-safe
//! serialization and deserialization. Each field within a struct is processed based on its type and
//! associated generics, allowing for custom encoding schemes and alignment with protocol
//! requirements. Additionally, the macros enable flexible handling of lifetimes and static
//! references to ensure compatibility across different use cases.
//!
//! ## Available Macros
//!
//! - **`Encodable`**: Automatically implements encoding logic, converting a struct's fields into a
//!   binary format.
//!   - **Attributes**: `#[already_sized]` (optional) to specify that a struct's size is fixed at
//!     compile-time.
//!   - **Generated Traits**: `EncodableField` (field-by-field encoding) and `GetSize` (size
//!     calculation).
//!
//! - **`Decodable`**: Automatically implements decoding logic, allowing a struct to be
//!   reconstructed from binary data.
//!   - **Generated Methods**: `get_structure` (defines field structure) and `from_decoded_fields`
//!     (builds the struct from decoded fields).
//!
//! ## Internal Structure
//!
//! ### `is_already_sized`
//! Checks if the `#[already_sized]` attribute is present on the struct, allowing certain
//! optimizations in generated code for fixed-size structs.
//!
//! ### `get_struct_properties`
//! Parses and captures a structs name, generics, and field data, enabling custom encoding and
//! decoding functionality.
//!
//! ### Custom Implementations
//! The `Encodable` macro generates an `EncodableField` implementation by serializing each field,
//! while `Decodable` constructs the struct from binary data. Both macros provide support for
//! structs with or without lifetimes, ensuring versatility in applications that require efficient,
//! protocol-level data handling.

#![no_std]

extern crate alloc;
extern crate proc_macro;

use alloc::{
    format,
    string::{String, ToString},
    vec::Vec,
};
use core::iter::FromIterator;
use proc_macro::{Group, TokenStream, TokenTree};

// Reserved field names to avoid conflicts
const RESERVED_FIELDS: [&str; 2] = ["__decodable_internal_data", "__decodable_internal_offset"];

// Checks if a `TokenStream` contains a group with a bracket delimiter (`[]`),
// and further examines if the group has an identifier called `already_sized`.
//
// Iterates through the `TokenStream`, searching for a group of tokens
// that is delimited by square brackets (`[]`). Once a group is found, it looks inside
// the group for an identifier named `already_sized`. If such an identifier is found,
// the function returns `true`. Otherwise, it returns `false`.
//
// # Example
//
// ```ignore
// use proc_macro2::{Delimiter, Group, Ident, Span, TokenStream, TokenTree};
// use quote::quote;
//
// let input: TokenStream = quote! {
//     [already_sized]
// };
//
// // Call the function to check if `already_sized` is present inside the bracket group.
// assert_eq!(is_already_sized(input), true);
//
// // Now let's try another TokenStream that doesn't contain the `already_sized` identifier.
// let input_without_already_sized: TokenStream = quote! {
//     [some_other_ident]
// };
//
// assert_eq!(is_already_sized(input_without_already_sized), false);
// ```
//
// In this example, the function successfully detects the presence of the `already_sized`
// identifier when it's wrapped inside brackets, and returns `true` accordingly.
fn is_already_sized(item: TokenStream) -> bool {
    let stream = item.into_iter();

    for next in stream {
        if let TokenTree::Group(g) = next.clone() {
            if g.delimiter() == proc_macro::Delimiter::Bracket {
                for t in g.stream().into_iter() {
                    if let TokenTree::Ident(i) = t {
                        if i.to_string() == "already_sized" {
                            return true;
                        }
                    }
                }
            }
        }
    }
    false
}

// Filters out attributes from a `TokenStream` that are prefixed with `#`.
//
// Removes all Rust attributes (e.g., `#[derive(...)]`, `#[cfg(...)]`)
// from the provided `TokenStream`, leaving behind only the core structure and
// its fields. This is useful in procedural macros when you want to manipulate
// the underlying data without the attributes.
//
// # Example
//
// ```ignore
// use proc_macro2::{Delimiter, Group, TokenStream, TokenTree};
// use quote::quote;
//
// let input: TokenStream = quote! {
//     #[derive(Debug, Clone)]
//     pub struct MyStruct {
//         pub field1: i32,
//         #[cfg(feature = "extra")]
//         pub field2: String,
//     }
// };
//
// let cleaned: TokenStream = remove_attributes(input);
//
// let expected_output: TokenStream = quote! {
//     pub struct MyStruct {
//         pub field1: i32,
//         pub field2: String,
//     }
// };
//
// assert_eq!(cleaned.to_string(), expected_output.to_string());
// ```
//
// In this example, the `#[derive(Debug, Clone)]` and `#[cfg(feature = "extra")]`
// attributes were removed, leaving just the plain `struct` with its fields.
fn remove_attributes(item: TokenStream) -> TokenStream {
    let stream = item.into_iter();
    let mut is_attribute = false;
    let mut result = Vec::new();

    for next in stream {
        match next.clone() {
            TokenTree::Punct(p) => {
                if p.to_string() == "#" {
                    is_attribute = true;
                } else {
                    result.push(next.clone());
                }
            }
            TokenTree::Group(g) => {
                if is_attribute {
                    continue;
                } else {
                    let delimiter = g.delimiter();
                    let cleaned_group = remove_attributes(g.stream());
                    let cleaned_group = TokenTree::Group(Group::new(delimiter, cleaned_group));
                    result.push(cleaned_group);
                }
            }
            _ => {
                is_attribute = false;
                result.push(next.clone());
            }
        }
    }

    TokenStream::from_iter(result)
}

// Represents the current state of the parser while processing a struct.
enum ParserState {
    // Indicates that the parser is processing the struct's name.
    Name,
    // Indicates that the parser is processing the struct's type.
    Type,
    // Indicates that the parser is inside the angle brackets for generics.
    //
    // The `usize` value represents the depth of nested generics
    Generics(usize),
}

// Represents a parsed struct, including its name, generics, and fields.
//
// # Examples
//
// ```ignore
// struct MyStruct<T> {
//     pub field1: i32,
//     pub field2: String,
// }
//
// let parsed = ParsedStruct {
//     name: "MyStruct".to_string(),
//     generics: "T".to_string(),
//     fields: vec![
//         ParsedField {
//             name: "field1".to_string(),
//             type_: "i32".to_string(),
//             generics: "".to_string(),
//         },
//         ParsedField {
//             name: "field2".to_string(),
//             type_: "String".to_string(),
//             generics: "".to_string(),
//         },
//     ],
// };
// ```
#[derive(Clone, Debug)]
struct ParsedStruct {
    // Name of the struct.
    pub name: String,
    // Generics associated with the struct, if any.
    pub generics: String,
    // List of fields within the struct.
    pub fields: Vec<ParsedField>,
}

// Represents a parsed field within a struct, including its name, type, and any associated generics.
//
// # Examples
//
// ```ignore
// // Given a struct field definition:
// // data: Option<T>,
//
// let field = ParsedField {
//     name: "data".to_string(),
//     type_: "Option".to_string(),
//     generics: "T".to_string(),
// };
// ```
#[derive(Clone, Debug)]
struct ParsedField {
    // Name of the field.
    name: String,
    // Type of the field.
    type_: String,
    // Generics associated with the field, if any.
    generics: String,
}

impl ParsedField {
    pub fn new() -> Self {
        ParsedField {
            name: "".to_string(),
            type_: "".to_string(),
            generics: "".to_string(),
        }
    }

    pub fn get_generics(&self) -> String {
        if self.generics == "<'decoder>" || self.generics.is_empty() {
            "".to_string()
        } else {
            format!("::{}", self.generics.clone())
        }
    }
    pub fn as_static(&self) -> String {
        if self.generics.is_empty() {
            "".to_string()
        } else {
            ".into_static()".to_string()
        }
    }
}

// Extracts properties of a struct, including its name, generics, and fields.
//
// Processes a token stream, filtering out attributes and extracting
// core components such as the struct's name, generics, and fields. It expects the
// token stream to represent a struct declaration.
//
// # Examples
//
// ```ignore
// use quote::quote;
//
// struct MyStruct<T> {
//     field1: i32,
// }
//
// let tokens = quote! {struct MyStream<T> { field1: i32 }};
// let parsed_struct = get_struct_properties(tokens);
//
// assert_eq!(parsed_struct.name, "MyStruct");
// assert_eq!(parsed_struct.generics, "T");
// assert_eq!(parsed_struct.fields.len(), 1);
// assert_eq!(parsed_struct.fields[0].name, "field1");
// assert_eq!(parsed_struct.fields[0].type_, "i32");
// ```
//
// This example demonstrates how `get_struct_properties` identifies the struct's
// name, any generic parameters, and its fields, including types and generic parameters.
fn get_struct_properties(item: TokenStream) -> ParsedStruct {
    let item = remove_attributes(item);
    let mut stream = item.into_iter();

    // Check if the stream is a struct
    loop {
        match stream.next().expect("Stream not a struct") {
            TokenTree::Ident(i) => {
                if i.to_string() == "struct" {
                    break;
                }
            }
            _ => continue,
        }
    }

    // Get the struct name
    let struct_name = match stream.next().expect("Struct has no name") {
        TokenTree::Ident(i) => i.to_string(),
        // Never executed at runtime it ok to panic
        _ => panic!("Strcut has no name"),
    };

    let mut struct_generics = "".to_string();
    let group: Vec<TokenTree>;

    // Get the struct generics if any
    loop {
        match stream
            .next()
            // Never executed at runtime it ok to panic
            .unwrap_or_else(|| panic!("Struct {} has no fields", struct_name))
        {
            TokenTree::Group(g) => {
                group = g.stream().into_iter().collect();
                break;
            }
            TokenTree::Punct(p) => {
                struct_generics = format!("{struct_generics}{p}");
            }
            TokenTree::Ident(i) => {
                struct_generics = format!("{struct_generics}{i}");
            }
            // Never executed at runtime it ok to panic
            _ => panic!("Struct {} has no fields", struct_name),
        };
    }

    let fields = parse_struct_fields(group);

    ParsedStruct {
        name: struct_name,
        generics: struct_generics,
        fields,
    }
}

// Parses the fields of a struct, scanning tokens to identify field names, types, and generics.
//
// Processes tokens for each field in a struct, managing parser states
// (`ParserState::Name`, `ParserState::Type`, and `ParserState::Generics`) to accurately parse
// complex types and nested generics within struct definitions.
//
// # Examples
//
// ```ignore
// struct MyStruct<T> {
//     field1: i32,
// }
//
// let tokens = vec![/* TokenTree representing `id: i32` */];
// let parsed_fields = parse_struct_fields(tokens);
//
// assert_eq!(parsed_fields.len(), 1);
// assert_eq!(parsed_fields[0].name, "field1");
// assert_eq!(parsed_fields[0].type_, "i32");
// assert_eq!(parsed_fields[0].generics, "");
// ```
//
// This example shows how `parse_struct_fields` handles both a primitive field (`id`) and a
// generic field (`data`) within a struct, including parsing of generic parameters.
fn parse_struct_fields(group: Vec<TokenTree>) -> Vec<ParsedField> {
    let mut fields = Vec::new();
    let mut field_ = ParsedField::new();
    let mut field_parser_state = ParserState::Name;
    for token in group {
        match (token, &field_parser_state) {
            (TokenTree::Ident(i), ParserState::Name) => {
                if i.to_string() == "pub" {
                    continue;
                } else {
                    field_.name = i.to_string();
                }
            }
            (TokenTree::Ident(i), ParserState::Type) => {
                field_.type_ = i.to_string();
            }
            (TokenTree::Ident(i), ParserState::Generics(_)) => {
                field_.generics = format!("{}{}", field_.generics, i);
            }
            (TokenTree::Punct(p), ParserState::Name) => {
                if p.to_string() == ":" {
                    field_parser_state = ParserState::Type
                } else {
                    // Never executed at runtime it ok to panic
                    panic!("Unexpected token '{}' in parsing {:#?}", p, field_);
                }
            }
            (TokenTree::Punct(p), ParserState::Type) => match p.to_string().as_ref() {
                "," => {
                    field_parser_state = ParserState::Name;
                    fields.push(field_.clone());
                    field_ = ParsedField::new();
                }
                "<" => {
                    field_.generics = "<".to_string();
                    field_parser_state = ParserState::Generics(0);
                }
                // Never executed at runtime it ok to panic
                _ => panic!("Unexpected token '{}' in parsing {:#?}", p, field_),
            },
            (TokenTree::Punct(p), ParserState::Generics(open_brackets)) => {
                match p.to_string().as_ref() {
                    "'" => {
                        field_.generics = format!("{}{}", field_.generics, p);
                    }
                    "<" => {
                        field_.generics = format!("{}{}", field_.generics, p);
                        field_parser_state = ParserState::Generics(open_brackets + 1);
                    }
                    ">" => {
                        field_.generics = format!("{}{}", field_.generics, p);
                        if open_brackets == &0 {
                            field_parser_state = ParserState::Type
                        } else {
                            field_parser_state = ParserState::Generics(open_brackets - 1);
                        }
                    }
                    _ => {
                        field_.generics = format!("{}{}", field_.generics, p);
                    }
                }
            }
            // Never executed at runtime it ok to panic
            _ => panic!("Unexpected token"),
        }
    }
    fields
}

/// Derives the `Decodable` trait, generating implementations for deserializing a struct from a
/// byte stream, including its structure, field decoding, and a method for creating a static
/// version.
///
/// This procedural macro generates the `Decodable` trait for a struct, which allows it to be
/// decoded from a binary stream, with support for handling fields of different types and
/// nested generics. The macro also includes implementations for `from_decoded_fields`,
/// `get_structure`, and methods to return a static version of the struct.
///
/// # Example
///
/// Given a struct:
///
/// ```ignore
/// struct Test {
///     a: u32,
///     b: u8,
///     c: U24,
/// }
/// ```
///
/// Using `#[derive(Decodable)]` on `Test` generates the following implementations:
///
/// ```ignore
/// mod impl_parse_decodable_test {
///     use super::{
///         binary_codec_sv2::{
///             decodable::{DecodableField, FieldMarker},
///             Decodable, Error, SizeHint,
///         },
///         *,
///     };
///
///     struct Test {
///         a: u32,
///         b: u8,
///         c: U24,
///     }
///
///     impl<'decoder> Decodable<'decoder> for Test {
///         fn get_structure(__decodable_internal_data: &[u8]) -> Result<Vec<FieldMarker>, Error> {
///             let mut fields = Vec::new();
///             let mut __decodable_internal_offset = 0;
///
///             let a: Vec<FieldMarker> = u32::get_structure(&__decodable_internal_data[__decodable_internal_offset..])?;
///             __decodable_internal_offset += a.size_hint_(&__decodable_internal_data, __decodable_internal_offset)?;
///             let a = a.try_into()?;
///             fields.push(a);
///
///             let b: Vec<FieldMarker> = u8::get_structure(&__decodable_internal_data[__decodable_internal_offset..])?;
///             __decodable_internal_offset += b.size_hint_(&__decodable_internal_data, __decodable_internal_offset)?;
///             let b = b.try_into()?;
///             fields.push(b);
///
///             let c: Vec<FieldMarker> = U24::get_structure(&__decodable_internal_data[__decodable_internal_offset..])?;
///             __decodable_internal_offset += c.size_hint_(&__decodable_internal_data, __decodable_internal_offset)?;
///             let c = c.try_into()?;
///             fields.push(c);
///
///             Ok(fields)
///         }
///
///         fn from_decoded_fields(mut __decodable_internal_data: Vec<DecodableField<'decoder>>) -> Result<Self, Error> {
///             Ok(Self {
///                 c: U24::from_decoded_fields(
///                     __decodable_internal_data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
///                 )?,
///                 b: u8::from_decoded_fields(
///                     __decodable_internal_data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
///                 )?,
///                 a: u32::from_decoded_fields(
///                     __decodable_internal_data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
///                 )?,
///             })
///         }
///     }
///
///     impl Test {
///         pub fn into_static(self) -> Test {
///             Test {
///                 a: self.a.clone(),
///                 b: self.b.clone(),
///                 c: self.c.clone(),
///             }
///         }
///     }
///
///     impl Test {
///         pub fn as_static(&self) -> Test {
///             Test {
///                 a: self.a.clone(),
///                 b: self.b.clone(),
///                 c: self.c.clone(),
///             }
///         }
///     }
/// }
/// ```
///
/// This generated code enables `Test` to be decoded from a binary stream, defines how each
/// field should be parsed, and provides `into_static` and `as_static` methods to facilitate
/// ownership and lifetime management of decoded fields in the struct.
#[proc_macro_derive(Decodable)]
pub fn decodable(item: TokenStream) -> TokenStream {
    let parsed_struct = get_struct_properties(item);

    let data_ident = RESERVED_FIELDS[0];
    let offset_ident = RESERVED_FIELDS[1];

    for field in &parsed_struct.fields {
        if RESERVED_FIELDS.contains(&field.name.as_str()) {
            return format!(
                "compile_error!(\"Field name '{}' is reserved and cannot be used in struct '{}'. Rename it to avoid conflicts.\");",
                field.name, parsed_struct.name
            )
            .parse()
            .unwrap();
        }
    }

    let mut derive_fields = String::new();

    for f in parsed_struct.fields.clone() {
        let field = format!(
            "
            let {}: Vec<FieldMarker> = {}{}::get_structure(& {}[{}..])?;
            {} += {}.size_hint_(&{}, {})?;
            let {} =  {}.try_into()?;
            fields.push({});
            ",
            f.name,
            f.type_,
            f.get_generics(),
            data_ident,
            offset_ident,
            offset_ident,
            f.name,
            data_ident,
            offset_ident,
            f.name,
            f.name,
            f.name
        );
        derive_fields.push_str(&field)
    }

    let mut derive_static_fields = String::new();
    for f in parsed_struct.fields.clone() {
        let field = format!(
            "
            {}: self.{}.clone(){},
            ",
            f.name,
            f.name,
            f.as_static(),
        );
        derive_static_fields.push_str(&field)
    }

    let mut derive_decoded_fields = String::new();
    let mut fields = parsed_struct.fields.clone();

    // Reverse the fields as they are popped out from the end of the vector but we want to pop out
    // from the front
    fields.reverse();

    // Create Struct from fields
    for f in fields.clone() {
        let field = format!(
            "
            {}: {}{}::from_decoded_fields({}.pop().ok_or(Error::NoDecodableFieldPassed)?.into())?,
            ",
            f.name,
            f.type_,
            f.get_generics(),
            data_ident
        );
        derive_decoded_fields.push_str(&field)
    }

    let impl_generics = if !parsed_struct.generics.is_empty() {
        parsed_struct.clone().generics
    } else {
        "<'decoder>".to_string()
    };

    let result = format!(
        "mod impl_parse_decodable_{} {{

    use super::binary_codec_sv2::{{decodable::DecodableField, decodable::FieldMarker, Decodable, Error, SizeHint}};
    use super::*;

    impl{} Decodable<'decoder> for {}{} {{
        fn get_structure({}: &[u8]) -> Result<Vec<FieldMarker>, Error> {{
            let mut fields = Vec::new();
            let mut {} = 0;
            {}
            Ok(fields)
        }}

        fn from_decoded_fields(mut {}: Vec<DecodableField<'decoder>>) -> Result<Self, Error> {{
            Ok(Self {{
                {}
            }})
        }}
    }}

    impl{} {}{} {{
        pub fn into_static(self) -> {}{} {{
            {} {{
                {}
            }}
        }}
    }}
    impl{} {}{} {{
        pub fn as_static(&self) -> {}{} {{
            {} {{
                {}
            }}
        }}
    }}
    }}",
        // imports
        parsed_struct.name.to_lowercase(),
        // derive decodable
        impl_generics,
        parsed_struct.name,
        parsed_struct.generics,
        data_ident,
        offset_ident,
        derive_fields,
        data_ident,
        derive_decoded_fields,
        // impl into_static
        impl_generics,
        parsed_struct.name,
        parsed_struct.generics,
        parsed_struct.name,
        get_static_generics(&parsed_struct.generics),
        parsed_struct.name,
        derive_static_fields,
        // impl into_static
        impl_generics,
        parsed_struct.name,
        parsed_struct.generics,
        parsed_struct.name,
        get_static_generics(&parsed_struct.generics),
        parsed_struct.name,
        derive_static_fields,

    );

    // Never executed at runtime it ok to panic
    result.parse().unwrap()
}

fn get_static_generics(gen: &str) -> &str {
    if gen.is_empty() {
        gen
    } else {
        "<'static>"
    }
}

/// Derives the `Encodable` trait, generating implementations for serializing a struct into an
/// encoded format, including methods for field serialization, calculating the encoded size,
/// and handling cases where the struct is already sized.
///
/// This procedural macro generates the `Encodable` trait for a struct, allowing each field
/// to be converted into an `EncodableField`, with support for recursive field encoding.
/// The macro also includes an implementation of the `GetSize` trait to calculate the
/// encoded size of the struct, depending on the `already_sized` attribute.
///
/// # Example
///
/// Given a struct:
///
/// ```ignore
/// struct Test {
///     a: u32,
///     b: u8,
///     c: U24,
/// }
/// ```
///
/// Using `#[derive(Encodable)]` on `Test` generates the following implementations:
///
/// ```ignore
/// mod impl_parse_encodable_test {
///     use super::binary_codec_sv2::{encodable::EncodableField, GetSize};
///     extern crate alloc;
///     use alloc::vec::Vec;
///
///     struct Test {
///         a: u32,
///         b: u8,
///         c: U24,
///     }
///
///     impl<'decoder> From<Test> for EncodableField<'decoder> {
///         fn from(v: Test) -> Self {
///             let mut fields: Vec<EncodableField> = Vec::new();
///
///             let val = v.a;
///             fields.push(val.into());
///
///             let val = v.b;
///             fields.push(val.into());
///
///             let val = v.c;
///             fields.push(val.into());
///
///             Self::Struct(fields)
///         }
///     }
///
///     impl<'decoder> GetSize for Test {
///         fn get_size(&self) -> usize {
///             let mut size = 0;
///
///             size += self.a.get_size();
///             size += self.b.get_size();
///             size += self.c.get_size();
///
///             size
///         }
///     }
/// }
/// ```
///
/// This generated code enables `Test` to be serialized into an encoded format, defines
/// how each field should be converted, and calculates the total encoded size of the struct,
/// depending on whether it is marked as `already_sized`.
#[proc_macro_derive(Encodable, attributes(already_sized))]
pub fn encodable(item: TokenStream) -> TokenStream {
    let is_already_sized = is_already_sized(item.clone());
    let parsed_struct = get_struct_properties(item);
    let fields = parsed_struct.fields.clone();

    let mut field_into_decoded_field = String::new();

    // Create DecodableField from fields
    for f in fields.clone() {
        let field = format!(
            "
            let val = v.{};
            fields.push(val.into());
            ",
            f.name
        );
        field_into_decoded_field.push_str(&field)
    }

    let mut sizes = String::new();

    for f in fields {
        let field = format!(
            "
            size += self.{}.get_size();
            ",
            f.name
        );
        sizes.push_str(&field)
    }
    let impl_generics = if !parsed_struct.generics.is_empty() {
        parsed_struct.clone().generics
    } else {
        "<'decoder>".to_string()
    };

    let get_size = if is_already_sized {
        String::new()
    } else {
        format!(
            "
            impl{} GetSize for {}{} {{
                fn get_size(&self) -> usize {{
                    let mut size = 0;
                    {}
                    size
                }}
            }}
            ",
            impl_generics, parsed_struct.name, parsed_struct.generics, sizes
        )
    };

    let result = format!(
        "mod impl_parse_encodable_{} {{

    use super::binary_codec_sv2::{{encodable::EncodableField, GetSize}};
    use super::{};
    extern crate alloc;
    use alloc::vec::Vec;

    impl{} From<{}{}> for EncodableField<'decoder> {{
        fn from(v: {}{}) -> Self {{
            let mut fields: Vec<EncodableField> = Vec::new();
            {}
            Self::Struct(fields)
        }}
    }}

    {}

    }}",
        // imports
        parsed_struct.name.to_lowercase(),
        parsed_struct.name,
        // impl From<Struct> for DecodableField
        impl_generics,
        parsed_struct.name,
        parsed_struct.generics,
        parsed_struct.name,
        parsed_struct.generics,
        field_into_decoded_field,
        // impl get_size
        get_size,
    );
    //println!("{}", result);

    // Never executed at runtime it ok to panic
    result.parse().unwrap()
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/examples/encode_decode.rs">
pub use binary_codec_sv2::{self, Decodable as Deserialize, Encodable as Serialize, *};
use core::convert::TryInto;
pub use derive_codec_sv2::{Decodable as Deserialize, Encodable as Serialize};

// The `Test` struct is expanded using the `Deserialize` and `Serialize` procedural macros.
// These macros provide the necessary methods for serializing and deserializing the struct.
//
// mod impl_parse_decodable_test {
//     use super::binary_codec_sv2::{
//         decodable::DecodableField, decodable::FieldMarker, Decodable, Error, SizeHint,
//     };
//     use super::*;
//     impl<'decoder> Decodable<'decoder> for Test {
//         fn get_structure(data: &[u8]) -> Result<Vec<FieldMarker>, Error> {
//             let mut fields = Vec::new();
//             let mut offset = 0;
//             let a: Vec<FieldMarker> = u32::get_structure(&data[offset..])?;
//             offset += a.size_hint_(&data, offset)?;
//             let a = a.try_into()?;
//             fields.push(a);
//             let b: Vec<FieldMarker> = u8::get_structure(&data[offset..])?;
//             offset += b.size_hint_(&data, offset)?;
//             let b = b.try_into()?;
//             fields.push(b);
//             let c: Vec<FieldMarker> = U24::get_structure(&data[offset..])?;
//             offset += c.size_hint_(&data, offset)?;
//             let c = c.try_into()?;
//             fields.push(c);
//             Ok(fields)
//         }
//         fn from_decoded_fields(
//             mut data: Vec<DecodableField<'decoder>>,
//         ) -> Result<Self, Error> {
//             Ok(Self {
//                 c: U24::from_decoded_fields(
//                     data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
//                 )?,
//                 b: u8::from_decoded_fields(
//                     data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
//                 )?,
//                 a: u32::from_decoded_fields(
//                     data.pop().ok_or(Error::NoDecodableFieldPassed)?.into(),
//                 )?,
//             })
//         }
//     }
//     impl<'decoder> Test {
//         pub fn into_static(self) -> Test {
//             Test {
//                 a: self.a.clone(),
//                 b: self.b.clone(),
//                 c: self.c.clone(),
//             }
//         }
//     }
//     impl<'decoder> Test {
//         pub fn as_static(&self) -> Test {
//             Test {
//                 a: self.a.clone(),
//                 b: self.b.clone(),
//                 c: self.c.clone(),
//             }
//         }
//     }
// }
// mod impl_parse_encodable_test {
//     use super::binary_codec_sv2::{encodable::EncodableField, GetSize};
//     use super::Test;
//     extern crate alloc;
//     use alloc::vec::Vec;
//     impl<'decoder> From<Test> for EncodableField<'decoder> {
//         fn from(v: Test) -> Self {
//             let mut fields: Vec<EncodableField> = Vec::new();
//             let val = v.a;
//             fields.push(val.into());
//             let val = v.b;
//             fields.push(val.into());
//             let val = v.c;
//             fields.push(val.into());
//             Self::Struct(fields)
//         }
//     }
//     impl<'decoder> GetSize for Test {
//         fn get_size(&self) -> usize {
//             let mut size = 0;
//             size += self.a.get_size();
//             size += self.b.get_size();
//             size += self.c.get_size();
//             size
//         }
//     }
// }
//

#[derive(Clone, Deserialize, Serialize, PartialEq, Debug)]
struct Test {
    a: u32,
    b: u8,
    c: U24,
}

fn main() {
    let expected = Test {
        a: 456,
        b: 9,
        c: 67_u32.try_into().unwrap(),
    };

    // `to_bytes` serves as the entry point to the `binary_sv2` crate. It acts as a serializer that
    // converts the struct into bytes.
    let mut bytes = to_bytes(expected.clone()).unwrap();

    // `from_bytes` is a deserializer that interprets the bytes and reconstructs the original
    // struct.
    let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

    assert_eq!(deserialized, expected);
}
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/README.md">
# binary-sv2

[![crates.io](https://img.shields.io/crates/v/binary-sv2.svg)](https://crates.io/crates/binary-sv2)
[![docs.rs](https://docs.rs/binary-sv2/badge.svg)](https://docs.rs/binary-sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)

`binary-sv2` is a Rust `no-std` crate that helps encode and decode binary data into Stratum V2 messages

## Key Capabilities

- **Protocol-Specific Types**: Supports fixed and dynamically-sized SV2 types.
- **Optimized Memory Use**: Supports buffer pooling to enhance memory efficiency.

## Features

- **prop_test**: Adds property testing support.
- **with_buffer_pool**: Optimizes memory usage during encoding.

## Usage

To include this crate in your project, run:

```sh
cargo add binary-sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/binary-sv2/src/lib.rs">
//! Export custom implementations of the `binary_sv2` protocol,
//! enabling encoding and decoding through custom traits.
//!
//! # Overview
//!
//! This crate will re-export implementations of the`Deserialize` and `Serialize` traits
//! from a custom implementation provided by `binary_codec_sv2` and `derive_codec_sv2`.
//! This allows for flexible integration of SV2  protocol types and binary serialization.
//!
//! ## Features
//! - **prop_test**: Adds support for property testing for protocol types.
//! - **with_buffer_pool**: Enables support for buffer pooling to optimize memory usage during
//!   serialization and deserialization.
#![no_std]

#[macro_use]
extern crate alloc;

use core::convert::TryInto;

pub use binary_codec_sv2::{self, Decodable as Deserialize, Encodable as Serialize, *};
pub use derive_codec_sv2::{Decodable as Deserialize, Encodable as Serialize};

/// Does nothing and will be removed during refactor
pub fn clone_message<T: Serialize>(_: T) -> T {
    todo!()
}

/// Converts a value implementing the `Into<u64>` trait into a custom `U256` type.
pub fn u256_from_int<V: Into<u64>>(value: V) -> U256<'static> {
    // initialize u256 as a bytes vec of len 24
    let mut u256 = vec![0_u8; 24];
    let val: u64 = value.into();
    for v in &(val.to_le_bytes()) {
        // add 8 bytes to u256
        u256.push(*v)
    }
    // Always safe cause u256 is 24 + 8 (32) bytes
    let u256: U256 = u256.try_into().unwrap();
    u256
}

#[cfg(test)]
mod test {
    use super::*;
    use alloc::vec::Vec;

    mod test_struct {
        use super::*;
        use core::convert::TryInto;

        #[derive(Clone, Deserialize, Serialize, PartialEq, Debug)]
        struct Test {
            a: u32,
            b: u8,
            c: U24,
        }

        #[test]
        fn test_struct() {
            let expected = Test {
                a: 456,
                b: 9,
                c: 67_u32.try_into().unwrap(),
            };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_f32 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Clone, Deserialize, Serialize, PartialEq, Debug)]
        struct Test {
            a: u8,
            b: U24,
            c: f32,
        }

        #[test]
        fn test_struct() {
            let expected = Test {
                c: 0.345,
                a: 9,
                b: 67_u32.try_into().unwrap(),
            };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_b0255 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Clone, Deserialize, Serialize, PartialEq, Debug)]
        struct Test<'decoder> {
            a: B0255<'decoder>,
        }

        #[test]
        fn test_b0255() {
            let mut b0255 = [6; 3];
            let b0255: B0255 = (&mut b0255[..]).try_into().unwrap();

            let expected = Test { a: b0255 };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }

        #[test]
        fn test_b0255_max() {
            let mut b0255 = [6; 255];
            let b0255: B0255 = (&mut b0255[..]).try_into().unwrap();

            let expected = Test { a: b0255 };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_u256 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Clone, Deserialize, Serialize, PartialEq, Debug)]
        struct Test<'decoder> {
            a: U256<'decoder>,
        }

        #[test]
        fn test_u256() {
            let mut u256 = [6_u8; 32];
            let u256: U256 = (&mut u256[..]).try_into().unwrap();

            let expected = Test { a: u256 };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_signature {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Signature<'decoder>,
        }

        #[test]
        fn test_signature() {
            let mut s = [6; 64];
            let s: Signature = (&mut s[..]).try_into().unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_b016m {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            b: bool,
            a: B016M<'decoder>,
        }

        #[test]
        fn test_b016m() {
            let mut b = [0_u8; 70000];
            let b: B016M = (&mut b[..]).try_into().unwrap();
            //println!("{:?}", to_bytes(&b).unwrap().len());

            let expected = Test { a: b, b: true };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }

        #[test]
        fn test_b016m_max() {
            let mut b = vec![0_u8; 16777215];
            let b: B016M = (&mut b[..]).try_into().unwrap();
            //println!("{:?}", to_bytes(&b).unwrap().len());

            let expected = Test { a: b, b: true };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_b064k {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            b: bool,
            a: B064K<'decoder>,
        }

        #[test]
        fn test_b064k() {
            let mut b = [1, 2, 9];
            let b: B064K = (&mut b[..])
                .try_into()
                .expect("vector smaller than 64K should not fail");

            let expected = Test { a: b, b: true };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq0255_u256 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, U256<'decoder>>,
        }

        #[test]
        fn test_seq0255_u256() {
            let mut u256_1 = [6; 32];
            let mut u256_2 = [5; 32];
            let mut u256_3 = [0; 32];
            let u256_1: U256 = (&mut u256_1[..]).try_into().unwrap();
            let u256_2: U256 = (&mut u256_2[..]).try_into().unwrap();
            let u256_3: U256 = (&mut u256_3[..]).try_into().unwrap();

            let val = vec![u256_1, u256_2, u256_3];
            let s = Seq0255::new(val).unwrap();

            let test = Test { a: s };

            let mut bytes = to_bytes(test.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            let bytes_2 = to_bytes(deserialized.clone()).unwrap();

            assert_eq!(bytes, bytes_2);
        }
    }

    mod test_0255_bool {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, bool>,
        }

        #[test]
        fn test_seq0255_bool() {
            let s: Seq0255<bool> = Seq0255::new(vec![true, false, true]).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq0255_u16 {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, u16>,
        }

        #[test]
        fn test_seq0255_u16() {
            let s: Seq0255<u16> = Seq0255::new(vec![10, 43, 89]).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq_0255_u24 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, U24>,
        }

        #[test]
        fn test_seq0255_u24() {
            let u24_1: U24 = 56_u32.try_into().unwrap();
            let u24_2: U24 = 59_u32.try_into().unwrap();
            let u24_3: U24 = 70999_u32.try_into().unwrap();

            let val = vec![u24_1, u24_2, u24_3];
            let s: Seq0255<U24> = Seq0255::new(val).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seqo255_u32 {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, u32>,
        }

        #[test]
        fn test_seq0255_u32() {
            let s: Seq0255<u32> = Seq0255::new(vec![546, 99999, 87, 32]).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq0255_signature {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq0255<'decoder, Signature<'decoder>>,
        }

        #[test]
        fn test_seq0255_signature() {
            let mut siganture_1 = [88_u8; 64];
            let mut siganture_2 = [99_u8; 64];
            let mut siganture_3 = [220_u8; 64];
            let siganture_1: Signature = (&mut siganture_1[..]).try_into().unwrap();
            let siganture_2: Signature = (&mut siganture_2[..]).try_into().unwrap();
            let siganture_3: Signature = (&mut siganture_3[..]).try_into().unwrap();

            let val = vec![siganture_1, siganture_2, siganture_3];
            let s: Seq0255<Signature> = Seq0255::new(val).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq_064_u256 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, U256<'decoder>>,
        }

        #[test]
        fn test_seq064k_u256() {
            let mut u256_1 = [6; 32];
            let mut u256_2 = [5; 32];
            let mut u256_3 = [0; 32];
            let u256_1: U256 = (&mut u256_1[..]).try_into().unwrap();
            let u256_2: U256 = (&mut u256_2[..]).try_into().unwrap();
            let u256_3: U256 = (&mut u256_3[..]).try_into().unwrap();

            let val = vec![u256_1, u256_2, u256_3];
            let s = Seq064K::new(val).unwrap();

            let test = Test { a: s };

            let mut bytes = to_bytes(test.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            let bytes_2 = to_bytes(deserialized.clone()).unwrap();

            assert_eq!(bytes, bytes_2);
        }
    }

    mod test_064_bool {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, bool>,
        }

        #[test]
        fn test_seq064k_bool() {
            let s: Seq064K<bool> = Seq064K::new(vec![true, false, true]).unwrap();
            let s2: Seq064K<bool> = Seq064K::new(vec![true; 64000]).unwrap();

            let expected = Test { a: s };
            let expected2 = Test { a: s2 };

            let mut bytes = to_bytes(expected.clone()).unwrap();
            let mut bytes2 = to_bytes(expected2.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();
            let deserialized2: Test = from_bytes(&mut bytes2[..]).unwrap();

            assert_eq!(deserialized, expected);
            assert_eq!(deserialized2, expected2);
        }
    }

    mod test_se1o64k_u16 {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, u16>,
        }

        #[test]
        fn test_seq064k_u16() {
            let s: Seq064K<u16> = Seq064K::new(vec![10, 43, 89]).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq064k_u24 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, U24>,
        }

        #[test]
        fn test_seq064k_u24() {
            let u24_1: U24 = 56_u32.try_into().unwrap();
            let u24_2: U24 = 59_u32.try_into().unwrap();
            let u24_3: U24 = 70999_u32.try_into().unwrap();

            let val = vec![u24_1, u24_2, u24_3];
            let s: Seq064K<U24> = Seq064K::new(val).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }

    mod test_seq064k_u32 {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, u32>,
        }

        #[test]
        fn test_seq064k_u32() {
            let s: Seq064K<u32> = Seq064K::new(vec![546, 99999, 87, 32]).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }
    mod test_seq064k_signature {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, Signature<'decoder>>,
        }

        #[test]
        fn test_seq064k_signature() {
            let mut siganture_1 = [88_u8; 64];
            let mut siganture_2 = [99_u8; 64];
            let mut siganture_3 = [220_u8; 64];
            let siganture_1: Signature = (&mut siganture_1[..]).try_into().unwrap();
            let siganture_2: Signature = (&mut siganture_2[..]).try_into().unwrap();
            let siganture_3: Signature = (&mut siganture_3[..]).try_into().unwrap();

            let val = vec![siganture_1, siganture_2, siganture_3];
            let s: Seq064K<Signature> = Seq064K::new(val).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }
    mod test_seq064k_b016m {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Seq064K<'decoder, B016M<'decoder>>,
        }

        #[test]
        fn test_seq064k_b016m() {
            let mut bytes_1 = [88_u8; 64];
            let mut bytes_2 = [99_u8; 64];
            let mut bytes_3 = [220_u8; 64];
            let bytes_1: B016M = (&mut bytes_1[..]).try_into().unwrap();
            let bytes_2: B016M = (&mut bytes_2[..]).try_into().unwrap();
            let bytes_3: B016M = (&mut bytes_3[..]).try_into().unwrap();

            let val = vec![bytes_1, bytes_2, bytes_3];
            let s: Seq064K<B016M> = Seq064K::new(val).unwrap();

            let expected = Test { a: s };

            let mut bytes = to_bytes(expected.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            assert_eq!(deserialized, expected);
        }
    }
    mod test_seq_0255_in_struct {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: u8,
            b: Seq0255<'decoder, u8>,
            c: u32,
        }

        #[test]
        fn test_seq_0255_in_struct() {
            let expected = Test {
                a: 89,
                b: Seq0255::new(vec![]).unwrap(),
                c: 32,
            };
            let len = expected.get_size();
            let mut buffer = vec![0; len];
            to_writer(expected.clone(), &mut buffer).unwrap();
            let deserialized: Test = from_bytes(&mut buffer[..]).unwrap();
            assert_eq!(deserialized, expected);
        }
    }
    mod test_sv2_option_u256 {
        use super::*;
        use core::convert::TryInto;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Sv2Option<'decoder, U256<'decoder>>,
        }

        #[test]
        fn test_sv2_option_u256() {
            let mut u256_1 = [6; 32];
            let u256_1: U256 = (&mut u256_1[..]).try_into().unwrap();

            let val = Some(u256_1);
            let s = Sv2Option::new(val);

            let test = Test { a: s };

            let mut bytes = to_bytes(test.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            let bytes_2 = to_bytes(deserialized.clone()).unwrap();

            assert_eq!(bytes, bytes_2);
        }
    }
    mod test_sv2_option_none {
        use super::*;

        #[derive(Deserialize, Serialize, PartialEq, Debug, Clone)]
        struct Test<'decoder> {
            a: Sv2Option<'decoder, U256<'decoder>>,
        }

        #[test]
        fn test_sv2_option_none() {
            let val = None;
            let s = Sv2Option::new(val);

            let test = Test { a: s };

            let mut bytes = to_bytes(test.clone()).unwrap();

            let deserialized: Test = from_bytes(&mut bytes[..]).unwrap();

            let bytes_2 = to_bytes(deserialized.clone()).unwrap();

            assert_eq!(bytes, bytes_2);
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/Cargo.toml">
[package]
name = "codec_sv2"
version = "2.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 data format"
documentation = "https://docs.rs/codec_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[dependencies]
framing_sv2 = { path = "../../../protocols/v2/framing-sv2", version = "^5.0.0" }
noise_sv2 = { path = "../../../protocols/v2/noise-sv2", default-features = false, optional = true, version = "^1.0.0" }
binary_sv2 = { path = "../../../protocols/v2/binary-sv2", version = "^3.0.0" }
buffer_sv2 = { path = "../../../utils/buffer", version = "^2.0.0" }
rand = { version = "0.8.5", default-features = false }
tracing = { version = "0.1", optional = true }

[dev-dependencies]
key-utils = { path = "../../../utils/key-utils", version = "^1.0.0" }

[features]
default = ["std"]
std = ["noise_sv2?/std", "rand/std", "rand/std_rng", "dep:tracing"]
with_buffer_pool = ["framing_sv2/with_buffer_pool"]
tracing = ["dep:tracing"]

[package.metadata.docs.rs]
features = ["with_buffer_pool", "noise_sv2", "std"]
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/examples/encrypted.rs">
// # Using Sv2 Codec with Noise Encryption
//
// This example demonstrates how to use the `codec-sv2` crate to encode and decode Sv2 frames
// with Noise protocol encryption over a TCP connection. It showcases how to:
//
// * Perform a Noise handshake between the sender and receiver.
// * Create an arbitrary custom message type (`CustomMessage`) for encryption/encoding and
//   decryption/decoding.
// * Encode the message into an encrypted Sv2 frame using Noise.
// * Send the encrypted frame over a TCP connection.
// * Decode the encrypted Sv2 frame on the receiving side after completing the Noise handshake.
//
// ## Run
//
// ```
// cargo run --example encrypted --features noise_sv2
// ```

use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};
#[cfg(feature = "noise_sv2")]
use codec_sv2::{
    Error, HandshakeRole, Initiator, NoiseEncoder, Responder, StandardEitherFrame,
    StandardNoiseDecoder, StandardSv2Frame, State, Sv2Frame,
};
#[cfg(feature = "noise_sv2")]
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};

#[cfg(feature = "noise_sv2")]
use noise_sv2::{ELLSWIFT_ENCODING_SIZE, INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE};

use std::convert::TryInto;
#[cfg(feature = "noise_sv2")]
use std::{
    io::{Read, Write},
    net::{TcpListener, TcpStream},
};

// Arbitrary message type.
// Supported Sv2 message types are listed in the [Sv2 Spec Message
// Types](https://github.com/stratum-mining/sv2-spec/blob/main/08-Message-Types.md).
#[cfg(feature = "noise_sv2")]
const CUSTOM_MSG_TYPE: u8 = 0xff;

// Emulate a TCP connection
#[cfg(feature = "noise_sv2")]
const TCP_ADDR: &str = "127.0.0.1:3333";

#[cfg(feature = "noise_sv2")]
const AUTHORITY_PUBLIC_K: &str = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72";
#[cfg(feature = "noise_sv2")]
const AUTHORITY_PRIVATE_K: &str = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n";
#[cfg(feature = "noise_sv2")]
const CERT_VALIDITY: std::time::Duration = std::time::Duration::from_secs(3600);

// Example message type.
// In practice, all Sv2 messages are defined in the following crates:
// * `common_messages_sv2`
// * `mining_sv2`
// * `job_declaration_sv2`
// * `template_distribution_sv2`
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CustomMessage {
    nonce: u16,
}

#[cfg(feature = "noise_sv2")]
fn main() {
    // Start a receiving listener
    let listener_receiver = TcpListener::bind(TCP_ADDR).expect("Failed to bind TCP listener");

    // Start a sender stream
    let mut stream_sender: TcpStream =
        TcpStream::connect(TCP_ADDR).expect("Failed to connect to TCP stream");

    // Start a receiver stream
    let mut stream_receiver: TcpStream = listener_receiver
        .incoming()
        .next()
        .expect("Failed to accept incoming TCP stream")
        .expect("Failed to connect to incoming TCP stream");

    // Handshake
    let authority_public_k: Secp256k1PublicKey = AUTHORITY_PUBLIC_K
        .to_string()
        .try_into()
        .expect("Failed to convert receiver public key to Secp256k1PublicKey");

    let authority_private_k: Secp256k1SecretKey = AUTHORITY_PRIVATE_K
        .to_string()
        .try_into()
        .expect("Failed to convert receiver private key to Secp256k1PublicKey");

    #[cfg(feature = "std")]
    let initiator = Initiator::from_raw_k(authority_public_k.into_bytes())
        .expect("Failed to create initiator role from raw pub key");
    #[cfg(not(feature = "std"))]
    let initiator =
        Initiator::from_raw_k_with_rng(authority_public_k.into_bytes(), &mut rand::thread_rng())
            .expect("Failed to create initiator role from raw pub key");

    #[cfg(feature = "std")]
    let responder = Responder::from_authority_kp(
        &authority_public_k.into_bytes(),
        &authority_private_k.into_bytes(),
        CERT_VALIDITY,
    )
    .expect("Failed to initialize responder from pub/key pair and/or cert");
    #[cfg(not(feature = "std"))]
    let responder = Responder::from_authority_kp_with_rng(
        &authority_public_k.into_bytes(),
        &authority_private_k.into_bytes(),
        CERT_VALIDITY,
        &mut rand::thread_rng(),
    )
    .expect("Failed to initialize responder from pub/key pair and/or cert");

    let mut sender_state = State::initialized(HandshakeRole::Initiator(initiator));
    let mut receiver_state = State::initialized(HandshakeRole::Responder(responder));

    let first_message = sender_state
        .step_0()
        .expect("Initiator failed first step of handshake");
    let first_message: [u8; ELLSWIFT_ENCODING_SIZE] = first_message
        .get_payload_when_handshaking()
        .try_into()
        .expect("Handshake remote invlaid message");

    #[cfg(feature = "std")]
    let (second_message, receiver_state) = receiver_state
        .step_1(first_message)
        .expect("Responder failed second step of handshake");
    #[cfg(not(feature = "std"))]
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    #[cfg(not(feature = "std"))]
    let (second_message, receiver_state) = receiver_state
        .step_1_with_now_rng(first_message, now, &mut rand::thread_rng())
        .expect("Responder failed second step of handshake");
    let second_message: [u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE] = second_message
        .get_payload_when_handshaking()
        .try_into()
        .expect("Handshake remote invlaid message");

    #[cfg(feature = "std")]
    let sender_state = sender_state
        .step_2(second_message)
        .expect("Initiator failed third step of handshake");
    #[cfg(not(feature = "std"))]
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    #[cfg(not(feature = "std"))]
    let sender_state = sender_state
        .step_2_with_now(second_message, now)
        .expect("Initiator failed third step of handshake");

    let mut sender_state = match sender_state {
        State::Transport(c) => State::with_transport_mode(c),
        _ => panic!("todo"),
    };

    let mut receiver_state = match receiver_state {
        State::Transport(c) => State::with_transport_mode(c),
        _ => panic!("todo"),
    };

    // Create a message
    let nonce = 1337;
    let msg = CustomMessage { nonce };
    let msg_type = CUSTOM_MSG_TYPE;
    // Unique identifier of the extension describing the protocol message, as defined by Sv2 Framing
    let extension_type = 0;
    // This message is intended for the receiver, so set to false
    let channel_msg = false;

    let frame = StandardEitherFrame::<CustomMessage>::Sv2(
        Sv2Frame::from_message(msg, msg_type, extension_type, channel_msg)
            .expect("Failed to create the frame"),
    );

    let mut encoder = NoiseEncoder::<CustomMessage>::new();
    let encoded_frame = encoder
        .encode(frame, &mut sender_state)
        .expect("Failed to encode the frame");

    // Send the encoded frame
    stream_sender
        .write_all(&encoded_frame[..])
        .expect("Failed to send the encoded frame");

    // Initialize the decoder
    let mut decoder = StandardNoiseDecoder::<CustomMessage>::new();

    let mut decoded_frame;

    // Continuously read the frame from the TCP stream into the decoder buffer until the full
    // message is received.
    //
    // Note: The length of the payload is defined in a header field. Every call to `next_frame`
    // will return a `MissingBytes` error, until the full payload is received.
    loop {
        let decoder_buf = decoder.writable();

        // Read the frame header into the decoder buffer
        stream_receiver
            .read_exact(decoder_buf)
            .expect("Failed to read the encoded frame header");

        let result = decoder.next_frame(&mut receiver_state);
        match result {
            Ok(frame) => {
                let frame: StandardSv2Frame<CustomMessage> = frame
                    .try_into()
                    .expect("Failed to decode frame into Sv2Frame");
                decoded_frame = frame;
                break;
            }
            Err(Error::MissingBytes(_)) => {}
            Err(_) => panic!("Failed to decode the frame"),
        }
    }

    // Parse the decoded frame header and payload
    let decoded_frame_header = decoded_frame
        .get_header()
        .expect("Failed to get the frame header");

    let decoded_msg: CustomMessage = binary_sv2::from_bytes(decoded_frame.payload())
        .expect("Failed to extract the message from the payload");

    // Assert that the decoded message is as expected
    assert_eq!(decoded_frame_header.msg_type(), CUSTOM_MSG_TYPE);
    assert_eq!(decoded_msg.nonce, nonce);
}

#[cfg(not(feature = "noise_sv2"))]
fn main() {
    eprintln!("Noise feature not enabled. Skipping example.");
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/examples/unencrypted.rs">
// # Using Sv2 Codec Without Encryption
//
// This example demonstrates how to use the `codec-sv2` crate to encode and decode Sv2 frames
// without encryption over a TCP connection. It showcases how to:
//
// * Create an arbitrary custom message type (`CustomMessage`) for encoding and decoding.
// * Encode the message into a Sv2 frame.
// * Send the encoded frame over a TCP connection.
// * Decode the Sv2 frame on the receiving side and extract the original message.
//
// ## Run
//
// ```
// cargo run --example unencrypted
// ```

use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};
use codec_sv2::{Encoder, Error, StandardDecoder, StandardSv2Frame, Sv2Frame};
use std::{
    convert::TryInto,
    io::{Read, Write},
    net::{TcpListener, TcpStream},
};

// The `with_buffer_pool` feature changes some type signatures.
#[cfg(not(feature = "with_buffer_pool"))]
type Slice = Vec<u8>;

#[cfg(feature = "with_buffer_pool")]
type Slice = buffer_sv2::Slice;

// Arbitrary message type.
// Supported Sv2 message types are listed in the [Sv2 Spec Message
// Types](https://github.com/stratum-mining/sv2-spec/blob/main/08-Message-Types.md).
const CUSTOM_MSG_TYPE: u8 = 0xff;

// Emulate a TCP connection
const TCP_ADDR: &str = "127.0.0.1:3333";

// Example message type.
// In practice, all Sv2 messages are defined in the following crates:
// * `common_messages_sv2`
// * `mining_sv2`
// * `job_declaration_sv2`
// * `template_distribution_sv2`
#[derive(Debug, Serialize, Deserialize, Clone)]
pub struct CustomMessage {
    nonce: u16,
}

fn main() {
    // Start a receiving listener
    let listener_receiver = TcpListener::bind(TCP_ADDR).expect("Failed to bind TCP listener");

    // Start a sender stream
    let stream_sender: TcpStream =
        TcpStream::connect(TCP_ADDR).expect("Failed to connect to TCP stream");

    // Start a receiver stream
    let stream_receiver: TcpStream = listener_receiver
        .incoming()
        .next()
        .expect("Failed to accept incoming TCP stream")
        .expect("Failed to connect to incoming TCP stream");

    // Server Side

    // Create a message
    let nonce = 1337;
    let msg = CustomMessage { nonce };
    let msg_type = CUSTOM_MSG_TYPE;
    // Unique identifier of the extension describing the protocol message, as defined by Sv2 Framing
    let extension_type = 0;
    // This message is intended for the receiver, so set to false
    let channel_msg = false;
    sender_side(stream_sender, msg, msg_type, extension_type, channel_msg);

    // Receiver Side
    let mut decoded_frame = receiver_side(stream_receiver);

    // Parse the decoded frame header and payload
    let decoded_frame_header = decoded_frame
        .get_header()
        .expect("Failed to get the frame header");
    let decoded_msg: CustomMessage = binary_sv2::from_bytes(decoded_frame.payload())
        .expect("Failed to extract the message from the payload");

    // Assert that the decoded message is as expected
    assert_eq!(decoded_frame_header.msg_type(), CUSTOM_MSG_TYPE);
    assert_eq!(decoded_msg.nonce, nonce);
}

fn sender_side(
    mut stream_sender: TcpStream,
    msg: CustomMessage,
    msg_type: u8,
    extension_type: u16,
    channel_msg: bool,
) {
    // Create the frame
    let frame =
        StandardSv2Frame::<CustomMessage>::from_message(msg, msg_type, extension_type, channel_msg)
            .expect("Failed to create the frame");

    // Encode the frame
    let mut encoder = Encoder::<CustomMessage>::new();
    let encoded_frame = encoder
        .encode(frame.clone())
        .expect("Failed to encode the frame");

    // Send the encoded frame
    stream_sender
        .write_all(encoded_frame)
        .expect("Failed to send the encoded frame");
}

fn receiver_side(mut stream_receiver: TcpStream) -> Sv2Frame<CustomMessage, Slice> {
    // Initialize the decoder
    let mut decoder = StandardDecoder::<CustomMessage>::new();

    // Continuously read the frame from the TCP stream into the decoder buffer until the full
    // message is received.
    //
    // Note: The length of the payload is defined in a header field. Every call to `next_frame`
    // will return a `MissingBytes` error, until the full payload is received.
    loop {
        let decoder_buf = decoder.writable();

        // Read the frame header into the decoder buffer
        stream_receiver
            .read_exact(decoder_buf)
            .expect("Failed to read the encoded frame header");

        match decoder.next_frame() {
            Ok(decoded_frame) => {
                return decoded_frame;
            }
            Err(Error::MissingBytes(_)) => {}
            Err(_) => panic!("Failed to decode the frame"),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/README.md">
# `codec_sv2`

[![crates.io](https://img.shields.io/crates/v/codec_sv2.svg)](https://crates.io/crates/codec_sv2)
[![docs.rs](https://docs.rs/codec_sv2/badge.svg)](https://docs.rs/codec_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=codec_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`codec_sv2` provides the message encoding and decoding functionality for the Stratum V2 (Sv2)
protocol, handling secure communication between Sv2 roles. This crate abstracts the complexity of
message encoding/decoding with optional Noise protocol support, ensuring both regular and encrypted
messages can be serialized, transmitted, and decoded consistently and reliably.

## Main Components

- **Encoder**: Encodes Sv2 messages with or without Noise protocol support.
- **Decoder**: Decodes Sv2 messages with or without Noise protocol support.
- **Handshake State**: Manages the current Noise protocol handshake state of the codec.


## Usage

To include this crate in your project, run:

```bash
cargo add codec_sv2
```

This crate can be built with the following feature flags:

- `std`: Enable usage of rust `std` library, enabled by default.
- `noise_sv2`: Enables support for Noise protocol encryption and decryption.
- `with_buffer_pool`: Enables buffer pooling for more efficient memory management.

In order to use this crate in a `#![no_std]` environment, use the `--no-default-features` to remove the `std` feature.

### Examples

This crate provides two examples demonstrating how to encode and decode Sv2 frames:

1. **[Unencrypted Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/codec-sv2/examples/unencrypted.rs)**:
   Encode and decode standard Sv2 frames, detailing the message serialization and transmission
   process for unencrypted communications.

2. **[Encrypted Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/codec-sv2/examples/encrypted.rs)**:
   Encode and decode Sv2 frames with Noise protocol encryption, detailing the entire encryption
   handshake and transport phase and serialization and transmission process for encrypted
   communications.
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/src/decoder.rs">
// # Decoder
//
// Provides utilities for decoding messages held by Sv2 frames, with or without Noise protocol
// support.
//
// It includes primitives to both decode encoded standard Sv2 frames and to decrypt and decode
// Noise-encrypted encoded Sv2 frames, ensuring secure communication when required.
//
// ## Usage
// All messages passed between Sv2 roles are encoded as Sv2 frames. These frames are decoded using
// primitives in this module. There are two types of decoders for reading these frames: one for
// regular Sv2 frames [`StandardDecoder`], and another for Noise-encrypted frames
// [`StandardNoiseDecoder`]. Both decoders manage the deserialization of incoming data and, when
// applicable, the decryption of the data upon receiving the transmitted message.
//
// ### Buffer Management
//
// The decoders rely on buffers to hold intermediate data during the decoding process.
//
// - When the `with_buffer_pool` feature is enabled, the internal `Buffer` type is backed by a
//   pool-allocated buffer [`binary_sv2::BufferPool`], providing more efficient memory usage,
//   particularly in high-throughput scenarios.
// - If this feature is not enabled, a system memory buffer [`binary_sv2::BufferFromSystemMemory`]
//   is used for simpler applications where memory efficiency is less critical.

#[cfg(feature = "noise_sv2")]
use binary_sv2::Deserialize;
#[cfg(feature = "noise_sv2")]
use binary_sv2::GetSize;
use binary_sv2::Serialize;
pub use buffer_sv2::AeadBuffer;
use core::marker::PhantomData;
#[cfg(feature = "noise_sv2")]
use framing_sv2::framing::HandShakeFrame;
use framing_sv2::{
    framing::{Frame, Sv2Frame},
    header::Header,
};
#[cfg(feature = "noise_sv2")]
use framing_sv2::{ENCRYPTED_SV2_FRAME_HEADER_SIZE, SV2_FRAME_CHUNK_SIZE, SV2_FRAME_HEADER_SIZE};
#[cfg(feature = "noise_sv2")]
use noise_sv2::NoiseCodec;
#[cfg(feature = "noise_sv2")]
use noise_sv2::NOISE_FRAME_HEADER_SIZE;

#[cfg(feature = "noise_sv2")]
use crate::error::Error;
use crate::error::Result;

use crate::Error::MissingBytes;
#[cfg(feature = "noise_sv2")]
use crate::State;

#[cfg(not(feature = "with_buffer_pool"))]
use buffer_sv2::{Buffer as IsBuffer, BufferFromSystemMemory as Buffer};

#[cfg(feature = "with_buffer_pool")]
use buffer_sv2::{Buffer as IsBuffer, BufferFromSystemMemory, BufferPool};

// The buffer type for holding intermediate data during decoding.
//
// When the `with_buffer_pool` feature is enabled, `Buffer` is a pool-allocated buffer type
// [`BufferPool`], which allows for more efficient memory management. Otherwise, it defaults to
// [`BufferFromSystemMemory`].
//
// `Buffer` is used for storing both serialized Sv2 frames and encrypted Noise data.
#[cfg(feature = "with_buffer_pool")]
type Buffer = BufferPool<BufferFromSystemMemory>;

/// An encoded or decoded Sv2 frame containing either a regular or Noise-protected message.
///
/// A wrapper around the [`Frame`] enum that represents either a regular or Noise-protected Sv2
/// frame containing the generic message type (`T`).
pub type StandardEitherFrame<T> = Frame<T, <Buffer as IsBuffer>::Slice>;

/// An encoded or decoded Sv2 frame.
///
/// A wrapper around the [`Sv2Frame`] that represents a regular Sv2 frame containing the generic
/// message type (`T`).
pub type StandardSv2Frame<T> = Sv2Frame<T, <Buffer as IsBuffer>::Slice>;

/// Standard Sv2 decoder with Noise protocol support.
///
/// Used for decoding and decrypting generic message types (`T`) encoded in Sv2 frames and
/// encrypted via the Noise protocol.
#[cfg(feature = "noise_sv2")]
pub type StandardNoiseDecoder<T> = WithNoise<Buffer, T>;

/// Standard Sv2 decoder without Noise protocol support.
///
/// Used for decoding generic message types (`T`) encoded in Sv2 frames.
pub type StandardDecoder<T> = WithoutNoise<Buffer, T>;

/// Decoder for Sv2 frames with Noise protocol support.
///
/// Accumulates the encrypted data into a dedicated buffer until the entire encrypted frame is
/// received. The Noise protocol is then used to decrypt the accumulated data into another
/// dedicated buffer, converting it back into its original serialized form. This decrypted data is
/// then deserialized into the original Sv2 frame and message format.
#[cfg(feature = "noise_sv2")]
#[derive(Debug)]
pub struct WithNoise<B: IsBuffer, T: Serialize + binary_sv2::GetSize> {
    // Marker for the type of frame being decoded.
    //
    // Used to maintain the generic type (`T`) information of the message payload held by the
    // frame. `T` refers to a type that implements the necessary traits for serialization
    // [`binary_sv2::Serialize`] and size calculation [`binary_sv2::GetSize`].
    frame: PhantomData<T>,

    // Tracks the number of bytes remaining until the full frame is received.
    //
    // Ensures that the full encrypted Noise frame has been received by keeping track of the
    // remaining bytes. Once the complete frame is received, decoding can proceed.
    missing_noise_b: usize,

    // Buffer for holding incoming encrypted Noise data to be decrypted.
    //
    // Stores the incoming encrypted data, allowing the decoder to accumulate the necessary bytes
    // for full decryption. Once the entire encrypted frame is received, the decoder processes the
    // buffer to extract the underlying frame.
    noise_buffer: B,

    // Buffer for holding decrypted data to be decoded.
    //
    // Stores the decrypted data until it is ready to be processed and converted into a Sv2 frame.
    sv2_buffer: B,
}

#[cfg(feature = "noise_sv2")]
impl<'a, T: Serialize + GetSize + Deserialize<'a>, B: IsBuffer + AeadBuffer> WithNoise<B, T> {
    /// Attempts to decode the next Noise encrypted frame.
    ///
    /// On success, the decoded and decrypted frame is returned. Otherwise, an error indicating the
    /// number of missing bytes required to complete the encoded frame, an error on a badly
    /// formatted message header, or an error on decryption failure is returned.
    ///
    /// In this case of the `Error::MissingBytes`, the user should resize the decoder buffer using
    /// `writable`, read another chunk from the incoming message stream, and then call `next_frame`
    /// again. This process should be repeated until `next_frame` returns `Ok`, indicating that the
    /// full message has been received, and the decoding and decryption of the frame can proceed.
    #[inline]
    pub fn next_frame(&mut self, state: &mut State) -> Result<Frame<T, B::Slice>> {
        match state {
            State::HandShake(_) => unreachable!(),
            State::NotInitialized(msg_len) => {
                let hint = *msg_len - self.noise_buffer.as_ref().len();
                match hint {
                    0 => {
                        self.missing_noise_b = NOISE_FRAME_HEADER_SIZE;
                        Ok(self.while_handshaking())
                    }
                    _ => {
                        self.missing_noise_b = hint;
                        Err(Error::MissingBytes(hint))
                    }
                }
            }
            State::Transport(noise_codec) => {
                let hint = if IsBuffer::len(&self.sv2_buffer) < SV2_FRAME_HEADER_SIZE {
                    let len = IsBuffer::len(&self.noise_buffer);
                    let src = self.noise_buffer.get_data_by_ref(len);
                    if src.len() < ENCRYPTED_SV2_FRAME_HEADER_SIZE {
                        ENCRYPTED_SV2_FRAME_HEADER_SIZE - src.len()
                    } else {
                        0
                    }
                } else {
                    let src = self.sv2_buffer.get_data_by_ref(SV2_FRAME_HEADER_SIZE);
                    let header = Header::from_bytes(src)?;
                    header.encrypted_len() - IsBuffer::len(&self.noise_buffer)
                };

                match hint {
                    0 => {
                        self.missing_noise_b = ENCRYPTED_SV2_FRAME_HEADER_SIZE;
                        self.decode_noise_frame(noise_codec)
                    }
                    _ => {
                        self.missing_noise_b = hint;
                        Err(Error::MissingBytes(hint))
                    }
                }
            }
        }
    }

    /// Provides a writable buffer for receiving incoming Noise-encrypted Sv2 data.
    ///
    /// This buffer is used to store incoming data, and its size is adjusted based on the number
    /// of missing bytes. As new data is read, it is written into this buffer until enough data has
    /// been received to fully decode a frame. The buffer must have the correct number of bytes
    /// available to progress to the decoding process.
    #[inline]
    pub fn writable(&mut self) -> &mut [u8] {
        self.noise_buffer.get_writable(self.missing_noise_b)
    }

    /// Determines whether the decoder's internal buffers can be safely dropped.
    ///
    /// For more information, refer to the [`buffer_sv2`
    /// crate](https://docs.rs/buffer_sv2/latest/buffer_sv2/).
    pub fn droppable(&self) -> bool {
        self.noise_buffer.is_droppable() && self.sv2_buffer.is_droppable()
    }

    // Processes and decodes a Sv2 frame during the Noise protocol handshake phase.
    //
    // Handles the decoding of a handshake frame from the `noise_buffer`. It converts the received
    // data into a `HandShakeFrame` and encapsulates it into a `Frame` for further processing by
    // the codec.
    //
    // This is used exclusively during the initial handshake phase of the Noise protocol, before
    // transitioning to regular frame encryption and decryption.
    fn while_handshaking(&mut self) -> Frame<T, B::Slice> {
        let src = self.noise_buffer.get_data_owned().as_mut().to_vec();

        // Since the frame length is already validated during the handshake process, this
        // operation is infallible
        let frame = HandShakeFrame::from_bytes_unchecked(src.into());

        frame.into()
    }

    // Decodes a Noise-encrypted Sv2 frame, handling both the message header and payload
    // decryption.
    //
    // Processes Noise-encrypted Sv2 frames by first decrypting the header, followed by the
    // payload. If the frame's data is received in chunks, it ensures that decryption occurs
    // incrementally as more encrypted data becomes available. The decrypted data is then stored in
    // the `sv2_buffer`, from which the resulting Sv2 frame is extracted and returned.
    //
    // On success, the decoded frame is returned. Otherwise, an error indicating the number of
    // missing bytes required to complete the encoded frame, an error on a badly formatted message
    // header, or a decryption failure error is returned. If there are still bytes missing to
    // complete the frame, the function will return an `Error::MissingBytes` with the number of
    // additional bytes required to fully decrypt the frame. Once all bytes are available, the
    // decryption process completes and the frame can be successfully decoded.
    #[inline]
    fn decode_noise_frame(&mut self, noise_codec: &mut NoiseCodec) -> Result<Frame<T, B::Slice>> {
        match (
            IsBuffer::len(&self.noise_buffer),
            IsBuffer::len(&self.sv2_buffer),
        ) {
            // HERE THE SV2 HEADER IS READY TO BE DECRYPTED
            (ENCRYPTED_SV2_FRAME_HEADER_SIZE, 0) => {
                let src = self.noise_buffer.get_data_owned();
                let decrypted_header = self
                    .sv2_buffer
                    .get_writable(ENCRYPTED_SV2_FRAME_HEADER_SIZE);
                decrypted_header.copy_from_slice(src.as_ref());
                self.sv2_buffer.as_ref();
                noise_codec.decrypt(&mut self.sv2_buffer)?;
                let header =
                    Header::from_bytes(self.sv2_buffer.get_data_by_ref(SV2_FRAME_HEADER_SIZE))?;
                self.missing_noise_b = header.encrypted_len();
                Err(Error::MissingBytes(header.encrypted_len()))
            }
            // HERE THE SV2 PAYLOAD IS READY TO BE DECRYPTED
            _ => {
                // DECRYPT THE PAYLOAD IN CHUNKS
                let encrypted_payload = self.noise_buffer.get_data_owned();
                let encrypted_payload_len = encrypted_payload.as_ref().len();
                let mut start = 0;
                let mut end = if encrypted_payload_len < SV2_FRAME_CHUNK_SIZE {
                    encrypted_payload_len
                } else {
                    SV2_FRAME_CHUNK_SIZE
                };
                // Do not try to decrypt the header cause it is already decrypted
                let mut decrypted_len = SV2_FRAME_HEADER_SIZE;

                while start < encrypted_payload_len {
                    let decrypted_payload = self.sv2_buffer.get_writable(end - start);
                    decrypted_payload.copy_from_slice(&encrypted_payload.as_ref()[start..end]);
                    self.sv2_buffer.danger_set_start(decrypted_len);
                    noise_codec.decrypt(&mut self.sv2_buffer)?;
                    start = end;
                    end = (start + SV2_FRAME_CHUNK_SIZE).min(encrypted_payload_len);
                    decrypted_len += self.sv2_buffer.as_ref().len();
                }
                self.sv2_buffer.danger_set_start(0);
                let src = self.sv2_buffer.get_data_owned();
                let frame = Sv2Frame::<T, B::Slice>::from_bytes_unchecked(src);
                Ok(frame.into())
            }
        }
    }
}

#[cfg(feature = "noise_sv2")]
impl<T: Serialize + binary_sv2::GetSize> WithNoise<Buffer, T> {
    /// Crates a new [`WithNoise`] decoder with default buffer sizes.
    ///
    /// Initializes the decoder with default buffer sizes and sets the number of missing bytes to
    /// 0.
    pub fn new() -> Self {
        Self {
            frame: PhantomData,
            missing_noise_b: 0,
            noise_buffer: Buffer::new(2_usize.pow(16) * 5),
            sv2_buffer: Buffer::new(2_usize.pow(16) * 5),
        }
    }
}

#[cfg(feature = "noise_sv2")]
impl<T: Serialize + binary_sv2::GetSize> Default for WithNoise<Buffer, T> {
    fn default() -> Self {
        Self::new()
    }
}

/// Decoder for standard Sv2 frames.
///
/// Accumulates the data into a dedicated buffer until the entire Sv2 frame is received. This data
/// is then deserialized into the original Sv2 frame and message format.
#[derive(Debug)]
pub struct WithoutNoise<B: IsBuffer, T: Serialize + binary_sv2::GetSize> {
    // Marker for the type of frame being decoded.
    //
    // Used to maintain the generic type (`T`) information of the message payload held by the
    // frame. `T` refers to a type that implements the necessary traits for serialization
    // [`binary_sv2::Serialize`] and size calculation [`binary_sv2::GetSize`].
    frame: PhantomData<T>,

    // Tracks the number of bytes remaining until the full frame is received.
    //
    // Ensures that the full Sv2 frame has been received by keeping track of the remaining bytes.
    // Once the complete frame is received, decoding can proceed.
    missing_b: usize,

    // Buffer for holding incoming data to be decoded into a Sv2 frame.
    //
    // This buffer stores incoming data as it is received, allowing the decoder to accumulate the
    // necessary bytes until a full frame is available. Once the full encoded frame has been
    // received, the buffer's contents are processed and decoded into an Sv2 frame.
    buffer: B,
}

impl<T: Serialize + binary_sv2::GetSize, B: IsBuffer> WithoutNoise<B, T> {
    /// Attempts to decode the next frame, returning either a frame or an error indicating how many
    /// bytes are missing.
    ///
    /// Attempts to decode the next Sv2 frame.
    ///
    /// On success, the decoded frame is returned. Otherwise, an error indicating the number of
    /// missing bytes required to complete the frame is returned.
    ///
    /// In the case of `Error::MissingBytes`, the user should resize the decoder buffer using
    /// `writable`, read another chunk from the incoming message stream, and then call `next_frame`
    /// again. This process should be repeated until `next_frame` returns `Ok`, indicating that the
    /// full message has been received, and the frame can be fully decoded.
    #[inline]
    pub fn next_frame(&mut self) -> Result<Sv2Frame<T, B::Slice>> {
        let len = self.buffer.len();
        let src = self.buffer.get_data_by_ref(len);
        let hint = Sv2Frame::<T, B::Slice>::size_hint(src) as usize;

        match hint {
            0 => {
                self.missing_b = Header::SIZE;
                let src = self.buffer.get_data_owned();
                let frame = Sv2Frame::<T, B::Slice>::from_bytes_unchecked(src);
                Ok(frame)
            }
            _ => {
                self.missing_b = hint;
                Err(MissingBytes(self.missing_b))
            }
        }
    }

    /// Provides a writable buffer for receiving incoming Sv2 data.
    ///
    /// This buffer is used to store incoming data, and its size is adjusted based on the number of
    /// missing bytes. As new data is read, it is written into this buffer until enough data has
    /// been received to fully decode a frame. The buffer must have the correct number of bytes
    /// available to progress to the decoding process.
    pub fn writable(&mut self) -> &mut [u8] {
        self.buffer.get_writable(self.missing_b)
    }
}

impl<T: Serialize + binary_sv2::GetSize> WithoutNoise<Buffer, T> {
    /// Creates a new [`WithoutNoise`] with a buffer of default size.
    ///
    /// Initializes the decoder with a default buffer size and sets the number of missing bytes to
    /// the size of the header.
    pub fn new() -> Self {
        Self {
            frame: PhantomData,
            missing_b: Header::SIZE,
            buffer: Buffer::new(2_usize.pow(16) * 5),
        }
    }
}

impl<T: Serialize + binary_sv2::GetSize> Default for WithoutNoise<Buffer, T> {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use binary_sv2::{binary_codec_sv2, Serialize};

    #[derive(Serialize)]
    pub struct TestMessage {}

    #[test]
    fn unencrypted_writable_with_missing_b_initialized_as_header_size() {
        let mut decoder = StandardDecoder::<TestMessage>::new();
        let actual = decoder.writable();
        let expect = [0u8; Header::SIZE];
        assert_eq!(actual, expect);
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/src/encoder.rs">
// # Encoder
//
// Provides utilities for encoding messages into Sv2 frames, with or without Noise protocol
// support.
//
// ## Usage
//
// All messages passed between Sv2 roles are encoded as Sv2 frames using primitives in this module.
// There are two types of encoders for creating these frames: one for regular Sv2 frames
// [`Encoder`], and another for Noise-encrypted frames [`NoiseEncoder`]. Both encoders manage the
// serialization of outgoing data and, when applicable, the encryption of the data before
// transmission.
//
// ### Buffer Management
//
// The encoders rely on buffers to hold intermediate data during the encoding process.
//
// - When the `with_buffer_pool` feature is enabled, the internal `Buffer` type is backed by a
//   pool-allocated buffer [`binary_sv2::BufferPool`], providing more efficient memory usage,
//   particularly in high-throughput scenarios.
// - If the feature is not enabled, a system memory buffer [`binary_sv2::BufferFromSystemMemory`] is
//   used for simpler applications where memory efficiency is less critical.

use alloc::vec::Vec;
use binary_sv2::{GetSize, Serialize};
#[cfg(feature = "noise_sv2")]
use core::convert::TryInto;
use core::marker::PhantomData;
use framing_sv2::framing::Sv2Frame;
#[cfg(feature = "noise_sv2")]
use framing_sv2::framing::{Frame, HandShakeFrame};
#[cfg(feature = "noise_sv2")]
use framing_sv2::{ENCRYPTED_SV2_FRAME_HEADER_SIZE, SV2_FRAME_CHUNK_SIZE, SV2_FRAME_HEADER_SIZE};
#[cfg(feature = "noise_sv2")]
use noise_sv2::AEAD_MAC_LEN;

#[cfg(feature = "tracing")]
use tracing::error;

#[cfg(feature = "noise_sv2")]
use crate::{Error, Result, State};

#[cfg(feature = "noise_sv2")]
#[cfg(not(feature = "with_buffer_pool"))]
use buffer_sv2::{Buffer as IsBuffer, BufferFromSystemMemory as Buffer};

#[cfg(feature = "noise_sv2")]
#[cfg(feature = "with_buffer_pool")]
use buffer_sv2::{Buffer as IsBuffer, BufferFromSystemMemory, BufferPool};

// The buffer type for holding intermediate data during encoding.
//
// When the `with_buffer_pool` feature is enabled, `Buffer` uses a pool-allocated buffer
// [`BufferPool`], providing more efficient memory management, particularly in high-throughput
// environments. If the feature is not enabled, it defaults to [`BufferFromSystemMemory`], a
// simpler system memory buffer.
//
// `Buffer` is utilized for storing both serialized Sv2 frames and encrypted Noise data during the
// encoding process, ensuring that all frames are correctly handled before transmission.
#[cfg(feature = "noise_sv2")]
#[cfg(feature = "with_buffer_pool")]
type Buffer = BufferPool<BufferFromSystemMemory>;

// A simple buffer slice for holding serialized Sv2 frame data before transmission.
//
// When the `with_buffer_pool` feature is disabled, [`Slice`] defaults to a `Vec<u8>`, which serves
// as a dynamically allocated array to hold the serialized bytes of Sv2 frames. This provides
// flexibility in managing the encoded data during transmission or further processing, though it
// may not offer the same memory efficiency as the pool-allocated version [`BufferPool`].
#[cfg(not(feature = "with_buffer_pool"))]
type Slice = Vec<u8>;

// A buffer slice used for holding serialized Sv2 frame data before transmission.
//
// When the `with_buffer_pool` feature is enabled, [`Slice`] defaults to a `buffer_sv2::Slice`,
// which serves as a slice of the `Buffer` that stores the encoded data. It holds the frame's
// serialized bytes temporarily, ensuring the data is ready for transmission or encryption,
// depending on whether Noise protocol support is enabled.
#[cfg(feature = "with_buffer_pool")]
type Slice = buffer_sv2::Slice;

/// Encoder for Sv2 frames with Noise protocol encryption.
///
/// Serializes the Sv2 frame into a dedicated buffer. Encrypts this serialized data using the Noise
/// protocol, storing it into another dedicated buffer. Encodes the serialized and encrypted data,
/// such that it is ready for transmission.
#[cfg(feature = "noise_sv2")]
pub struct NoiseEncoder<T: Serialize + binary_sv2::GetSize> {
    // Buffer for holding encrypted Noise data to be transmitted.
    //
    // Stores the encrypted data after the Sv2 frame has been processed by the Noise protocol
    // and is ready for transmission. This buffer holds the outgoing encrypted data, ensuring
    // that the full frame is correctly prepared before being sent.
    noise_buffer: Buffer,

    // Buffer for holding serialized Sv2 data before encryption.
    //
    // Stores the data after it has been serialized into an Sv2 frame but before it is encrypted
    // by the Noise protocol. The buffer accumulates the frame's serialized bytes before they are
    // encrypted and then encoded for transmission.
    sv2_buffer: Buffer,

    // Marker for the type of frame being encoded.
    //
    // Used to maintain the generic type information for `T`, which represents the message payload
    // contained within the Sv2 frame. `T` refers to a type that implements the necessary traits
    // for serialization [`binary_sv2::Serialize`] and size calculation [`binary_sv2::GetSize`],
    // ensuring that the encoder can handle different message types correctly during the encoding
    // process.
    frame: PhantomData<T>,
}

// A Sv2 frame that will be encoded and optionally encrypted using the Noise protocol.
//
// Represent a Sv2 frame during the encoding process. It encapsulates the frame's generic payload
// message type (`T`) and is stored in a [`Slice`] buffer. The `Item` is passed to the encoder,
// which either processes it for normal transmission or applies Noise encryption, depending on the
// codec's state.
#[cfg(feature = "noise_sv2")]
type Item<T> = Frame<T, Slice>;

#[cfg(feature = "noise_sv2")]
impl<T: Serialize + GetSize> NoiseEncoder<T> {
    /// Encodes an Sv2 frame and encrypts it using the Noise protocol.
    ///
    /// Takes an `item`, which is an Sv2 frame containing a payload of type `T`, and encodes it for
    /// transmission. The frame is encrypted after being serialized. The `state` parameter
    /// determines whether the encoder is in the handshake or transport phase, guiding the
    /// appropriate encoding and encryption action.
    ///
    /// - In the handshake phase, the initial handshake messages are processed to establish secure
    ///   communication.
    /// - In the transport phase, the full frame is serialized, encrypted, and stored in a buffer
    ///   for transmission.
    ///
    /// On success, the method returns an encrypted (`Slice`) (buffer) ready for transmission.
    /// Otherwise, errors on an encryption or serialization failure.
    #[inline]
    pub fn encode(&mut self, item: Item<T>, state: &mut State) -> Result<Slice> {
        match state {
            State::Transport(noise_codec) => {
                let len = item.encoded_length();
                let writable = self.sv2_buffer.get_writable(len);

                // ENCODE THE SV2 FRAME
                let i: Sv2Frame<T, Slice> = item.try_into().map_err(|e| {
                    #[cfg(feature = "tracing")]
                    error!("Error while encoding 1 frame: {:?}", e);
                    Error::FramingError(e)
                })?;
                i.serialize(writable)?;

                let sv2 = self.sv2_buffer.get_data_owned();
                let sv2: &[u8] = sv2.as_ref();

                // ENCRYPT THE HEADER
                let to_encrypt = self.noise_buffer.get_writable(SV2_FRAME_HEADER_SIZE);
                to_encrypt.copy_from_slice(&sv2[..SV2_FRAME_HEADER_SIZE]);
                noise_codec.encrypt(&mut self.noise_buffer)?;

                // ENCRYPT THE PAYLOAD IN CHUNKS
                let mut start = SV2_FRAME_HEADER_SIZE;
                let mut end = if sv2.len() - start < (SV2_FRAME_CHUNK_SIZE - AEAD_MAC_LEN) {
                    sv2.len()
                } else {
                    SV2_FRAME_CHUNK_SIZE + start - AEAD_MAC_LEN
                };
                let mut encrypted_len = ENCRYPTED_SV2_FRAME_HEADER_SIZE;

                while start < sv2.len() {
                    let to_encrypt = self.noise_buffer.get_writable(end - start);
                    to_encrypt.copy_from_slice(&sv2[start..end]);
                    self.noise_buffer.danger_set_start(encrypted_len);
                    noise_codec.encrypt(&mut self.noise_buffer)?;
                    encrypted_len += self.noise_buffer.as_ref().len();
                    start = end;
                    end = (start + SV2_FRAME_CHUNK_SIZE - AEAD_MAC_LEN).min(sv2.len());
                }
                self.noise_buffer.danger_set_start(0);
            }
            State::HandShake(_) => self.while_handshaking(item)?,
            State::NotInitialized(_) => self.while_handshaking(item)?,
        };

        // Clear sv2_buffer
        self.sv2_buffer.get_data_owned();
        // Return noise_buffer
        Ok(self.noise_buffer.get_data_owned())
    }

    // Encodes Sv2 frames during the handshake phase of the Noise protocol.
    //
    // Used when the encoder is in the handshake phase, before secure communication is fully
    // established. It encodes the provided `item` into a handshake frame, storing the resulting
    // data in the `noise_buffer`. The handshake phase is necessary to exchange initial messages
    // and set up the Noise encryption state before transitioning to the transport phase, where
    // full frames are encrypted and transmitted.
    #[inline(never)]
    fn while_handshaking(&mut self, item: Item<T>) -> Result<()> {
        // ENCODE THE SV2 FRAME
        let i: HandShakeFrame = item.try_into().map_err(|e| {
            #[cfg(feature = "tracing")]
            error!("Error while encoding 2 frame - while_handshaking: {:?}", e);
            Error::FramingError(e)
        })?;
        let payload = i.get_payload_when_handshaking();
        let wrtbl = self.noise_buffer.get_writable(payload.len());
        for (i, b) in payload.iter().enumerate() {
            wrtbl[i] = *b;
        }
        Ok(())
    }

    /// Determines whether the encoder's internal buffers can be safely dropped.
    pub fn droppable(&self) -> bool {
        self.noise_buffer.is_droppable() && self.sv2_buffer.is_droppable()
    }
}

#[cfg(feature = "noise_sv2")]
impl<T: Serialize + binary_sv2::GetSize> NoiseEncoder<T> {
    /// Creates a new `NoiseEncoder` with default buffer sizes.
    pub fn new() -> Self {
        #[cfg(not(feature = "with_buffer_pool"))]
        let size = 512;
        #[cfg(feature = "with_buffer_pool")]
        let size = 2_usize.pow(16) * 5;
        Self {
            sv2_buffer: Buffer::new(size),
            noise_buffer: Buffer::new(size),
            frame: core::marker::PhantomData,
        }
    }
}

#[cfg(feature = "noise_sv2")]
impl<T: Serialize + GetSize> Default for NoiseEncoder<T> {
    fn default() -> Self {
        Self::new()
    }
}

/// Encoder for standard Sv2 frames.
///
/// Serializes the Sv2 frame into a dedicated buffer then encodes it, such that it is ready for
/// transmission.
#[derive(Debug)]
pub struct Encoder<T> {
    // Buffer for holding serialized Sv2 data.
    //
    // Stores the serialized bytes of the Sv2 frame after it has been encoded. Once the frame is
    // serialized, the resulting bytes are stored in this buffer to be transmitted. The buffer is
    // dynamically resized to accommodate the size of the encoded frame.
    buffer: Vec<u8>,

    // Marker for the type of frame being encoded.
    //
    // Used to maintain the generic type information for `T`, which represents the message payload
    // contained within the Sv2 frame. `T` refers to a type that implements the necessary traits
    // for serialization [`binary_sv2::Serialize`] and size calculation [`binary_sv2::GetSize`],
    // ensuring that the encoder can handle different message types correctly during the encoding
    // process.
    frame: PhantomData<T>,
}

impl<T: Serialize + GetSize> Encoder<T> {
    /// Encodes a standard Sv2 frame for transmission.
    ///
    /// Takes a standard Sv2 frame containing a payload of type `T` and serializes it into a byte
    /// stream. The resulting serialized bytes are stored in the internal `buffer`, preparing the
    /// frame for transmission. On success, the method returns a reference to the serialized bytes
    /// stored in the internal buffer. Otherwise, errors on a serialization failure.
    pub fn encode(
        &mut self,
        item: Sv2Frame<T, Slice>,
    ) -> core::result::Result<&[u8], crate::Error> {
        let len = item.encoded_length();

        self.buffer.resize(len, 0);

        item.serialize(&mut self.buffer)?;

        Ok(&self.buffer[..])
    }

    /// Creates a new `Encoder` with a buffer of default size.
    pub fn new() -> Self {
        Self {
            buffer: Vec::with_capacity(512),
            frame: core::marker::PhantomData,
        }
    }
}

impl<T: Serialize + GetSize> Default for Encoder<T> {
    fn default() -> Self {
        Self::new()
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/src/error.rs">
//! # Error Handling and Result Types
//!
//! This module defines error types and utilities for handling errors in the `codec_sv2` module.
//! It includes the [`Error`] enum for representing various errors, a C-compatible [`CError`] enum
//! for FFI, and a `Result` type alias for convenience.

use core::fmt;
use framing_sv2::Error as FramingError;
#[cfg(feature = "noise_sv2")]
use noise_sv2::{AeadError, Error as NoiseError};

/// A type alias for results returned by the `codec_sv2` modules.
///
/// `Result` is a convenient wrapper around the [`core::result::Result`] type, using the [`Error`]
/// enum defined in this crate as the error type.
pub type Result<T> = core::result::Result<T, Error>;

/// Enumeration of possible errors in the `codec_sv2` module.
///
/// This enum represents various errors that can occur within the `codec_sv2` module, including
/// errors from related crates like [`binary_sv2`], [`framing_sv2`], and [`noise_sv2`].
#[derive(Debug, PartialEq, Eq)]
pub enum Error {
    /// AEAD (`snow`) error in the Noise protocol.
    #[cfg(feature = "noise_sv2")]
    AeadError(AeadError),

    /// Binary Sv2 data format error.
    BinarySv2Error(binary_sv2::Error),

    /// Framing Sv2 error.
    FramingError(FramingError),

    /// Framing Sv2 error.
    FramingSv2Error(framing_sv2::Error),

    /// Invalid step for initiator in the Noise protocol.
    #[cfg(feature = "noise_sv2")]
    InvalidStepForInitiator,

    /// Invalid step for responder in the Noise protocol.
    #[cfg(feature = "noise_sv2")]
    InvalidStepForResponder,

    /// Incomplete frame with the number of missing bytes remaining to completion.
    MissingBytes(usize),

    /// Sv2 Noise protocol error.
    #[cfg(feature = "noise_sv2")]
    NoiseSv2Error(NoiseError),

    /// Noise protocol is not in the expected handshake state.
    #[cfg(feature = "noise_sv2")]
    NotInHandShakeState,

    /// Unexpected state in the Noise protocol.
    UnexpectedNoiseState,
}

impl fmt::Display for Error {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        use Error::*;
        match self {
            #[cfg(feature = "noise_sv2")]
            AeadError(e) => write!(f, "Aead Error: `{e:?}`"),
            BinarySv2Error(e) => write!(f, "Binary Sv2 Error: `{e:?}`"),
            FramingError(e) => write!(f, "Framing error in codec: `{e:?}`"),
            FramingSv2Error(e) => write!(f, "Framing Sv2 Error: `{e:?}`"),
            #[cfg(feature = "noise_sv2")]
            InvalidStepForInitiator => write!(
                f,
                "This noise handshake step can not be executed by an initiato"
            ),
            #[cfg(feature = "noise_sv2")]
            InvalidStepForResponder => write!(
                f,
                "This noise handshake step can not be executed by a responder"
            ),
            MissingBytes(u) => write!(f, "Missing `{u}` Noise bytes"),
            #[cfg(feature = "noise_sv2")]
            NoiseSv2Error(e) => write!(f, "Noise SV2 Error: `{e:?}`"),
            #[cfg(feature = "noise_sv2")]
            NotInHandShakeState => write!(
                f,
                "This operation can be executed only during the noise handshake"
            ),
            UnexpectedNoiseState => {
                write!(f, "Noise state is incorrect")
            }
        }
    }
}

#[cfg(feature = "noise_sv2")]
impl From<AeadError> for Error {
    fn from(e: AeadError) -> Self {
        Error::AeadError(e)
    }
}

impl From<binary_sv2::Error> for Error {
    fn from(e: binary_sv2::Error) -> Self {
        Error::BinarySv2Error(e)
    }
}

impl From<framing_sv2::Error> for Error {
    fn from(e: framing_sv2::Error) -> Self {
        Error::FramingSv2Error(e)
    }
}

#[cfg(feature = "noise_sv2")]
impl From<NoiseError> for Error {
    fn from(e: NoiseError) -> Self {
        Error::NoiseSv2Error(e)
    }
}

/// C-compatible enumeration of possible errors in the `codec_sv2` module.
///
/// This enum mirrors the [`Error`] enum but is designed to be used in C code through FFI. It
/// represents the same set of errors as [`Error`], making them accessible to C programs.
#[repr(C)]
#[derive(Debug)]
pub enum CError {
    /// AEAD (`snow`) error in the Noise protocol.
    AeadError,

    /// Binary Sv2 data format error.
    BinarySv2Error,

    /// Framing Sv2 error.
    FramingError,

    /// Framing Sv2 error.
    FramingSv2Error,

    /// Invalid step for initiator in the Noise protocol.
    InvalidStepForInitiator,

    /// Invalid step for responder in the Noise protocol.
    InvalidStepForResponder,

    /// Missing bytes in the Noise protocol.
    MissingBytes(usize),

    /// Sv2 Noise protocol error.
    NoiseSv2Error,

    /// Noise protocol is not in the expected handshake state.
    NotInHandShakeState,

    /// Unexpected state in the Noise protocol.
    UnexpectedNoiseState,
}

/// Force `cbindgen` to create a header for [`CError`].
///
/// It ensures that [`CError`] is included in the generated C header file. This function is not
/// meant to be called and will panic if called. Its only purpose is to make [`CError`] visible to
/// `cbindgen`.
#[no_mangle]
pub extern "C" fn export_cerror() -> CError {
    unimplemented!()
}

impl From<Error> for CError {
    fn from(e: Error) -> CError {
        match e {
            #[cfg(feature = "noise_sv2")]
            Error::AeadError(_) => CError::AeadError,
            Error::BinarySv2Error(_) => CError::BinarySv2Error,
            Error::FramingSv2Error(_) => CError::FramingSv2Error,
            Error::FramingError(_) => CError::FramingError,
            #[cfg(feature = "noise_sv2")]
            Error::InvalidStepForInitiator => CError::InvalidStepForInitiator,
            #[cfg(feature = "noise_sv2")]
            Error::InvalidStepForResponder => CError::InvalidStepForResponder,
            Error::MissingBytes(u) => CError::MissingBytes(u),
            #[cfg(feature = "noise_sv2")]
            Error::NoiseSv2Error(_) => CError::NoiseSv2Error,
            #[cfg(feature = "noise_sv2")]
            Error::NotInHandShakeState => CError::NotInHandShakeState,
            Error::UnexpectedNoiseState => CError::UnexpectedNoiseState,
        }
    }
}

impl Drop for CError {
    fn drop(&mut self) {
        match self {
            CError::AeadError => (),
            CError::BinarySv2Error => (),
            CError::FramingError => (),
            CError::FramingSv2Error => (),
            CError::InvalidStepForInitiator => (),
            CError::InvalidStepForResponder => (),
            CError::MissingBytes(_) => (),
            CError::NoiseSv2Error => (),
            CError::NotInHandShakeState => (),
            CError::UnexpectedNoiseState => (),
        };
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/codec-sv2/src/lib.rs">
//! # Stratum V2 Codec Library
//!
//! `codec_sv2` provides the message encoding and decoding functionality for the Stratum V2 (Sv2)
//! protocol, handling secure communication between Sv2 roles.
//!
//! This crate abstracts the complexity of message encoding/decoding with optional Noise protocol
//! support, ensuring both regular and encrypted messages can be serialized, transmitted, and
//! decoded consistently and reliably.
//!
//!
//! ## Usage
//! `codec-sv2` supports both standard Sv2 frames (unencrypted) and Noise-encrypted Sv2 frames to
//! ensure secure communication. To encode messages for transmission, choose between the
//! [`Encoder`] for standard Sv2 frames or the [`NoiseEncoder`] for encrypted frames. To decode
//! received messages, choose between the [`StandardDecoder`] for standard Sv2 frames or
//! [`StandardNoiseDecoder`] to decrypt Noise frames.
//!
//! ## Build Options
//!
//! This crate can be built with the following features:
//!
//! - `std`: Enable usage of rust `std` library, enabled by default.
//! - `noise_sv2`: Enables support for Noise protocol encryption and decryption.
//! - `with_buffer_pool`: Enables buffer pooling for more efficient memory management.
//!
//! In order to use this crate in a `#![no_std]` environment, use the `--no-default-features` to
//! remove the `std` feature.
//!
//! ## Examples
//!
//! See the examples for more information:
//!
//! - [Unencrypted Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/codec-sv2/examples/unencrypted.rs)
//! - [Encrypted Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/codec-sv2/examples/encrypted.rs)

#![cfg_attr(not(feature = "std"), no_std)]

pub use framing_sv2::framing::Frame;

extern crate alloc;

#[cfg(feature = "noise_sv2")]
use alloc::boxed::Box;

mod decoder;
mod encoder;
pub mod error;

pub use error::{CError, Error, Result};

pub use decoder::{StandardEitherFrame, StandardSv2Frame};

pub use decoder::StandardDecoder;
#[cfg(feature = "noise_sv2")]
pub use decoder::StandardNoiseDecoder;

pub use encoder::Encoder;
#[cfg(feature = "noise_sv2")]
pub use encoder::NoiseEncoder;

#[cfg(feature = "noise_sv2")]
pub use framing_sv2::framing::HandShakeFrame;
pub use framing_sv2::framing::Sv2Frame;

#[cfg(feature = "noise_sv2")]
pub use noise_sv2::{self, Initiator, NoiseCodec, Responder};

pub use buffer_sv2;

pub use binary_sv2;

pub use framing_sv2::{self, framing::handshake_message_to_frame as h2f};

/// Represents the role in the Noise handshake process, either as an initiator or a responder.
///
/// The Noise protocol requires two roles during the handshake process:
/// - **Initiator**: The party that starts the handshake by sending the initial message.
/// - **Responder**: The party that waits for the initiator's message and responds to it.
///
/// This enum distinguishes between these two roles, allowing the codec to handle the handshake
/// process accordingly.
#[allow(clippy::large_enum_variant)]
#[cfg(feature = "noise_sv2")]
#[derive(Debug, Clone)]
pub enum HandshakeRole {
    /// The initiator role in the Noise handshake process.
    ///
    /// The initiator starts the handshake by sending the initial message. This variant stores an
    /// `Initiator` object, which contains the necessary state and cryptographic materials for the
    /// initiator's part in the Noise handshake.
    Initiator(Box<noise_sv2::Initiator>),

    /// The responder role in the Noise handshake process.
    ///
    /// The responder waits for the initiator's handshake message and then responds. This variant
    /// stores a `Responder` object, which contains the necessary state and cryptographic materials
    /// for the responder's part in the Noise handshake.
    Responder(Box<noise_sv2::Responder>),
}

/// Represents the state of the Noise protocol codec during different phases: initialization,
/// handshake, or transport mode, where encryption and decryption are fully operational.
///
/// The state transitions from initialization [`State::NotInitialized`] to handshake
/// [`State::HandShake`] and finally to transport mode [`State::Transport`] as the encryption
/// handshake is completed.
#[cfg(feature = "noise_sv2")]
#[derive(Debug, Clone)]
#[allow(clippy::large_enum_variant)]
pub enum State {
    /// The codec has not been initialized yet.
    ///
    /// This state is used when the codec is first created or reset, before the handshake process
    /// begins. The variant carries the expected size of the handshake message, which can vary
    /// depending on whether the codec is acting as an initiator or a responder.
    NotInitialized(usize),

    /// The codec is in the handshake phase, where cryptographic keys are being negotiated.
    ///
    /// In this state, the codec is in the process of establishing secure communication by
    /// exchanging handshake messages. Once the handshake is complete, the codec transitions to
    /// [`State::Transport`] mode.
    HandShake(HandshakeRole),

    /// The codec is in transport mode, where AEAD encryption and decryption are fully operational.
    ///
    /// In this state, the codec is performing full encryption and decryption using the Noise
    /// protocol in transport mode. The [`NoiseCodec`] object is responsible for handling the
    /// encryption and decryption of data.
    Transport(NoiseCodec),
}

#[cfg(feature = "noise_sv2")]
impl State {
    /// Initiates the first step of the handshake process for the initiator.
    ///
    /// Creates and sends the initial handshake message for the initiator. It is the first step in
    /// establishing a secure communication channel. Responders cannot perform this step.
    ///
    /// nb: This method returns a [`HandShakeFrame`] but does not change the current state
    /// (`self`). The state remains `State::HandShake(HandshakeRole::Initiator)` until `step_1` is
    /// called to advance the handshake process.
    pub fn step_0(&mut self) -> core::result::Result<HandShakeFrame, Error> {
        match self {
            Self::HandShake(h) => match h {
                HandshakeRole::Initiator(i) => i.step_0().map_err(|e| e.into()).map(h2f),
                HandshakeRole::Responder(_) => Err(Error::InvalidStepForResponder),
            },
            _ => Err(Error::NotInHandShakeState),
        }
    }

    /// Processes the second step of the handshake process for the responder.
    ///
    /// The responder receives the public key from the initiator, generates a response message
    /// containing the handshake frame, and prepares the [`NoiseCodec`] for transitioning the
    /// initiator state to transport mode in `step_2`.
    ///
    /// nb: Returns a new state [`State::Transport`] but does not update the current state
    /// (`self`). The caller is responsible for updating the state, allowing for more flexible
    /// control over the handshake process as the caller decides what to do with this state.
    #[cfg(feature = "std")]
    pub fn step_1(
        &mut self,
        re_pub: [u8; noise_sv2::ELLSWIFT_ENCODING_SIZE],
    ) -> core::result::Result<(HandShakeFrame, Self), Error> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;

        self.step_1_with_now_rng(re_pub, now, &mut rand::thread_rng())
    }

    /// Processes the second step of the handshake process for the responder given
    /// the current time and a custom random number generator.
    ///
    /// See [`Self::step_1`] for more details.
    ///
    /// The current time and the custom random number generatorshould be provided in order to not
    /// implicitely rely on `std` and allow `no_std` environments to provide a hardware random
    /// number generator for example.
    #[inline]
    pub fn step_1_with_now_rng<R: rand::Rng + rand::CryptoRng>(
        &mut self,
        re_pub: [u8; noise_sv2::ELLSWIFT_ENCODING_SIZE],
        now: u32,
        rng: &mut R,
    ) -> core::result::Result<(HandShakeFrame, Self), Error> {
        match self {
            Self::HandShake(h) => match h {
                HandshakeRole::Responder(r) => {
                    let (message, codec) = r.step_1_with_now_rng(re_pub, now, rng)?;
                    Ok((h2f(message), Self::Transport(codec)))
                }
                HandshakeRole::Initiator(_) => Err(Error::InvalidStepForInitiator),
            },
            _ => Err(Error::NotInHandShakeState),
        }
    }

    /// Processes the final step of the handshake process for the initiator.
    ///
    /// Receives the response message from the responder containing the handshake frame, and
    /// transitions the state to transport mode. This finalizes the secure communication setup and
    /// enables full encryption and decryption in [`State::Transport`] mode.
    ///
    /// nb: Directly updates the current state (`self`) to [`State::Transport`], completing the
    /// handshake process.
    #[cfg(feature = "std")]
    pub fn step_2(
        &mut self,
        message: [u8; noise_sv2::INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE],
    ) -> core::result::Result<Self, Error> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;
        self.step_2_with_now(message, now)
    }

    /// Processes the final step of the handshake process for the initiator given the
    /// current system time.
    ///
    /// See [`Self::step_2`] for more details.
    ///
    /// The current system time should be provided to avoid relying on `std` and allow `no_std`
    /// environments to use another source of time.
    #[inline]
    pub fn step_2_with_now(
        &mut self,
        message: [u8; noise_sv2::INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE],
        now: u32,
    ) -> core::result::Result<Self, Error> {
        match self {
            Self::HandShake(h) => match h {
                HandshakeRole::Initiator(i) => i
                    .step_2_with_now(message, now)
                    .map_err(|e| e.into())
                    .map(Self::Transport),
                HandshakeRole::Responder(_) => Err(Error::InvalidStepForResponder),
            },
            _ => Err(Error::NotInHandShakeState),
        }
    }
}

#[cfg(feature = "noise_sv2")]
impl State {
    /// Creates a new uninitialized handshake [`State`].
    ///
    /// Sets the codec to the initial state, [`State::NotInitialized`], based on the provided
    /// handshake role. This state is used before the handshake process begins, and the handshake
    /// message size guides the codec on how much data to expect before advancing to the next step.
    /// The expected size of the handshake message is determined by whether the codec is acting as
    /// an initiator or responder.
    pub fn not_initialized(role: &HandshakeRole) -> Self {
        match role {
            HandshakeRole::Initiator(_) => {
                Self::NotInitialized(noise_sv2::INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE)
            }
            HandshakeRole::Responder(_) => Self::NotInitialized(noise_sv2::ELLSWIFT_ENCODING_SIZE),
        }
    }

    /// Initializes the codec state to [`State::HandShake`] mode with the given handshake role.
    ///
    /// Transitions the codec into the handshake phase by accepting a [`HandshakeRole`], which
    /// determines whether the codec is the initiator or responder in the handshake process. Once
    /// in [`State::HandShake`] mode, the codec begins negotiating cryptographic keys with the
    /// peer, eventually transitioning to the secure [`State::Transport`] phase.
    ///
    /// The role passed to this method defines how the handshake proceeds:
    /// - [`HandshakeRole::Initiator`]: The codec will start the handshake process.
    /// - [`HandshakeRole::Responder`]: The codec will wait for the initiator's handshake message.
    pub fn initialized(inner: HandshakeRole) -> Self {
        Self::HandShake(inner)
    }

    /// Transitions the codec state to [`State::Transport`] mode with the given [`NoiseCodec`].
    ///
    /// Finalizes the handshake process and transitions the codec into [`State::Transport`] mode,
    /// where full encryption and decryption are active. The codec uses the provided [`NoiseCodec`]
    /// to perform encryption and decryption for all communication in this mode, ensuring secure
    /// data transmission.
    ///
    /// Once in [`State::Transport`] mode, the codec is fully operational for secure communication.
    pub fn with_transport_mode(tm: NoiseCodec) -> Self {
        Self::Transport(tm)
    }
}

#[cfg(test)]
#[cfg(feature = "noise_sv2")]
mod tests {
    use super::*;

    #[test]
    fn handshake_step_fails_if_state_is_not_initialized() {
        let mut state = State::NotInitialized(32);
        let actual = state.step_0().unwrap_err();
        let expect = Error::NotInHandShakeState;
        assert_eq!(actual, expect);
    }

    #[test]
    fn handshake_step_fails_if_state_is_in_transport_mode() {
        let mut state = State::NotInitialized(32);
        let actual = state.step_0().unwrap_err();
        let expect = Error::NotInHandShakeState;
        assert_eq!(actual, expect);
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/Cargo.toml">
[package]
name = "framing_sv2"
version = "5.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 frames"
documentation = "https://docs.rs/framing_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../../protocols/v2/binary-sv2", version = "^3.0.0" }
buffer_sv2 = { path = "../../../utils/buffer", optional=true, version = "^2.0.0" }
noise_sv2 = { path = "../../../protocols/v2/noise-sv2", version = "^1.0.0" }

[dev-dependencies]
noise_sv2 = { path = "../../../protocols/v2/noise-sv2", version = "^1.0.0" }
rand = "0.8.3"
secp256k1 = { version = "0.28.2", default-features = false, features =["alloc","rand","rand-std"] }

[features]
with_buffer_pool = ["binary_sv2/with_buffer_pool", "buffer_sv2"]

[package.metadata.docs.rs]
features = ["with_buffer_pool"]
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/examples/sv2_frame.rs">
// # Sv2 Frame Example
//
// This example demonstrates how to use the `framing_sv2` crate to construct, serialize, and
// deserialize a regular Sv2 message frame (`Sv2Frame`). It showcases how to:
//
// - Define a message payload and frame it using the Sv2 protocol.
// - Define a custom message type (`CustomMessage`) to be framed.
// - Construct an Sv2 frame with the custom message, message type, and extension type.
// - Serialize the frame into a byte array.
// - Deserialize the frame from the byte array back into an Sv2 frame.
//
// In the Sv2 protocol, these frames can then be encoded and transmitted between Sv2 roles.
//
// ## Run
//
// ```
// cargo run --example sv2_frame
// ```

use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};
use framing_sv2::framing::Sv2Frame;
use std::convert::TryInto;

// Example message type (e.g., SetupConnection)
const MSG_TYPE: u8 = 1;
// Example extension type (e.g., a standard Sv2 message)
const EXT_TYPE: u16 = 0x0001;

#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct CustomMessage {
    pub nonce: u32,
}

fn main() {
    // Create the message payload
    let message = CustomMessage { nonce: 42 };

    // Create the frame from the message
    let frame: Sv2Frame<CustomMessage, Vec<u8>> =
        Sv2Frame::from_message(message.clone(), MSG_TYPE, EXT_TYPE, false)
            .expect("Failed to frame the message");

    // Serialize the frame into a byte array for transmission
    let mut serialized_frame = vec![0u8; frame.encoded_length()];
    frame
        .serialize(&mut serialized_frame)
        .expect("Failed to serialize the frame");

    // Deserialize the frame from bytes back into an Sv2Frame
    let mut deserialized_frame = Sv2Frame::<CustomMessage, Vec<u8>>::from_bytes(serialized_frame)
        .expect("Failed to deserialize frame");

    // Assert that deserialized header has the original content
    let deserialized_header = deserialized_frame
        .get_header()
        .expect("Frame has no header");
    assert_eq!(deserialized_header.msg_type(), MSG_TYPE);
    assert_eq!(deserialized_header.ext_type(), EXT_TYPE);

    // Assert that deserialized message has the original content
    let deserialized_message: CustomMessage = binary_sv2::from_bytes(deserialized_frame.payload())
        .expect("Failed to extract the message from the payload");
    assert_eq!(deserialized_message.nonce, message.nonce);
}
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/README.md">
# `framing_sv2`

[![crates.io](https://img.shields.io/crates/v/framing_sv2.svg)](https://crates.io/crates/framing_sv2)
[![docs.rs](https://docs.rs/framing_sv2/badge.svg)](https://docs.rs/framing_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=framing_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`framing_sv2` provides utilities for framing messages sent between Sv2 roles, handling both Sv2
message and Noise handshake frames.

The Sv2 protocol is binary, with fixed message framing. Each message begins with the extension
type, message type, and message length (six bytes in total), followed by a variable length
message. The message framing is outlined below ([according to Sv2 specs
](https://github.com/stratum-mining/sv2-spec/blob/main/03-Protocol-Overview.md#32-framing)):

| Field Name  | Byte Length | Description |
|----------------|-------------|-------------|
| `extension_type` | `U16` | Unique identifier of the extension associated with this protocol message. |
| `msg_type` | `U8` | Unique identifier of this protocol message. |
| `msg_length` | `U24` | Length of the protocol message, not including this header. |
| `payload` | `BYTES` | Message-specific payload of length `msg_length`. If the MSB in `extension_type` (the `channel_msg` bit) is set the first four bytes are defined as a `U32` `"channel_id"`, though this definition is repeated in the message definitions below and these 4 bytes are included in `msg_length`. |

Some bits of the `extension_type` field can also be repurposed for signaling on how the frame
should be handled across channels.

The least significant bit of `extension_type` (i.e.bit 15, 0-indexed, aka `channel_msg`) indicates
a message which is specific to a channel, whereas if the most significant bit is unset, the message
is to be interpreted by the immediate receiving device.

Note that the `channel_msg` bit is ignored in the extension lookup, i.e.an `extension_type` of
`0x8ABC` is for the same "extension" as `0x0ABC`.

If the `channel_msg` bit is set, the first four bytes of the payload field is a `U32` representing
the `channel_id` this message is destined for (these bytes are repeated in the message framing
descriptions below).

Note that for the Job Declaration and Template Distribution Protocols the `channel_msg` bit is
always unset.

## Main Components

- **Header**: Defines the 6-byte Sv2 message header with information about the message payload,
  including its extension type, if it is associated with a specific mining channel, the type of
  message (e.g. `SetupConnection`, `NewMiningJob`, etc.) and the payload length.
- **Sv2 Framing**: Use for serializing Sv2 messages.
- **Noise Handshake Framing**: Use for serializing Noise protocol handshake messages.

## Usage

To include this crate in your project, run:

```bash
cargo add framing_sv2
```

This crate can be built with the following feature flags:

- `with_buffer_pool`: Enables buffer pooling for more efficient memory management.

### Examples

This crate provides an example demonstrating how to serialize and deserialize Sv2 message frames:

1. **[Sv2 Frame](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/framing-sv2/examples/sv2_frame.rs)**:
   Constructs, serializes, and deserialize a regular Sv2 message frame (`Sv2Frame`).
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/src/error.rs">
//! # Error Handling
//!
//! This module defines error types and utilities for handling errors in the `framing_sv2` module.

// use crate::framing2::EitherFrame;
use core::fmt;

use crate::SV2_FRAME_HEADER_SIZE;

// pub type FramingResult<T> = core::result::Result<T, Error>;

#[derive(Debug, PartialEq, Eq)]
pub enum Error {
    /// Binary Sv2 data format error.
    BinarySv2Error(binary_sv2::Error),
    ExpectedHandshakeFrame,
    ExpectedSv2Frame,
    UnexpectedHeaderLength(isize),
}

impl fmt::Display for Error {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        use Error::*;
        match self {
            BinarySv2Error(ref e) => {
                write!(f, "BinarySv2Error: `{e:?}`")
            }
            ExpectedHandshakeFrame => {
                write!(f, "Expected `HandshakeFrame`, received `Sv2Frame`")
            }
            ExpectedSv2Frame => {
                write!(f, "Expected `Sv2Frame`, received `HandshakeFrame`")
            }
            UnexpectedHeaderLength(actual_size) => {
                write!(
                    f,
                    "Unexpected `Header` length: `{actual_size}`, should be equal or more to {SV2_FRAME_HEADER_SIZE}"
                )
            }
        }
    }
}

impl From<binary_sv2::Error> for Error {
    fn from(e: binary_sv2::Error) -> Self {
        Error::BinarySv2Error(e)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/src/framing.rs">
//! # Sv2 Frame
//!
//! Handles the serializing and deserializing of both Sv2 and Noise handshake messages into frames.
//!
//! It handles the serialization and deserialization of frames, ensuring that messages can be
//! correctly encoded and transmitted and then received and decoded between Sv2 roles.
//!
//! # Usage
//!
//! Two types of frames are defined. The most common frame is [`crate::framing::Sv2Frame`] and is
//! used for almost all messages passed between Sv2 roles. It consists of a
//! [`crate::header::Header`] followed by the serialized message payload. The
//! [`crate::framing::HandShakeFrame`] is used exclusively during the Noise handshake process,
//! performed between Sv2 roles at the beginning of their communication. This frame is used until
//! the handshake state progresses to transport mode. After that, all subsequent messages use
//! [`crate::framing::Sv2Frame`]. No header is included in the handshake frame.

use crate::{header::Header, Error};
use alloc::vec::Vec;
use binary_sv2::{to_writer, GetSize, Serialize};
use core::convert::TryFrom;

#[cfg(not(feature = "with_buffer_pool"))]
type Slice = Vec<u8>;

#[cfg(feature = "with_buffer_pool")]
type Slice = buffer_sv2::Slice;

/// Represents either an Sv2 frame or a handshake frame.
///
/// A wrapper used when generic reference to a frame is needed, but the kind of frame ([`Sv2Frame`]
/// or [`HandShakeFrame`]) does not matter. Note that after the initial handshake is complete
/// between two Sv2 roles, all further messages are framed with [`Sv2Frame`].
#[derive(Debug)]
pub enum Frame<T, B> {
    HandShake(HandShakeFrame),
    Sv2(Sv2Frame<T, B>),
}

impl<T: Serialize + GetSize, B: AsMut<[u8]> + AsRef<[u8]>> Frame<T, B> {
    pub fn encoded_length(&self) -> usize {
        match &self {
            Self::HandShake(frame) => frame.encoded_length(),
            Self::Sv2(frame) => frame.encoded_length(),
        }
    }
}

impl<T, B> From<HandShakeFrame> for Frame<T, B> {
    fn from(v: HandShakeFrame) -> Self {
        Self::HandShake(v)
    }
}

impl<T, B> From<Sv2Frame<T, B>> for Frame<T, B> {
    fn from(v: Sv2Frame<T, B>) -> Self {
        Self::Sv2(v)
    }
}

/// Abstraction for a Sv2 frame.
///
/// Represents a regular Sv2 frame, used for all communication outside of the Noise protocol
/// handshake process. It contains a [`Header`] and a message payload, which can be serialized for
/// encoding and transmission or decoded and deserialized upon receipt.
#[derive(Debug, Clone)]
pub struct Sv2Frame<T, B> {
    header: Header,
    payload: Option<T>,
    // Serialized header + payload
    serialized: Option<B>,
}

impl<T: Serialize + GetSize, B: AsMut<[u8]> + AsRef<[u8]>> Sv2Frame<T, B> {
    /// Writes the serialized [`Sv2Frame`] into `dst`.
    ///
    /// This operation when called on an already serialized frame is very cheap. When called on a
    /// non serialized frame, it is not so cheap (because it serializes it).
    #[inline]
    pub fn serialize(self, dst: &mut [u8]) -> Result<(), Error> {
        if let Some(mut serialized) = self.serialized {
            dst.swap_with_slice(serialized.as_mut());
            Ok(())
        } else if let Some(payload) = self.payload {
            to_writer(self.header, dst).map_err(Error::BinarySv2Error)?;
            to_writer(payload, &mut dst[Header::SIZE..]).map_err(Error::BinarySv2Error)?;
            Ok(())
        } else {
            // Sv2Frame always has a payload or a serialized payload
            panic!("Impossible state")
        }
    }

    /// Returns the message payload.
    ///
    /// `self` can be either serialized (`self.serialized` is `Some()`) or deserialized
    /// (`self.serialized` is `None`, `self.payload` is `Some()`).
    ///
    /// This function is only intended as a fast way to get a reference to an already serialized
    /// payload. If the frame has not yet been serialized, this function should never be used (it
    /// will panic).
    pub fn payload(&mut self) -> &mut [u8] {
        if let Some(serialized) = self.serialized.as_mut() {
            &mut serialized.as_mut()[Header::SIZE..]
        } else {
            // panic here is the expected behaviour
            panic!("Sv2Frame is not yet serialized.")
        }
    }

    /// [`Sv2Frame`] always returns `Some(self.header)`.
    pub fn get_header(&self) -> Option<crate::header::Header> {
        Some(self.header)
    }

    /// Tries to build a [`Sv2Frame`] from raw bytes.
    ///
    /// It assumes the raw bytes represent a serialized [`Sv2Frame`] frame (`Self.serialized`).
    /// Returns a [`Sv2Frame`] on success, or the number of the bytes needed to complete the frame
    /// as an error. `Self.serialized` is [`Some`], but nothing is assumed or checked about the
    /// correctness of the payload.
    #[inline]
    pub fn from_bytes(mut bytes: B) -> Result<Self, isize> {
        let hint = Self::size_hint(bytes.as_mut());

        if hint == 0 {
            Ok(Self::from_bytes_unchecked(bytes))
        } else {
            Err(hint)
        }
    }

    /// Constructs an [`Sv2Frame`] from raw bytes without performing byte content validation.
    #[inline]
    pub fn from_bytes_unchecked(mut bytes: B) -> Self {
        // Unchecked function caller is supposed to already know that the passed bytes are valid
        let header = Header::from_bytes(bytes.as_mut()).expect("Invalid header");
        Self {
            header,
            payload: None,
            serialized: Some(bytes),
        }
    }

    /// After parsing `bytes` into a [`Header`], this function helps to determine if the
    /// `msg_length` field is correctly representing the size of the frame.
    /// - Returns `0` if the byte slice is of the expected size according to the header.
    /// - Returns a negative value if the byte slice is shorter than expected; this value represents
    ///   how many bytes are missing.
    /// - Returns a positive value if the byte slice is longer than expected; this value indicates
    ///   the surplus of bytes beyond the expected size.
    #[inline]
    pub fn size_hint(bytes: &[u8]) -> isize {
        match Header::from_bytes(bytes) {
            Err(_) => {
                // Returns how many bytes are missing from the expected frame size
                (Header::SIZE - bytes.len()) as isize
            }
            Ok(header) => {
                if bytes.len() - Header::SIZE == header.len() {
                    // expected frame size confirmed
                    0
                } else {
                    // Returns how many excess bytes are beyond the expected frame size
                    (bytes.len() - Header::SIZE) as isize + header.len() as isize
                }
            }
        }
    }

    /// If [`Sv2Frame`] is serialized, returns the length of `self.serialized`, otherwise, returns
    /// the length of `self.payload`.
    #[inline]
    pub fn encoded_length(&self) -> usize {
        if let Some(serialized) = self.serialized.as_ref() {
            serialized.as_ref().len()
        } else if let Some(payload) = self.payload.as_ref() {
            payload.get_size() + Header::SIZE
        } else {
            // Sv2Frame always has a payload or a serialized payload
            panic!("Impossible state")
        }
    }

    /// Tries to build a [`Sv2Frame`] from a non-serialized payload.
    ///
    /// Returns a [`Sv2Frame`] if the size of the payload fits in the frame, [`None`] otherwise.
    pub fn from_message(
        message: T,
        message_type: u8,
        extension_type: u16,
        channel_msg: bool,
    ) -> Option<Self> {
        let extension_type = update_extension_type(extension_type, channel_msg);
        let len = message.get_size() as u32;
        Header::from_len(len, message_type, extension_type).map(|header| Self {
            header,
            payload: Some(message),
            serialized: None,
        })
    }
}

impl<A, B> Sv2Frame<A, B> {
    /// Maps a `Sv2Frame<A, B>` to `Sv2Frame<C, B>` by applying `fun`, which is assumed to be a
    /// closure that converts `A` to `C`
    pub fn map<C>(self, fun: fn(A) -> C) -> Sv2Frame<C, B> {
        let serialized = self.serialized;
        let header = self.header;
        let payload = self.payload.map(fun);
        Sv2Frame {
            header,
            payload,
            serialized,
        }
    }
}

impl<T, B> TryFrom<Frame<T, B>> for Sv2Frame<T, B> {
    type Error = Error;

    fn try_from(v: Frame<T, B>) -> Result<Self, Error> {
        match v {
            Frame::Sv2(frame) => Ok(frame),
            Frame::HandShake(_) => Err(Error::ExpectedSv2Frame),
        }
    }
}

/// Abstraction for a Noise handshake frame.
///
/// Contains only the serialized payload with a fixed length and is only used during Noise
/// handshake process. Once the handshake is complete, regular Sv2 communication switches to
/// [`Sv2Frame`] for ongoing communication.
#[derive(Debug)]
pub struct HandShakeFrame {
    payload: Slice,
}

impl HandShakeFrame {
    /// Returns payload of [`HandShakeFrame`] as a [`Vec<u8>`].
    pub fn get_payload_when_handshaking(&self) -> Vec<u8> {
        self.payload[0..].to_vec()
    }

    /// Builds a [`HandShakeFrame`] from raw bytes. Nothing is assumed or checked about the
    /// correctness of the payload.
    pub fn from_bytes(bytes: Slice) -> Result<Self, isize> {
        Ok(Self::from_bytes_unchecked(bytes))
    }

    #[inline]
    pub fn from_bytes_unchecked(bytes: Slice) -> Self {
        Self { payload: bytes }
    }

    // Returns the size of the [`HandShakeFrame`] payload.
    #[inline]
    fn encoded_length(&self) -> usize {
        self.payload.len()
    }
}

impl<T, B> TryFrom<Frame<T, B>> for HandShakeFrame {
    type Error = Error;

    fn try_from(v: Frame<T, B>) -> Result<Self, Error> {
        match v {
            Frame::HandShake(frame) => Ok(frame),
            Frame::Sv2(_) => Err(Error::ExpectedHandshakeFrame),
        }
    }
}

/// Returns a [`HandShakeFrame`] from a generic byte array.
#[allow(clippy::useless_conversion)]
pub fn handshake_message_to_frame<T: AsRef<[u8]>>(message: T) -> HandShakeFrame {
    let mut payload = Vec::new();
    payload.extend_from_slice(message.as_ref());
    HandShakeFrame {
        payload: payload.into(),
    }
}

// Basically a Boolean bit filter for `extension_type`.
//
// Takes an `extension_type` represented as a `u16` and a Boolean flag (`channel_msg`). If
// `channel_msg` is true, it sets the most significant bit of `extension_type` to `1`, otherwise,
// it clears the most significant bit to `0`.
fn update_extension_type(extension_type: u16, channel_msg: bool) -> u16 {
    if channel_msg {
        let mask = 0b1000_0000_0000_0000;
        extension_type | mask
    } else {
        let mask = 0b0111_1111_1111_1111;
        extension_type & mask
    }
}

#[cfg(test)]
use binary_sv2::binary_codec_sv2;

#[cfg(test)]
#[derive(Serialize)]
struct T {}

#[test]
fn test_size_hint() {
    let h = Sv2Frame::<T, Vec<u8>>::size_hint(&[0, 128, 30, 46, 0, 0][..]);
    assert!(h == 46);
}
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/src/header.rs">
//! # Sv2 Frame Header
//!
//! Defines the [`crate::header::Header`] structure used in the framing of Sv2 messages.
//!
//! Each [`crate::framing::Sv2Frame`] starts with a 6-byte header with information about the
//! message payload, including its extension type, if it is associated with a specific mining
//! channel, the type of message (e.g. `SetupConnection`, `NewMiningJob`, etc.) and the payload
//! length.
//!
//! ## Header Structure
//!
//! The Sv2 header includes the following fields:
//!
//! - `extension_type`: A `16`-bit field that describes the protocol extension associated with the
//!   message. It also contains a special bit (the `channel_msg` bit) to indicate if the message is
//!   tied to a specific channel.
//! - `msg_type`: An `8`-bit field representing the specific message type within the given
//!   extension.
//! - `msg_length`: A `24`-bit field that indicates the length of the message payload, excluding the
//!   header itself.

use crate::Error;
use alloc::vec::Vec;
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, U24};
use core::convert::TryInto;

use crate::{SV2_FRAME_CHUNK_SIZE, SV2_FRAME_HEADER_SIZE};
use noise_sv2::AEAD_MAC_LEN;

/// Abstraction for a Sv2 Frame Header.
#[derive(Debug, Serialize, Deserialize, Copy, Clone)]
pub struct Header {
    // Unique identifier of the extension describing this protocol message.  Most significant bit
    // (i.e.bit 15, 0-indexed, aka channel_msg) indicates a message which is specific to a channel,
    // whereas if the most significant bit is unset, the message is to be interpreted by the
    // immediate receiving device.  Note that the channel_msg bit is ignored in the extension
    // lookup, i.e.an extension_type of 0x8ABC is for the same "extension" as 0x0ABC.  If the
    // channel_msg bit is set, the first four bytes of the payload field is a U32 representing the
    // channel_id this message is destined for. Note that for the Job Declaration and Template
    // Distribution Protocols the channel_msg bit is always unset.
    extension_type: u16, // fix: use U16 type
    //
    // Unique identifier of the extension describing this protocol message
    msg_type: u8, // fix: use specific type?

    // Length of the protocol message, not including this header
    msg_length: U24,
}

impl Header {
    pub const SIZE: usize = SV2_FRAME_HEADER_SIZE;

    /// Construct a [`Header`] from raw bytes
    #[inline]
    pub fn from_bytes(bytes: &[u8]) -> Result<Self, Error> {
        if bytes.len() < Self::SIZE {
            return Err(Error::UnexpectedHeaderLength(bytes.len() as isize));
        };
        let extension_type = u16::from_le_bytes([bytes[0], bytes[1]]);
        let msg_type = bytes[2];
        let msg_length: U24 = u32::from_le_bytes([bytes[3], bytes[4], bytes[5], 0]).try_into()?;
        Ok(Self {
            extension_type,
            msg_type,
            msg_length,
        })
    }

    // Get the payload length
    #[allow(clippy::len_without_is_empty)]
    #[inline]
    pub(crate) fn len(&self) -> usize {
        let inner: u32 = self.msg_length.into();
        inner as usize
    }

    // Construct a [`Header`] from payload length, type and extension type.
    #[inline]
    pub(crate) fn from_len(msg_length: u32, msg_type: u8, extension_type: u16) -> Option<Header> {
        Some(Self {
            extension_type,
            msg_type,
            msg_length: msg_length.try_into().ok()?,
        })
    }

    /// Get the [`Header`] message type.
    pub fn msg_type(&self) -> u8 {
        self.msg_type
    }

    /// Get the [`Header`[ extension type.
    pub fn ext_type(&self) -> u16 {
        self.extension_type
    }

    /// Check if [`Header`] represents a channel message.
    ///
    /// A header can represent a channel message if the MSB(Most Significant Bit) is set.
    pub fn channel_msg(&self) -> bool {
        const CHANNEL_MSG_MASK: u16 = 0b0000_0000_0000_0001;
        self.extension_type & CHANNEL_MSG_MASK == self.extension_type
    }

    /// Calculates the total length of a chunked message, accounting for MAC overhead.
    ///
    /// Determines the total length of the message frame, including the overhead introduced by
    /// MACs. If the message is split into multiple chunks (due to its size exceeding the maximum
    /// frame chunk size), each chunk requires a MAC for integrity verification.
    ///
    /// This method is particularly relevant when the message is being encrypted using the Noise
    /// protocol, where the payload is divided into encrypted chunks, and each chunk is appended
    /// with a MAC. However, it can also be applied to non-encrypted chunked messages to calculate
    /// their total length.
    ///
    /// The calculated length includes the full payload length and any additional space required
    /// for the MACs.
    pub fn encrypted_len(&self) -> usize {
        let len = self.len();
        let mut chunks = len / (SV2_FRAME_CHUNK_SIZE - AEAD_MAC_LEN);
        if len % (SV2_FRAME_CHUNK_SIZE - AEAD_MAC_LEN) != 0 {
            chunks += 1;
        }
        let mac_len = chunks * AEAD_MAC_LEN;
        len + mac_len
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use alloc::vec;

    #[test]
    fn test_header_from_bytes() {
        let bytes = vec![0x01, 0x02, 0x03, 0x04, 0x05, 0x06];
        let header = Header::from_bytes(&bytes).unwrap();
        assert_eq!(header.extension_type, 0x0201);
        assert_eq!(header.msg_type, 0x03);
        assert_eq!(header.msg_length, 0x060504_u32.try_into().unwrap());
    }

    #[test]
    fn test_header_from_len() {
        let header = Header::from_len(0x1234, 0x56, 0x789a).unwrap();
        assert_eq!(header.extension_type, 0x789a);
        assert_eq!(header.msg_type, 0x56);
        assert_eq!(header.msg_length, 0x1234_u32.try_into().unwrap());

        let extension_type = 0;
        let msg_type = 0x1;
        let msg_length = 0x1234_u32;
        let header = Header::from_len(msg_length, msg_type, extension_type).unwrap();
        assert_eq!(header.extension_type, 0);
        assert_eq!(header.msg_type, 0x1);
        assert_eq!(header.msg_length, 0x1234_u32.try_into().unwrap());
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/framing-sv2/src/lib.rs">
//! # Stratum V2 Framing Library
//!
//! `framing_sv2` provides utilities for framing messages sent between Sv2 roles, handling both Sv2
//! message and Noise handshake frames.
//!
//! ## Message Format
//!
//! The Sv2 protocol is binary, with fixed message framing. Each message begins with the extension
//! type, message type, and message length (six bytes in total), followed by a variable length
//! message.
//!
//! The message framing is outlined below ([according to Sv2 specs
//! ](https://stratumprotocol.org/specification/03-Protocol-Overview/#32-framing)):
//!
//! | Protocol Type  | Byte Length | Description |
//! |----------------|-------------|-------------|
//! | `extension_type` | `U16` | Unique identifier of the extension describing this protocol message. <br><br> Most significant bit (i.e.bit `15`, `0`-indexed, aka `channel_msg`) indicates a message which is specific to a channel, whereas if the most significant bit is unset, the message is to be interpreted by the immediate receiving device. <br><br> Note that the `channel_msg` bit is ignored in the extension lookup, i.e.an `extension_type` of `0x8ABC` is for the same "extension" as `0x0ABC`. <br><br> If the `channel_msg` bit is set, the first four bytes of the payload field is a `U32` representing the `channel_id` this message is destined for (these bytes are repeated in the message framing descriptions below). <br><br> Note that for the Job Declaration and Template Distribution Protocols the `channel_msg` bit is always unset. |
//! | `msg_type` | `U8` | Unique identifier of the extension describing this protocol message. |
//! | `msg_length` | `U24` | Length of the protocol message, not including this header. |
//! | `payload` | `BYTES` | Message-specific payload of length `msg_length`. If the MSB in `extension_type` (the `channel_msg` bit) is set the first four bytes are defined as a `U32` `"channel_id"`, though this definition is repeated in the message definitions below and these 4 bytes are included in `msg_length`. |
//!
//! ## Usage
//!
//! Nearly all messages sent between Sv2 roles are serialized with the [`framing::Sv2Frame`]. The
//! exception is when two Sv2 roles exchange Noise protocol handshake messages.
//!
//! Before Sv2 roles can communicate securely, they must perform a Noise handshake (note that Noise
//! encryption is optional for communication between two local Sv2 roles (i.e. a local mining
//! device and a local mining proxy), but required between two remote Sv2 roles (i.e. a local
//! mining proxy and a remote pool)). During this process, the [`framing::HandShakeFrame`] is used
//! to transmit encrypted messages between the roles. After the handshake is completed and the
//! connection transitions into transport mode, [`framing::Sv2Frame`] is used for all messages.
//!
//! Once the Noise handshake is complete (if it was performed at all), all subsequent messages are
//! framed using the [`framing::Sv2Frame`]. Each frame consists of a [`header::Header`] followed by
//! a serialized payload.
//!
//! ## Build Options
//!
//! This crate can be built with the following features:
//!
//! - `with_buffer_pool`: Enables buffer pooling for more efficient memory management.
//!
//! ## Examples
//!
//! See the example for more information:
//!
//! - [Sv2 Frame Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/framing-sv2/examples/sv2_frame.rs)

#![no_std]

extern crate alloc;

/// Sv2 framing types
pub mod framing;

/// Sv2 framing errors
pub mod error;

/// Sv2 framing header
pub mod header;
pub use error::Error;

use noise_sv2::AEAD_MAC_LEN;

/// Size of the SV2 frame header in bytes.
pub const SV2_FRAME_HEADER_SIZE: usize = 6;

/// Size of the encrypted SV2 frame header, including the MAC.
pub const ENCRYPTED_SV2_FRAME_HEADER_SIZE: usize = SV2_FRAME_HEADER_SIZE + AEAD_MAC_LEN;

/// Maximum size of an SV2 frame chunk in bytes.
pub const SV2_FRAME_CHUNK_SIZE: usize = 65535;
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/Cargo.toml">
[package]
name = "noise_sv2"
version = "1.4.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 noise"
documentation = "https://docs.rs/noise_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[dependencies]
secp256k1 = { version = "0.28.2", default-features = false, features = ["hashes", "alloc", "rand"] }
rand = {version = "0.8.5", default-features = false }
aes-gcm = { version = "0.10.2", features = ["alloc", "aes"], default-features = false }
chacha20poly1305 = { version = "0.10.1", default-features = false, features = ["alloc"]}
rand_chacha = { version = "0.3.1", default-features = false }

[features]
default = ["std"]
std = ["rand/std", "rand/std_rng", "rand_chacha/std", "secp256k1/rand-std"]

[dev-dependencies]
quickcheck = "1.0.3"
quickcheck_macros = "1"
rand = {version = "0.8.5", default-features = false, features = ["std", "std_rng"] }

[profile.dev]
panic = "unwind"

[profile.release]
panic = "abort"

[package.metadata.docs.rs]
features = ["std"]
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/examples/handshake.rs">
// # Noise Protocol Handshake
//
// This example demonstrates how to use the `noise-sv2` crate to establish a Noise handshake
// between and initiator and responder, and encrypt and decrypt a secret message. It showcases how
// to:
//
// - Generate a cryptographic keypair using the `secp256k1` library.
// - Perform a Noise handshake between an initiator and responder.
// - Transition from handshake to secure communication mode.
// - Encrypt a message as the initiator role.
// - Decrypt the message as the responder role.
//
// ## Run
//
// ```sh
// cargo run --example handshake
// ```

use noise_sv2::{Initiator, Responder};
use secp256k1::{Keypair, Parity, Secp256k1};

// Even parity used in the Schnorr signature process
const PARITY: Parity = Parity::Even;
// Validity duration of the responder's certificate, seconds
const RESPONDER_CERT_VALIDITY: u32 = 3600;

// Generates a secp256k1 public/private key pair for the responder.
fn generate_key() -> Keypair {
    let secp = Secp256k1::new();
    let (secret_key, _) = secp.generate_keypair(&mut rand::thread_rng());
    let kp = Keypair::from_secret_key(&secp, &secret_key);
    if kp.x_only_public_key().1 == PARITY {
        kp
    } else {
        generate_key()
    }
}

fn main() {
    let mut secret_message = "Ciao, Mondo!".as_bytes().to_vec();

    let responder_key_pair = generate_key();

    #[cfg(feature = "std")]
    let mut initiator = Initiator::new(Some(responder_key_pair.public_key().into()));
    #[cfg(not(feature = "std"))]
    let mut initiator = Initiator::new_with_rng(
        Some(responder_key_pair.public_key().into()),
        &mut rand::thread_rng(),
    );
    #[cfg(feature = "std")]
    let mut responder = Responder::new(responder_key_pair, RESPONDER_CERT_VALIDITY);
    #[cfg(not(feature = "std"))]
    let mut responder = Responder::new_with_rng(
        responder_key_pair,
        RESPONDER_CERT_VALIDITY,
        &mut rand::thread_rng(),
    );

    let first_message = initiator
        .step_0()
        .expect("Initiator failed first step of handshake");

    #[cfg(feature = "std")]
    let (second_message, mut responder_state) = responder
        .step_1(first_message)
        .expect("Responder failed second step of handshake");
    #[cfg(not(feature = "std"))]
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    #[cfg(not(feature = "std"))]
    let (second_message, mut responder_state) = responder
        .step_1_with_now_rng(first_message, now, &mut rand::thread_rng())
        .expect("Responder failed second step of handshake");

    #[cfg(feature = "std")]
    let mut initiator_state = initiator
        .step_2(second_message)
        .expect("Initiator failed third step of handshake");
    #[cfg(not(feature = "std"))]
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    #[cfg(not(feature = "std"))]
    let mut initiator_state = initiator
        .step_2_with_now(second_message, now)
        .expect("Initiator failed third step of handshake");

    initiator_state
        .encrypt(&mut secret_message)
        .expect("Initiator failed to encrypt the secret message");
    assert!(secret_message != "Ciao, Mondo!".as_bytes().to_vec());

    responder_state
        .decrypt(&mut secret_message)
        .expect("Responder failed to decrypt the secret message");
    assert!(secret_message == "Ciao, Mondo!".as_bytes().to_vec());
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/README.md">
# noise_sv2

[![crates.io](https://img.shields.io/crates/v/noise_sv2.svg)](https://crates.io/crates/noise_sv2)
[![docs.rs](https://docs.rs/noise_sv2/badge.svg)](https://docs.rs/noise_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=noise_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`noise_sv2` is primarily intended to secure communication in the Stratum V2 (Sv2) protocol. It handles the necessary Noise handshakes, encrypts outgoing messages, and decrypts incoming responses, ensuring privacy and integrity across the communication link between Sv2 roles. See the [Protocol Security specification](https://github.com/stratum-mining/sv2-spec/blob/main/04-Protocol-Security.md) for more details.

## Key Capabilities
* **Secure Communication**: Provides encryption and authentication for messages exchanged between different Sv2 roles.
* **Cipher Support**: Includes support for both `AES-GCM` and `ChaCha20-Poly1305`.
* **Handshake Roles**: Implements the `Initiator` and `Responder` roles required by the Noise handshake, allowing both sides of a connection to establish secure communication.
* **Cryptographic Helpers**: Facilitates the management of cryptographic state and encryption operations.

## Usage
To include this crate in your project, run:

```bash
cargo add noise_sv2
```

This crate can be built with the following feature flags:

- `std`: Enable usage of rust `std` library, enabled by default.

In order to use this crate in a `#![no_std]` environment, use the `--no-default-features` to remove the `std` feature.

### Examples

This crate provides example on establishing a secure line:

1. **[Noise Handshake Example](https://github.com/stratum-mining/stratum/blob/main/protocols/v2/noise-sv2/examples/handshake.rs)**:
   Establish a secure line of communication between an Initiator and Responder via the Noise
   protocol, allowing for the encryption and decryption of a secret message.
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/aed_cipher.rs">
// # AEAD Cipher
//
// Abstracts the encryption and decryption operations for authenticated encryption with associated
// data (AEAD) ciphers used in the Noise protocol.
//
// The [`AeadCipher`] trait provides a unified interface for AEAD ciphers, including
// [`ChaCha20Poly1305`] and [`Aes256Gcm`], allowing flexible cryptographic operations in different
// contexts.
//
// The trait supports core AEAD operations, including:
//
// - Key initialization via the `from_key` method to derive a cipher instance from a 32-byte key.
// - Authenticated encryption via the `encrypt` method to securely encrypt data with a nonce and
//   additional associated data (AAD).
// - Authenticated decryption via the `decrypt` method to securely decrypt data using the provided
//   nonce and AAD.
//
// ## Usage
//
// The `AeadCipher` trait can be implemented for any AEAD cipher, enabling encryption and decryption
// of Noise protocol messages. Two default implementations are provided for the
// [`ChaCha20Poly1305`] and [`Aes256Gcm`] ciphers.

use aes_gcm::Aes256Gcm;
use chacha20poly1305::{aead::Buffer, AeadInPlace, ChaCha20Poly1305, ChaChaPoly1305, KeyInit};

// Defines the interface for AEAD ciphers.
//
// The [`AeadCipher`] trait provides a standard interface for initializing AEAD ciphers, and for
// performing encryption and decryption operations with additional Authenticated Associated Data
// (AAD). This trait is implemented by either the [`ChaCha20Poly1305`] or [`Aes256Gcm`] specific
// cipher types, allowing them to be used interchangeably in cryptographic protocols. It is utilized
// by the [`crate::handshake::HandshakeOp`] trait to secure the handshake process.
//
// The `T: Buffer` represents the data buffer to be encrypted or decrypted. The buffer must
// implement the [`Buffer`] trait, which provides necessary operations for in-place encryption and
// decryption.
pub trait AeadCipher {
    // Creates a new instance of the cipher from a 32-byte key.
    //
    // Initializes the AEAD cipher with the provided key (`k`), preparing it for
    // encryption and decryption operations.
    fn from_key(k: [u8; 32]) -> Self;

    // Encrypts the data in place using the provided 12-byte `nonce` and AAD (`ad`).
    //
    // Performs authenticated encryption on the provided mutable data buffer (`data`), modifying
    // it in place to contain the ciphertext. The encryption is performed using the provided nonce
    // and AAD, which ensures that the data has not been tampered with during transit.
    fn encrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error>;

    // Decrypts the data in place using the provided 12-byte nonce (`n`) and AAD (`ad`).
    //
    // Performs authenticated decryption on the provided mutable data buffer, modifying it in
    // place to contain the plaintext. The decryption is performed using the provided nonce and
    // AAD, ensuring that the data has not been tampered with during transit.
    fn decrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error>;
}

impl AeadCipher for ChaCha20Poly1305 {
    fn from_key(k: [u8; 32]) -> Self {
        ChaChaPoly1305::new(&k.into())
    }

    fn encrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        self.encrypt_in_place(nonce.into(), ad, data)
    }

    fn decrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        self.decrypt_in_place(nonce.into(), ad, data)
    }
}

impl AeadCipher for Aes256Gcm {
    fn from_key(k: [u8; 32]) -> Self {
        Aes256Gcm::new(&k.into())
    }

    fn encrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        self.encrypt_in_place(nonce.into(), ad, data)
    }

    fn decrypt<T: Buffer>(
        &mut self,
        nonce: &[u8; 12],
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        self.decrypt_in_place(nonce.into(), ad, data)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/cipher_state.rs">
// # Cipher State Management
//
// Defines the [`CipherState`] trait and the [`GenericCipher`] enum, which manage the state of
// AEAD ciphers used in the Noise protocol. This includes managing the encryption key, nonce, and
// the cipher instance itself, facilitating secure encryption and decryption during communication.
//
// The [`CipherState`] trait abstracts the management of core elements for AEAD ciphers:
// - Manages the encryption key lifecycle used by the AEAD cipher.
// - Generates and tracks unique nonces for each encryption operation, preventing replay attacks.
// - Initializes the appropriate cipher (e.g., [`ChaCha20Poly1305`] or [`Aes256Gcm`]) for secure
//   communication.
//
// The trait provides methods for encrypting and decrypting data using additional associated data
// (AAD) and securely erasing sensitive cryptographic material when no longer needed.
//
// The [`GenericCipher`] enum enables flexible use of either [`ChaCha20Poly1305`] or [`Aes256Gcm`]
// ciphers. It abstracts away the specific cipher being used while ensuring consistent handling of
// cryptographic operations (e.g., encryption, decryption, key erasure) across both ciphers.
//
// ## Usage
//
// The [`CipherState`] trait is used by the [`crate::handshake::HandshakeOp`] trait to manage
// stateful encryption and decryption tasks during the Noise protocol handshake. By implementing
// [`CipherState`], the handshake process securely manages cryptographic material and transforms
// messages exchanged between the initiator and responder.
//
// Once the Noise handshake is complete, the [`crate::Initiator`] and [`crate::Responder`] use
// [`GenericCipher`] instances (`c1` and `c2`) to perform symmetric encryption and decryption.
// These ciphers, initialized and managed through the [`CipherState`] trait, ensure ongoing
// communication remains confidential and authenticated.
//
// The [`CipherState`] trait and [`GenericCipher`] enum are essential for managing AEAD ciphers
// within the Noise protocol, ensuring secure data handling, key management, and nonce tracking
// throughout the communication session.

use core::ptr;

use crate::aed_cipher::AeadCipher;
use aes_gcm::Aes256Gcm;
use chacha20poly1305::{aead::Buffer, ChaCha20Poly1305};

// The `CipherState` trait manages AEAD ciphers for secure communication, handling the encryption
// key, nonce, and cipher instance. It supports encryption and decryption with ciphers like
// [`ChaCha20Poly1305`] and [`Aes256Gcm`], ensuring proper key and nonce management.
//
// Key responsibilities:
// - **Key management**: Set and retrieve the 32-byte encryption key.
// - **Nonce management**: Track unique nonces for encryption operations.
// - **Cipher handling**: Initialize and manage AEAD ciphers for secure data encryption.
//
// Used in protocols like Noise, `CipherState` ensures secure communication by managing
// cryptographic material during and after handshakes.
pub trait CipherState<Cipher_: AeadCipher>
where
    Self: Sized,
{
    // Retrieves a mutable reference to the 32-byte encryption key (`k`).
    fn get_k(&mut self) -> &mut Option<[u8; 32]>;

    // Sets the 32-byte encryption key to the optionally provided value (`k`).
    //
    // Allows the encryption key to be explicitly set, typically after it has been derived or
    // initialized during the handshake process. If `None`, the encryption key is unset.
    fn set_k(&mut self, k: Option<[u8; 32]>);

    // Retrieves the current nonce (`n`) used for encryption.
    //
    // The nonce is a counter that is incremented with each encryption/decryption operations to
    // ensure that each encryption operation with the same key produces a unique ciphertext.
    fn get_n(&self) -> u64;

    // Sets the nonce (`n`) to the provided value.
    //
    // Allows the nonce to be explicitly set, typically after it has been initialized, incremented
    // during the encryption process, or reset.
    fn set_n(&mut self, n: u64);

    // Retrieves a mutable reference to the optional cipher instance.
    //
    // Provides access to the underlying AEAD cipher instance used for encryption and decryption
    // operations.
    fn get_cipher(&mut self) -> &mut Option<Cipher_>;

    // Converts the current 64-bit nonce value (`n`) to a 12-byte array.
    //
    // Converts the 64-bit nonce value  to a 12-byte array suitable for use with AEAD ciphers,
    // which typically expect a 96-bit (12-byte) nonce. The result is a correctly formatted nonce
    // for use in encryption and decryption operations.
    fn nonce_to_bytes(&self) -> [u8; 12] {
        let mut res = [0u8; 12];
        let n = self.get_n();
        let bytes = n.to_le_bytes();
        let len = res.len();
        res[4..].copy_from_slice(&bytes[..(len - 4)]);
        res
    }

    #[allow(dead_code)]
    fn into_aesg(mut self) -> Option<Cipher<Aes256Gcm>> {
        #[allow(clippy::clone_on_copy)]
        let k = self.get_k().clone()?;
        let c = Aes256Gcm::from_key(k);
        Some(Cipher::from_cipher(c))
    }

    #[allow(dead_code)]
    fn into_chacha(mut self) -> Option<Cipher<ChaCha20Poly1305>> {
        #[allow(clippy::clone_on_copy)]
        let k = self.get_k().clone()?;
        let c = ChaCha20Poly1305::from_key(k);
        Some(Cipher::from_cipher(c))
    }

    // Encrypts the provided `data` in place using the cipher and AAD (`ad`).
    //
    // Performs authenticated encryption on the provided `data` buffer, modifying it in place to
    // contain the ciphertext. The encryption is performed using the current nonce and the AAD.
    // The nonce is incremented after each successful encryption.
    fn encrypt_with_ad<T: Buffer>(
        &mut self,
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        let n = self.nonce_to_bytes();
        self.set_n(self.get_n() + 1);
        if let Some(c) = self.get_cipher() {
            match c.encrypt(&n, ad, data) {
                Ok(_) => Ok(()),
                Err(e) => {
                    self.set_n(self.get_n() - 1);
                    Err(e)
                }
            }
        } else {
            self.set_n(self.get_n() - 1);
            Ok(())
        }
    }

    // Decrypts the data in place using the cipher and AAD (`ad`).
    //
    // Performs authenticated decryption on the provided `data` buffer, modifying it in place to
    // contain the plaintext. The decryption is performed using the current nonce and the provided
    // AAD. The nonce is incremented after each successful decryption.
    fn decrypt_with_ad<T: Buffer>(
        &mut self,
        ad: &[u8],
        data: &mut T,
    ) -> Result<(), aes_gcm::Error> {
        let n = self.nonce_to_bytes();
        self.set_n(self.get_n() + 1);
        if let Some(c) = self.get_cipher() {
            match c.decrypt(&n, ad, data) {
                Ok(_) => Ok(()),
                Err(e) => {
                    self.set_n(self.get_n() - 1);
                    Err(e)
                }
            }
        } else {
            self.set_n(self.get_n() - 1);
            Ok(())
        }
    }
}

// The `GenericCipher` enum abstracts the use of two AEAD ciphers: [`ChaCha20Poly1305`] and
// [`Aes256Gcm`]. It provides a unified interface for secure encryption and decryption, allowing
// flexibility in choosing the cipher while ensuring consistent cryptographic operations.
//
// Variants:
// - **ChaCha20Poly1305**: Uses the `ChaCha20Poly1305` cipher for encryption.
// - **Aes256Gcm**: Uses the `Aes256Gcm` cipher for encryption.
//
// `GenericCipher` enables easy switching between ciphers while maintaining secure key and nonce
// management.
#[allow(clippy::large_enum_variant)]
#[derive(Clone)]
pub enum GenericCipher {
    ChaCha20Poly1305(Cipher<ChaCha20Poly1305>),
    #[allow(dead_code)]
    Aes256Gcm(Cipher<Aes256Gcm>),
}

impl Drop for GenericCipher {
    // Securely erases the encryption key when the [`GenericCipher`] is dropped.
    //
    // Ensures that the encryption key is securely erased from memory when the [`GenericCipher`]
    // instance is dropped, preventing any potential leakage of sensitive cryptographic material.
    fn drop(&mut self) {
        self.erase_k();
    }
}

impl GenericCipher {
    // Encrypts the data (`msg`) in place using the underlying cipher.
    //
    // Performs authenticated encryption on the provided data buffer, modifying it in place to
    // contain the ciphertext. The encryption is performed using the current nonce and an empty
    // additional associated data (AAD) buffer.
    pub fn encrypt<T: Buffer>(&mut self, msg: &mut T) -> Result<(), aes_gcm::Error> {
        match self {
            GenericCipher::ChaCha20Poly1305(c) => c.encrypt_with_ad(&[], msg),
            GenericCipher::Aes256Gcm(c) => c.encrypt_with_ad(&[], msg),
        }
    }

    // Decrypts the data (`msg`) in place using the underlying cipher.
    //
    // Performs authenticated decryption on the provided data buffer, modifying it in place to
    // contain the plaintext. The decryption is performed using the current nonce and an empty
    // additional associated data (AAD) buffer.
    pub fn decrypt<T: Buffer>(&mut self, msg: &mut T) -> Result<(), aes_gcm::Error> {
        match self {
            GenericCipher::ChaCha20Poly1305(c) => c.decrypt_with_ad(&[], msg),
            GenericCipher::Aes256Gcm(c) => c.decrypt_with_ad(&[], msg),
        }
    }

    // Securely erases the encryption key (`k`) from memory.
    //
    // Overwrites the encryption key stored within the [`GenericCipher`] with zeros and sets it to
    // `None`, ensuring that the key cannot be recovered after the [`GenericCipher`] is dropped or
    // no longer needed.
    pub fn erase_k(&mut self) {
        match self {
            GenericCipher::ChaCha20Poly1305(c) => {
                if let Some(k) = c.k.as_mut() {
                    for b in k {
                        unsafe { ptr::write_volatile(b, 0) };
                    }
                    c.k = None;
                }
            }
            GenericCipher::Aes256Gcm(c) => {
                if let Some(k) = c.k.as_mut() {
                    for b in k {
                        unsafe { ptr::write_volatile(b, 0) };
                    }
                    c.k = None;
                }
            }
        }
    }

    #[allow(dead_code)]
    pub fn into_aesg(mut self) -> GenericCipher {
        match &mut self {
            GenericCipher::ChaCha20Poly1305(c) => {
                let c = Cipher::from_cipher(Aes256Gcm::from_key(c.get_k().unwrap()));
                self.erase_k();
                GenericCipher::Aes256Gcm(c)
            }
            GenericCipher::Aes256Gcm(_) => {
                self.erase_k();
                self
            }
        }
    }
}

impl CipherState<Aes256Gcm> for GenericCipher {
    fn get_k(&mut self) -> &mut Option<[u8; 32]> {
        match self {
            GenericCipher::Aes256Gcm(c) => c.get_k(),
            _ => unreachable!(),
        }
    }

    fn set_k(&mut self, k: Option<[u8; 32]>) {
        match self {
            GenericCipher::Aes256Gcm(c) => c.set_k(k),
            _ => unreachable!(),
        }
    }

    fn get_n(&self) -> u64 {
        match self {
            GenericCipher::Aes256Gcm(c) => c.get_n(),
            _ => unreachable!(),
        }
    }

    fn set_n(&mut self, n: u64) {
        match self {
            GenericCipher::Aes256Gcm(c) => c.set_n(n),
            _ => unreachable!(),
        }
    }

    fn get_cipher(&mut self) -> &mut Option<Aes256Gcm> {
        match self {
            GenericCipher::Aes256Gcm(c) => c.get_cipher(),
            _ => unreachable!(),
        }
    }
}

// Represents the state of an AEAD cipher, including the optional 32-byte encryption key (`k`),
// nonce (`n`), and optional cipher instance (`cipher`).
//
// Manages the cryptographic state required to perform AEAD encryption and decryption operations.
// It stores the optional encryption key, the nonce, and the optional cipher instance itself. The
// [`CipherState`] trait is implemented to provide a consistent interface for managing cipher
// state across different AEAD ciphers.
#[derive(Clone)]
pub struct Cipher<C: AeadCipher> {
    // Optional 32-byte encryption key.
    k: Option<[u8; 32]>,
    // Nonce value.
    n: u64,
    // Optional cipher instance.
    cipher: Option<C>,
}

// Ensures that the `Cipher` type is not `Sync`, which prevents multiple threads from
// simultaneously accessing the same instance of `Cipher`. This eliminates the need to handle
// potential issues related to visibility of changes across threads.
//
// After sending the `k` value, we immediately clear it to prevent the original thread from
// accessing the value again, thereby enhancing security by ensuring the sensitive data is no
// longer available in memory.
//
// The `Cipher` struct is neither `Sync` nor `Copy` due to its `cipher` field, which implements
// the `AeadCipher` trait. This trait requires mutable access, making the entire struct non-`Sync`
// and non-`Copy`, even though the key and nonce are simple types.
impl<C: AeadCipher> Cipher<C> {
    // Internal use only, we need k for handshake
    pub fn from_key_and_cipher(k: [u8; 32], c: C) -> Self {
        Self {
            k: Some(k),
            n: 0,
            cipher: Some(c),
        }
    }

    // At the end of the handshake we return a cipher with hidden key
    #[allow(dead_code)]
    pub fn from_cipher(c: C) -> Self {
        Self {
            k: None,
            n: 0,
            cipher: Some(c),
        }
    }
}

impl<C: AeadCipher> CipherState<C> for Cipher<C> {
    fn get_k(&mut self) -> &mut Option<[u8; 32]> {
        &mut self.k
    }
    fn get_n(&self) -> u64 {
        self.n
    }
    fn set_n(&mut self, n: u64) {
        self.n = n;
    }
    fn get_cipher(&mut self) -> &mut Option<C> {
        &mut self.cipher
    }

    fn set_k(&mut self, k: Option<[u8; 32]>) {
        self.k = k;
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/error.rs">
// # Error Handling
//
// Defines error types and utilities for handling errors in the `noise_sv2` module.

use alloc::vec::Vec;

use aes_gcm::Error as AesGcm;

/// Noise protocol error handling.
#[derive(Debug, PartialEq, Eq)]
pub enum Error {
    /// The handshake has not been completed when a finalization step is executed.
    HandshakeNotFinalized,

    /// Error on an empty cipher list is provided where one is required.
    CipherListMustBeNonEmpty,

    /// Error on unsupported ciphers.
    UnsupportedCiphers(Vec<u8>),

    /// Provided cipher list is invalid or malformed.
    InvalidCipherList(Vec<u8>),

    /// Chosen cipher is invalid or unsupported.
    InvalidCipherChosed(Vec<u8>),

    /// Wraps AES-GCM errors during encryption/decryption.
    AesGcm(AesGcm),

    /// Cipher is in an invalid state during encryption/decryption operations.
    InvalidCipherState,

    /// Provided certificate is invalid or cannot be verified.
    InvalidCertificate([u8; 74]),

    /// A raw public key is invalid or cannot be parsed.
    InvalidRawPublicKey,

    /// A raw private key is invalid or cannot be parsed.
    InvalidRawPrivateKey,

    /// An incoming handshake message is expected but not received.
    ExpectedIncomingHandshakeMessage,

    /// A message has an incorrect or unexpected length.
    InvalidMessageLength,
}

impl From<AesGcm> for Error {
    fn from(value: AesGcm) -> Self {
        Self::AesGcm(value)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/handshake.rs">
// # Noise Handshake Operations
//
// The [`HandshakeOp`] trait defines the cryptographic operations and utilities required to perform
// the Noise protocol handshake between Sv2 roles.
//
// This trait abstracts key management, encryption, and hashing for the Noise protocol handshake,
// outlining core operations implemented by the [`crate::Initiator`] and [`crate::Responder`]
// roles. The trait governs the following processes:
//
// - Elliptic curve Diffie-Hellman (ECDH) key exchange using the [`secp256k1`] curve to establish a
//   shared secret.
// - HMAC and HKDF for deriving encryption keys from the shared secret.
// - AEAD encryption and decryption using either [`ChaCha20Poly1305`] or `AES-GCM` ciphers to ensure
//   message confidentiality and integrity.
// - Chaining key and handshake hash updates to maintain the security of the session.
//
// The handshake begins with the exchange of ephemeral key pairs, followed by the derivation of
// shared secrets, which are then used to securely encrypt all subsequent communication.
//
// ## Usage
// The handshake secures communication between two Sv2 roles, with one acting as the
// [`crate::Initiator`] (e.g., a local mining proxy) and the other as the [`crate::Responder`]
// (e.g., a remote pool). Both roles implement the [`HandshakeOp`] trait to manage cryptographic
// state, updating the handshake hash (`h`), chaining key (`ck`), and encryption key (`k`) to
// ensure confidentiality and integrity throughout the handshake.
//
// Securing communication via the Noise protocol guarantees the confidentiality and authenticity of
// sensitive data, such as share submissions. While the use of a secure channel is optional for Sv2
// roles within a local network (e.g., between a local mining device and mining proxy), it is
// mandatory for communication across external networks (e.g., between a local mining proxy and a
// remote pool).

use alloc::{string::String, vec::Vec};

use crate::{aed_cipher::AeadCipher, cipher_state::CipherState, NOISE_HASHED_PROTOCOL_NAME_CHACHA};
use chacha20poly1305::ChaCha20Poly1305;
use secp256k1::{
    ecdh::SharedSecret,
    hashes::{sha256::Hash as Sha256Hash, Hash},
    rand, Keypair, Secp256k1, SecretKey, XOnlyPublicKey,
};

// Represents the operations needed during a Noise protocol handshake.
//
// The [`HandshakeOp`] trait defines the necessary functions for managing the state and
// cryptographic operations required during the Noise protocol handshake. It provides methods for
// key generation, hash mixing, encryption, decryption, and key derivation, ensuring that the
// handshake process is secure and consistent.
pub trait HandshakeOp<Cipher: AeadCipher>: CipherState<Cipher> {
    // Returns the name of the entity implementing the handshake operation.
    //
    // Provides a string that identifies the entity (e.g., "Initiator" or "Responder") that is
    // performing the handshake. It is primarily used for debugging or logging purposes.
    #[allow(dead_code)]
    fn name(&self) -> String;

    // Retrieves a mutable reference to the handshake hash (`h`).
    //
    // The handshake hash accumulates the state of the handshake, incorporating all exchanged
    // messages to ensure integrity and prevent tampering. This method provides access to the
    // current state of the handshake hash, allowing it to be updated as the handshake progresses.
    fn get_h(&mut self) -> &mut [u8; 32];

    // Retrieves a mutable reference to the chaining key (`ck`).
    //
    // The chaining key is used during the key derivation process to generate new keys throughout
    // the handshake. This method provides access to the current chaining key, which is updated
    // as the handshake progresses and new keys are derived.
    fn get_ck(&mut self) -> &mut [u8; 32];

    // Sets the handshake hash (`h`) to the provided value.
    //
    // This method allows the handshake hash to be explicitly set, typically after it has been
    // initialized or updated during the handshake process. The handshake hash ensures the
    // integrity of the handshake by incorporating all exchanged messages.
    fn set_h(&mut self, data: [u8; 32]);

    // Sets the chaining key (`ck`) to the provided value.
    //
    // This method allows the chaining key to be explicitly set, typically after it has been
    // initialized or updated during the handshake process. The chaining key is crucial for
    // deriving new keys as the handshake progresses.
    fn set_ck(&mut self, data: [u8; 32]);

    // Mixes the data into the handshake hash (`h`).
    //
    // Updates the current handshake hash by combining it with the provided `data`. The result is
    // a new SHA-256 hash digest that reflects all previous handshake messages, ensuring the
    // integrity of the handshake process. This method is typically called whenever a new piece of
    // data (e.g., a public key or ciphertext) needs to be incorporated into the handshake state.
    fn mix_hash(&mut self, data: &[u8]) {
        let h = self.get_h();
        let mut to_hash = Vec::with_capacity(32 + data.len());
        to_hash.extend_from_slice(h);
        to_hash.extend_from_slice(data);
        *h = Sha256Hash::hash(&to_hash).to_byte_array();
    }

    // Generates a new cryptographic key pair using the [`Secp256k1`] curve.
    //
    // Generates a fresh key pair, consisting of a secret key and a corresponding public key,
    // using the [`Secp256k1`] elliptic curve. If the generated public key does not match the
    // expected parity, a new key pair is generated to ensure consistency.
    #[allow(dead_code)]
    #[cfg(feature = "std")]
    fn generate_key() -> Keypair {
        Self::generate_key_with_rng(&mut rand::thread_rng())
    }
    #[inline]
    fn generate_key_with_rng<R: rand::Rng + ?Sized>(rng: &mut R) -> Keypair {
        let secp = Secp256k1::new();
        let (secret_key, _) = secp.generate_keypair(rng);
        let kp = Keypair::from_secret_key(&secp, &secret_key);
        if kp.x_only_public_key().1 == crate::PARITY {
            kp
        } else {
            Self::generate_key_with_rng(rng)
        }
    }

    // Computes an HMAC-SHA256 (Hash-based Message Authentication Code) hash of the provided data
    // using the given key.
    //
    // This method implements the HMAC-SHA256 hashing algorithm, which combines a key and data to
    // produce a 32-byte hash. It is used during the handshake to securely derive new keys from
    // existing material, ensuring that the resulting keys are cryptographically strong.
    //
    // This method uses a two-step process:
    // 1. The key is XORed with an inner padding (`ipad`) and hashed with the data.
    // 2. The result is XORed with the outer padding (`opad`) and hashed again to produce the final
    //    HMAC.
    fn hmac_hash(key: &[u8; 32], data: &[u8]) -> [u8; 32] {
        #[allow(clippy::identity_op)]
        let mut ipad = [(0 ^ 0x36); 64];
        #[allow(clippy::identity_op)]
        let mut opad = [(0 ^ 0x5c); 64];
        for i in 0..32 {
            ipad[i] = key[i] ^ 0x36;
        }
        for i in 0..32 {
            opad[i] = key[i] ^ 0x5c;
        }

        let mut to_hash = Vec::with_capacity(64 + data.len());
        to_hash.extend_from_slice(&ipad);
        to_hash.extend_from_slice(data);
        let temp = Sha256Hash::hash(&to_hash).to_byte_array();

        to_hash.clear();
        to_hash.extend_from_slice(&opad);
        to_hash.extend_from_slice(&temp);

        Sha256Hash::hash(&to_hash).to_byte_array()
    }

    // Derives two new keys using the HKDF (HMAC-based Key Derivation Function) process.
    //
    // Performs the HKDF key derivation process, which uses an initial chaining key and input key
    // material to produce two new 32-byte keys. This process is used throughout the handshake to
    // generate fresh keys for encryption and authentication, ensuring that each step of the
    // handshake is securely linked.
    //
    // This method performs the following steps:
    // 1. Performs a HMAC hash on the chaining key and input key material to derive a temporary key.
    // 2. Performs a HMAC hash on the temporary key and specific byte sequence (`0x01`) to derive
    //    the first output.
    // 3. Performs a HMAC hash on the temporary key and the concatenation of the first output and a
    //    specific byte sequence (`0x02`).
    // 4. Returns both outputs.
    fn hkdf_2(chaining_key: &[u8; 32], input_key_material: &[u8]) -> ([u8; 32], [u8; 32]) {
        let temp_key = Self::hmac_hash(chaining_key, input_key_material);
        let out_1 = Self::hmac_hash(&temp_key, &[0x1]);
        let out_2 = Self::hmac_hash(&temp_key, &[&out_1[..], &[0x2][..]].concat());
        (out_1, out_2)
    }

    #[allow(dead_code)]
    fn hkdf_3(
        chaining_key: &[u8; 32],
        input_key_material: &[u8],
    ) -> ([u8; 32], [u8; 32], [u8; 32]) {
        let temp_key = Self::hmac_hash(chaining_key, input_key_material);
        let out_1 = Self::hmac_hash(&temp_key, &[0x1]);
        let out_2 = Self::hmac_hash(&temp_key, &[&out_1[..], &[0x2][..]].concat());
        let out_3 = Self::hmac_hash(&temp_key, &[&out_2[..], &[0x3][..]].concat());
        (out_1, out_2, out_3)
    }

    // Mixes the input key material into the current chaining key (`ck`) and initializes the
    // handshake cipher with an updated encryption key (`k`).
    //
    // Updates the chaining key by incorporating the provided input key material (e.g., the result
    // of a Diffie-Hellman exchange) and uses the updated chaining key to derive a new encryption
    // key. The encryption key is then used to initialize the handshake cipher, preparing it for
    // use in the next step of the handshake.
    fn mix_key(&mut self, input_key_material: &[u8]) {
        let ck = self.get_ck();
        let (ck, temp_k) = Self::hkdf_2(ck, input_key_material);
        self.set_ck(ck);
        self.initialize_key(temp_k);
    }

    // Encrypts the provided plaintext and updates the hash `h` value.
    //
    // The `encrypt_and_hash` method encrypts the given plaintext using the
    // current encryption key and then updates `h` with the resulting ciphertext.
    // If an encryption key is present `k`, the method encrypts the data using
    // using AEAD, where the associated data is the current hash value. After
    // encryption, the ciphertext is mixed into the hash to ensure integrity
    // and authenticity of the messages exchanged during the handshake.
    fn encrypt_and_hash(&mut self, plaintext: &mut Vec<u8>) -> Result<(), aes_gcm::Error> {
        if self.get_k().is_some() {
            #[allow(clippy::clone_on_copy)]
            let h = self.get_h().clone();
            self.encrypt_with_ad(&h, plaintext)?;
        };
        let ciphertext = plaintext;
        self.mix_hash(ciphertext);
        Ok(())
    }

    // Decrypts the provided ciphertext and updates the handshake hash (`h`).
    //
    // Decrypts the given ciphertext using the handshake cipher and then mixes the ciphertext
    // (before decryption) into the handshake hash. If the encryption key (`k`) is present, the
    // data is decrypted using AEAD, where the associated data is the current handshake hash. This
    // ensures that each decryption step is securely linked to the previous handshake state,
    // maintaining the integrity of the
    // handshake.
    fn decrypt_and_hash(&mut self, ciphertext: &mut Vec<u8>) -> Result<(), aes_gcm::Error> {
        let encrypted = ciphertext.clone();
        if self.get_k().is_some() {
            #[allow(clippy::clone_on_copy)]
            let h = self.get_h().clone();
            self.decrypt_with_ad(&h, ciphertext)?;
        };
        self.mix_hash(&encrypted);
        Ok(())
    }

    #[allow(dead_code)]
    fn ecdh(private: &[u8], public: &[u8]) -> [u8; 32] {
        let private = SecretKey::from_slice(private).expect("Wrong key");
        let x_public = XOnlyPublicKey::from_slice(public).expect("Wrong key");
        let res = SharedSecret::new(&x_public.public_key(crate::PARITY), &private);
        res.secret_bytes()
    }

    // Initializes the handshake state by setting the initial chaining key (`ck`) and handshake
    // hash (`h`).
    //
    // Prepares the handshake state for use by setting the initial chaining key and handshake
    // hash. The chaining key is typically derived from a protocol name or other agreed-upon
    // value, and the handshake hash is initialized to reflect this starting state.
    fn initialize_self(&mut self) {
        let ck = NOISE_HASHED_PROTOCOL_NAME_CHACHA;
        let h = Sha256Hash::hash(&ck[..]);
        self.set_h(h.to_byte_array());
        self.set_ck(ck);
        self.set_k(None);
    }

    // Initializes the handshake cipher with the provided encryption key (`k`).
    //
    // Resets the nonce (`n`) to 0 and initializes the handshake cipher using the given 32-byte
    // encryption key. It also updates the internal key storage (`k`) with the new key, preparing
    // the cipher for encrypting or decrypting subsequent messages in the handshake.
    fn initialize_key(&mut self, key: [u8; 32]) {
        self.set_n(0);
        let cipher = ChaCha20Poly1305::from_key(key);
        self.set_handshake_cipher(cipher);
        if let Some(k) = self.get_k() {
            *k = key;
        } else {
            let set_k = self.get_k();
            *set_k = Some(key);
        }
    }

    fn set_handshake_cipher(&mut self, cipher: ChaCha20Poly1305);
}

#[cfg(test)]
mod test {
    use super::*;
    use alloc::string::ToString;
    use core::convert::TryInto;
    use quickcheck::{Arbitrary, TestResult};

    use secp256k1::SecretKey;

    struct TestHandShake {
        k: Option<[u8; 32]>,
        n: u64,
        cipher: Option<ChaCha20Poly1305>,
        h: [u8; 32],
        ck: [u8; 32],
    }

    impl TestHandShake {
        pub fn new() -> Self {
            let mut self_ = TestHandShake {
                k: None,
                n: 0,
                cipher: None,
                h: [0; 32],
                ck: [0; 32],
            };
            self_.initialize_self();
            self_
        }
    }

    impl CipherState<ChaCha20Poly1305> for TestHandShake {
        fn get_k(&mut self) -> &mut Option<[u8; 32]> {
            &mut self.k
        }

        fn set_k(&mut self, k: Option<[u8; 32]>) {
            self.k = k
        }

        fn get_n(&self) -> u64 {
            self.n
        }

        fn set_n(&mut self, n: u64) {
            self.n = n
        }

        fn get_cipher(&mut self) -> &mut Option<ChaCha20Poly1305> {
            &mut self.cipher
        }
    }

    impl HandshakeOp<ChaCha20Poly1305> for TestHandShake {
        fn name(&self) -> String {
            "Test".to_string()
        }

        fn get_h(&mut self) -> &mut [u8; 32] {
            &mut self.h
        }

        fn get_ck(&mut self) -> &mut [u8; 32] {
            &mut self.ck
        }

        fn set_h(&mut self, data: [u8; 32]) {
            self.h = data
        }

        fn set_ck(&mut self, data: [u8; 32]) {
            self.ck = data
        }

        fn set_handshake_cipher(&mut self, cipher: ChaCha20Poly1305) {
            self.cipher = Some(cipher)
        }
    }

    #[test]
    fn is_a_cypher() {
        let mut cipher_1 = TestHandShake::new();
        let mut cipher_2 = TestHandShake::new();
        cipher_1.initialize_key([0; 32]);
        cipher_2.initialize_key([0; 32]);

        let ad = [1, 2, 3];
        let data = vec![1, 7, 92, 3, 4, 5];

        let mut encrypted = data.clone();
        cipher_1.encrypt_with_ad(&ad, &mut encrypted).unwrap();

        cipher_2.decrypt_with_ad(&ad, &mut encrypted).unwrap();

        assert!(encrypted == data);
    }

    #[test]
    fn test_hmac_hash_with_0s() {
        let k = [0; 32];
        let data = [0; 90];
        let value = TestHandShake::hmac_hash(&k, &data);

        // xor padded key with repeted 0x36
        let xored = [0x36; 64];
        let mut to_hash = vec![];
        for b in xored {
            to_hash.push(b);
        }
        for b in data {
            to_hash.push(b);
        }
        let temp = Sha256Hash::hash(&to_hash).to_byte_array();
        // xor padded key with repeted 0x5x 01011100
        let xored = [0x5c; 64];
        let mut to_hash = vec![];
        for b in xored {
            to_hash.push(b);
        }
        for b in temp {
            to_hash.push(b);
        }
        let expected = Sha256Hash::hash(&to_hash).to_byte_array();

        assert!(value == expected);
    }

    #[test]
    fn test_hkdf2() {
        let chaining_key = [0; 32];
        let input_key_material = [0; 32];
        let temp_k = TestHandShake::hmac_hash(&chaining_key, &input_key_material);
        let expected_1 = TestHandShake::hmac_hash(&temp_k, &[0x1]);
        let mut temp_2 = expected_1.to_vec();
        temp_2.push(0x2);
        let expected_2 = TestHandShake::hmac_hash(&temp_k, &temp_2);
        let (out_1, out_2) = TestHandShake::hkdf_2(&chaining_key, &input_key_material);
        assert!(out_1 == expected_1);
        assert!(out_2 == expected_2);
    }

    #[test]
    fn test_mix_key() {
        let input_key_material = [0; 32];
        let ck = [0; 32];
        let mut tester = TestHandShake::new();
        tester.set_ck(ck);

        let (mut ck, temp_k) = TestHandShake::hkdf_2(&ck, &input_key_material);

        tester.mix_key(&input_key_material);

        assert!(tester.get_ck() == &mut ck);
        assert!(tester.get_k().unwrap() == temp_k);
    }

    #[test]
    fn test_mix_hash() {
        let data = [0; 32];
        let h = [0; 32];
        let mut tester = TestHandShake::new();
        tester.set_h(h);

        let mut to_hash = h.to_vec();
        to_hash.extend_from_slice(&data);
        let mut expected = Sha256Hash::hash(&to_hash).to_byte_array();

        tester.mix_hash(&data);

        assert!(tester.get_h() == &mut expected);
    }

    #[test]
    fn test_decrypt_encrypt_with_hash() {
        let mut cipher_1 = TestHandShake::new();
        let mut cipher_2 = TestHandShake::new();
        cipher_1.initialize_key([0; 32]);
        cipher_2.initialize_key([0; 32]);

        cipher_1.set_h([0; 32]);
        cipher_2.set_h([0; 32]);

        let data = vec![1, 7, 92, 3, 4, 5];

        let mut encrypted = data.clone();
        cipher_1.encrypt_and_hash(&mut encrypted).unwrap();
        assert!(encrypted != data);

        cipher_2.decrypt_and_hash(&mut encrypted).unwrap();

        assert!(encrypted == data);
        assert!(cipher_1.get_h() == cipher_2.get_h());
    }

    #[test]
    #[cfg(feature = "std")]
    fn test_ecdh() {
        let key_pair_1 = TestHandShake::generate_key();
        let key_pair_2 = TestHandShake::generate_key();

        let secret_1 = key_pair_1.secret_bytes();
        let secret_2 = key_pair_2.secret_bytes();

        let pub_1 = key_pair_1.x_only_public_key();
        let pub_2 = key_pair_2.x_only_public_key();

        let ecdh_1 = TestHandShake::ecdh(&secret_1, &pub_2.0.serialize());
        let ecdh_2 = TestHandShake::ecdh(&secret_2, &pub_1.0.serialize());

        assert!(ecdh_1 == ecdh_2);
    }

    #[test]
    fn test_ecdh_with_rng() {
        let key_pair_1 = TestHandShake::generate_key_with_rng(&mut rand::thread_rng());
        let key_pair_2 = TestHandShake::generate_key_with_rng(&mut rand::thread_rng());

        let secret_1 = key_pair_1.secret_bytes();
        let secret_2 = key_pair_2.secret_bytes();

        let pub_1 = key_pair_1.x_only_public_key();
        let pub_2 = key_pair_2.x_only_public_key();

        let ecdh_1 = TestHandShake::ecdh(&secret_1, &pub_2.0.serialize());
        let ecdh_2 = TestHandShake::ecdh(&secret_2, &pub_1.0.serialize());

        assert!(ecdh_1 == ecdh_2);
    }

    #[derive(Clone, Debug)]
    struct KeypairWrapper(pub Option<Keypair>);

    impl Arbitrary for KeypairWrapper {
        fn arbitrary(g: &mut quickcheck::Gen) -> Self {
            let secp = Secp256k1::new();
            let mut secret = Vec::<u8>::arbitrary(g);
            if secret.len() < 32 {
                while secret.len() < 32 {
                    secret.push(0)
                }
            }
            if secret.len() > 32 {
                secret.truncate(32);
            }
            assert!(secret.len() == 32);
            let secret: [u8; 32] = secret.try_into().unwrap();
            match SecretKey::from_slice(&secret) {
                Ok(secret) => KeypairWrapper(Some(Keypair::from_secret_key(&secp, &secret))),
                Err(_) => KeypairWrapper(None),
            }
        }
    }

    #[quickcheck_macros::quickcheck]
    fn test_ecdh_1(kp1: KeypairWrapper, kp2: KeypairWrapper) -> TestResult {
        let (kp1, kp2) = match (kp1.0, kp2.0) {
            (Some(kp1), Some(kp2)) => (kp1, kp2),
            _ => return TestResult::discard(),
        };
        if kp1.x_only_public_key().1 == crate::PARITY && kp2.x_only_public_key().1 == crate::PARITY
        {
            let secret_1 = kp1.secret_bytes();
            let secret_2 = kp2.secret_bytes();

            let pub_1 = kp1.x_only_public_key();
            let pub_2 = kp2.x_only_public_key();

            let ecdh_1 = TestHandShake::ecdh(&secret_1, &pub_2.0.serialize());
            let ecdh_2 = TestHandShake::ecdh(&secret_2, &pub_1.0.serialize());

            if ecdh_1 == ecdh_2 {
                TestResult::passed()
            } else {
                TestResult::failed()
            }
        } else {
            TestResult::discard()
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/initiator.rs">
// # Initiator Role
//
// Manages the [`Initiator`] role in the Noise protocol handshake for communication between Sv2
// roles. The initiator is responsible for starting the handshake process by sending the first
// cryptographic message to the [`crate::Responder`] role (e.g., a mining pool).
//
// The [`Initiator`] role is equipped with utilities for generating and managing the initiator's
// key pairs, performing elliptic curve Diffie-Hellman (ECDH) key exchanges, and encrypting
// messages during the handshake phase. The initiator's responsibilities include:
//
// - Generating an ephemeral key pair for the handshake.
// - Using the [`secp256k1`] elliptic curve for ECDH to derive a shared secret.
// - Encrypting the initial handshake message to securely exchange cryptographic material.
// - Managing the state transitions between handshake steps, including updating the handshake hash,
//   chaining key, and encryption key as the session progresses.
//
// ## Usage
// The initiator role is typically used by a downstream Sv2 role (e.g., a local mining proxy) to
// establish a secure connection with an upstream responder (e.g., a remote mining pool). The
// initiator begins the handshake by generating a public key and sending it to the responder. After
// receiving a response, the initiator computes a shared secret and encrypts further messages using
// this key.
//
// The [`Initiator`] struct implements the [`HandshakeOp`] trait, which defines the core
// cryptographic operations during the handshake. It ensures secure communication by supporting
// both the [`ChaCha20Poly1305`] or `AES-GCM` cipher, providing both confidentiality and message
// authentication for all subsequent communication.
//
// ### Secure Data Erasure
//
// The [`Initiator`] includes functionality for securely erasing sensitive cryptographic material,
// ensuring that private keys and other sensitive data are wiped from memory when no longer needed.
//
// The [`Drop`] trait is implemented to automatically trigger secure erasure when the [`Initiator`]
// instance goes out of scope, preventing potential misuse or leakage of cryptographic material.

use alloc::{
    boxed::Box,
    string::{String, ToString},
};
use core::{convert::TryInto, ptr};

use crate::{
    cipher_state::{Cipher, CipherState, GenericCipher},
    error::Error,
    handshake::HandshakeOp,
    signature_message::SignatureNoiseMessage,
    NoiseCodec, ELLSWIFT_ENCODING_SIZE, ENCRYPTED_ELLSWIFT_ENCODING_SIZE,
    ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE, INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE,
    SIGNATURE_NOISE_MESSAGE_SIZE,
};
use aes_gcm::KeyInit;
use chacha20poly1305::ChaCha20Poly1305;
use secp256k1::{
    ellswift::{ElligatorSwift, ElligatorSwiftParty},
    Keypair, PublicKey, XOnlyPublicKey,
};

/// Manages the initiator's role in the Noise NX handshake, handling key exchange, encryption, and
/// handshake state. It securely generates and manages cryptographic keys, performs Diffie-Hellman
/// exchanges, and maintains the handshake hash, chaining key, and nonce for message encryption.
/// After the handshake, it facilitates secure communication using either [`ChaCha20Poly1305`] or
/// `AES-GCM` ciphers. Sensitive data is securely erased when no longer needed.
#[derive(Clone)]
pub struct Initiator {
    // Cipher used for encrypting and decrypting messages during the handshake.
    //
    // It is initialized once enough information is available from the handshake process.
    handshake_cipher: Option<ChaCha20Poly1305>,
    // Optional static key used in the handshake. This key may be derived from the pre-shared key
    // (PSK) or generated during the handshake.
    k: Option<[u8; 32]>,
    // Current nonce used in the encryption process.
    //
    // Ensures that the same plaintext encrypted twice will produce different ciphertexts.
    n: u64,
    // Chaining key used in the key derivation process to generate new keys throughout the
    // handshake.
    ck: [u8; 32],
    // Handshake hash which accumulates all handshake messages to ensure integrity and prevent
    // tampering.
    h: [u8; 32],
    // Ephemeral key pair generated by the initiator for this session, used for generating the
    // shared secret with the responder.
    e: Keypair,
    // Optional public key of the responder, used to authenticate the responder during the
    // handshake.
    #[allow(unused)]
    responder_authority_pk: Option<XOnlyPublicKey>,
    // First [`CipherState`] used for encrypting messages from the initiator to the responder
    // after the handshake is complete.
    c1: Option<GenericCipher>,
    // Second [`CipherState`] used for encrypting messages from the responder to the initiator
    // after the handshake is complete.
    c2: Option<GenericCipher>,
}

impl core::fmt::Debug for Initiator {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        f.debug_struct("Initiator").finish()
    }
}

// Ensures that the `Cipher` type is not `Sync`, which prevents multiple threads from
// simultaneously accessing the same instance of `Cipher`. This eliminates the need to handle
// potential issues related to visibility of changes across threads.
//
// After sending the `k` value, we immediately clear it to prevent the original thread from
// accessing the value again, thereby enhancing security by ensuring the sensitive data is no
// longer available in memory.
//
// The `Cipher` struct is neither `Sync` nor `Copy` due to its `cipher` field, which implements
// the `AeadCipher` trait. This trait requires mutable access, making the entire struct non-`Sync`
// and non-`Copy`, even though the key and nonce are simple types.
impl CipherState<ChaCha20Poly1305> for Initiator {
    fn get_k(&mut self) -> &mut Option<[u8; 32]> {
        &mut self.k
    }

    fn get_n(&self) -> u64 {
        self.n
    }

    fn set_n(&mut self, n: u64) {
        self.n = n;
    }

    fn get_cipher(&mut self) -> &mut Option<ChaCha20Poly1305> {
        &mut self.handshake_cipher
    }

    fn set_k(&mut self, k: Option<[u8; 32]>) {
        self.k = k;
    }
}

impl HandshakeOp<ChaCha20Poly1305> for Initiator {
    fn name(&self) -> String {
        "Initiator".to_string()
    }

    fn get_h(&mut self) -> &mut [u8; 32] {
        &mut self.h
    }

    fn get_ck(&mut self) -> &mut [u8; 32] {
        &mut self.ck
    }

    fn set_h(&mut self, data: [u8; 32]) {
        self.h = data;
    }

    fn set_ck(&mut self, data: [u8; 32]) {
        self.ck = data;
    }

    fn set_handshake_cipher(&mut self, cipher: ChaCha20Poly1305) {
        self.handshake_cipher = Some(cipher);
    }
}

impl Initiator {
    /// Creates a new [`Initiator`] instance with an optional responder public key.
    ///
    /// If the responder public key is provided, the initiator uses this key to authenticate the
    /// responder during the handshake. The initial initiator state is instantiated with the
    /// ephemeral key pair and handshake hash.
    #[cfg(feature = "std")]
    pub fn new(pk: Option<XOnlyPublicKey>) -> Box<Self> {
        Self::new_with_rng(pk, &mut rand::thread_rng())
    }

    /// Creates a new [`Initiator`] instance with an optional responder public key and a custom
    /// random number generator.
    ///
    /// See [`Self::new`] for more details.
    ///
    /// The custom random number generator is used to generate the ephemeral key pair. It should be
    /// provided in order to not implicitely rely on `std` and allow `no_std` environments to
    /// provide a hardware random number generator for example.
    #[inline]
    pub fn new_with_rng<R: rand::Rng + ?Sized>(
        pk: Option<XOnlyPublicKey>,
        rng: &mut R,
    ) -> Box<Self> {
        let mut self_ = Self {
            handshake_cipher: None,
            k: None,
            n: 0,
            ck: [0; 32],
            h: [0; 32],
            e: Self::generate_key_with_rng(rng),
            responder_authority_pk: pk,
            c1: None,
            c2: None,
        };
        self_.initialize_self();
        Box::new(self_)
    }

    /// Creates a new [`Initiator`] instance using a raw 32-byte public key.
    ///
    /// Constructs a [`XOnlyPublicKey`] from the provided raw key slice and initializes a new
    /// [`Initiator`] with the derived public key. If the provided key cannot be converted into a
    /// valid [`XOnlyPublicKey`], an [`Error::InvalidRawPublicKey`] error is returned.
    ///
    /// Typically used when the initiator is aware of the responder's public key in advance.
    #[cfg(feature = "std")]
    pub fn from_raw_k(key: [u8; 32]) -> Result<Box<Self>, Error> {
        Self::from_raw_k_with_rng(key, &mut rand::thread_rng())
    }

    /// Creates a new [`Initiator`] instance using a raw 32-byte public key and a custom random
    /// number generator.
    ///
    /// See [`Self::from_raw_k`] for more details.
    ///
    /// The custom random number generator should be provided in order to not implicitely rely on
    /// `std` and allow `no_std` environments to provide a hardware random number generator for
    /// example.
    #[inline]
    pub fn from_raw_k_with_rng<R: rand::Rng + ?Sized>(
        key: [u8; 32],
        rng: &mut R,
    ) -> Result<Box<Self>, Error> {
        let pk =
            secp256k1::XOnlyPublicKey::from_slice(&key).map_err(|_| Error::InvalidRawPublicKey)?;
        Ok(Self::new_with_rng(Some(pk), rng))
    }

    /// Creates a new [`Initiator`] without requiring the responder's authority public key.
    /// This function initializes the [`Initiator`] with a default empty state and is intended
    /// for use when both the initiator and responder are within the same network. In this case,
    /// the initiator does not validate the responder's static key from a certificate. However,
    /// the connection remains encrypted.
    #[cfg(feature = "std")]
    pub fn without_pk() -> Result<Box<Self>, Error> {
        Self::without_pk_with_rng(&mut rand::thread_rng())
    }

    /// Creates a new [`Initiator`] instance without a responder's public key and using a custom
    /// random number generator.
    ///
    /// See [`Self::without_pk`] for more details.
    ///
    /// The custom random number generator should be provided in order to not implicitely rely on
    /// `std` and allow `no_std` environments to provide a hardware random number generator for
    /// example.
    #[inline]
    pub fn without_pk_with_rng<R: rand::Rng + ?Sized>(rng: &mut R) -> Result<Box<Self>, Error> {
        Ok(Self::new_with_rng(None, rng))
    }

    /// Executes the initial step of the Noise NX protocol handshake.
    ///
    /// This step involves generating an ephemeral keypair and encoding the public key using
    /// Elligator Swift encoding, which obscures the key to prevent identification. The encoded
    /// public key is then mixed into the handshake state, and an empty payload is encrypted.
    /// This operation currently only affects the handshake hash, as the key (`k`) is not yet
    /// established. The function returns the encoded public key, which is ready to be sent to
    /// the responder.
    ///
    /// On success, the function returns a 64-byte array containing the encoded public key.
    /// If an error occurs during encryption, it returns an [`aes_gcm::Error`].
    pub fn step_0(&mut self) -> Result<[u8; ELLSWIFT_ENCODING_SIZE], aes_gcm::Error> {
        let elliswift_enc_pubkey = ElligatorSwift::from_pubkey(self.e.public_key()).to_array();
        self.mix_hash(&elliswift_enc_pubkey);
        self.encrypt_and_hash(&mut vec![])?;

        let mut message = [0u8; ELLSWIFT_ENCODING_SIZE];
        message[..64].copy_from_slice(&elliswift_enc_pubkey[..ELLSWIFT_ENCODING_SIZE]);
        Ok(message)
    }

    /// Processes the second step of the Noise NX protocol handshake for the initiator.
    ///
    /// This method handles the responder's reply in the Noise NX protocol handshake, processing
    /// the message to derive shared secrets and authenticate the responder. It interprets the
    /// first 64 bytes of the message as the responder's ephemeral public key, decodes it, and
    /// mixes it into the handshake state. It then derives a shared secret from the ephemeral keys
    /// and updates the state accordingly.
    ///
    /// The next 80 bytes of the message contain the responder's static public key, encrypted and
    /// authenticated. The method decrypts this segment and derives another shared secret using the
    /// responder's static public key, further securing the handshake state. Finally, the method
    /// decrypts and verifies the signature included in the message to ensure the responder's
    /// authenticity.
    ///
    /// On success, this method returns a [`NoiseCodec`] instance initialized with session ciphers
    /// for secure communication. If the provided `message` has an incorrect length, it returns an
    /// [`Error::InvalidMessageLength`]. If decryption or signature verification fails, it returns
    /// an [`Error::InvalidCertificate`].
    #[cfg(feature = "std")]
    pub fn step_2(
        &mut self,
        message: [u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE],
    ) -> Result<NoiseCodec, Error> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;
        self.step_2_with_now(message, now)
    }

    /// Processes the second step of the Noise NX protocol handshake for the initiator given the
    /// current system time.
    ///
    /// See [`Self::step_2`] for more details.
    ///
    /// The current system time should be provided to avoid relying on `std` and allow `no_std`
    /// environments to use another source of time.
    pub fn step_2_with_now(
        &mut self,
        message: [u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE],
        now: u32,
    ) -> Result<NoiseCodec, Error> {
        // 2. interprets first 64 bytes as ElligatorSwift encoding of x-coordinate of public key
        // from this is derived the 32-bytes remote ephemeral public key `re.public_key`
        let mut elliswift_theirs_ephemeral_serialized: [u8; ELLSWIFT_ENCODING_SIZE] =
            [0; ELLSWIFT_ENCODING_SIZE];
        elliswift_theirs_ephemeral_serialized.clone_from_slice(&message[0..ELLSWIFT_ENCODING_SIZE]);
        self.mix_hash(&elliswift_theirs_ephemeral_serialized);

        // 3. calls `MixHash(re.public_key)`
        // 4. calls `MixKey(ECDH(e.private_key, re.public_key))`
        let e_private_key = self.e.secret_key();
        let elligatorswift_ours_ephemeral = ElligatorSwift::from_pubkey(self.e.public_key());
        let elligatorswift_theirs_ephemeral =
            ElligatorSwift::from_array(elliswift_theirs_ephemeral_serialized);
        let ecdh_ephemeral: [u8; 32] = ElligatorSwift::shared_secret(
            elligatorswift_ours_ephemeral,
            elligatorswift_theirs_ephemeral,
            e_private_key,
            ElligatorSwiftParty::A,
            None,
        )
        .to_secret_bytes();
        self.mix_key(&ecdh_ephemeral);

        // 5. decrypts next 80 bytes with `DecryptAndHash()` and stores the results as
        // `rs.public_key` which is **server's static public key** (note that 64 bytes is the
        // elligatorswift encoded public key and 16 bytes is MAC)
        let mut to_decrypt = message
            [ELLSWIFT_ENCODING_SIZE..ELLSWIFT_ENCODING_SIZE + ENCRYPTED_ELLSWIFT_ENCODING_SIZE]
            .to_vec();
        self.decrypt_and_hash(&mut to_decrypt)?;

        // 6. calls `MixKey(ECDH(e.private_key, rs.public_key)`
        let elligatorswift_theirs_static_serialized: [u8; ELLSWIFT_ENCODING_SIZE] = to_decrypt[..]
            .try_into()
            .expect("slice with incorrect length");
        let elligatorswift_theirs_static =
            ElligatorSwift::from_array(elligatorswift_theirs_static_serialized);
        let ecdh_static: [u8; 32] = ElligatorSwift::shared_secret(
            elligatorswift_ours_ephemeral,
            elligatorswift_theirs_static,
            e_private_key,
            ElligatorSwiftParty::A,
            None,
        )
        .to_secret_bytes();
        self.mix_key(&ecdh_static);

        // Decrypt and verify the SignatureNoiseMessage
        let mut to_decrypt = message[ELLSWIFT_ENCODING_SIZE + ENCRYPTED_ELLSWIFT_ENCODING_SIZE
            ..INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE]
            .to_vec();
        if to_decrypt.len() != ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE {
            return Err(Error::InvalidMessageLength);
        }

        self.decrypt_and_hash(&mut to_decrypt)?;
        let plaintext: [u8; SIGNATURE_NOISE_MESSAGE_SIZE] = to_decrypt.try_into().unwrap();
        let signature_message: SignatureNoiseMessage = plaintext.into();
        let rs_pub_key = PublicKey::from_ellswift(elligatorswift_theirs_static)
            .x_only_public_key()
            .0
            .serialize();
        let rs_pk_xonly = XOnlyPublicKey::from_slice(&rs_pub_key).unwrap();
        if signature_message.verify_with_now(&rs_pk_xonly, &self.responder_authority_pk, now) {
            let (temp_k1, temp_k2) = Self::hkdf_2(self.get_ck(), &[]);
            let c1 = ChaCha20Poly1305::new(&temp_k1.into());
            let c2 = ChaCha20Poly1305::new(&temp_k2.into());
            let c1: Cipher<ChaCha20Poly1305> = Cipher::from_key_and_cipher(temp_k1, c1);
            let c2: Cipher<ChaCha20Poly1305> = Cipher::from_key_and_cipher(temp_k2, c2);
            self.c1 = None;
            self.c2 = None;
            let mut encryptor = GenericCipher::ChaCha20Poly1305(c1);
            let mut decryptor = GenericCipher::ChaCha20Poly1305(c2);
            encryptor.erase_k();
            decryptor.erase_k();
            let codec = crate::NoiseCodec {
                encryptor,
                decryptor,
            };
            Ok(codec)
        } else {
            Err(Error::InvalidCertificate(plaintext))
        }
    }

    // Securely erases sensitive data from the [`Initiator`] memory.
    //
    // Clears all sensitive cryptographic material within the [`Initiator`] to prevent any
    // accidental leakage or misuse. It overwrites the stored keys, chaining key, handshake hash,
    // and session ciphers with zeros. This method is typically
    // called when the [`Initiator`] instance is no longer needed or before deallocation.
    fn erase(&mut self) {
        if let Some(k) = self.k.as_mut() {
            for b in k {
                unsafe { ptr::write_volatile(b, 0) };
            }
        }
        for mut b in self.ck {
            unsafe { ptr::write_volatile(&mut b, 0) };
        }
        for mut b in self.h {
            unsafe { ptr::write_volatile(&mut b, 0) };
        }
        if let Some(c1) = self.c1.as_mut() {
            c1.erase_k()
        }
        if let Some(c2) = self.c2.as_mut() {
            c2.erase_k()
        }
        self.e.non_secure_erase();
    }
}
impl Drop for Initiator {
    fn drop(&mut self) {
        self.erase();
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/lib.rs">
//! # Noise-SV2: Noise Protocol Implementation for Stratum V2
//!
//! `noise_sv2` ensures secure communication between Sv2 roles by handling encryption, decryption,
//! and authentication through Noise protocol handshakes and cipher operations.
//!
//! Implementation of the [Sv2 Noise protocol specification](https://github.com/stratum-mining/sv2-spec/blob/main/04-Protocol-Security.md#4-protocol-security).
//!
//! ## Features
//! - Noise Protocol: Establishes secure communication via the Noise protocol handshake between the
//!   [`Initiator`] and [`Responder`] roles.
//! - Diffie-Hellman with [`secp256k1`]: Securely establishes a shared secret between two Sv2 roles,
//!   using the same elliptic curve used in Bitcoin.
//! - AEAD: Ensures confidentiality and integrity of the data.
//! - `AES-GCM` and `ChaCha20-Poly1305`: Provides encryption, with hardware-optimized and
//!   software-optimized options.
//! - Schnorr Signatures: Authenticates messages and verifies the identity of the Sv2 roles. In
//!   practice, the primitives exposed by this crate should be used to secure communication channels
//!   between Sv2 roles. Securing communication between two Sv2 roles on the same local network
//!   (e.g., local mining devices communicating with a local mining proxy) is optional. However, it
//!   is mandatory to secure the communication between two Sv2 roles communicating over a remote
//!   network (e.g., a local mining proxy communicating with a remote pool sever).
//!
//! The Noise protocol establishes secure communication between two Sv2 roles via a handshake
//! performed at the beginning of the connection. The initiator (e.g., a local mining proxy) and
//! the responder (e.g., a mining pool) establish a shared secret using Elliptic Curve
//! Diffie-Hellman (ECDH) with the [`secp256k1`] elliptic curve (the same elliptic curve used by
//! Bitcoin). Once both Sv2 roles compute the shared secret from the ECDH exchange, the Noise
//! protocol derives symmetric encryption keys for secure communication. These keys are used with
//! AEAD (using either `AES-GCM` or `ChaCha20-Poly1305`) to encrypt and authenticate all
//! communication between the roles. This encryption ensures that sensitive data, such as share
//! submissions, remains confidential and tamper-resistant. Additionally, Schnorr signatures are
//! used to authenticate messages and validate the identities of the Sv2 roles, ensuring that
//! critical messages like job templates and share submissions originate from legitimate sources.

#![cfg_attr(all(not(feature = "std"), not(test)), no_std)]

#[macro_use]
extern crate alloc;

use aes_gcm::aead::Buffer;
pub use aes_gcm::aead::Error as AeadError;
use cipher_state::GenericCipher;
mod aed_cipher;
mod cipher_state;
mod error;
mod handshake;
mod initiator;
mod responder;
mod signature_message;
#[cfg(test)]
mod test;

/// Size of the MAC for supported AEAD encryption algorithm (ChaChaPoly).
pub const AEAD_MAC_LEN: usize = 16;

/// Size of the Noise protocol frame header in bytes.
pub const NOISE_FRAME_HEADER_SIZE: usize = 2;

/// Size in bytes of the SIGNATURE_NOISE_MESSAGE, which contains information and
/// a signature for the handshake initiator, formatted according to the Noise
/// Protocol specifications.
pub const SIGNATURE_NOISE_MESSAGE_SIZE: usize = 74;

/// Size in bytes of the encrypted signature noise message, which includes the
/// SIGNATURE_NOISE_MESSAGE and a MAC for integrity verification.
pub const ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE: usize =
    SIGNATURE_NOISE_MESSAGE_SIZE + AEAD_MAC_LEN;

/// Size in bytes of the encoded elliptic curve point using ElligatorSwift
/// encoding. This encoding produces a 64-byte representation of the
/// X-coordinate of a secp256k1 curve point.
pub const ELLSWIFT_ENCODING_SIZE: usize = 64;

/// Size in bytes of the encrypted ElligatorSwift encoded data, which includes
/// the original ElligatorSwift encoded data and a MAC for integrity
/// verification.
pub const ENCRYPTED_ELLSWIFT_ENCODING_SIZE: usize = ELLSWIFT_ENCODING_SIZE + AEAD_MAC_LEN;

/// Size in bytes of the handshake message expected by the initiator,
/// encompassing:
/// - ElligatorSwift encoded public key
/// - Encrypted ElligatorSwift encoding
/// - Encrypted SIGNATURE_NOISE_MESSAGE
pub const INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE: usize = ELLSWIFT_ENCODING_SIZE
    + ENCRYPTED_ELLSWIFT_ENCODING_SIZE
    + ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE;

/// If protocolName is less than or equal to 32 bytes in length, use
/// protocolName with zero bytes appended to make 32 bytes. Otherwise, apply
/// HASH to it. For name = "Noise_NX_Secp256k1+EllSwift_ChaChaPoly_SHA256", we
/// need the hash. More info can be found [at this link](https://github.com/stratum-mining/sv2-spec/blob/main/04-Protocol-Security.md#451-handshake-act-1-nx-handshake-part-1---e).
pub const NOISE_HASHED_PROTOCOL_NAME_CHACHA: [u8; 32] = [
    46, 180, 120, 129, 32, 142, 158, 238, 31, 102, 159, 103, 198, 110, 231, 14, 169, 234, 136, 9,
    13, 80, 63, 232, 48, 220, 75, 200, 62, 41, 191, 16,
];

// The parity value used in the Schnorr signature process.
//
// Used to define whether a public key corresponds to an even or odd point on the elliptic curve.
// In this case, `Parity::Even` is used.
const PARITY: secp256k1::Parity = secp256k1::Parity::Even;

/// A codec for managing encrypted communication in the Noise protocol.
///
/// Manages the encryption and decryption of messages between two parties, the [`Initiator`] and
/// [`Responder`], using the Noise protocol. A symmetric cipher is used for both encrypting
/// outgoing messages and decrypting incoming messages.
#[derive(Clone)]
pub struct NoiseCodec {
    // Cipher to encrypt outgoing messages.
    encryptor: GenericCipher,

    // Cipher to decrypt incoming messages.
    decryptor: GenericCipher,
}

impl core::fmt::Debug for NoiseCodec {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        f.debug_struct("NoiseCodec").finish()
    }
}

impl NoiseCodec {
    /// Encrypts a message (`msg`) in place using the stored cipher.
    pub fn encrypt<T: Buffer>(&mut self, msg: &mut T) -> Result<(), aes_gcm::Error> {
        self.encryptor.encrypt(msg)
    }

    /// Decrypts a message (`msg`) in place using the stored cipher.
    pub fn decrypt<T: Buffer>(&mut self, msg: &mut T) -> Result<(), aes_gcm::Error> {
        self.decryptor.decrypt(msg)
    }
}

pub use error::Error;
pub use initiator::Initiator;
pub use responder::Responder;
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/responder.rs">
// # Responder Role
//
// Manages the [`Responder`] role in the Noise protocol handshake for secure communication between
// Sv2 roles. The responder is responsible for handling incoming handshake messages from the
// [`crate::Initiator`] (e.g., a mining proxy) and respond with the appropriate cryptographic
// data.
//
// The [`Responder`] role is equipped with utilities for handling elliptic curve Diffie-Hellman
// (ECDH) key exchanges, decrypting messages, and securely managing cryptographic state during the
// handshake phase. The responder's responsibilities include:
//
// - Generating an ephemeral key pair for the handshake.
// - Using the [`secp256k1`] elliptic curve for ECDH to compute a shared secret based on the
//   initiator's public key.
// - Decrypting and processing incoming handshake messages from the initiator.
// - Managing state transitions, including updates to the handshake hash, chaining key, and
//   encryption key as the session progresses.
//
// ## Usage
// The responder role is typically used by an upstream Sv2 role (e.g., a remote mining pool) to
// respond to an incoming handshake initiated by a downstream role (e.g., a local mining proxy).
// After receiving the initiator's public key, the responder computes a shared secret, which is
// used to securely encrypt further communication.
//
// The [`Responder`] struct implements the [`HandshakeOp`] trait, which defines the core
// cryptographic operations during the handshake. It ensures secure communication by supporting
// both the [`ChaCha20Poly1305`] or `AES-GCM` cipher, providing both confidentiality and message
// authentication for all subsequent communication.
//
// ### Secure Data Erasure
//
// The [`Responder`] includes functionality for securely erasing sensitive cryptographic material,
// ensuring that private keys and other sensitive data are wiped from memory when no longer needed.
// The [`Drop`] trait is implemented to automatically trigger secure erasure when the [`Responder`]
// instance goes out of scope, preventing potential misuse or leakage of cryptographic material.

use core::{ptr, time::Duration};

use crate::{
    cipher_state::{Cipher, CipherState, GenericCipher},
    error::Error,
    handshake::HandshakeOp,
    signature_message::SignatureNoiseMessage,
    NoiseCodec, ELLSWIFT_ENCODING_SIZE, ENCRYPTED_ELLSWIFT_ENCODING_SIZE,
    ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE, INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE,
};
use aes_gcm::KeyInit;
use alloc::{
    boxed::Box,
    string::{String, ToString},
    vec::Vec,
};
use chacha20poly1305::ChaCha20Poly1305;
use secp256k1::{ellswift::ElligatorSwift, Keypair, Secp256k1, SecretKey};

const VERSION: u16 = 0;

/// Represents the state and operations of the responder in the Noise NX protocol handshake.
/// It handles cryptographic key exchanges, manages handshake state, and securely establishes
/// a connection with the initiator. The responder manages key generation, Diffie-Hellman exchanges,
/// message decryption, and state transitions, ensuring secure communication. Sensitive
/// cryptographic material is securely erased when no longer needed.
#[derive(Clone)]
pub struct Responder {
    // Cipher used for encrypting and decrypting messages during the handshake.
    //
    // It is initialized once enough information is available from the handshake process.
    handshake_cipher: Option<ChaCha20Poly1305>,
    // Optional static key used in the handshake. This key may be derived from the pre-shared key
    // (PSK) or generated during the handshake.
    k: Option<[u8; 32]>,
    // Current nonce used in the encryption process.
    //
    // Ensures that the same plaintext encrypted twice will produce different ciphertexts.
    n: u64,
    // Chaining key used in the key derivation process to generate new keys throughout the
    // handshake.
    ck: [u8; 32],
    // Handshake hash which accumulates all handshake messages to ensure integrity and prevent
    // tampering.
    h: [u8; 32],
    // Ephemeral key pair generated by the responder for this session, used for generating the
    // shared secret with the initiator.
    e: Keypair,
    // Static key pair of the responder, used to establish long-term identity and authenticity.
    //
    // Remains consistent across handshakes.
    s: Keypair,
    // Authority key pair, representing the responder's authority credentials.
    //
    // Used to sign messages and verify the identity of the responder.
    a: Keypair,
    // First [`CipherState`] used for encrypting messages from the initiator to the responder
    // after the handshake is complete.
    c1: Option<GenericCipher>,
    // Second [`CipherState`] used for encrypting messages from the responder to the initiator
    // after the handshake is complete.
    c2: Option<GenericCipher>,
    // Validity duration of the responder's certificate, in seconds.
    cert_validity: u32,
}

impl core::fmt::Debug for Responder {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        f.debug_struct("Responder").finish()
    }
}

// Ensures that the `Cipher` type is not `Sync`, which prevents multiple threads from
// simultaneously accessing the same instance of `Cipher`. This eliminates the need to handle
// potential issues related to visibility of changes across threads.
//
// After sending the `k` value, we immediately clear it to prevent the original thread from
// accessing the value again, thereby enhancing security by ensuring the sensitive data is no
// longer available in memory.
//
// The `Cipher` struct is neither `Sync` nor `Copy` due to its `cipher` field, which implements
// the `AeadCipher` trait. This trait requires mutable access, making the entire struct non-`Sync`
// and non-`Copy`, even though the key and nonce are simple types.

impl CipherState<ChaCha20Poly1305> for Responder {
    fn get_k(&mut self) -> &mut Option<[u8; 32]> {
        &mut self.k
    }

    fn get_n(&self) -> u64 {
        self.n
    }

    fn set_n(&mut self, n: u64) {
        self.n = n;
    }

    fn set_k(&mut self, k: Option<[u8; 32]>) {
        self.k = k;
    }

    fn get_cipher(&mut self) -> &mut Option<ChaCha20Poly1305> {
        &mut self.handshake_cipher
    }
}

impl HandshakeOp<ChaCha20Poly1305> for Responder {
    fn name(&self) -> String {
        "Responder".to_string()
    }

    fn get_h(&mut self) -> &mut [u8; 32] {
        &mut self.h
    }

    fn get_ck(&mut self) -> &mut [u8; 32] {
        &mut self.ck
    }

    fn set_h(&mut self, data: [u8; 32]) {
        self.h = data;
    }

    fn set_ck(&mut self, data: [u8; 32]) {
        self.ck = data;
    }

    fn set_handshake_cipher(&mut self, cipher: ChaCha20Poly1305) {
        self.handshake_cipher = Some(cipher);
    }
}

impl Responder {
    /// Creates a new [`Responder`] instance with the provided authority keypair and certificate
    /// validity.
    ///
    /// Constructs a new [`Responder`] with the necessary cryptographic state for the Noise NX
    /// protocol handshake. It generates ephemeral and static key pairs for the responder and
    /// prepares the handshake state. The authority keypair and certificate validity period are
    /// also configured.
    #[cfg(feature = "std")]
    pub fn new(a: Keypair, cert_validity: u32) -> Box<Self> {
        Self::new_with_rng(a, cert_validity, &mut rand::thread_rng())
    }

    /// Creates a new [`Responder`] instance with the provided authority keypair, certificate
    /// validity, and a custom random number generator.
    ///
    /// See [`Self::new`] for more details.
    ///
    /// The custom random number generator should be provided in order to not implicitely rely on
    /// `std` and allow `no_std` environments to provide a hardware random number generator for
    /// example.
    #[inline]
    pub fn new_with_rng<R: rand::Rng + ?Sized>(
        a: Keypair,
        cert_validity: u32,
        rng: &mut R,
    ) -> Box<Self> {
        let mut self_ = Self {
            handshake_cipher: None,
            k: None,
            n: 0,
            ck: [0; 32],
            h: [0; 32],
            e: Self::generate_key_with_rng(rng),
            s: Self::generate_key_with_rng(rng),
            a,
            c1: None,
            c2: None,
            cert_validity,
        };
        Self::initialize_self(&mut self_);
        Box::new(self_)
    }

    /// Creates a new [`Responder`] instance with the provided 32-byte authority key pair.
    ///
    /// Constructs a new [`Responder`] with a given public and private key pair, which represents
    /// the responder's authority credentials. It verifies that the provided public key matches the
    /// corresponding private key, ensuring the authenticity of the authority key pair. The
    /// certificate validity duration is also set here. Fails if the key pair is mismatched.
    #[cfg(feature = "std")]
    pub fn from_authority_kp(
        public: &[u8; 32],
        private: &[u8; 32],
        cert_validity: Duration,
    ) -> Result<Box<Self>, Error> {
        Self::from_authority_kp_with_rng(public, private, cert_validity, &mut rand::thread_rng())
    }

    /// Creates a new [`Responder`] instance with the provided 32-byte authority key pair and a
    /// custom random number generator.
    ///
    /// See [`Self::from_authority_kp`] for more details.
    ///
    /// The custom random number generator should be provided in order to not implicitely rely on
    /// `std` and allow `no_std` environments to provide a hardware random number generator for
    /// example.
    #[inline]
    pub fn from_authority_kp_with_rng<R: rand::Rng + ?Sized>(
        public: &[u8; 32],
        private: &[u8; 32],
        cert_validity: Duration,
        rng: &mut R,
    ) -> Result<Box<Self>, Error> {
        let secp = Secp256k1::new();
        let secret = SecretKey::from_slice(private).map_err(|_| Error::InvalidRawPrivateKey)?;
        let kp = Keypair::from_secret_key(&secp, &secret);
        let pub_ = kp.x_only_public_key().0.serialize();
        if public == &pub_[..] {
            Ok(Self::new_with_rng(kp, cert_validity.as_secs() as u32, rng))
        } else {
            Err(Error::InvalidRawPublicKey)
        }
    }

    /// Processes the first step of the Noise NX protocol handshake for the responder.
    ///
    /// This function manages the responder's side of the handshake after receiving the initiator's
    /// initial message. It processes the ephemeral public key provided by the initiator, derives
    /// the necessary shared secrets, and constructs the response message. The response includes
    /// the responder's ephemeral public key (in its ElligatorSwift-encoded form), the encrypted
    /// static public key, and a signature noise message. Additionally, it establishes the session
    /// ciphers for encrypting and decrypting further communication.
    ///
    /// On success, it returns a tuple containing the response message to be sent back to the
    /// initiator and a [`NoiseCodec`] instance, which is configured with the session ciphers for
    /// secure transmission of subsequent messages.
    ///
    /// On failure, the method returns an error if there is an issue during encryption, decryption,
    /// or any other step of the handshake process.
    #[cfg(feature = "std")]
    pub fn step_1(
        &mut self,
        elligatorswift_theirs_ephemeral_serialized: [u8; ELLSWIFT_ENCODING_SIZE],
    ) -> Result<([u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE], NoiseCodec), aes_gcm::Error> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;

        self.step_1_with_now_rng(
            elligatorswift_theirs_ephemeral_serialized,
            now,
            &mut rand::thread_rng(),
        )
    }

    /// Executes the first step of the Noise NX protocol handshake for the responder given
    /// the current time and a custom random number generator.
    ///
    /// See [`Self::step_1`] for more details.
    ///
    /// The current time and the custom random number generatorshould be provided in order to not
    /// implicitely rely on `std` and allow `no_std` environments to provide a hardware random
    /// number generator for example.
    #[inline]
    pub fn step_1_with_now_rng<R: rand::Rng + rand::CryptoRng>(
        &mut self,
        elligatorswift_theirs_ephemeral_serialized: [u8; ELLSWIFT_ENCODING_SIZE],
        now: u32,
        rng: &mut R,
    ) -> Result<([u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE], NoiseCodec), aes_gcm::Error> {
        // 4.5.1.2 Responder
        Self::mix_hash(self, &elligatorswift_theirs_ephemeral_serialized[..]);
        Self::decrypt_and_hash(self, &mut vec![])?;

        // 4.5.2.1 Responder
        let mut out = [0; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE];
        let keypair = self.e;
        let elligatorswitf_ours_ephemeral = ElligatorSwift::from_pubkey(keypair.public_key());
        let elligatorswift_ours_ephemeral_serialized = elligatorswitf_ours_ephemeral.to_array();
        out[..ELLSWIFT_ENCODING_SIZE]
            .copy_from_slice(&elligatorswift_ours_ephemeral_serialized[..ELLSWIFT_ENCODING_SIZE]);

        // 3. calls `MixHash(e.public_key)`
        // what is here is not the public key encoded with ElligatorSwift, but the x-coordinate of
        // the public key (which is a point in the EC).

        Self::mix_hash(self, &elligatorswift_ours_ephemeral_serialized);

        // 4. calls `MixKey(ECDH(e.private_key, re.public_key))`
        let e_private_key = keypair.secret_key();
        let elligatorswift_theirs_ephemeral =
            ElligatorSwift::from_array(elligatorswift_theirs_ephemeral_serialized);
        let ecdh_ephemeral = ElligatorSwift::shared_secret(
            elligatorswift_theirs_ephemeral,
            elligatorswitf_ours_ephemeral,
            e_private_key,
            secp256k1::ellswift::ElligatorSwiftParty::B,
            None,
        )
        .to_secret_bytes();
        Self::mix_key(self, &ecdh_ephemeral);

        // 5. appends `EncryptAndHash(s.public_key)` (64 bytes encrypted elligatorswift  public key,
        //    16 bytes MAC)
        let mut encrypted_static_pub_k = vec![0; ELLSWIFT_ENCODING_SIZE];
        let elligatorswift_ours_static = ElligatorSwift::from_pubkey(self.s.public_key());
        let elligatorswift_ours_static_serialized: [u8; ELLSWIFT_ENCODING_SIZE] =
            elligatorswift_ours_static.to_array();
        encrypted_static_pub_k[..ELLSWIFT_ENCODING_SIZE]
            .copy_from_slice(&elligatorswift_ours_static_serialized[0..ELLSWIFT_ENCODING_SIZE]);
        self.encrypt_and_hash(&mut encrypted_static_pub_k)?;
        out[ELLSWIFT_ENCODING_SIZE..(ELLSWIFT_ENCODING_SIZE + ENCRYPTED_ELLSWIFT_ENCODING_SIZE)]
            .copy_from_slice(&encrypted_static_pub_k[..(ENCRYPTED_ELLSWIFT_ENCODING_SIZE)]);
        // note: 64+16+64 = 144

        // 6. calls `MixKey(ECDH(s.private_key, re.public_key))`
        let s_private_key = self.s.secret_key();
        let ecdh_static = ElligatorSwift::shared_secret(
            elligatorswift_theirs_ephemeral,
            elligatorswift_ours_static,
            s_private_key,
            secp256k1::ellswift::ElligatorSwiftParty::B,
            None,
        )
        .to_secret_bytes();
        Self::mix_key(self, &ecdh_static[..]);

        // 7. appends `EncryptAndHash(SIGNATURE_NOISE_MESSAGE)` to the buffer
        let valid_from = now;
        let not_valid_after = now + self.cert_validity;
        let signature_noise_message = self.get_signature(VERSION, valid_from, not_valid_after, rng);
        let mut signature_part = Vec::with_capacity(ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE);
        signature_part.extend_from_slice(&signature_noise_message[..]);
        Self::encrypt_and_hash(self, &mut signature_part)?;
        let ephemeral_plus_static_encrypted_length =
            ELLSWIFT_ENCODING_SIZE + ENCRYPTED_ELLSWIFT_ENCODING_SIZE;
        out[ephemeral_plus_static_encrypted_length..(INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE)]
            .copy_from_slice(&signature_part[..ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE]);

        // 9. return pair of CipherState objects, the first for encrypting transport messages from
        //    initiator to responder, and the second for messages in the other direction:
        let ck = Self::get_ck(self);
        let (temp_k1, temp_k2) = Self::hkdf_2(ck, &[]);
        let c1 = ChaCha20Poly1305::new(&temp_k1.into());
        let c2 = ChaCha20Poly1305::new(&temp_k2.into());
        let c1: Cipher<ChaCha20Poly1305> = Cipher::from_key_and_cipher(temp_k1, c1);
        let c2: Cipher<ChaCha20Poly1305> = Cipher::from_key_and_cipher(temp_k2, c2);
        let to_send = out;
        self.c1 = None;
        self.c2 = None;
        let mut encryptor = GenericCipher::ChaCha20Poly1305(c2);
        let mut decryptor = GenericCipher::ChaCha20Poly1305(c1);
        encryptor.erase_k();
        decryptor.erase_k();
        let codec = crate::NoiseCodec {
            encryptor,
            decryptor,
        };
        Ok((to_send, codec))
    }

    // Generates a signature noise message for the responder's certificate.
    //
    // This method creates a signature noise message that includes the protocol version,
    // certificate validity period, and a cryptographic signature. The signature is created using
    // the responder's static public key and authority keypair, ensuring that the responder's
    // identity and certificate validity are cryptographically verifiable.
    #[inline]
    fn get_signature<R: rand::Rng + rand::CryptoRng>(
        &self,
        version: u16,
        valid_from: u32,
        not_valid_after: u32,
        rng: &mut R,
    ) -> [u8; 74] {
        let mut ret = [0; 74];
        let version = version.to_le_bytes();
        let valid_from = valid_from.to_le_bytes();
        let not_valid_after = not_valid_after.to_le_bytes();
        ret[0] = version[0];
        ret[1] = version[1];
        ret[2] = valid_from[0];
        ret[3] = valid_from[1];
        ret[4] = valid_from[2];
        ret[5] = valid_from[3];
        ret[6] = not_valid_after[0];
        ret[7] = not_valid_after[1];
        ret[8] = not_valid_after[2];
        ret[9] = not_valid_after[3];
        SignatureNoiseMessage::sign_with_rng(&mut ret, &self.s.x_only_public_key().0, &self.a, rng);
        ret
    }

    // Securely erases sensitive data in the responder's memory.
    //
    // Clears all sensitive cryptographic material within the [`Responder`] to prevent any
    // accidental leakage or misuse. It overwrites the stored keys, chaining key, handshake hash,
    // and session ciphers with zeros. This function is typically
    // called when the [`Responder`] instance is no longer needed or before deallocation.
    fn erase(&mut self) {
        if let Some(k) = self.k.as_mut() {
            for b in k {
                unsafe { ptr::write_volatile(b, 0) };
            }
        }
        for mut b in self.ck {
            unsafe { ptr::write_volatile(&mut b, 0) };
        }
        for mut b in self.h {
            unsafe { ptr::write_volatile(&mut b, 0) };
        }
        if let Some(c1) = self.c1.as_mut() {
            c1.erase_k()
        }
        if let Some(c2) = self.c2.as_mut() {
            c2.erase_k()
        }
        self.e.non_secure_erase();
        self.s.non_secure_erase();
        self.a.non_secure_erase();
    }
}

impl Drop for Responder {
    /// Ensures that sensitive data is securely erased when the [`Responder`] instance is dropped,
    /// preventing any potential leakage of cryptographic material.
    fn drop(&mut self) {
        self.erase();
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/signature_message.rs">
// # Signature-Based Message Handling
//
// Defines the [`SignatureNoiseMessage`] struct, which represents a signed message used in the
// Noise protocol to authenticate and verify the identity of a party during the handshake.
//
// This module provides utilities for creating, signing, and verifying Noise protocol messages
// using Schnorr signatures over the [`secp256k1`] elliptic curve. It encapsulates signed messages
// along with versioning and validity timestamps. The following capabilities are supported:
//
// - Conversion of raw byte arrays into structured [`SignatureNoiseMessage`] instances.
// - Message signing using Schnorr signatures and the [`secp256k1`] curve.
// - Verification of signed messages, ensuring they fall within valid time periods and are signed by
//   an authorized public key.
//
// ## Usage
//
// The [`SignatureNoiseMessage`] is used by both the [`crate::Responder`] and [`crate::Initiator`]
// roles. The [`crate::Responder`] uses the `sign` method to generate a Schnorr signature over the
// initial message sent by the initiator. The [`crate::Initiator`] uses the `verify` method to
// check the validity of the signed message from the responder, comparing it against the provided
// public key and optional authority key, while ensuring the message falls within the specified
// validity period.

use core::convert::TryInto;

use secp256k1::{hashes::sha256, schnorr::Signature, Keypair, Message, Secp256k1, XOnlyPublicKey};

/// `SignatureNoiseMessage` represents a signed message used in the Noise NX protocol
/// for authentication during the handshake process. It encapsulates the necessary
/// details for signature verification, including protocol versioning, validity periods,
/// and a Schnorr signature over the message.
///
/// This structure ensures that messages are authenticated and valid only within
/// a specified time window, using Schnorr signatures over the `secp256k1` elliptic curve.
pub struct SignatureNoiseMessage {
    // Version of the protocol being used.
    pub version: u16,
    // Start of the validity period for the message, expressed as a Unix timestamp.
    pub valid_from: u32,
    // End of the validity period for the message, expressed as a Unix timestamp.
    pub not_valid_after: u32,
    // 64-byte Schnorr signature that authenticates the message.
    pub signature: [u8; 64],
}

impl From<[u8; 74]> for SignatureNoiseMessage {
    // Converts a 74-byte array into a [`SignatureNoiseMessage`].
    //
    // Allows a raw 74-byte array to be converted into a [`SignatureNoiseMessage`], extracting the
    // version, validity periods, and signature from the provided data. Panics if the byte array
    // cannot be correctly converted into the struct fields.
    fn from(value: [u8; 74]) -> Self {
        let version = u16::from_le_bytes(value[0..2].try_into().unwrap());
        let valid_from = u32::from_le_bytes(value[2..6].try_into().unwrap());
        let not_valid_after = u32::from_le_bytes(value[6..10].try_into().unwrap());
        let signature = value[10..74].try_into().unwrap();
        Self {
            version,
            valid_from,
            not_valid_after,
            signature,
        }
    }
}

impl SignatureNoiseMessage {
    // Verifies the [`SignatureNoiseMessage`] against the provided public key and an optional
    // authority public key. The verification checks that the message is currently valid
    // (i.e., within the `valid_from` and `not_valid_after` time window) and that the signature
    // is correctly signed by the authority.
    //
    // If an authority public key is not provided, the function assumes that the signature
    // is already valid without further verification.
    #[allow(dead_code)]
    #[cfg(feature = "std")]
    pub fn verify(self, pk: &XOnlyPublicKey, authority_pk: &Option<XOnlyPublicKey>) -> bool {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;
        self.verify_with_now(pk, authority_pk, now)
    }

    /// Verifies the validity and authenticity of the `SignatureNoiseMessage` at a given timestamp.
    ///
    /// See [`Self::verify`] for more details.
    ///
    /// The current system time should be provided to avoid relying on `std` and allow `no_std`
    /// environments to use another source of time.
    #[inline]
    pub fn verify_with_now(
        self,
        pk: &XOnlyPublicKey,
        authority_pk: &Option<XOnlyPublicKey>,
        now: u32,
    ) -> bool {
        if let Some(authority_pk) = authority_pk {
            if self.valid_from <= now && self.not_valid_after >= now {
                let secp = Secp256k1::verification_only();
                let (m, s) = self.split();
                // m = SHA-256(version || valid_from || not_valid_after || server_static_key)
                let m = [&m[0..10], &pk.serialize()].concat();
                let m = Message::from_hashed_data::<sha256::Hash>(&m);
                let s = match Signature::from_slice(&s) {
                    Ok(s) => s,
                    _ => return false,
                };
                secp.verify_schnorr(&s, &m, authority_pk).is_ok()
            } else {
                false
            }
        } else {
            true
        }
    }

    // Signs a [`SignatureNoiseMessage`] using the provided keypair (`kp`).
    //
    // Creates a Schnorr signature for the message, combining the version, validity period, and
    // the static public key of the server (`static_pk`). The resulting signature is then written
    // into the provided message buffer (`msg`).
    #[allow(dead_code)]
    #[cfg(feature = "std")]
    pub fn sign(msg: &mut [u8; 74], static_pk: &XOnlyPublicKey, kp: &Keypair) {
        Self::sign_with_rng(msg, static_pk, kp, &mut rand::thread_rng());
    }

    /// Signs a [`SignatureNoiseMessage`] using the provided keypair (`kp`) and a custom random
    /// number generator.
    ///
    /// See [`Self::sign`] for more details.
    ///
    /// The random number generator is used in the signature generation. It should be provided in
    /// order to not implicitely rely on `std` and allow `no_std` environments to provide a
    /// hardware random number generator for example.
    #[inline]
    pub fn sign_with_rng<R: rand::Rng + rand::CryptoRng>(
        msg: &mut [u8; 74],
        static_pk: &XOnlyPublicKey,
        kp: &Keypair,
        rng: &mut R,
    ) {
        let secp = Secp256k1::signing_only();
        let m = [&msg[0..10], &static_pk.serialize()].concat();
        let m = Message::from_hashed_data::<sha256::Hash>(&m);
        let signature = secp.sign_schnorr_with_rng(&m, kp, rng);
        for (i, b) in signature.as_ref().iter().enumerate() {
            msg[10 + i] = *b;
        }
    }

    // Splits the [`SignatureNoiseMessage`] into its component parts: the message hash and the
    // signature.
    //
    // Separates the message into the first 10 bytes (containing the version and validity period)
    // and the 64-byte Schnorr signature, returning them in a tuple. Used internally during the
    // verification process.
    fn split(self) -> ([u8; 10], [u8; 64]) {
        let mut m = [0; 10];
        m[0] = self.version.to_le_bytes()[0];
        m[1] = self.version.to_le_bytes()[1];
        m[2] = self.valid_from.to_le_bytes()[0];
        m[3] = self.valid_from.to_le_bytes()[1];
        m[4] = self.valid_from.to_le_bytes()[2];
        m[5] = self.valid_from.to_le_bytes()[3];
        m[6] = self.not_valid_after.to_le_bytes()[0];
        m[7] = self.not_valid_after.to_le_bytes()[1];
        m[8] = self.not_valid_after.to_le_bytes()[2];
        m[9] = self.not_valid_after.to_le_bytes()[3];
        (m, self.signature)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/noise-sv2/src/test.rs">
use crate::{handshake::HandshakeOp, initiator::Initiator, responder::Responder};

#[test]
#[cfg(feature = "std")]
fn test_1() {
    let key_pair = Responder::generate_key();

    let mut initiator = Initiator::new(Some(key_pair.public_key().into()));
    let mut responder = Responder::new(key_pair, 31449600);
    let first_message = initiator.step_0().unwrap();
    let (second_message, mut codec_responder) = responder.step_1(first_message).unwrap();
    let mut codec_initiator = initiator.step_2(second_message).unwrap();
    let mut message = "ciao".as_bytes().to_vec();
    codec_initiator.encrypt(&mut message).unwrap();
    assert!(message != "ciao".as_bytes().to_vec());
    codec_responder.decrypt(&mut message).unwrap();

    assert!(message == "ciao".as_bytes().to_vec());
}
#[test]
fn test_1_with_rng() {
    let key_pair = Responder::generate_key_with_rng(&mut rand::thread_rng());

    let mut initiator: Box<Initiator> =
        Initiator::new_with_rng(Some(key_pair.public_key().into()), &mut rand::thread_rng());
    let mut responder = Responder::new_with_rng(key_pair, 31449600, &mut rand::thread_rng());
    let first_message = initiator.step_0().unwrap();
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    let (second_message, mut codec_responder) = responder
        .step_1_with_now_rng(first_message, now, &mut rand::thread_rng())
        .unwrap();
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs() as u32;
    let mut codec_initiator = initiator.step_2_with_now(second_message, now).unwrap();
    let mut message = "ciao".as_bytes().to_vec();
    codec_initiator.encrypt(&mut message).unwrap();
    assert!(message != "ciao".as_bytes().to_vec());
    codec_responder.decrypt(&mut message).unwrap();

    assert!(message == "ciao".as_bytes().to_vec());
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/Cargo.toml">
[package]
name = "roles_logic_sv2"
version = "3.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "Common handlers for use within SV2 roles"
documentation = "https://docs.rs/roles_logic_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
bitcoin = { version = "0.32.5" }
common_messages_sv2 = { path = "../../../protocols/v2/subprotocols/common-messages", version = "^5.0.0" }
mining_sv2 = { path = "../../../protocols/v2/subprotocols/mining", version = "^4.0.0" }
template_distribution_sv2 = { path = "../../../protocols/v2/subprotocols/template-distribution", version = "^3.0.0" }
job_declaration_sv2 = { path = "../../../protocols/v2/subprotocols/job-declaration", version = "^4.0.0" }
tracing = { version = "0.1"}
chacha20poly1305 = { version = "0.10.1"}
nohash-hasher = "0.2.0"
primitive-types = "0.13.1"
hex = {package = "hex-conservative", version = "0.3.0"}
codec_sv2 = { path = "../../../protocols/v2/codec-sv2", version = "^2.0.0", features = ["noise_sv2", "with_buffer_pool"] }

[dev-dependencies]
quickcheck = "1.0.3"
quickcheck_macros = "1"
rand = "0.8.5"
toml =  {git = "https://github.com/diondokter/toml-rs", default-features = false, rev="c4161aa"}
serde = { version = "1.0.89", features = ["derive", "alloc"], default-features = false}
tracing-subscriber = "0.3"

[features]
prop_test = ["template_distribution_sv2/prop_test"]
# Code coverage tools may conflict with the nopanic logic, so we can disable it when needed
disable_nopanic = []
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/README.md">
# `roles_logic_sv2`

[![crates.io](https://img.shields.io/crates/v/roles_logic_sv2.svg)](https://crates.io/crates/roles_logic_sv2)
[![docs.rs](https://docs.rs/roles_logic_sv2/badge.svg)](https://docs.rs/roles_logic_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=roles_logic_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`roles_logic_sv2` provides the core logic and utilities for implementing roles in the Stratum V2 (Sv2) protocol, such as miners, pools, and proxies. It abstracts message handling, channel management, job creation, and routing logic, enabling efficient and secure communication across upstream and downstream connections.

## Main Components

- **Channel Logic**: Manages the lifecycle and settings of communication channels (standard, extended, and group ones) between roles.
- **Handlers**: Provides traits for handling logic of Sv2 protocol messages.
- **Job Management**: Facilitates the creation, validation, and dispatching of mining jobs.
- **Parsers**: Handles serialization and deserialization of Sv2 messages via [`binary_sv2`](https://docs.rs/binary_sv2/latest/binary_sv2/index.html).
- **Routing Logic**: Implements message routing and downstream/upstream selector utilities. Useful for advanced proxy implementations with multiplexing of Standard Channels across different upstreams.
- **Utilities**: Provides helpers for safe mutex locking, mining-specific calculations, and more.

## Usage

To include this crate in your project, run:

```bash
cargo add roles_logic_sv2
```

This crate can be built with the following feature flags:

- `prop_test`: Enables property-based testing features for template distribution logic, leveraging dependencies' testing capabilities such as `template_distribution_sv2` crate.
- `disable_nopanic`: Disables the nopanic logic in scenarios where code coverage tools might conflict with it.
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/reg-test-block.toml">
# Block with 1 transaction (not including the cb tx) generated using regtest
block_hash  = "59202ef47d684ab51866e91d5f40e61a94787d02d899fc3da28e4f4bcb8fd0a4"

version     = 0x20000000 # LE
prev_hash   = "74f34a78cf87740b67fa545734ce7ea86e917e53001c3bd6eaf5d32f5d89264d" # BE
merkle_root = "3a158fb4278f27d01cafcac465ba1c5b8f3461687681465f6734649f2cb8d5a7" #BE
nbits       = 0x207fffff # BE
time        = 0x6238b86a # BE
nonce       = 0x00000001 # BE

coinbase_tx_prefix = "02000000010000000000000000000000000000000000000000000000000000000000000000ffffffff040172"
coinbase_script    = "0101"
coinbase_tx_suffix = "ffffffff02d0751b2a010000001600146e556b6f3ad312726bc501da9656ac1329a84d040000000000000000266a24aa21a9edd8b6a6c9c32ffe19d40f1109b33186c59d580c7e7d591194df4e57b05ef9824100000000"
path               = ["55f88d00edeeca6cdb853624a34500e076394abd35fc39bf9a3a9865f0540731"]
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channel_logic/channel_factory.rs">
//! # Channel Factory
//!
//! This module contains logic for creating and managing channels.

use crate::{
    job_creator::{self, JobsCreators},
    parsers::Mining,
    utils::{GroupId, Id, Mutex},
    Error,
};

use codec_sv2::binary_sv2;
use mining_sv2::{
    ExtendedExtranonce, NewExtendedMiningJob, OpenExtendedMiningChannelSuccess,
    OpenMiningChannelError, SetCustomMiningJob, SetCustomMiningJobSuccess, SetNewPrevHash,
    SubmitSharesError, SubmitSharesExtended, SubmitSharesStandard, Target,
};

use hex::DisplayHex;
use nohash_hasher::BuildNoHashHasher;
use std::{collections::HashMap, convert::TryInto, sync::Arc};
use template_distribution_sv2::{NewTemplate, SetNewPrevHash as SetNewPrevHashFromTp};

use tracing::{debug, error, info, trace, warn};

use bitcoin::{
    block::{Header, Version},
    hash_types,
    hashes::sha256d::Hash,
    CompactTarget, TxOut,
};

/// A stripped type of `SetCustomMiningJob` without the (`channel_id, `request_id` and `token`)
/// fields
#[derive(Debug)]
pub struct PartialSetCustomMiningJob {
    pub version: u32,
    pub prev_hash: binary_sv2::U256<'static>,
    pub min_ntime: u32,
    pub nbits: u32,
    pub coinbase_tx_version: u32,
    pub coinbase_prefix: binary_sv2::B0255<'static>,
    pub coinbase_tx_input_n_sequence: u32,
    pub coinbase_tx_value_remaining: u64,
    pub coinbase_tx_outputs: binary_sv2::B064K<'static>,
    pub coinbase_tx_locktime: u32,
    pub merkle_path: binary_sv2::Seq0255<'static, binary_sv2::U256<'static>>,
    pub future_job: bool,
}

/// Represents the action that needs to be done when a new share is received.
#[derive(Debug, Clone)]
pub enum OnNewShare {
    /// Used when the received is malformed, is for an inexistent channel or do not meet downstream
    /// target.
    SendErrorDownstream(SubmitSharesError<'static>),
    /// Used when an extended channel in a proxy receive a share, and the share meet upstream
    /// target, in this case a new share must be sent upstream. Also an optional template id is
    /// returned, when a job declarator want to send a valid share upstream could use the
    /// template for get the up job id.
    SendSubmitShareUpstream((Share, Option<u64>)),
    /// Used when a group channel in a proxy receive a share that is not malformed and is for a
    /// valid channel in that case we relay the same exact share upstream with a new request id.
    RelaySubmitShareUpstream,
    /// Indicate that the share meet bitcoin target, when there is an upstream the we should send
    /// the share upstream, whenever possible we should also notify the TP about it.
    /// When a pool negotiate a job with downstream we do not have the template_id so we set it to
    /// None
    /// (share, template id, coinbase,complete extranonce)
    ShareMeetBitcoinTarget((Share, Option<u64>, Vec<u8>, Vec<u8>)),
    /// Indicate that the share meet downstream target, in the case we could send a success
    /// response downstream.
    ShareMeetDownstreamTarget,
}

impl OnNewShare {
    /// Converts standard share into extended share
    pub fn into_extended(&mut self, extranonce: Vec<u8>, up_id: u32) {
        match self {
            OnNewShare::SendErrorDownstream(_) => (),
            OnNewShare::SendSubmitShareUpstream((share, template_id)) => match share {
                Share::Extended(_) => (),
                Share::Standard((share, _)) => {
                    let share = SubmitSharesExtended {
                        channel_id: up_id,
                        sequence_number: share.sequence_number,
                        job_id: share.job_id,
                        nonce: share.nonce,
                        ntime: share.ntime,
                        version: share.version,
                        extranonce: extranonce.try_into().unwrap(),
                    };
                    *self = Self::SendSubmitShareUpstream((Share::Extended(share), *template_id));
                }
            },
            OnNewShare::RelaySubmitShareUpstream => (),
            OnNewShare::ShareMeetBitcoinTarget((share, t_id, coinbase, ext)) => match share {
                Share::Extended(_) => (),
                Share::Standard((share, _)) => {
                    let share = SubmitSharesExtended {
                        channel_id: up_id,
                        sequence_number: share.sequence_number,
                        job_id: share.job_id,
                        nonce: share.nonce,
                        ntime: share.ntime,
                        version: share.version,
                        extranonce: extranonce.try_into().unwrap(),
                    };
                    *self = Self::ShareMeetBitcoinTarget((
                        Share::Extended(share),
                        *t_id,
                        coinbase.clone(),
                        ext.to_vec(),
                    ));
                }
            },
            OnNewShare::ShareMeetDownstreamTarget => todo!(),
        }
    }
}

/// A share can be either extended or standard
#[derive(Clone, Debug)]
pub enum Share {
    Extended(SubmitSharesExtended<'static>),
    // share, group id
    Standard((SubmitSharesStandard, u32)),
}

/// Helper type used before a `SetNewPrevHash` has a channel_id
#[derive(Clone, Debug)]
pub struct StagedPhash {
    job_id: u32,
    prev_hash: binary_sv2::U256<'static>,
    min_ntime: u32,
    nbits: u32,
}

impl StagedPhash {
    /// Converts a Staged PrevHash into a SetNewPrevHash message
    pub fn into_set_p_hash(
        &self,
        channel_id: u32,
        new_job_id: Option<u32>,
    ) -> SetNewPrevHash<'static> {
        SetNewPrevHash {
            channel_id,
            job_id: new_job_id.unwrap_or(self.job_id),
            prev_hash: self.prev_hash.clone(),
            min_ntime: self.min_ntime,
            nbits: self.nbits,
        }
    }
}

impl Share {
    /// Get share sequence number
    pub fn get_sequence_number(&self) -> u32 {
        match self {
            Share::Extended(s) => s.sequence_number,
            Share::Standard(s) => s.0.sequence_number,
        }
    }

    /// Get share channel id
    pub fn get_channel_id(&self) -> u32 {
        match self {
            Share::Extended(s) => s.channel_id,
            Share::Standard(s) => s.0.channel_id,
        }
    }

    /// Get share timestamp
    pub fn get_n_time(&self) -> u32 {
        match self {
            Share::Extended(s) => s.ntime,
            Share::Standard(s) => s.0.ntime,
        }
    }

    /// Get share nonce
    pub fn get_nonce(&self) -> u32 {
        match self {
            Share::Extended(s) => s.nonce,
            Share::Standard(s) => s.0.nonce,
        }
    }

    /// Get share job id
    pub fn get_job_id(&self) -> u32 {
        match self {
            Share::Extended(s) => s.job_id,
            Share::Standard(s) => s.0.job_id,
        }
    }

    /// Get share version
    pub fn get_version(&self) -> u32 {
        match self {
            Share::Extended(s) => s.version,
            Share::Standard(s) => s.0.version,
        }
    }
}

#[derive(Debug)]
/// Basic logic shared between all the channel factories
struct ChannelFactory {
    ids: Arc<Mutex<GroupId>>,
    extended_channels:
        HashMap<u32, OpenExtendedMiningChannelSuccess<'static>, BuildNoHashHasher<u32>>,
    extranonces: ExtendedExtranonce,
    share_per_min: f32,
    // (NewExtendedMiningJob,group ids that already received the future job)
    future_jobs: Vec<(NewExtendedMiningJob<'static>, Vec<u32>)>,
    // (SetNewPrevHash,group ids that already received the set prev_hash)
    last_prev_hash: Option<(StagedPhash, Vec<u32>)>,
    last_prev_hash_: Option<hash_types::BlockHash>,
    // (NewExtendedMiningJob,group ids that already received the job)
    last_valid_job: Option<(NewExtendedMiningJob<'static>, Vec<u32>)>,
    kind: ExtendedChannelKind,
    job_ids: Id,
    channel_to_group_id: HashMap<u32, u32, BuildNoHashHasher<u32>>,
    future_templates: HashMap<u32, NewTemplate<'static>, BuildNoHashHasher<u32>>,
}

impl ChannelFactory {
    /// Called when a `OpenExtendedMiningChannel` message is received.
    /// Here we save the downstream's target (based on hashrate) and the
    /// channel's extranonce details before returning the relevant SV2 mining messages
    /// to be sent downstream. For the mining messages, we will first return an
    /// `OpenExtendedMiningChannelSuccess` if the channel is successfully opened. Then we add
    /// the `NewExtendedMiningJob` and `SetNewPrevHash` messages if the relevant data is
    /// available. If the channel opening fails, we return `OpenExtendedMiningChannelError`.
    pub fn new_extended_channel(
        &mut self,
        request_id: u32,
        hash_rate: f32,
        min_extranonce_size: u16,
    ) -> Result<Vec<Mining<'static>>, Error> {
        let extended_channels_group = 0;
        let max_extranonce_size = self.extranonces.get_range2_len() as u16;
        if min_extranonce_size <= max_extranonce_size {
            // SECURITY is very unlikely to finish the ids btw this unwrap could be used by an
            // attacker that want to disrupt the service maybe we should have a method
            // to reuse ids that are no longer connected?
            let channel_id = self
                .ids
                .safe_lock(|ids| ids.new_channel_id(extended_channels_group))
                .unwrap();
            self.channel_to_group_id.insert(channel_id, 0);
            let target = match crate::utils::hash_rate_to_target(
                hash_rate.into(),
                self.share_per_min.into(),
            ) {
                Ok(target) => target,
                Err(e) => {
                    error!(
                        "Impossible to get target: {:?}. Request id: {:?}",
                        e, request_id
                    );
                    return Err(e);
                }
            };
            let extranonce_prefix = self
                .extranonces
                .next_prefix_extended(max_extranonce_size as usize)
                .unwrap()
                .into_b032();
            let success = OpenExtendedMiningChannelSuccess {
                request_id,
                channel_id,
                target,
                extranonce_size: max_extranonce_size,
                extranonce_prefix,
            };
            self.extended_channels.insert(channel_id, success.clone());
            let mut result = vec![Mining::OpenExtendedMiningChannelSuccess(success)];
            if let Some((job, _)) = &self.last_valid_job {
                let mut job = job.clone();
                job.set_future();
                let j_id = job.job_id;
                result.push(Mining::NewExtendedMiningJob(job));
                if let Some((new_prev_hash, _)) = &self.last_prev_hash {
                    let mut new_prev_hash = new_prev_hash.into_set_p_hash(channel_id, None);
                    new_prev_hash.job_id = j_id;
                    result.push(Mining::SetNewPrevHash(new_prev_hash.clone()))
                };
            } else if let Some((new_prev_hash, _)) = &self.last_prev_hash {
                let new_prev_hash = new_prev_hash.into_set_p_hash(channel_id, None);
                result.push(Mining::SetNewPrevHash(new_prev_hash.clone()))
            };
            for (job, _) in &self.future_jobs {
                result.push(Mining::NewExtendedMiningJob(job.clone()))
            }
            Ok(result)
        } else {
            Ok(vec![Mining::OpenMiningChannelError(
                OpenMiningChannelError::unsupported_extranonce_size(request_id),
            )])
        }
    }

    /// Called when we want to replicate a channel already opened by another actor.
    /// It is used only in the jd client from the template provider module to mock a pool.
    /// Anything else should open channel with the new_extended_channel function
    pub fn replicate_upstream_extended_channel_only_jd(
        &mut self,
        target: binary_sv2::U256<'static>,
        extranonce: mining_sv2::Extranonce,
        channel_id: u32,
        extranonce_size: u16,
    ) -> Option<()> {
        self.channel_to_group_id.insert(channel_id, 0);
        let extranonce_prefix = extranonce.into();
        let success = OpenExtendedMiningChannelSuccess {
            request_id: 0,
            channel_id,
            target,
            extranonce_size,
            extranonce_prefix,
        };
        self.extended_channels.insert(channel_id, success.clone());
        Some(())
    }

    /// Called when a new prev hash is received. If the respective job is available in the future
    /// job queue, we move the future job into the valid job slot and store the prev hash as the
    /// current prev hash to be referenced.
    fn on_new_prev_hash(&mut self, m: StagedPhash) -> Result<(), Error> {
        while let Some(mut job) = self.future_jobs.pop() {
            if job.0.job_id == m.job_id {
                let now = std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs() as u32;
                job.0.set_no_future(now);
                self.last_valid_job = Some(job);
                break;
            }
            self.last_valid_job = None;
        }
        self.future_jobs = vec![];
        self.last_prev_hash_ = Some(crate::utils::u256_to_block_hash(m.prev_hash.clone()));
        self.last_prev_hash = Some((m, vec![]));
        Ok(())
    }

    /// Called when a `NewExtendedMiningJob` arrives. If the job is future, we add it to the future
    /// queue. If the job is not future, we pair it with a the most recent prev hash
    fn on_new_extended_mining_job(
        &mut self,
        m: NewExtendedMiningJob<'static>,
    ) -> Result<HashMap<u32, Mining<'static>, BuildNoHashHasher<u32>>, Error> {
        match (m.is_future(), &self.last_prev_hash) {
            (true, _) => {
                let mut result = HashMap::with_hasher(BuildNoHashHasher::default());
                self.prepare_jobs_for_downstream_on_new_extended(&mut result, &m)?;
                self.future_jobs.push((m, vec![]));
                Ok(result)
            }
            (false, Some(_)) => {
                let mut result = HashMap::with_hasher(BuildNoHashHasher::default());
                self.prepare_jobs_for_downstream_on_new_extended(&mut result, &m)?;
                // If job is not future it must always be paired with the last received prev hash
                self.last_valid_job = Some((m, vec![]));
                if let Some((_p_hash, _)) = &self.last_prev_hash {
                    Ok(result)
                } else {
                    Err(Error::JobIsNotFutureButPrevHashNotPresent)
                }
            }
            // This should not happen when a non future job is received we always need to have a
            // prev hash
            (false, None) => Err(Error::JobIsNotFutureButPrevHashNotPresent),
        }
    }

    // When a new extended job is received we use this function to prepare the jobs to be sent
    // downstream (standard for hom and this job for non hom)
    fn prepare_jobs_for_downstream_on_new_extended(
        &mut self,
        result: &mut HashMap<u32, Mining, BuildNoHashHasher<u32>>,
        m: &NewExtendedMiningJob<'static>,
    ) -> Result<(), Error> {
        for id in self.extended_channels.keys() {
            let mut extended = m.clone();
            extended.channel_id = *id;
            let extended_job = Mining::NewExtendedMiningJob(extended);
            result.insert(*id, extended_job);
        }
        Ok(())
    }

    // If there is job creator, bitcoin_target is retrieved from there. If not, it is set to 0.
    // If there is a job creator we pass the correct template id. If not, we pass `None`
    // allow comparison chain because clippy wants to make job management assertion into a match
    // clause
    #[allow(clippy::comparison_chain)]
    #[allow(clippy::too_many_arguments)]
    fn check_target<TxHash: std::convert::AsRef<[u8]>>(
        &mut self,
        mut m: Share,
        bitcoin_target: Target,
        template_id: Option<u64>,
        up_id: u32,
        merkle_path: Vec<TxHash>,
        coinbase_tx_prefix: &[u8],
        coinbase_tx_suffix: &[u8],
        prev_blockhash: hash_types::BlockHash,
        bits: u32,
    ) -> Result<OnNewShare, Error> {
        debug!("Checking target for share {:?}", m);
        let upstream_target = match &self.kind {
            ExtendedChannelKind::Pool => Target::new(0, 0),
            ExtendedChannelKind::Proxy {
                upstream_target, ..
            }
            | ExtendedChannelKind::ProxyJd {
                upstream_target, ..
            } => upstream_target.clone(),
        };

        let (downstream_target, extranonce) = self
            .get_channel_specific_mining_info(&m)
            .ok_or(Error::ShareDoNotMatchAnyChannel)?;
        let extranonce_1_len = self.extranonces.get_range0_len();
        let extranonce_2 = extranonce[extranonce_1_len..].to_vec();
        match &mut m {
            Share::Extended(extended_share) => {
                extended_share.extranonce = extranonce_2.try_into()?;
            }
            Share::Standard(_) => (),
        };
        trace!(
            "On checking target coinbase prefix is: {:?}",
            coinbase_tx_prefix
        );
        trace!(
            "On checking target coinbase suffix is: {:?}",
            coinbase_tx_suffix
        );
        // Safe unwrap a sha256 can always be converted into [u8;32]
        let merkle_root: [u8; 32] = crate::utils::merkle_root_from_path(
            coinbase_tx_prefix,
            coinbase_tx_suffix,
            &extranonce[..],
            &merkle_path[..],
        )
        .ok_or(Error::InvalidCoinbase)?
        .try_into()
        .unwrap();
        let version = match &m {
            Share::Extended(share) => share.version as i32,
            Share::Standard(share) => share.0.version as i32,
        };

        let header = Header {
            version: Version::from_consensus(version),
            prev_blockhash,
            merkle_root: (*Hash::from_bytes_ref(&merkle_root)).into(),
            time: m.get_n_time(),
            bits: CompactTarget::from_consensus(bits),
            nonce: m.get_nonce(),
        };

        trace!("On checking target header is: {:?}", header);
        let hash_ = header.block_hash();
        let hash: [u8; 32] = *hash_.to_raw_hash().as_ref();

        if tracing::level_enabled!(tracing::Level::DEBUG)
            || tracing::level_enabled!(tracing::Level::TRACE)
        {
            let bitcoin_target_log: binary_sv2::U256 = bitcoin_target.clone().into();
            let mut bitcoin_target_log = bitcoin_target_log.to_vec();
            bitcoin_target_log.reverse();
            debug!("Bitcoin target : {:?}", bitcoin_target_log.as_hex());
            let upstream_target: binary_sv2::U256 = upstream_target.clone().into();
            let mut upstream_target = upstream_target.to_vec();
            upstream_target.reverse();
            debug!("Upstream target: {:?}", upstream_target.to_vec().as_hex());
            let mut hash = hash;
            hash.reverse();
            debug!("Hash           : {:?}", hash.to_vec().as_hex());
        }
        let hash: Target = hash.into();

        if hash <= bitcoin_target {
            let mut print_hash: [u8; 32] = *hash_.to_raw_hash().as_ref();
            print_hash.reverse();

            info!(
                "Share hash meet bitcoin target: {:?}",
                print_hash.to_vec().as_hex()
            );

            let coinbase = [coinbase_tx_prefix, &extranonce[..], coinbase_tx_suffix]
                .concat()
                .to_vec();
            match self.kind {
                ExtendedChannelKind::Proxy { .. } | ExtendedChannelKind::ProxyJd { .. } => {
                    let upstream_extranonce_space = self.extranonces.get_range0_len();
                    let extranonce_ = extranonce[upstream_extranonce_space..].to_vec();
                    let mut res = OnNewShare::ShareMeetBitcoinTarget((
                        m,
                        template_id,
                        coinbase,
                        extranonce.to_vec(),
                    ));
                    res.into_extended(extranonce_, up_id);
                    Ok(res)
                }
                ExtendedChannelKind::Pool => Ok(OnNewShare::ShareMeetBitcoinTarget((
                    m,
                    template_id,
                    coinbase,
                    extranonce.to_vec(),
                ))),
            }
        } else if hash <= upstream_target {
            match self.kind {
                ExtendedChannelKind::Proxy { .. } | ExtendedChannelKind::ProxyJd { .. } => {
                    let upstream_extranonce_space = self.extranonces.get_range0_len();
                    let extranonce = extranonce[upstream_extranonce_space..].to_vec();
                    let mut res = OnNewShare::SendSubmitShareUpstream((m, template_id));
                    res.into_extended(extranonce, up_id);
                    Ok(res)
                }
                ExtendedChannelKind::Pool => {
                    Ok(OnNewShare::SendSubmitShareUpstream((m, template_id)))
                }
            }
        } else if hash <= downstream_target {
            Ok(OnNewShare::ShareMeetDownstreamTarget)
        } else {
            error!("Share does not meet any target: {:?}", m);
            let error = SubmitSharesError {
                channel_id: m.get_channel_id(),
                sequence_number: m.get_sequence_number(),
                // Infallible unwrap we already know the len of the error code (is a
                // static string)
                error_code: SubmitSharesError::difficulty_too_low_error_code()
                    .to_string()
                    .try_into()
                    .unwrap(),
            };
            Ok(OnNewShare::SendErrorDownstream(error))
        }
    }

    /// Returns the downstream target and extranonce for the channel
    fn get_channel_specific_mining_info(&self, m: &Share) -> Option<(mining_sv2::Target, Vec<u8>)> {
        match m {
            Share::Extended(share) => {
                let channel = self.extended_channels.get(&m.get_channel_id())?;
                let extranonce_prefix = channel.extranonce_prefix.to_vec();
                let dowstream_target = channel.target.clone().into();
                let extranonce = [&extranonce_prefix[..], &share.extranonce.to_vec()[..]]
                    .concat()
                    .to_vec();
                if extranonce.len() != self.extranonces.get_len() {
                    error!(
                        "Extranonce is not of the right len expected {} actual {}",
                        self.extranonces.get_len(),
                        extranonce.len()
                    );
                }
                Some((dowstream_target, extranonce))
            }
            Share::Standard((_share, _group_id)) => {
                unimplemented!()
            }
        }
    }
    /// Updates the downstream target for the given channel_id
    fn update_target_for_channel(&mut self, channel_id: u32, new_target: Target) -> Option<bool> {
        let channel = self.extended_channels.get_mut(&channel_id)?;
        channel.target = new_target.into();
        Some(true)
    }
}

/// Used by a pool to in order to manage all downstream channel. It adds job creation capabilities
/// to ChannelFactory.
#[derive(Debug)]
pub struct PoolChannelFactory {
    inner: ChannelFactory,
    job_creator: JobsCreators,
    pool_coinbase_outputs: Vec<TxOut>,
    // extended_channel_id -> SetCustomMiningJob
    negotiated_jobs: HashMap<u32, SetCustomMiningJob<'static>, BuildNoHashHasher<u32>>,
}

impl PoolChannelFactory {
    /// constructor
    pub fn new(
        ids: Arc<Mutex<GroupId>>,
        extranonces: ExtendedExtranonce,
        job_creator: JobsCreators,
        share_per_min: f32,
        kind: ExtendedChannelKind,
        pool_coinbase_outputs: Vec<TxOut>,
    ) -> Self {
        let inner = ChannelFactory {
            ids,
            extended_channels: HashMap::with_hasher(BuildNoHashHasher::default()),
            extranonces,
            share_per_min,
            future_jobs: Vec::new(),
            last_prev_hash: None,
            last_prev_hash_: None,
            last_valid_job: None,
            kind,
            job_ids: Id::new(),
            channel_to_group_id: HashMap::with_hasher(BuildNoHashHasher::default()),
            future_templates: HashMap::with_hasher(BuildNoHashHasher::default()),
        };

        Self {
            inner,
            job_creator,
            pool_coinbase_outputs,
            negotiated_jobs: HashMap::with_hasher(BuildNoHashHasher::default()),
        }
    }

    /// Calls [`ChannelFactory::new_extended_channel`]
    pub fn new_extended_channel(
        &mut self,
        request_id: u32,
        hash_rate: f32,
        min_extranonce_size: u16,
    ) -> Result<Vec<Mining<'static>>, Error> {
        self.inner
            .new_extended_channel(request_id, hash_rate, min_extranonce_size)
    }

    /// Called when we want to replicate a channel already opened by another actor.
    /// is used only in the jd client from the template provider module to mock a pool.
    /// Anything else should open channel with the new_extended_channel function
    pub fn replicate_upstream_extended_channel_only_jd(
        &mut self,
        target: binary_sv2::U256<'static>,
        extranonce: mining_sv2::Extranonce,
        channel_id: u32,
        extranonce_size: u16,
    ) -> Option<()> {
        self.inner.replicate_upstream_extended_channel_only_jd(
            target,
            extranonce,
            channel_id,
            extranonce_size,
        )
    }

    /// Called only when a new prev hash is received by a Template Provider. It matches the
    /// message with a `job_id` and calls [`ChannelFactory::on_new_prev_hash`]
    /// it return the job_id
    pub fn on_new_prev_hash_from_tp(
        &mut self,
        m: &SetNewPrevHashFromTp<'static>,
    ) -> Result<u32, Error> {
        let job_id = self.job_creator.on_new_prev_hash(m).unwrap_or(0);
        let new_prev_hash = StagedPhash {
            job_id,
            prev_hash: m.prev_hash.clone(),
            min_ntime: m.header_timestamp,
            nbits: m.n_bits,
        };
        self.inner.on_new_prev_hash(new_prev_hash)?;
        Ok(job_id)
    }

    /// Called only when a new template is received by a Template Provider
    pub fn on_new_template(
        &mut self,
        m: &mut NewTemplate<'static>,
    ) -> Result<HashMap<u32, Mining<'static>, BuildNoHashHasher<u32>>, Error> {
        let new_job =
            self.job_creator
                .on_new_template(m, true, self.pool_coinbase_outputs.clone())?;
        self.inner.on_new_extended_mining_job(new_job)
    }

    /// Called when a `SubmitSharesStandard` message is received from the downstream. We check the
    /// shares against the channel's respective target and return `OnNewShare` to let us know if
    /// and where the shares should be relayed
    pub fn on_submit_shares_standard(
        &mut self,
        m: SubmitSharesStandard,
    ) -> Result<OnNewShare, Error> {
        match self.inner.channel_to_group_id.get(&m.channel_id) {
            Some(g_id) => {
                let referenced_job = self
                    .inner
                    .last_valid_job
                    .clone()
                    .ok_or(Error::ShareDoNotMatchAnyJob)?
                    .0;
                let merkle_path = referenced_job.merkle_path.to_vec();
                let template_id = self
                    .job_creator
                    .get_template_id_from_job(referenced_job.job_id)
                    .ok_or(Error::NoTemplateForId)?;
                let target = self.job_creator.last_target();
                let prev_blockhash = self
                    .inner
                    .last_prev_hash_
                    .ok_or(Error::ShareDoNotMatchAnyJob)?;
                let bits = self
                    .inner
                    .last_prev_hash
                    .as_ref()
                    .ok_or(Error::ShareDoNotMatchAnyJob)?
                    .0
                    .nbits;
                self.inner.check_target(
                    Share::Standard((m, *g_id)),
                    target,
                    Some(template_id),
                    0,
                    merkle_path,
                    referenced_job.coinbase_tx_prefix.as_ref(),
                    referenced_job.coinbase_tx_suffix.as_ref(),
                    prev_blockhash,
                    bits,
                )
            }
            None => {
                let err = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: SubmitSharesError::invalid_channel_error_code()
                        .to_string()
                        .try_into()
                        .unwrap(),
                };
                Ok(OnNewShare::SendErrorDownstream(err))
            }
        }
    }

    /// Called when a `SubmitSharesExtended` message is received from the downstream. We check the
    /// shares against the channel's respective target and return `OnNewShare` to let us know if
    /// and where the shares should be relayed
    pub fn on_submit_shares_extended(
        &mut self,
        m: SubmitSharesExtended,
    ) -> Result<OnNewShare, Error> {
        let target = self.job_creator.last_target();
        // When downstream set a custom mining job we add the job to the negotiated job
        // hashmap, with the extended channel id as a key. Whenever the pool receive a share must
        // first check if the channel have a negotiated job if so we can not retrieve the template
        // via the job creator but we create a new one from the set custom job.
        if self.negotiated_jobs.contains_key(&m.channel_id) {
            let referenced_job = self.negotiated_jobs.get(&m.channel_id).unwrap();
            let merkle_path = referenced_job.merkle_path.to_vec();
            let extended_job = job_creator::extended_job_from_custom_job(
                referenced_job,
                self.inner.extranonces.get_len() as u8,
            )
            .unwrap();
            let prev_blockhash = crate::utils::u256_to_block_hash(referenced_job.prev_hash.clone());
            let bits = referenced_job.nbits;
            self.inner.check_target(
                Share::Extended(m.into_static()),
                target,
                None,
                0,
                merkle_path,
                extended_job.coinbase_tx_prefix.as_ref(),
                extended_job.coinbase_tx_suffix.as_ref(),
                prev_blockhash,
                bits,
            )
        } else {
            let referenced_job = self
                .inner
                .last_valid_job
                .clone()
                .ok_or(Error::ShareDoNotMatchAnyJob)?
                .0;
            let merkle_path = referenced_job.merkle_path.to_vec();
            let template_id = self
                .job_creator
                .get_template_id_from_job(referenced_job.job_id)
                .ok_or(Error::NoTemplateForId)?;
            let prev_blockhash = self
                .inner
                .last_prev_hash_
                .ok_or(Error::ShareDoNotMatchAnyJob)?;
            let bits = self
                .inner
                .last_prev_hash
                .as_ref()
                .ok_or(Error::ShareDoNotMatchAnyJob)?
                .0
                .nbits;
            self.inner.check_target(
                Share::Extended(m.into_static()),
                target,
                Some(template_id),
                0,
                merkle_path,
                referenced_job.coinbase_tx_prefix.as_ref(),
                referenced_job.coinbase_tx_suffix.as_ref(),
                prev_blockhash,
                bits,
            )
        }
    }

    /// Utility function to return a new group id
    pub fn new_group_id(&mut self) -> u32 {
        let new_id = self.inner.ids.safe_lock(|ids| ids.new_group_id()).unwrap();
        new_id
    }

    /// Utility function to return a new standard channel id
    pub fn new_standard_id_for_hom(&mut self) -> u32 {
        let hom_group_id = 0;
        let new_id = self
            .inner
            .ids
            .safe_lock(|ids| ids.new_channel_id(hom_group_id))
            .unwrap();
        new_id
    }

    /// Returns the full extranonce, extranonce1 (static for channel) + extranonce2 (miner nonce
    /// space)
    pub fn extranonce_from_downstream_extranonce(
        &self,
        ext: mining_sv2::Extranonce,
    ) -> Option<mining_sv2::Extranonce> {
        self.inner
            .extranonces
            .extranonce_from_downstream_extranonce(ext)
            .ok()
    }

    /// Called when a new custom mining job arrives
    pub fn on_new_set_custom_mining_job(
        &mut self,
        set_custom_mining_job: SetCustomMiningJob<'static>,
    ) -> SetCustomMiningJobSuccess {
        if self.check_set_custom_mining_job(&set_custom_mining_job) {
            self.negotiated_jobs.insert(
                set_custom_mining_job.channel_id,
                set_custom_mining_job.clone(),
            );
            SetCustomMiningJobSuccess {
                channel_id: set_custom_mining_job.channel_id,
                request_id: set_custom_mining_job.request_id,
                job_id: self.inner.job_ids.next(),
            }
        } else {
            todo!()
        }
    }

    fn check_set_custom_mining_job(
        &self,
        _set_custom_mining_job: &SetCustomMiningJob<'static>,
    ) -> bool {
        true
    }

    /// Get extended channel ids
    pub fn get_extended_channels_ids(&self) -> Vec<u32> {
        self.inner.extended_channels.keys().copied().collect()
    }

    pub fn get_shares_per_minute(&self) -> f32 {
        self.inner.share_per_min
    }

    /// Update coinbase outputs
    pub fn update_pool_outputs(&mut self, outs: Vec<TxOut>) {
        self.pool_coinbase_outputs = outs;
    }

    /// Calls [`ChannelFactory::update_target_for_channel`]
    /// Set a particular downstream channel target.
    pub fn update_target_for_channel(
        &mut self,
        channel_id: u32,
        new_target: Target,
    ) -> Option<bool> {
        self.inner.update_target_for_channel(channel_id, new_target)
    }

    /// Set the target for this channel. This is the upstream target.
    pub fn set_target(&mut self, new_target: &mut Target) {
        self.inner.kind.set_target(new_target);
    }
}

/// Used by proxies that want to open extended channels with upstream. If the proxy has job
/// declaration capabilities, we set the job creator and the coinbase outs.
#[derive(Debug)]
pub struct ProxyExtendedChannelFactory {
    inner: ChannelFactory,
    job_creator: Option<JobsCreators>,
    pool_coinbase_outputs: Option<Vec<TxOut>>,
    // Id assigned to the extended channel by upstream
    extended_channel_id: u32,
}

impl ProxyExtendedChannelFactory {
    /// Constructor
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        ids: Arc<Mutex<GroupId>>,
        extranonces: ExtendedExtranonce,
        job_creator: Option<JobsCreators>,
        share_per_min: f32,
        kind: ExtendedChannelKind,
        pool_coinbase_outputs: Option<Vec<TxOut>>,
        extended_channel_id: u32,
    ) -> Self {
        match &kind {
            ExtendedChannelKind::Proxy { .. } => {
                if job_creator.is_some() {
                    panic!("Channel factory of kind Proxy can not be initialized with a JobCreators");
                };
            },
            ExtendedChannelKind::ProxyJd { .. } => {
                if job_creator.is_none() {
                    panic!("Channel factory of kind ProxyJd must be initialized with a JobCreators");
                };
            }
            ExtendedChannelKind::Pool => panic!("Try to construct an ProxyExtendedChannelFactory with pool kind, kind must be Proxy or ProxyJd"),
        };
        let inner = ChannelFactory {
            ids,
            extended_channels: HashMap::with_hasher(BuildNoHashHasher::default()),
            extranonces,
            share_per_min,
            future_jobs: Vec::new(),
            last_prev_hash: None,
            last_prev_hash_: None,
            last_valid_job: None,
            kind,
            job_ids: Id::new(),
            channel_to_group_id: HashMap::with_hasher(BuildNoHashHasher::default()),
            future_templates: HashMap::with_hasher(BuildNoHashHasher::default()),
        };
        ProxyExtendedChannelFactory {
            inner,
            job_creator,
            pool_coinbase_outputs,
            extended_channel_id,
        }
    }

    /// Calls [`ChannelFactory::new_extended_channel`]
    pub fn new_extended_channel(
        &mut self,
        request_id: u32,
        hash_rate: f32,
        min_extranonce_size: u16,
    ) -> Result<Vec<Mining>, Error> {
        self.inner
            .new_extended_channel(request_id, hash_rate, min_extranonce_size)
    }

    /// Called only when a new prev hash is received by a Template Provider when job declaration is
    /// used. It matches the message with a `job_id`, creates a new custom job, and calls
    /// [`ChannelFactory::on_new_prev_hash`]
    pub fn on_new_prev_hash_from_tp(
        &mut self,
        m: &SetNewPrevHashFromTp<'static>,
    ) -> Result<Option<(PartialSetCustomMiningJob, u32)>, Error> {
        if let Some(job_creator) = self.job_creator.as_mut() {
            let job_id = job_creator.on_new_prev_hash(m).unwrap_or(0);
            let new_prev_hash = StagedPhash {
                job_id,
                prev_hash: m.prev_hash.clone(),
                min_ntime: m.header_timestamp,
                nbits: m.n_bits,
            };
            let mut custom_job = None;
            if let Some(template) = self.inner.future_templates.get(&job_id) {
                custom_job = Some((
                    PartialSetCustomMiningJob {
                        version: template.version,
                        prev_hash: new_prev_hash.prev_hash.clone(),
                        min_ntime: new_prev_hash.min_ntime,
                        nbits: new_prev_hash.nbits,
                        coinbase_tx_version: template.coinbase_tx_version,
                        coinbase_prefix: template.coinbase_prefix.clone(),
                        coinbase_tx_input_n_sequence: template.coinbase_tx_input_sequence,
                        coinbase_tx_value_remaining: template.coinbase_tx_value_remaining,
                        coinbase_tx_outputs: template.coinbase_tx_outputs.clone(),
                        coinbase_tx_locktime: template.coinbase_tx_locktime,
                        merkle_path: template.merkle_path.clone(),
                        future_job: template.future_template,
                    },
                    job_id,
                ));
            }
            self.inner.future_templates = HashMap::with_hasher(BuildNoHashHasher::default());
            self.inner.on_new_prev_hash(new_prev_hash)?;
            Ok(custom_job)
        } else {
            panic!("A channel factory without job creator do not have declaration capabilities")
        }
    }

    /// Called only when a new template is received by a Template Provider when job declaration is
    /// used. It creates a new custom job and calls
    /// [`ChannelFactory::on_new_extended_mining_job`]
    #[allow(clippy::type_complexity)]
    pub fn on_new_template(
        &mut self,
        m: &mut NewTemplate<'static>,
    ) -> Result<
        (
            // downstream job_id -> downstream message (newextjob or newjob)
            HashMap<u32, Mining<'static>, BuildNoHashHasher<u32>>,
            // PartialSetCustomMiningJob to send to the pool
            Option<PartialSetCustomMiningJob>,
            // job_id registered in the channel, the one that SetNewPrevHash refer to (upstsream
            // job id)
            u32,
        ),
        Error,
    > {
        if let (Some(job_creator), Some(pool_coinbase_outputs)) = (
            self.job_creator.as_mut(),
            self.pool_coinbase_outputs.as_mut(),
        ) {
            let new_job = job_creator.on_new_template(m, true, pool_coinbase_outputs.clone())?;
            let id = new_job.job_id;
            if !new_job.is_future() && self.inner.last_prev_hash.is_some() {
                let prev_hash = self.last_prev_hash().unwrap();
                let min_ntime = self.last_min_ntime().unwrap();
                let nbits = self.last_nbits().unwrap();
                let custom_mining_job = PartialSetCustomMiningJob {
                    version: m.version,
                    prev_hash,
                    min_ntime,
                    nbits,
                    coinbase_tx_version: m.coinbase_tx_version,
                    coinbase_prefix: m.coinbase_prefix.clone(),
                    coinbase_tx_input_n_sequence: m.coinbase_tx_input_sequence,
                    coinbase_tx_value_remaining: m.coinbase_tx_value_remaining,
                    coinbase_tx_outputs: m.coinbase_tx_outputs.clone(),
                    coinbase_tx_locktime: m.coinbase_tx_locktime,
                    merkle_path: m.merkle_path.clone(),
                    future_job: m.future_template,
                };
                return Ok((
                    self.inner.on_new_extended_mining_job(new_job)?,
                    Some(custom_mining_job),
                    id,
                ));
            } else if new_job.is_future() {
                self.inner
                    .future_templates
                    .insert(new_job.job_id, m.clone());
            }
            Ok((self.inner.on_new_extended_mining_job(new_job)?, None, id))
        } else {
            panic!("Either channel factory has no job creator or pool_coinbase_outputs are not yet set")
        }
    }

    /// Called when a `SubmitSharesStandard` message is received from the downstream. We check the
    /// shares against the channel's respective target and return `OnNewShare` to let us know if
    /// and where the shares should be relayed
    pub fn on_submit_shares_extended(
        &mut self,
        m: SubmitSharesExtended<'static>,
    ) -> Result<OnNewShare, Error> {
        let merkle_path = self
            .inner
            .last_valid_job
            .as_ref()
            .ok_or(Error::ShareDoNotMatchAnyJob)?
            .0
            .merkle_path
            .to_vec();

        let referenced_job = self
            .inner
            .last_valid_job
            .clone()
            .ok_or(Error::ShareDoNotMatchAnyJob)?
            .0;

        if referenced_job.job_id != m.job_id {
            let error = SubmitSharesError {
                channel_id: m.channel_id,
                sequence_number: m.sequence_number,
                // Infallible unwrap we already know the len of the error code (is a
                // static string)
                error_code: SubmitSharesError::invalid_job_id_error_code()
                    .to_string()
                    .try_into()
                    .unwrap(),
            };
            return Ok(OnNewShare::SendErrorDownstream(error));
        }

        if let Some(job_creator) = self.job_creator.as_mut() {
            let template_id = job_creator
                .get_template_id_from_job(referenced_job.job_id)
                .ok_or(Error::NoTemplateForId)?;
            let bitcoin_target = job_creator.last_target();
            let prev_blockhash = self
                .inner
                .last_prev_hash_
                .ok_or(Error::ShareDoNotMatchAnyJob)?;
            let bits = self
                .inner
                .last_prev_hash
                .as_ref()
                .ok_or(Error::ShareDoNotMatchAnyJob)?
                .0
                .nbits;
            self.inner.check_target(
                Share::Extended(m),
                bitcoin_target,
                Some(template_id),
                self.extended_channel_id,
                merkle_path,
                referenced_job.coinbase_tx_prefix.as_ref(),
                referenced_job.coinbase_tx_suffix.as_ref(),
                prev_blockhash,
                bits,
            )
        } else {
            let bitcoin_target = [0; 32];
            // if there is not job_creator is not proxy duty to check if target is below or above
            // bitcoin target so we set bitcoin_target = 0.
            let prev_blockhash = self
                .inner
                .last_prev_hash_
                .ok_or(Error::ShareDoNotMatchAnyJob)?;
            let bits = self
                .inner
                .last_prev_hash
                .as_ref()
                .ok_or(Error::ShareDoNotMatchAnyJob)?
                .0
                .nbits;
            self.inner.check_target(
                Share::Extended(m),
                bitcoin_target.into(),
                None,
                self.extended_channel_id,
                merkle_path,
                referenced_job.coinbase_tx_prefix.as_ref(),
                referenced_job.coinbase_tx_suffix.as_ref(),
                prev_blockhash,
                bits,
            )
        }
    }

    /// Called when a `SubmitSharesStandard` message is received from the Downstream. We check the
    /// shares against the channel's respective target and return `OnNewShare` to let us know if
    /// and where the shares should be relayed
    pub fn on_submit_shares_standard(
        &mut self,
        m: SubmitSharesStandard,
    ) -> Result<OnNewShare, Error> {
        let merkle_path = self
            .inner
            .last_valid_job
            .as_ref()
            .ok_or(Error::ShareDoNotMatchAnyJob)?
            .0
            .merkle_path
            .to_vec();
        let referenced_job = self
            .inner
            .last_valid_job
            .clone()
            .ok_or(Error::ShareDoNotMatchAnyJob)?
            .0;
        match self.inner.channel_to_group_id.get(&m.channel_id) {
            Some(g_id) => {
                if let Some(job_creator) = self.job_creator.as_mut() {
                    let template_id = job_creator
                        .get_template_id_from_job(
                            self.inner.last_valid_job.as_ref().unwrap().0.job_id,
                        )
                        .ok_or(Error::NoTemplateForId)?;
                    let bitcoin_target = job_creator.last_target();
                    let prev_blockhash = self
                        .inner
                        .last_prev_hash_
                        .ok_or(Error::ShareDoNotMatchAnyJob)?;
                    let bits = self
                        .inner
                        .last_prev_hash
                        .as_ref()
                        .ok_or(Error::ShareDoNotMatchAnyJob)?
                        .0
                        .nbits;
                    self.inner.check_target(
                        Share::Standard((m, *g_id)),
                        bitcoin_target,
                        Some(template_id),
                        self.extended_channel_id,
                        merkle_path,
                        referenced_job.coinbase_tx_prefix.as_ref(),
                        referenced_job.coinbase_tx_suffix.as_ref(),
                        prev_blockhash,
                        bits,
                    )
                } else {
                    let bitcoin_target = [0; 32];
                    let prev_blockhash = self
                        .inner
                        .last_prev_hash_
                        .ok_or(Error::ShareDoNotMatchAnyJob)?;
                    let bits = self
                        .inner
                        .last_prev_hash
                        .as_ref()
                        .ok_or(Error::ShareDoNotMatchAnyJob)?
                        .0
                        .nbits;
                    // if there is not job_creator is not proxy duty to check if target is below or
                    // above bitcoin target so we set bitcoin_target = 0.
                    self.inner.check_target(
                        Share::Standard((m, *g_id)),
                        bitcoin_target.into(),
                        None,
                        self.extended_channel_id,
                        merkle_path,
                        referenced_job.coinbase_tx_prefix.as_ref(),
                        referenced_job.coinbase_tx_suffix.as_ref(),
                        prev_blockhash,
                        bits,
                    )
                }
            }
            None => {
                let err = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: SubmitSharesError::invalid_channel_error_code()
                        .to_string()
                        .try_into()
                        .unwrap(),
                };
                Ok(OnNewShare::SendErrorDownstream(err))
            }
        }
    }

    /// Calls [`ChannelFactory::on_new_prev_hash`]
    pub fn on_new_prev_hash(&mut self, m: SetNewPrevHash<'static>) -> Result<(), Error> {
        self.inner.on_new_prev_hash(StagedPhash {
            job_id: m.job_id,
            prev_hash: m.prev_hash.clone().into_static(),
            min_ntime: m.min_ntime,
            nbits: m.nbits,
        })
    }

    /// Calls [`ChannelFactory::on_new_extended_mining_job`]
    pub fn on_new_extended_mining_job(
        &mut self,
        m: NewExtendedMiningJob<'static>,
    ) -> Result<HashMap<u32, Mining<'static>, BuildNoHashHasher<u32>>, Error> {
        self.inner.on_new_extended_mining_job(m)
    }

    /// Set new target
    pub fn set_target(&mut self, new_target: &mut Target) {
        self.inner.kind.set_target(new_target);
    }

    /// Get last valid job version
    pub fn last_valid_job_version(&self) -> Option<u32> {
        self.inner.last_valid_job.as_ref().map(|j| j.0.version)
    }

    /// Returns the full extranonce, extranonce1 (static for channel) + extranonce2 (miner nonce
    /// space)
    pub fn extranonce_from_downstream_extranonce(
        &self,
        ext: mining_sv2::Extranonce,
    ) -> Option<mining_sv2::Extranonce> {
        self.inner
            .extranonces
            .extranonce_from_downstream_extranonce(ext)
            .ok()
    }

    /// Returns the most recent prev hash
    pub fn last_prev_hash(&self) -> Option<binary_sv2::U256<'static>> {
        self.inner
            .last_prev_hash
            .as_ref()
            .map(|f| f.0.prev_hash.clone())
    }

    /// Get last min ntime
    pub fn last_min_ntime(&self) -> Option<u32> {
        self.inner.last_prev_hash.as_ref().map(|f| f.0.min_ntime)
    }

    /// Get last nbits
    pub fn last_nbits(&self) -> Option<u32> {
        self.inner.last_prev_hash.as_ref().map(|f| f.0.nbits)
    }

    /// Get extranonce_size
    pub fn extranonce_size(&self) -> usize {
        self.inner.extranonces.get_len()
    }

    /// Get extranonce_2 size
    pub fn channel_extranonce2_size(&self) -> usize {
        self.inner.extranonces.get_len() - self.inner.extranonces.get_range0_len()
    }

    // Only used when the proxy is using Job Declaration
    /// Updates pool outputs
    pub fn update_pool_outputs(&mut self, outs: Vec<TxOut>) {
        self.pool_coinbase_outputs = Some(outs);
    }

    /// Get this channel id
    pub fn get_this_channel_id(&self) -> u32 {
        self.extended_channel_id
    }

    /// Returns the extranonce1 len of the upstream. For a proxy, this would
    /// be the extranonce_prefix len
    pub fn get_upstream_extranonce1_len(&self) -> usize {
        self.inner.extranonces.get_range0_len()
    }

    /// Calls [`ChannelFactory::update_target_for_channel`]
    pub fn update_target_for_channel(
        &mut self,
        channel_id: u32,
        new_target: Target,
    ) -> Option<bool> {
        self.inner.update_target_for_channel(channel_id, new_target)
    }
}

/// Used by proxies for tracking upstream targets.
#[derive(Debug, Clone)]
pub enum ExtendedChannelKind {
    Proxy { upstream_target: Target },
    ProxyJd { upstream_target: Target },
    Pool,
}
impl ExtendedChannelKind {
    /// Set target
    pub fn set_target(&mut self, new_target: &mut Target) {
        match self {
            ExtendedChannelKind::Proxy { upstream_target }
            | ExtendedChannelKind::ProxyJd { upstream_target } => {
                std::mem::swap(upstream_target, new_target)
            }
            ExtendedChannelKind::Pool => warn!("Try to set upstream target for a pool"),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channel_logic/mod.rs">
//! # Channel Logic
//!
//! A module for managing channels on applications.
//!
//! Divided in two submodules:
//! - [`channel_factory`]
//! - [`proxy_group_channel`]

pub mod channel_factory;

use mining_sv2::{NewExtendedMiningJob, NewMiningJob};
use std::convert::TryInto;

/// Convert extended to standard job by calculating the merkle root
pub fn extended_to_standard_job<'a>(
    extended: &NewExtendedMiningJob,
    coinbase_script: &[u8],
    channel_id: u32,
    job_id: Option<u32>,
) -> Option<NewMiningJob<'a>> {
    let merkle_root = crate::utils::merkle_root_from_path(
        extended.coinbase_tx_prefix.inner_as_ref(),
        extended.coinbase_tx_suffix.inner_as_ref(),
        coinbase_script,
        &extended.merkle_path.inner_as_ref(),
    );

    Some(NewMiningJob {
        channel_id,
        job_id: job_id.unwrap_or(extended.job_id),
        min_ntime: extended.min_ntime.clone().into_static(),
        version: extended.version,
        merkle_root: merkle_root?.try_into().ok()?,
    })
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/chain_tip.rs">
//! # Chain Tip
use codec_sv2::binary_sv2::U256;

/// An abstraction over the chain tip, carrying information from `SetNewPrevHash` messages.
///
/// Used while creating non-future jobs.
#[derive(Debug, Clone)]
pub struct ChainTip {
    prev_hash: U256<'static>,
    nbits: u32,
    min_ntime: u32,
}

impl ChainTip {
    pub fn new(prev_hash: U256<'static>, nbits: u32, min_ntime: u32) -> Self {
        Self {
            prev_hash,
            nbits,
            min_ntime,
        }
    }

    pub fn prev_hash(&self) -> U256<'static> {
        self.prev_hash.clone()
    }

    pub fn nbits(&self) -> u32 {
        self.nbits
    }

    pub fn min_ntime(&self) -> u32 {
        self.min_ntime
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/error.rs">
#[derive(Debug)]
pub enum ExtendedChannelError {
    NewExtranoncePrefixTooLarge,
    JobIdNotFound,
}

#[derive(Debug)]
pub enum StandardChannelError {
    JobIdNotFound,
    NewExtranoncePrefixTooLarge,
}

#[derive(Debug)]
pub enum GroupChannelError {
    JobIdNotFound,
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/extended.rs">
//! Mining Client abstraction over the state of a Sv2 Extended Channel

use crate::{
    channels::{
        chain_tip::ChainTip,
        client::{
            error::ExtendedChannelError,
            share_accounting::{ShareAccounting, ShareValidationError, ShareValidationResult},
        },
    },
    utils::{bytes_to_hex, merkle_root_from_path, target_to_difficulty, u256_to_block_hash},
};
use bitcoin::{
    blockdata::block::{Header, Version},
    hashes::sha256d::Hash,
    CompactTarget, Target as BitcoinTarget,
};
use codec_sv2::binary_sv2::{self, Sv2Option};
use mining_sv2::{
    NewExtendedMiningJob, SetNewPrevHash as SetNewPrevHashMp, SubmitSharesExtended, Target,
    MAX_EXTRANONCE_LEN,
};
use std::{collections::HashMap, convert::TryInto};
use tracing::debug;

// ExtendedJob is a tuple of:
// - the NewExtendedMiningJob message
// - the extranonce_prefix associated with the channel at the time of job creation
pub type ExtendedJob<'a> = (NewExtendedMiningJob<'a>, Vec<u8>);

/// Mining Client abstraction over the state of a Sv2 Extended Channel.
///
/// It keeps track of:
/// - the channel's unique `channel_id`
/// - the channel's `user_identity`
/// - the channel's unique `extranonce_prefix`
/// - the channel's rollable extranonce size
/// - the channel's target
/// - the channel's nominal hashrate
/// - the channel's version rolling
/// - the channel's future jobs (indexed by `job_id`, to be activated upon receipt of a
///   `SetNewPrevHash` message)
/// - the channel's active job
/// - the channel's past jobs (which were active jobs under the current chain tip, indexed by
///   `job_id`)
/// - the channel's stale jobs (which were past and active jobs under the previous chain tip,
///   indexed by `job_id`)
/// - the channel's share accounting (as seen by the client)
/// - the channel's chain tip
#[derive(Clone, Debug)]
pub struct ExtendedChannel<'a> {
    channel_id: u32,
    user_identity: String,
    extranonce_prefix: Vec<u8>,
    rollable_extranonce_size: u16,
    target: Target, // todo: try to use Target from rust-bitcoin
    nominal_hashrate: f32,
    version_rolling: bool,
    // future jobs are indexed with job_id (u32)
    future_jobs: HashMap<u32, ExtendedJob<'a>>,
    active_job: Option<ExtendedJob<'a>>,
    // past jobs are indexed with job_id (u32)
    past_jobs: HashMap<u32, ExtendedJob<'a>>,
    // stale jobs are indexed with job_id (u32)
    stale_jobs: HashMap<u32, ExtendedJob<'a>>,
    share_accounting: ShareAccounting,
    chain_tip: Option<ChainTip>,
}

impl<'a> ExtendedChannel<'a> {
    pub fn new(
        channel_id: u32,
        user_identity: String,
        extranonce_prefix: Vec<u8>,
        target: Target,
        nominal_hashrate: f32,
        version_rolling: bool,
        rollable_extranonce_size: u16,
    ) -> Self {
        Self {
            channel_id,
            user_identity,
            extranonce_prefix,
            rollable_extranonce_size,
            target,
            nominal_hashrate,
            version_rolling,
            future_jobs: HashMap::new(),
            active_job: None,
            past_jobs: HashMap::new(),
            stale_jobs: HashMap::new(),
            share_accounting: ShareAccounting::new(),
            chain_tip: None,
        }
    }

    pub fn get_channel_id(&self) -> u32 {
        self.channel_id
    }

    pub fn get_user_identity(&self) -> &String {
        &self.user_identity
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn is_version_rolling(&self) -> bool {
        self.version_rolling
    }

    pub fn get_chain_tip(&self) -> Option<&ChainTip> {
        self.chain_tip.as_ref()
    }

    /// Sets the extranonce prefix.
    ///
    /// Note: after this, all new jobs will be associated with the new extranonce prefix.
    /// Jobs created before this call will remain associated with the previous extranonce prefix,
    /// and share validation will be done accordingly.
    pub fn set_extranonce_prefix(
        &mut self,
        new_extranonce_prefix: Vec<u8>,
    ) -> Result<(), ExtendedChannelError> {
        let new_rollable_extranonce_size =
            MAX_EXTRANONCE_LEN as u16 - new_extranonce_prefix.len() as u16;

        // we return an error if the new extranonce_prefix would violate
        // min_rollable_extranonce_size that was already established with the client when the
        // channel was created
        if new_rollable_extranonce_size < self.rollable_extranonce_size {
            return Err(ExtendedChannelError::NewExtranoncePrefixTooLarge);
        }

        self.extranonce_prefix = new_extranonce_prefix;
        self.rollable_extranonce_size = new_rollable_extranonce_size;

        Ok(())
    }

    pub fn get_rollable_extranonce_size(&self) -> u16 {
        self.rollable_extranonce_size
    }

    pub fn get_target(&self) -> &Target {
        &self.target
    }

    pub fn set_target(&mut self, new_target: Target) {
        self.target = new_target;
    }

    pub fn get_nominal_hashrate(&self) -> f32 {
        self.nominal_hashrate
    }

    pub fn get_active_job(&self) -> Option<&ExtendedJob<'a>> {
        self.active_job.as_ref()
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        &self.future_jobs
    }

    pub fn get_past_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        &self.past_jobs
    }

    pub fn get_stale_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        &self.stale_jobs
    }

    pub fn get_share_accounting(&self) -> &ShareAccounting {
        &self.share_accounting
    }

    /// Called when a `NewExtendedMiningJob` message is received from upstream.
    pub fn on_new_extended_mining_job(
        &mut self,
        new_extended_mining_job: NewExtendedMiningJob<'a>,
    ) {
        match new_extended_mining_job.min_ntime.clone().into_inner() {
            Some(_min_ntime) => {
                if let Some(active_job) = self.active_job.clone() {
                    self.past_jobs.insert(active_job.0.job_id, active_job);
                }
                self.active_job = Some((new_extended_mining_job, self.extranonce_prefix.clone()));
            }
            None => {
                self.future_jobs.insert(
                    new_extended_mining_job.job_id,
                    (new_extended_mining_job, self.extranonce_prefix.clone()),
                );
            }
        }
    }

    /// Called when a `SetNewPrevHash` message is received from upstream.
    ///
    /// If the job_id addressed in the `SetNewPrevHash` is not a future job,
    /// returns an error.
    ///
    /// If the job_id addressed in the `SetNewPrevHash` is a future job,
    /// it is "activated" and set as the active job.
    ///
    /// All past jobs are marked as stale, so that shares are not propagated.
    ///
    /// The chain tip information is not kept in the channel state.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHashMp<'a>,
    ) -> Result<(), ExtendedChannelError> {
        match self.future_jobs.remove(&set_new_prev_hash.job_id) {
            Some(mut activated_job) => {
                activated_job.0.min_ntime = Sv2Option::new(Some(set_new_prev_hash.min_ntime));
                self.active_job = Some(activated_job);
            }
            None => {
                return Err(ExtendedChannelError::JobIdNotFound);
            }
        }

        // all other future jobs are now useless
        self.future_jobs.clear();

        // mark all past jobs as stale, so that shares are not propagated
        self.stale_jobs = self.past_jobs.clone();

        // clear past jobs, as we're no longer going to propagate shares for them
        self.past_jobs.clear();

        // clear seen shares, as shares for past chain tip will be rejected as stale
        self.share_accounting.flush_seen_shares();

        let set_new_prev_hash_static = set_new_prev_hash.into_static();
        let new_chain_tip = ChainTip::new(
            set_new_prev_hash_static.prev_hash,
            set_new_prev_hash_static.nbits,
            set_new_prev_hash_static.min_ntime,
        );
        self.chain_tip = Some(new_chain_tip);

        Ok(())
    }

    /// Validates a share, to be used before submission upstream.
    ///
    /// Updates the channel state with the result of the share validation.
    ///
    /// - Allows the mining client to avoid propagating stale, duplicate or low-diff shares.
    /// - Allows the mining client to know whether a block was found on some share.
    /// - Allows the mining client to keep a local version of the share accounting for comparison
    ///   with the acknowledgements coming from the upstream server.
    pub fn validate_share(
        &mut self,
        share: SubmitSharesExtended,
    ) -> Result<ShareValidationResult, ShareValidationError> {
        let job_id = share.job_id;

        // check if job_id is active job
        let is_active_job = self
            .active_job
            .as_ref()
            .is_some_and(|job| job.0.job_id == job_id);

        // check if job_id is past job
        let is_past_job = self.past_jobs.contains_key(&job_id);

        // check if job_id is stale job
        let is_stale_job = self.stale_jobs.contains_key(&job_id);

        if is_stale_job {
            return Err(ShareValidationError::Stale);
        }

        let job = if is_active_job {
            self.active_job.as_ref().expect("active job must exist")
        } else if is_past_job {
            self.past_jobs.get(&job_id).expect("past job must exist")
        } else {
            return Err(ShareValidationError::InvalidJobId);
        };

        let mut full_extranonce = vec![];
        full_extranonce.extend_from_slice(job.1.as_slice());
        full_extranonce.extend_from_slice(share.extranonce.inner_as_ref());

        // calculate the merkle root from:
        // - job coinbase_tx_prefix
        // - full extranonce
        // - job coinbase_tx_suffix
        // - job merkle_path
        let merkle_root: [u8; 32] = merkle_root_from_path(
            job.0.coinbase_tx_prefix.inner_as_ref(),
            job.0.coinbase_tx_suffix.inner_as_ref(),
            full_extranonce.as_ref(),
            &job.0.merkle_path.inner_as_ref(),
        )
        .ok_or(ShareValidationError::Invalid)?
        .try_into()
        .expect("merkle root must be 32 bytes");

        let chain_tip = self
            .chain_tip
            .as_ref()
            .ok_or(ShareValidationError::NoChainTip)?;

        let prev_hash = chain_tip.prev_hash();
        let nbits: CompactTarget = CompactTarget::from_consensus(chain_tip.nbits());

        // validate when version rolling is not allowed
        if !job.0.version_rolling_allowed {
            // If version rolling is not allowed, ensure bits 13-28 are 0
            // This is done by checking if the version & 0x1fffe000 == 0
            // ref: https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki
            if (share.version & 0x1fffe000) != 0 {
                return Err(ShareValidationError::VersionRollingNotAllowed);
            }
        }

        // create the header for validation
        let header = Header {
            version: Version::from_consensus(share.version as i32),
            prev_blockhash: u256_to_block_hash(prev_hash.clone()),
            merkle_root: (*Hash::from_bytes_ref(&merkle_root)).into(),
            time: share.ntime,
            bits: nbits,
            nonce: share.nonce,
        };

        // convert the header hash to a target type for easy comparison
        let hash = header.block_hash();
        let raw_hash: [u8; 32] = *hash.to_raw_hash().as_ref();
        let hash_as_target: Target = raw_hash.into();
        let hash_as_diff = target_to_difficulty(hash_as_target.clone());

        let network_target = BitcoinTarget::from_compact(nbits);

        // print hash_as_target and self.target as human readable hex
        let hash_as_u256: binary_sv2::U256 = hash_as_target.clone().into();
        let mut hash_bytes = hash_as_u256.to_vec();
        hash_bytes.reverse(); // Convert to big-endian for display
        let target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "share validation \nshare:\t\t{}\nchannel target:\t{}\nnetwork target:\t{}",
            bytes_to_hex(&hash_bytes),
            bytes_to_hex(&target_bytes),
            format!("{:x}", network_target)
        );

        // check if a block was found
        if network_target.is_met_by(hash) {
            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );
            return Ok(ShareValidationResult::BlockFound);
        }

        // check if the share hash meets the channel target
        if hash_as_target < self.target {
            if self.share_accounting.is_share_seen(hash.to_raw_hash()) {
                return Err(ShareValidationError::DuplicateShare);
            }

            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            // update the best diff
            self.share_accounting.update_best_diff(hash_as_diff);

            return Ok(ShareValidationResult::Valid);
        }

        Err(ShareValidationError::DoesNotMeetTarget)
    }
}

#[cfg(test)]
mod tests {
    use crate::channels::client::{
        extended::ExtendedChannel,
        share_accounting::{ShareValidationError, ShareValidationResult},
    };
    use codec_sv2::binary_sv2::Sv2Option;
    use mining_sv2::{
        NewExtendedMiningJob, SetNewPrevHash as SetNewPrevHashMp, SubmitSharesExtended,
        MAX_EXTRANONCE_LEN,
    };
    use std::convert::TryInto;

    #[test]
    fn test_future_job_activation_flow() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;
        let version_rolling = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            target,
            nominal_hashrate,
            version_rolling,
            rollable_extranonce_size,
        );

        let future_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        channel.on_new_extended_mining_job(future_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 1);
        assert_eq!(channel.get_active_job(), None);
        assert_eq!(channel.get_past_jobs().len(), 0);

        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            nbits: 503543726,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        assert!(channel.get_future_jobs().is_empty());

        let mut previously_future_job = future_job.clone();
        previously_future_job.min_ntime = Sv2Option::new(Some(ntime));

        assert_eq!(
            channel.get_active_job(),
            Some(&(previously_future_job, extranonce_prefix))
        );
    }

    #[test]
    fn test_past_jobs_flow() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;
        let version_rolling = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            target,
            nominal_hashrate,
            version_rolling,
            rollable_extranonce_size,
        );

        let ntime: u32 = 1746839905;
        let active_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(Some(ntime)),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        channel.on_new_extended_mining_job(active_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 0);
        assert_eq!(
            channel.get_active_job(),
            Some(&(active_job.clone(), extranonce_prefix.clone()))
        );
        assert_eq!(channel.get_past_jobs().len(), 0);

        let mut new_active_job = active_job.clone();
        new_active_job.job_id = 2;
        channel.on_new_extended_mining_job(new_active_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 0);
        assert_eq!(
            channel.get_active_job(),
            Some(&(new_active_job, extranonce_prefix))
        );
        assert_eq!(channel.get_past_jobs().len(), 1);
    }

    #[test]
    fn test_share_validation_block_found() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;
        let version_rolling = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            target,
            nominal_hashrate,
            version_rolling,
            rollable_extranonce_size,
        );

        let future_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        channel.on_new_extended_mining_job(future_job.clone());

        // network target: 7fffff0000000000000000000000000000000000000000000000000000000000
        let nbits = 545259519;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash 38b6a7d5b2cae08bc6c8b4b4fc13ff129ae0a07309240108f46ddf48c498b120
        // which satisfies network target
        // 7fffff0000000000000000000000000000000000000000000000000000000000
        let share_valid_block = SubmitSharesExtended {
            channel_id,
            sequence_number: 0,
            job_id: 1,
            nonce: 741057,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(share_valid_block);

        assert!(matches!(res, Ok(ShareValidationResult::BlockFound)));
    }

    #[test]
    fn test_share_validation_does_not_meet_target() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        // channel target: 0000ffff00000000000000000000000000000000000000000000000000000000
        let target = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0xff, 0xff, 0x00, 0x00,
        ]
        .into();
        let nominal_hashrate = 1.0;
        let version_rolling = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            target,
            nominal_hashrate,
            version_rolling,
            rollable_extranonce_size,
        );

        let future_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        channel.on_new_extended_mining_job(future_job.clone());

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let nbits = 453040064;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash bc1f25bceec05b1cc60fd0f0a3ede685efbb00d2a7d39c879d2c187b2af3538d
        // which does not meet the channel target
        // 0000ffff00000000000000000000000000000000000000000000000000000000
        let share_low_diff = SubmitSharesExtended {
            channel_id,
            sequence_number: 0,
            job_id: 1,
            nonce: 741057,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(share_low_diff);

        assert!(matches!(
            res.unwrap_err(),
            ShareValidationError::DoesNotMeetTarget
        ));
    }

    #[test]
    fn test_share_validation_valid_share() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        // channel target: 0000ffff00000000000000000000000000000000000000000000000000000000
        let target = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0xff, 0xff, 0x00, 0x00,
        ]
        .into();
        let nominal_hashrate = 1.0;
        let version_rolling = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            target,
            nominal_hashrate,
            version_rolling,
            rollable_extranonce_size,
        );

        let future_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        channel.on_new_extended_mining_job(future_job.clone());

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let nbits: u32 = 453040064;
        let ntime: u32 = 1746839905;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash 0000d769e5ab58b7309b7507834cb0bc60749315c93015e8bba97b9752ced5b7
        // which does meet the channel target
        // 0000ffff00000000000000000000000000000000000000000000000000000000
        let valid_share = SubmitSharesExtended {
            channel_id,
            sequence_number: 0,
            job_id: 1,
            nonce: 2426,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(valid_share);

        assert!(matches!(res, Ok(ShareValidationResult::Valid)));

        // try to cheat by re-submitting the same share
        // with a different sequence number
        let repeated_share = SubmitSharesExtended {
            channel_id,
            sequence_number: 1,
            job_id: 1,
            nonce: 2426,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(repeated_share);

        assert!(matches!(
            res.unwrap_err(),
            ShareValidationError::DuplicateShare
        ));
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/group.rs">
//! Abstraction over the state of a Sv2 Group Channel, as seen by a Mining Client

use crate::channels::client::error::GroupChannelError;

use std::collections::{HashMap, HashSet};

use mining_sv2::{NewExtendedMiningJob, SetNewPrevHash as SetNewPrevHashMp};

/// Mining Client abstraction over the state of a Sv2 Group Channel.
///
/// It keeps track of:
/// - the group channel's unique `group_channel_id`
/// - the group channel's `standard_channel_ids` (indexed by `channel_id`)
/// - the group channel's future jobs (indexed by `job_id`, to be activated upon receipt of a
///   `SetNewPrevHash` message)
/// - the group channel's active job
///
/// Since share validation happens at the Standard Channel level, we don't really keep track of:
/// - the group channel's past jobs
/// - the group channel's stale jobs
/// - the group channel's share validation state
#[derive(Debug, Clone)]
pub struct GroupChannel<'a> {
    group_channel_id: u32,
    standard_channel_ids: HashSet<u32>,
    // future jobs are indexed with job_id (u32)
    future_jobs: HashMap<u32, NewExtendedMiningJob<'a>>,
    active_job: Option<NewExtendedMiningJob<'a>>,
}

impl<'a> GroupChannel<'a> {
    pub fn new(group_channel_id: u32) -> Self {
        Self {
            group_channel_id,
            standard_channel_ids: HashSet::new(),
            future_jobs: HashMap::new(),
            active_job: None,
        }
    }

    pub fn add_standard_channel_id(&mut self, standard_channel_id: u32) {
        self.standard_channel_ids.insert(standard_channel_id);
    }

    pub fn remove_standard_channel_id(&mut self, standard_channel_id: u32) {
        self.standard_channel_ids.remove(&standard_channel_id);
    }

    pub fn get_group_channel_id(&self) -> u32 {
        self.group_channel_id
    }

    pub fn get_standard_channel_ids(&self) -> &HashSet<u32> {
        &self.standard_channel_ids
    }

    pub fn get_active_job(&self) -> Option<&NewExtendedMiningJob<'a>> {
        self.active_job.as_ref()
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, NewExtendedMiningJob<'a>> {
        &self.future_jobs
    }

    /// Called when a `NewExtendedMiningJob` message is received from upstream.
    ///
    /// If the job is a future job, it is added to the `future_jobs` map.
    /// If the job is an active job, it is set as the active job.
    pub fn on_new_extended_mining_job(
        &mut self,
        new_extended_mining_job: NewExtendedMiningJob<'a>,
    ) {
        match new_extended_mining_job.min_ntime.clone().into_inner() {
            Some(_min_ntime) => {
                self.active_job = Some(new_extended_mining_job);
            }
            None => {
                self.future_jobs
                    .insert(new_extended_mining_job.job_id, new_extended_mining_job);
            }
        }
    }

    /// Called when a `SetNewPrevHash` message is received from upstream.
    ///
    /// If there is some future job matching the `job_id` that `SetNewPrevHash` points to,
    /// this future job is "activated" and set as the active job.
    ///
    /// If there is not future job matching the `job_id` that `SetNewPrevHash` points to,
    /// returns an error.
    ///
    /// All other future jobs are cleared.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHashMp<'a>,
    ) -> Result<(), GroupChannelError> {
        match self.future_jobs.remove(&set_new_prev_hash.job_id) {
            Some(job) => {
                self.active_job = Some(job);
            }
            None => return Err(GroupChannelError::JobIdNotFound),
        }

        // all other future jobs are now useless
        self.future_jobs.clear();
        Ok(())
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/mod.rs">
//! Abstractions for channels to be used by mining clients.

pub mod error;
pub mod extended;
pub mod group;
pub mod share_accounting;
pub mod standard;
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/share_accounting.rs">
//! Abstractions for share validation for a Mining Client

use bitcoin::hashes::sha256d::Hash;
use std::collections::HashSet;

/// The outcome of share validation, from the perspective of a Mining Client.
#[derive(Debug)]
pub enum ShareValidationResult {
    Valid,
    BlockFound,
}

/// The error variants that can occur during share validation
#[derive(Debug)]
pub enum ShareValidationError {
    Invalid,
    Stale,
    InvalidJobId,
    DoesNotMeetTarget,
    VersionRollingNotAllowed,
    DuplicateShare,
    NoChainTip,
}

/// The state of share validation on the context of some specific channel (either Extended or
/// Standard)
///
/// Only meant for usage on Mining Clients.
#[derive(Clone, Debug)]
pub struct ShareAccounting {
    last_share_sequence_number: u32,
    shares_accepted: u32,
    share_work_sum: u64,
    seen_shares: HashSet<Hash>,
    best_diff: f64,
}

impl Default for ShareAccounting {
    fn default() -> Self {
        Self::new()
    }
}

impl ShareAccounting {
    pub fn new() -> Self {
        Self {
            last_share_sequence_number: 0,
            shares_accepted: 0,
            share_work_sum: 0,
            seen_shares: HashSet::new(),
            best_diff: 0.0,
        }
    }

    pub fn update_share_accounting(
        &mut self,
        share_work: u64,
        share_sequence_number: u32,
        share_hash: Hash,
    ) {
        self.last_share_sequence_number = share_sequence_number;
        self.shares_accepted += 1;
        self.share_work_sum += share_work;
        self.seen_shares.insert(share_hash);
    }

    /// clears the hashset of seen shares
    ///
    /// should be called on every chain tip update
    /// to avoid unbounded growth of memory
    pub fn flush_seen_shares(&mut self) {
        self.seen_shares.clear();
    }

    pub fn get_last_share_sequence_number(&self) -> u32 {
        self.last_share_sequence_number
    }

    pub fn get_shares_accepted(&self) -> u32 {
        self.shares_accepted
    }

    pub fn get_share_work_sum(&self) -> u64 {
        self.share_work_sum
    }

    /// Checks if the share has been seen.
    /// Useful to avoid duplicate shares.
    pub fn is_share_seen(&self, share_hash: Hash) -> bool {
        self.seen_shares.contains(&share_hash)
    }

    pub fn get_best_diff(&self) -> f64 {
        self.best_diff
    }

    /// Updates the best diff if the new diff is higher.
    pub fn update_best_diff(&mut self, diff: f64) {
        if diff > self.best_diff {
            self.best_diff = diff;
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/client/standard.rs">
//! Abstraction over the state of a Sv2 Standard Channel, as seen by a Mining Client

use crate::{
    channels::{
        chain_tip::ChainTip,
        client::{
            error::StandardChannelError,
            share_accounting::{ShareAccounting, ShareValidationError, ShareValidationResult},
        },
    },
    utils::{bytes_to_hex, merkle_root_from_path, target_to_difficulty, u256_to_block_hash},
};
use bitcoin::{
    blockdata::block::{Header, Version},
    hashes::sha256d::Hash,
    CompactTarget, Target as BitcoinTarget,
};
use codec_sv2::binary_sv2::{self, Sv2Option};
use mining_sv2::{
    NewExtendedMiningJob, NewMiningJob, SetNewPrevHash as SetNewPrevHashMp, SubmitSharesStandard,
    Target, MAX_EXTRANONCE_LEN,
};
use std::{collections::HashMap, convert::TryInto};
use tracing::debug;

/// Mining Client abstraction over the state of a Sv2 Standard Channel.
///
/// It keeps track of:
/// - the channel's unique `channel_id`
/// - the channel's `user_identity`
/// - the channel's unique `extranonce_prefix`
/// - the channel's target
/// - the channel's nominal hashrate
/// - the channel's future jobs (indexed by `job_id`, to be activated upon receipt of a
///   `NewMiningJob` message)
/// - the channel's active job
/// - the channel's past jobs (which were active jobs under the current chain tip, indexed by
///   `job_id`)
/// - the channel's stale jobs (which were past and active jobs under the previous chain tip,
///   indexed by `job_id`)
/// - the channel's share accounting (as seen by the client)
/// - the channel's chain tip
#[derive(Debug, Clone)]
pub struct StandardChannel<'a> {
    channel_id: u32,
    user_identity: String,
    extranonce_prefix: Vec<u8>,
    target: Target,
    nominal_hashrate: f32,
    future_jobs: HashMap<u32, NewMiningJob<'a>>,
    active_job: Option<NewMiningJob<'a>>,
    past_jobs: HashMap<u32, NewMiningJob<'a>>,
    stale_jobs: HashMap<u32, NewMiningJob<'a>>,
    share_accounting: ShareAccounting,
    chain_tip: Option<ChainTip>,
}

impl<'a> StandardChannel<'a> {
    pub fn new(
        channel_id: u32,
        user_identity: String,
        extranonce_prefix: Vec<u8>,
        target: Target,
        nominal_hashrate: f32,
    ) -> Self {
        Self {
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
            future_jobs: HashMap::new(),
            active_job: None,
            past_jobs: HashMap::new(),
            stale_jobs: HashMap::new(),
            share_accounting: ShareAccounting::new(),
            chain_tip: None,
        }
    }

    pub fn get_channel_id(&self) -> u32 {
        self.channel_id
    }

    pub fn get_user_identity(&self) -> &String {
        &self.user_identity
    }

    pub fn get_chain_tip(&self) -> Option<&ChainTip> {
        self.chain_tip.as_ref()
    }

    pub fn set_extranonce_prefix(
        &mut self,
        extranonce_prefix: Vec<u8>,
    ) -> Result<(), StandardChannelError> {
        if extranonce_prefix.len() > MAX_EXTRANONCE_LEN {
            return Err(StandardChannelError::NewExtranoncePrefixTooLarge);
        }

        self.extranonce_prefix = extranonce_prefix;

        Ok(())
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn get_target(&self) -> &Target {
        &self.target
    }

    pub fn set_target(&mut self, target: Target) {
        self.target = target;
    }

    pub fn get_nominal_hashrate(&self) -> f32 {
        self.nominal_hashrate
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, NewMiningJob<'a>> {
        &self.future_jobs
    }

    pub fn get_active_job(&self) -> Option<&NewMiningJob<'a>> {
        self.active_job.as_ref()
    }

    pub fn get_past_jobs(&self) -> &HashMap<u32, NewMiningJob<'a>> {
        &self.past_jobs
    }

    pub fn get_stale_jobs(&self) -> &HashMap<u32, NewMiningJob<'a>> {
        &self.stale_jobs
    }

    pub fn get_share_accounting(&self) -> &ShareAccounting {
        &self.share_accounting
    }

    /// Called when the Group Channel receives a new extended job.
    ///
    /// Essentially converts the extended job into a standard job (with the current channel's
    /// extranonce_prefix) and then calls `on_new_mining_job` to update the channel state.
    pub fn on_new_group_channel_job(&mut self, new_extended_mining_job: NewExtendedMiningJob<'a>) {
        let merkle_root = merkle_root_from_path(
            new_extended_mining_job.coinbase_tx_prefix.inner_as_ref(),
            new_extended_mining_job.coinbase_tx_suffix.inner_as_ref(),
            &self.extranonce_prefix,
            &new_extended_mining_job.merkle_path.inner_as_ref(),
        )
        .expect("merkle root must be valid")
        .try_into()
        .expect("merkle root must be 32 bytes");

        let new_mining_job = NewMiningJob {
            channel_id: self.channel_id,
            job_id: new_extended_mining_job.job_id,
            merkle_root,
            version: new_extended_mining_job.version,
            min_ntime: new_extended_mining_job.min_ntime,
        };

        self.on_new_mining_job(new_mining_job);
    }

    /// Called when a `NewMiningJob` message is received from upstream.
    pub fn on_new_mining_job(&mut self, new_mining_job: NewMiningJob<'a>) {
        match new_mining_job.min_ntime.clone().into_inner() {
            Some(_min_ntime) => {
                println!();
                if let Some(active_job) = self.active_job.as_ref() {
                    self.past_jobs.insert(active_job.job_id, active_job.clone());
                }
                self.active_job = Some(new_mining_job);
            }
            None => {
                self.future_jobs
                    .insert(new_mining_job.job_id, new_mining_job);
            }
        }
    }

    /// Called when a `SetNewPrevHash` message is received from upstream.
    ///
    /// If the job_id addressed in the `SetNewPrevHash` is not a future job,
    /// returns an error.
    ///
    /// If the job_id addressed in the `SetNewPrevHash` is a future job,
    /// it is "activated" and set as the active job.
    ///
    /// All past jobs are marked as stale, so that shares are not propagated.
    ///
    /// The chain tip information is not kept in the channel state.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHashMp<'a>,
    ) -> Result<(), StandardChannelError> {
        match self.future_jobs.remove(&set_new_prev_hash.job_id) {
            Some(mut activated_job) => {
                activated_job.min_ntime = Sv2Option::new(Some(set_new_prev_hash.min_ntime));
                self.active_job = Some(activated_job);
            }
            None => return Err(StandardChannelError::JobIdNotFound),
        }

        // all other future jobs are now useless
        self.future_jobs.clear();

        // mark all past jobs as stale, so that shares are not propagated
        self.stale_jobs = self.past_jobs.clone();

        // clear past jobs, as we're no longer going to propagate shares for them
        self.past_jobs.clear();

        // clear seen shares, as shares for past chain tip will be rejected as stale
        self.share_accounting.flush_seen_shares();

        let set_new_prev_hash_static = set_new_prev_hash.into_static();
        let new_chain_tip = ChainTip::new(
            set_new_prev_hash_static.prev_hash,
            set_new_prev_hash_static.nbits,
            set_new_prev_hash_static.min_ntime,
        );
        self.chain_tip = Some(new_chain_tip);

        Ok(())
    }

    /// Validates a share, to be used before submission upstream.
    ///
    /// Updates the channel state with the result of the share validation.
    ///
    /// - Allows the mining client to avoid propagating stale, duplicate or low-diff shares.
    /// - Allows the mining client to know whether a block was found on some share.
    /// - Allows the mining client to keep a local version of the share accounting for comparison
    ///   with the acknowledgements coming from the upstream server.
    pub fn validate_share(
        &mut self,
        share: SubmitSharesStandard,
    ) -> Result<ShareValidationResult, ShareValidationError> {
        let job_id = share.job_id;

        // check if job_id is active job
        let is_active_job = self
            .active_job
            .as_ref()
            .is_some_and(|job| job.job_id == job_id);

        // check if job_id is past job
        let is_past_job = self.past_jobs.contains_key(&job_id);

        // check if job_id is stale job
        let is_stale_job = self.stale_jobs.contains_key(&job_id);

        if is_stale_job {
            return Err(ShareValidationError::Stale);
        }

        let job = if is_active_job {
            self.active_job.as_ref().expect("active job must exist")
        } else if is_past_job {
            self.past_jobs.get(&job_id).expect("past job must exist")
        } else {
            return Err(ShareValidationError::InvalidJobId);
        };

        let merkle_root: [u8; 32] = job
            .merkle_root
            .inner_as_ref()
            .try_into()
            .expect("merkle root must be 32 bytes");

        let chain_tip = self
            .chain_tip
            .as_ref()
            .ok_or(ShareValidationError::NoChainTip)?;

        let prev_hash = chain_tip.prev_hash();
        let nbits = CompactTarget::from_consensus(chain_tip.nbits());

        // create the header for validation
        let header = Header {
            version: Version::from_consensus(share.version as i32),
            prev_blockhash: u256_to_block_hash(prev_hash.clone()),
            merkle_root: (*Hash::from_bytes_ref(&merkle_root)).into(),
            time: share.ntime,
            bits: nbits,
            nonce: share.nonce,
        };

        // convert the header hash to a target type for easy comparison
        let hash = header.block_hash();
        let raw_hash: [u8; 32] = *hash.to_raw_hash().as_ref();
        let hash_as_target: Target = raw_hash.into();
        let hash_as_diff = target_to_difficulty(hash_as_target.clone());
        let network_target = BitcoinTarget::from_compact(nbits);

        // print hash_as_target and self.target as human readable hex
        let hash_as_u256: binary_sv2::U256 = hash_as_target.clone().into();
        let mut hash_bytes = hash_as_u256.to_vec();
        hash_bytes.reverse(); // Convert to big-endian for display
        let target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "share validation \nshare:\t\t{}\nchannel target:\t{}\nnetwork target:\t{}",
            bytes_to_hex(&hash_bytes),
            bytes_to_hex(&target_bytes),
            format!("{:x}", network_target)
        );

        // check if a block was found
        if network_target.is_met_by(hash) {
            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );
            return Ok(ShareValidationResult::BlockFound);
        }

        // check if the share hash meets the channel target
        if hash_as_target < self.target {
            if self.share_accounting.is_share_seen(hash.to_raw_hash()) {
                return Err(ShareValidationError::DuplicateShare);
            }

            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            // update the best diff
            self.share_accounting.update_best_diff(hash_as_diff);

            return Ok(ShareValidationResult::Valid);
        }

        Err(ShareValidationError::DoesNotMeetTarget)
    }
}

#[cfg(test)]
mod tests {
    use crate::channels::client::{
        share_accounting::{ShareValidationError, ShareValidationResult},
        standard::StandardChannel,
    };
    use codec_sv2::binary_sv2::Sv2Option;
    use mining_sv2::{NewMiningJob, SetNewPrevHash as SetNewPrevHashMp, SubmitSharesStandard};

    #[test]
    fn test_future_job_activation_flow() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
        );

        let future_job = NewMiningJob {
            channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(None),
        };

        channel.on_new_mining_job(future_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 1);
        assert_eq!(channel.get_active_job(), None);
        assert_eq!(channel.get_past_jobs().len(), 0);

        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            nbits: 503543726,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();
        assert!(channel.get_future_jobs().is_empty());

        let mut previously_future_job = future_job.clone();
        previously_future_job.min_ntime = Sv2Option::new(Some(ntime));

        assert_eq!(channel.get_active_job(), Some(&previously_future_job));
    }

    #[test]
    fn test_past_jobs_flow() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
        );

        let ntime: u32 = 1746839905;
        let active_job = NewMiningJob {
            channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(Some(ntime)),
        };

        channel.on_new_mining_job(active_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 0);
        assert_eq!(channel.get_active_job(), Some(&active_job));
        assert_eq!(channel.get_past_jobs().len(), 0);

        let mut new_active_job = active_job.clone();
        new_active_job.job_id = 2;
        channel.on_new_mining_job(new_active_job.clone());

        assert_eq!(channel.get_future_jobs().len(), 0);
        assert_eq!(channel.get_active_job(), Some(&new_active_job));
        assert_eq!(channel.get_past_jobs().len(), 1);
    }

    #[test]
    fn test_share_validation_block_found() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let target = [0xff; 32].into();
        let nominal_hashrate = 1.0;

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
        );

        let future_job = NewMiningJob {
            channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(None),
        };

        channel.on_new_mining_job(future_job.clone());

        // network target: 7fffff0000000000000000000000000000000000000000000000000000000000
        let nbits = 545259519;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash 61e8fe82487d10282fdededed636403eb2c8cb05ce792951dd410a9011a94ebb
        // which satisfied the network target
        // 7fffff0000000000000000000000000000000000000000000000000000000000
        let share_valid_block = SubmitSharesStandard {
            channel_id,
            sequence_number: 0,
            job_id: future_job.job_id,
            nonce: 3,
            ntime: 1745596932,
            version: 536870912,
        };

        let res = channel.validate_share(share_valid_block);

        assert!(matches!(res, Ok(ShareValidationResult::BlockFound)));
    }

    #[test]
    fn test_share_validation_does_not_meet_target() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        // channel target: 0000ffff00000000000000000000000000000000000000000000000000000000
        let target = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0xff, 0xff, 0x00, 0x00,
        ]
        .into();
        let nominal_hashrate = 1.0;

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
        );

        let future_job = NewMiningJob {
            channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(None),
        };

        channel.on_new_mining_job(future_job.clone());

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let nbits = 453040064;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash 45ec7dbd7b599599e6724ab32e6936dad033f46ccff97e743579d8c047cf3243
        // which does not meet the channel target
        // 0000ffff00000000000000000000000000000000000000000000000000000000
        let share_low_diff = SubmitSharesStandard {
            channel_id,
            sequence_number: 0,
            job_id: future_job.job_id,
            nonce: 3,
            ntime: 1745596932,
            version: 536870912,
        };

        let res = channel.validate_share(share_low_diff);

        assert!(matches!(
            res.unwrap_err(),
            ShareValidationError::DoesNotMeetTarget
        ));
    }

    #[test]
    fn test_share_validation_valid_share() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        // channel target: 0000ffff00000000000000000000000000000000000000000000000000000000
        let target = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0xff, 0xff, 0x00, 0x00,
        ]
        .into();
        let nominal_hashrate = 1.0;

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            target,
            nominal_hashrate,
        );

        let future_job = NewMiningJob {
            channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(None),
        };

        channel.on_new_mining_job(future_job.clone());

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let nbits = 453040064;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ];
        let ntime: u32 = 1746839905;
        let set_new_prev_hash = SetNewPrevHashMp {
            channel_id,
            job_id: future_job.job_id,
            prev_hash: prev_hash.into(),
            nbits,
            min_ntime: ntime,
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // this share has hash 0000762e88282a2ed8e7097aef06f413a962a47e32206a80ecbfc1f0b4bd1493
        // which meets the channel target
        // 0000ffff00000000000000000000000000000000000000000000000000000000
        let valid_share = SubmitSharesStandard {
            channel_id,
            sequence_number: 0,
            job_id: future_job.job_id,
            nonce: 244405,
            ntime: 1745596932,
            version: 536870912,
        };

        let res = channel.validate_share(valid_share);

        assert!(matches!(res, Ok(ShareValidationResult::Valid)));
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/mod.rs">
pub mod chain_tip;
pub mod client;
pub mod server;
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/error.rs">
use crate::channels::server::jobs::error::JobFactoryError;

#[derive(Debug)]
pub enum ExtendedChannelError {
    JobFactoryError(JobFactoryError),
    InvalidNominalHashrate,
    RequestedMaxTargetOutOfRange,
    ChainTipNotSet,
    TemplateIdNotFound,
    JobIdNotFound,
    RequestedMinExtranonceSizeTooLarge,
    NewExtranoncePrefixTooLarge,
}

#[derive(Debug)]
pub enum GroupChannelError {
    ChainTipNotSet,
    TemplateIdNotFound,
    JobFactoryError(JobFactoryError),
}

#[derive(Debug)]
pub enum StandardChannelError {
    TemplateIdNotFound,
    InvalidNominalHashrate,
    RequestedMaxTargetOutOfRange,
    NewExtranoncePrefixTooLarge,
    JobFactoryError(JobFactoryError),
    ChainTipNotSet,
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/extended.rs">
//! Mining Server abstraction over the state of a Sv2 Extended Channel

use crate::{
    channels::{
        chain_tip::ChainTip,
        server::{
            error::ExtendedChannelError,
            jobs::{extended::ExtendedJob, factory::JobFactory, job_store::JobStore, JobOrigin},
            share_accounting::{ShareAccounting, ShareValidationError, ShareValidationResult},
        },
    },
    utils::{
        bytes_to_hex, hash_rate_to_target, merkle_root_from_path, target_to_difficulty,
        u256_to_block_hash,
    },
};
use bitcoin::{
    blockdata::block::{Header, Version},
    hashes::sha256d::Hash,
    transaction::TxOut,
    CompactTarget, Target as BitcoinTarget,
};
use codec_sv2::binary_sv2;
use mining_sv2::{SetCustomMiningJob, SubmitSharesExtended, Target, MAX_EXTRANONCE_LEN};
use std::{collections::HashMap, convert::TryInto};
use template_distribution_sv2::{NewTemplate, SetNewPrevHash as SetNewPrevHashTdp};
use tracing::debug;

/// Mining Server abstraction of a Sv2 Extended Channel.
///
/// It keeps track of:
/// - the channel's unique `channel_id`
/// - the channel's `user_identity`
/// - the channel's unique `extranonce_prefix`
/// - the channel's rollable extranonce size
/// - the channel's requested max target (limit established by the client)
/// - the channel's target
/// - the channel's nominal hashrate
/// - the channels' mapping between `template_id`s and `job_id`s
/// - the channel's future jobs (indexed by `template_id`, to be activated upon receipt of a
///   `SetNewPrevHash` message)
/// - the channel's active job
/// - the channel's past jobs (which were active jobs under the current chain tip, indexed by
///   `job_id`)
/// - the channel's stale jobs (which were past and active jobs under the previous chain tip,
///   indexed by `job_id`)
/// - the channel's share validation state
/// - the channel's job factory
/// - the channel's chain tip
#[derive(Debug)]
pub struct ExtendedChannel<'a> {
    channel_id: u32,
    user_identity: String,
    extranonce_prefix: Vec<u8>,
    rollable_extranonce_size: u16,
    requested_max_target: Target,
    target: Target, // todo: try to use Target from rust-bitcoin
    nominal_hashrate: f32,
    job_store: Box<dyn JobStore<ExtendedJob<'a>>>,
    job_factory: JobFactory,
    share_accounting: ShareAccounting,
    expected_share_per_minute: f32,
    chain_tip: Option<ChainTip>,
}

impl<'a> ExtendedChannel<'a> {
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        channel_id: u32,
        user_identity: String,
        extranonce_prefix: Vec<u8>,
        max_target: Target,
        nominal_hashrate: f32,
        version_rolling_allowed: bool,
        requested_min_rollable_extranonce_size: u16,
        share_batch_size: usize,
        expected_share_per_minute: f32,
        job_store: Box<dyn JobStore<ExtendedJob<'a>>>,
    ) -> Result<Self, ExtendedChannelError> {
        let target_u256 =
            match hash_rate_to_target(nominal_hashrate.into(), expected_share_per_minute.into()) {
                Ok(target_u256) => target_u256,
                Err(_) => {
                    return Err(ExtendedChannelError::InvalidNominalHashrate);
                }
            };

        let target: Target = target_u256.clone().into();

        if target > max_target {
            return Err(ExtendedChannelError::RequestedMaxTargetOutOfRange);
        }

        let available_rollable_extranonce_size =
            (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        if requested_min_rollable_extranonce_size > available_rollable_extranonce_size {
            return Err(ExtendedChannelError::RequestedMinExtranonceSizeTooLarge);
        }

        Ok(Self {
            channel_id,
            user_identity,
            extranonce_prefix,
            rollable_extranonce_size: available_rollable_extranonce_size,
            requested_max_target: max_target,
            target,
            nominal_hashrate,
            job_store,
            job_factory: JobFactory::new(version_rolling_allowed),
            share_accounting: ShareAccounting::new(share_batch_size),
            expected_share_per_minute,
            chain_tip: None,
        })
    }

    pub fn get_channel_id(&self) -> u32 {
        self.channel_id
    }

    pub fn get_user_identity(&self) -> &String {
        &self.user_identity
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn get_chain_tip(&self) -> Option<&ChainTip> {
        self.chain_tip.as_ref()
    }

    pub fn get_shares_per_minute(&self) -> f32 {
        self.expected_share_per_minute
    }

    /// Only for testing purposes, not meant to be used in real apps.
    #[cfg(test)]
    fn set_chain_tip(&mut self, chain_tip: ChainTip) {
        self.chain_tip = Some(chain_tip);
    }

    /// Sets the extranonce prefix.
    ///
    /// Note: after this, all new jobs will be associated with the new extranonce prefix.
    /// Jobs created before this call will remain associated with the previous extranonce prefix,
    /// and share validation will be done accordingly.
    pub fn set_extranonce_prefix(
        &mut self,
        extranonce_prefix: Vec<u8>,
    ) -> Result<(), ExtendedChannelError> {
        let new_rollable_extranonce_size =
            MAX_EXTRANONCE_LEN as u16 - extranonce_prefix.len() as u16;

        // we return an error if the new extranonce_prefix would violate
        // min_rollable_extranonce_size that was already established with the client when the
        // channel was created
        if new_rollable_extranonce_size < self.rollable_extranonce_size {
            return Err(ExtendedChannelError::NewExtranoncePrefixTooLarge);
        }

        self.extranonce_prefix = extranonce_prefix;
        self.rollable_extranonce_size = new_rollable_extranonce_size;

        Ok(())
    }

    pub fn get_rollable_extranonce_size(&self) -> u16 {
        self.rollable_extranonce_size
    }

    pub fn get_requested_max_target(&self) -> &Target {
        &self.requested_max_target
    }

    pub fn get_target(&self) -> &Target {
        &self.target
    }

    pub fn set_target(&mut self, target: Target) {
        self.target = target;
    }

    pub fn get_future_template_to_job_id(&self) -> &HashMap<u64, u32> {
        self.job_store.get_future_template_to_job_id()
    }

    pub fn get_nominal_hashrate(&self) -> f32 {
        self.nominal_hashrate
    }

    pub fn set_nominal_hashrate(&mut self, hashrate: f32) {
        self.nominal_hashrate = hashrate;
    }

    /// Updates the channel's nominal hashrate and target.
    ///
    /// If requested_max_target is None, we use the cached value in the channel state.
    pub fn update_channel(
        &mut self,
        new_nominal_hashrate: f32,
        requested_max_target: Option<Target>,
    ) -> Result<(), ExtendedChannelError> {
        let target_u256 = match hash_rate_to_target(
            new_nominal_hashrate.into(),
            self.expected_share_per_minute.into(),
        ) {
            Ok(target_u256) => target_u256,
            Err(_) => {
                return Err(ExtendedChannelError::InvalidNominalHashrate);
            }
        };

        let requested_max_target = match requested_max_target {
            Some(ref requested_max_target) => requested_max_target.clone(),
            None => self.requested_max_target.clone(),
        };

        // debug hex of target_u256 and max_Target
        // just like in share validation
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display
        let max_target_u256: binary_sv2::U256 = requested_max_target.clone().into();
        let mut max_target_bytes = max_target_u256.to_vec();
        max_target_bytes.reverse(); // Convert to big-endian for display

        // Get the old target for comparison on the debug log
        // Not really needed for the actual method functionality
        // But it's useful to have for debugging purposes
        let old_target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut old_target_bytes = old_target_u256.to_vec();
        old_target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "updating channel target \nold target:\t{}\nnew target:\t{}\nmax_target:\t{}",
            bytes_to_hex(&old_target_bytes),
            bytes_to_hex(&target_bytes),
            bytes_to_hex(&max_target_bytes)
        );

        let new_target: Target = target_u256.into();

        if new_target > requested_max_target {
            return Err(ExtendedChannelError::RequestedMaxTargetOutOfRange);
        }

        self.nominal_hashrate = new_nominal_hashrate;
        self.target = new_target;
        self.requested_max_target = requested_max_target;

        Ok(())
    }

    pub fn get_active_job(&self) -> Option<&ExtendedJob<'a>> {
        self.job_store.get_active_job()
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        self.job_store.get_future_jobs()
    }

    pub fn get_past_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        self.job_store.get_past_jobs()
    }

    pub fn get_share_accounting(&self) -> &ShareAccounting {
        &self.share_accounting
    }

    /// Updates the channel state with a new template.
    ///
    /// If the template is a future template, the chain tip is not used.
    /// If the template is not a future template, the chain tip must be set.
    ///
    /// Only meant for usage on a Sv2 Pool Server or a Sv2 Job Declaration Client,
    /// but not on mining clients such as Mining Devices or Proxies.
    pub fn on_new_template(
        &mut self,
        template: NewTemplate<'a>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<(), ExtendedChannelError> {
        match template.future_template {
            true => {
                let new_job = self
                    .job_factory
                    .new_extended_job(
                        self.channel_id,
                        None,
                        self.extranonce_prefix.clone(),
                        template.clone(),
                        coinbase_reward_outputs,
                    )
                    .map_err(ExtendedChannelError::JobFactoryError)?;
                self.job_store.add_future_job(template.template_id, new_job);
            }
            false => {
                match self.chain_tip.clone() {
                    // we can only create non-future jobs if we have a chain tip
                    None => return Err(ExtendedChannelError::ChainTipNotSet),
                    Some(chain_tip) => {
                        let new_job = self
                            .job_factory
                            .new_extended_job(
                                self.channel_id,
                                Some(chain_tip),
                                self.extranonce_prefix.clone(),
                                template.clone(),
                                coinbase_reward_outputs,
                            )
                            .map_err(ExtendedChannelError::JobFactoryError)?;
                        self.job_store.add_active_job(new_job);
                    }
                }
            }
        }

        Ok(())
    }

    /// Updates the channel state with a new `SetNewPrevHash` message (Template Distribution
    /// Protocol variant).
    ///
    /// If there are no future jobs, returns an error.
    /// If there is some future job matching the `template_id`` that `SetNewPrevHash` points to,
    /// this future job is "activated" and set as the active job.
    ///
    /// All past jobs are cleared.
    ///
    /// The chain tip information is not kept in the channel state.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHashTdp<'a>,
    ) -> Result<(), ExtendedChannelError> {
        match self.job_store.get_future_jobs().is_empty() {
            true => {
                return Err(ExtendedChannelError::TemplateIdNotFound);
            }
            false => {
                // the SetNewPrevHash message was addressed to a specific future template
                self.job_store.activate_future_job(
                    set_new_prev_hash.template_id,
                    set_new_prev_hash.header_timestamp,
                );
            }
        }

        // clear seen shares, as shares for past chain tip will be rejected as stale
        self.share_accounting.flush_seen_shares();

        // update the chain tip
        let set_new_prev_hash_static = set_new_prev_hash.into_static();
        let new_chain_tip = ChainTip::new(
            set_new_prev_hash_static.prev_hash,
            set_new_prev_hash_static.n_bits,
            set_new_prev_hash_static.header_timestamp,
        );
        self.chain_tip = Some(new_chain_tip);

        Ok(())
    }

    /// Updates the channel state with a new custom mining job.
    ///
    /// If there is an active job, it is moved to the past jobs.
    /// The new custom mining job is then set as the active job.
    ///
    /// Returns the job id of the new custom mining job.
    ///
    /// To be used by a Sv2 Pool Server upon receiving a `SetCustomMiningJob` message.
    pub fn on_set_custom_mining_job(
        &mut self,
        set_custom_mining_job: SetCustomMiningJob<'a>,
    ) -> Result<u32, ExtendedChannelError> {
        let new_job = self
            .job_factory
            .new_custom_job(set_custom_mining_job, self.extranonce_prefix.clone())
            .map_err(ExtendedChannelError::JobFactoryError)?;

        let job_id = new_job.get_job_id();

        self.job_store.add_active_job(new_job);

        Ok(job_id)
    }

    /// Validates a share.
    ///
    /// Updates the channel state with the result of the share validation.
    pub fn validate_share(
        &mut self,
        share: SubmitSharesExtended,
    ) -> Result<ShareValidationResult, ShareValidationError> {
        let job_id = share.job_id;

        // check if job_id is active job
        let is_active_job = self
            .job_store
            .get_active_job()
            .is_some_and(|job| job.get_job_id() == job_id);

        // check if job_id is past job
        let is_past_job = self.job_store.get_past_jobs().contains_key(&job_id);

        // check if job_id is stale job
        let is_stale_job = self.job_store.get_stale_jobs().contains_key(&job_id);

        if is_stale_job {
            return Err(ShareValidationError::Stale);
        }

        // if job_id is not active, past or stale, return error
        if !is_active_job && !is_past_job && !is_stale_job {
            return Err(ShareValidationError::InvalidJobId);
        }

        let job = if is_active_job {
            self.job_store
                .get_active_job()
                .expect("active job must exist")
        } else if is_past_job {
            self.job_store
                .get_past_jobs()
                .get(&job_id)
                .expect("past job must exist")
        } else {
            self.job_store
                .get_stale_jobs()
                .get(&job_id)
                .expect("stale job must exist")
        };

        let extranonce_prefix = job.get_extranonce_prefix();
        let mut full_extranonce = vec![];
        full_extranonce.extend(extranonce_prefix.clone());
        full_extranonce.extend(share.extranonce.inner_as_ref());

        // calculate the merkle root from:
        // - job coinbase_tx_prefix
        // - full extranonce
        // - job coinbase_tx_suffix
        // - job merkle_path
        let merkle_root: [u8; 32] = merkle_root_from_path(
            job.get_coinbase_tx_prefix().inner_as_ref(),
            job.get_coinbase_tx_suffix().inner_as_ref(),
            full_extranonce.as_ref(),
            &job.get_merkle_path().inner_as_ref(),
        )
        .ok_or(ShareValidationError::Invalid)?
        .try_into()
        .expect("merkle root must be 32 bytes");

        let chain_tip = self
            .chain_tip
            .as_ref()
            .ok_or(ShareValidationError::NoChainTip)?;

        let prev_hash = chain_tip.prev_hash();
        let nbits = CompactTarget::from_consensus(chain_tip.nbits());

        // validate when version rolling is not allowed
        if !job.version_rolling_allowed() {
            // If version rolling is not allowed, ensure bits 13-28 are 0
            // This is done by checking if the version & 0x1fffe000 == 0
            // ref: https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki
            if (share.version & 0x1fffe000) != 0 {
                return Err(ShareValidationError::VersionRollingNotAllowed);
            }
        }

        // create the header for validation
        let header = Header {
            version: Version::from_consensus(share.version as i32),
            prev_blockhash: u256_to_block_hash(prev_hash.clone()),
            merkle_root: (*Hash::from_bytes_ref(&merkle_root)).into(),
            time: share.ntime,
            bits: nbits,
            nonce: share.nonce,
        };

        // convert the header hash to a target type for easy comparison
        let hash = header.block_hash();
        let raw_hash: [u8; 32] = *hash.to_raw_hash().as_ref();
        let hash_as_target: Target = raw_hash.into();
        let hash_as_diff = target_to_difficulty(hash_as_target.clone());

        let network_target = BitcoinTarget::from_compact(nbits);

        // print hash_as_target and self.target as human readable hex
        let hash_as_u256: binary_sv2::U256 = hash_as_target.clone().into();
        let mut hash_bytes = hash_as_u256.to_vec();
        hash_bytes.reverse(); // Convert to big-endian for display
        let target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "share validation \nshare:\t\t{}\nchannel target:\t{}\nnetwork target:\t{}",
            bytes_to_hex(&hash_bytes),
            bytes_to_hex(&target_bytes),
            format!("{:x}", network_target)
        );

        // check if a block was found
        if network_target.is_met_by(hash) {
            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            let mut coinbase = vec![];
            coinbase.extend(job.get_coinbase_tx_prefix().inner_as_ref());
            coinbase.extend(full_extranonce);
            coinbase.extend(job.get_coinbase_tx_suffix().inner_as_ref());

            match job.get_origin() {
                JobOrigin::NewTemplate(template) => {
                    let template_id = template.template_id;
                    return Ok(ShareValidationResult::BlockFound(
                        Some(template_id),
                        coinbase,
                    ));
                }
                JobOrigin::SetCustomMiningJob(_set_custom_mining_job) => {
                    return Ok(ShareValidationResult::BlockFound(None, coinbase));
                }
            }
        }

        // check if the share hash meets the channel target
        if hash_as_target <= self.target {
            if self.share_accounting.is_share_seen(hash.to_raw_hash()) {
                return Err(ShareValidationError::DuplicateShare);
            }

            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            // update the best diff
            self.share_accounting.update_best_diff(hash_as_diff);

            let last_sequence_number = self.share_accounting.get_last_share_sequence_number();
            let new_submits_accepted_count = self.share_accounting.get_shares_accepted();
            let new_shares_sum = self.share_accounting.get_share_work_sum();

            // if sequence number is a multiple of share_batch_size
            // it's time to send a SubmitShares.Success
            if self.share_accounting.should_acknowledge() {
                Ok(ShareValidationResult::ValidWithAcknowledgement(
                    last_sequence_number,
                    new_submits_accepted_count,
                    new_shares_sum,
                ))
            } else {
                Ok(ShareValidationResult::Valid)
            }
        } else {
            Err(ShareValidationError::DoesNotMeetTarget)
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::channels::{
        chain_tip::ChainTip,
        server::{
            error::ExtendedChannelError,
            extended::ExtendedChannel,
            jobs::{job_store::DefaultJobStore, JobOrigin},
            share_accounting::{ShareValidationError, ShareValidationResult},
        },
    };
    use bitcoin::{transaction::TxOut, Amount, ScriptBuf};
    use codec_sv2::binary_sv2::Sv2Option;
    use mining_sv2::{NewExtendedMiningJob, SubmitSharesExtended, Target, MAX_EXTRANONCE_LEN};
    use std::convert::TryInto;
    use template_distribution_sv2::{NewTemplate, SetNewPrevHash};

    const SATS_AVAILABLE_IN_TEMPLATE: u64 = 5000000000;

    #[test]
    fn test_future_job_activation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        assert!(channel.get_future_jobs().is_empty());
        channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();
        assert!(channel.get_active_job().is_none());

        let future_job_id = channel
            .get_future_template_to_job_id()
            .get(&template.template_id)
            .unwrap();

        let future_job = channel
            .get_future_jobs()
            .get(future_job_id)
            .unwrap()
            .clone();

        // we know that the provided template + coinbase_reward_outputs should generate this future
        // job
        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        assert_eq!(future_job.get_job_message(), &expected_job);

        let ntime = 1746839905;
        let set_new_prev_hash = SetNewPrevHash {
            template_id: 1,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            header_timestamp: ntime,
            n_bits: 503543726,
            target: [
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                174, 119, 3, 0, 0,
            ]
            .into(),
        };

        channel.on_set_new_prev_hash(set_new_prev_hash).unwrap();

        // we just activated the only future job
        assert!(channel.get_future_jobs().is_empty());

        let mut previously_future_job = future_job.clone();
        previously_future_job.activate(ntime);

        let activated_job = channel.get_active_job().unwrap();

        // assert that the activated job is the same as the previously future job
        assert_eq!(
            activated_job.get_job_message(),
            previously_future_job.get_job_message()
        );
    }

    #[test]
    fn test_non_future_job_creation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let ntime = 1746839905;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ]
        .into();
        let n_bits = 503543726;

        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        channel.set_chain_tip(chain_tip);

        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        assert!(channel.get_future_jobs().is_empty());

        let active_job = channel.get_active_job().unwrap().clone();

        // we know that the provided template + coinbase_reward_outputs should generate this
        // non-future job
        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(Some(ntime)),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        assert_eq!(active_job.get_job_message(), &expected_job);
    }

    #[test]
    fn test_custom_job_creation_flow() {
        // this extended channel lives on JDC
        let jdc_channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        // this extended channel lives on JDC
        let mut jdc_extended_channel = ExtendedChannel::new(
            jdc_channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        jdc_extended_channel
            .on_new_template(template.clone(), coinbase_reward_outputs.clone())
            .unwrap();

        let ntime = 1746839905;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ]
        .into();
        let n_bits = 503543726;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        let set_new_prev_hash = SetNewPrevHash {
            template_id: 1,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            header_timestamp: ntime,
            n_bits: 503543726,
            target: [
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                174, 119, 3, 0, 0,
            ]
            .into(),
        };

        jdc_extended_channel
            .on_set_new_prev_hash(set_new_prev_hash)
            .unwrap();

        let jdc_active_job = jdc_extended_channel.get_active_job().unwrap().clone();

        let mining_job_token = vec![0].try_into().unwrap();
        let set_custom_mining_job = jdc_active_job
            .clone()
            .into_custom_job(0, mining_job_token, chain_tip)
            .unwrap();

        // this extended channel lives on Pool Mining Server
        let pool_channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        // this extended channel lives on Pool Mining Server
        let mut pool_extended_channel = ExtendedChannel::new(
            pool_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        pool_extended_channel
            .on_set_custom_mining_job(set_custom_mining_job.clone())
            .unwrap();

        let pool_active_job = pool_extended_channel.get_active_job().unwrap().clone();
        assert_eq!(
            pool_active_job.get_origin(),
            &JobOrigin::SetCustomMiningJob(set_custom_mining_job)
        );

        assert_eq!(
            pool_active_job.get_job_message(),
            jdc_active_job.get_job_message()
        );

        assert_eq!(pool_active_job.get_extranonce_prefix(), &extranonce_prefix);

        assert_eq!(
            pool_active_job.get_coinbase_outputs(),
            jdc_active_job.get_coinbase_outputs()
        );
    }

    #[test]
    fn test_coinbase_reward_outputs_sum_above_template_value() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);

        let invalid_coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE + 1), /* oops: one too many extra
                                                                      * sats */
            script_pubkey: script,
        }];

        let res = channel.on_new_template(template.clone(), invalid_coinbase_reward_outputs);

        assert!(res.is_err());
        assert!(channel.get_future_jobs().is_empty());
    }

    #[test]
    fn test_share_validation_block_found() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation and share
        // validation

        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target: 04325c53ef368eb04325c53ef368eb04325c53ef368eb04325c53ef368eb0431

        let template_id = 1;
        let template = NewTemplate {
            template_id,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network target: 7fffff0000000000000000000000000000000000000000000000000000000000
        let ntime = 1745596910;
        let prev_hash = [
            251, 175, 106, 40, 35, 87, 122, 90, 58, 51, 78, 32, 202, 236, 228, 36, 154, 174, 206,
            144, 147, 195, 21, 224, 195, 103, 214, 189, 51, 190, 24, 98,
        ]
        .into();
        let n_bits = 545259519;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        channel.set_chain_tip(chain_tip);

        // prepare channel with non-future job
        channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        // this share has hash 00009a270ad03f1256312c7f196ab1a66bf8951f282fc75d9c81393cbb6427a8
        // which satisfies network target
        // 7fffff0000000000000000000000000000000000000000000000000000000000
        let share_valid_block = SubmitSharesExtended {
            channel_id,
            sequence_number: 0,
            job_id: 1,
            nonce: 741057,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(share_valid_block);

        assert!(matches!(res, Ok(ShareValidationResult::BlockFound(_, _))));
    }

    #[test]
    fn test_share_validation_does_not_meet_target() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation and share
        // validation

        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 100.0; // bigger hashrate to get higher difficulty
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target: 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435

        let template_id = 1;
        let template = NewTemplate {
            template_id,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let ntime = 1745596910;
        let prev_hash = [
            154, 124, 239, 231, 221, 122, 160, 173, 164, 175, 87, 33, 74, 214, 191, 107, 73, 34, 0,
            162, 227, 16, 44, 40, 33, 73, 0, 0, 0, 0, 0, 0,
        ]
        .into();
        let n_bits = 453040064;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        channel.set_chain_tip(chain_tip);

        // prepare channel with non-future job
        channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        // this share has hash 6f33ea329093baa13e37d11b3afa91960f8d84f0ec064c1376522548c0852d79
        // which does not meet the channel target
        // 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435
        let share_low_diff = SubmitSharesExtended {
            channel_id,
            sequence_number: 0,
            job_id: 1,
            nonce: 741057,
            ntime: 1745596971,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(share_low_diff);

        assert!(matches!(
            res.unwrap_err(),
            ShareValidationError::DoesNotMeetTarget
        ));
    }

    #[test]
    fn test_share_validation_valid_share() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation and share
        // validation

        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1_000.0; // bigger hashrate to get higher difficulty
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target is:
        // 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205

        let template_id = 1;
        let template = NewTemplate {
            template_id,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network tarkget is: 000000000000d7c0000000000000000000000000000000000000000000000000
        let n_bits = 453040064;
        let ntime = 1745611105;
        let prev_hash = [
            23, 205, 72, 134, 153, 86, 220, 153, 224, 28, 216, 146, 228, 120, 227, 157, 213, 99,
            160, 163, 128, 59, 139, 190, 158, 62, 0, 0, 0, 0, 0, 0,
        ]
        .into();
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        channel.set_chain_tip(chain_tip);

        // prepare channel with non-future job
        channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        // this share has hash 0001099d7c957a0502952177aada0254921f04306a174543389263d1dd487cce
        // which does meet the channel target
        // 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205
        // but does not meet network target
        // 000000000000d7c0000000000000000000000000000000000000000000000000
        let valid_share = SubmitSharesExtended {
            channel_id,
            sequence_number: 1,
            job_id: 1,
            nonce: 159386,
            ntime: 1745611105,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(valid_share);
        assert!(matches!(res, Ok(ShareValidationResult::Valid)));

        // try to cheat by re-submitting the same share
        // with a different sequence number
        let repeated_share = SubmitSharesExtended {
            channel_id,
            sequence_number: 2,
            job_id: 1,
            nonce: 159386,
            ntime: 1745611105,
            version: 536870912,
            extranonce: vec![1, 0, 0, 0, 0].try_into().unwrap(),
        };

        let res = channel.validate_share(repeated_share);

        // assert duplicate share is rejected
        assert!(matches!(res, Err(ShareValidationError::DuplicateShare)));
    }

    #[test]
    fn test_update_channel() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let expected_share_per_minute = 1.0;
        let initial_hashrate = 10.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        // this is the most permissive possible max_target
        let max_target: Target = [0xff; 32].into();

        // Create a channel with initial hashrate
        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target.clone(),
            initial_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // Get the initial target
        let initial_target = channel.get_target().clone();

        // Update the channel with a new hashrate (higher)
        let new_hashrate = 100.0;
        channel
            .update_channel(new_hashrate, Some(max_target.clone()))
            .unwrap();

        // Get the new target after update
        let new_target = channel.get_target().clone();

        // The target should be different after updating with a different hashrate
        // old target: 006d0b803685c01b42e00da17006d0b803685c01b42e00da17006d0b803685bf
        // new target: 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435
        assert_ne!(initial_target, new_target);

        // The nominal hashrate should be updated
        assert_eq!(channel.get_nominal_hashrate(), new_hashrate);

        // Test invalid hashrate (negative)
        let result = channel.update_channel(-1.0, Some(max_target.clone()));
        assert!(result.is_err());
        assert!(matches!(
            result,
            Err(ExtendedChannelError::InvalidNominalHashrate)
        ));

        // Create a not so permissive max_target so we can test a target that exceeds it
        let not_so_permissive_max_target: Target = [
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0x00,
        ]
        .into();

        // Try to update with a hashrate that would result in a target exceeding the max_target
        // new target: 2492492492492492492492492492492492492492492492492492492492492491
        // max target: 00ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
        let very_small_hashrate = 0.1;
        let result = channel.update_channel(
            very_small_hashrate,
            Some(not_so_permissive_max_target.clone()),
        );
        assert!(result.is_err());
        assert!(matches!(
            result,
            Err(ExtendedChannelError::RequestedMaxTargetOutOfRange)
        ));

        // Test successful update with not_so_permissive_max_target
        // new target: 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205
        // max target: 00ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
        let sufficiently_big_hashrate = 1000.0;
        let result = channel.update_channel(
            sufficiently_big_hashrate,
            Some(not_so_permissive_max_target),
        );
        assert!(result.is_ok());
    }

    #[test]
    fn test_update_extranonce_prefix() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1_000.0;
        let version_rolling_allowed = true;
        let rollable_extranonce_size = (MAX_EXTRANONCE_LEN - extranonce_prefix.len()) as u16;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::new());

        let mut channel = ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            version_rolling_allowed,
            rollable_extranonce_size,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let current_extranonce_prefix = channel.get_extranonce_prefix();
        assert_eq!(current_extranonce_prefix, &extranonce_prefix);

        let new_extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 2,
        ]
        .to_vec();

        channel
            .set_extranonce_prefix(new_extranonce_prefix.clone())
            .unwrap();
        let current_extranonce_prefix = channel.get_extranonce_prefix();
        assert_eq!(current_extranonce_prefix, &new_extranonce_prefix);

        let new_extranonce_prefix_too_large = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 2, 0,
        ]
        .to_vec();

        // try to set a new extranonce_prefix that is too large
        let result = channel.set_extranonce_prefix(new_extranonce_prefix_too_large.clone());
        assert!(result.is_err());
        assert!(matches!(
            result,
            Err(ExtendedChannelError::NewExtranoncePrefixTooLarge)
        ));
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/group.rs">
//! Abstraction over the state of a Sv2 Group Channel, as seen by a Mining Server
use crate::channels::{
    chain_tip::ChainTip,
    server::{
        error::GroupChannelError,
        jobs::{extended::ExtendedJob, factory::JobFactory, job_store::JobStore},
    },
};
use bitcoin::transaction::TxOut;
use template_distribution_sv2::{NewTemplate, SetNewPrevHash as SetNewPrevHashTdp};

use std::collections::{HashMap, HashSet};

/// Abstraction of a Group Channel.
///
/// It keeps track of:
/// - the group channel's unique `group_channel_id`
/// - the group channel's `standard_channels` (indexed by `channel_id`)
/// - the group channel's job factory
/// - the group channel's future jobs (indexed by `template_id`, to be activated upon receipt of a
///   `SetNewPrevHash` message)
/// - the group channel's active job
/// - the group channel's chain tip
///
/// Since share validation happens at the Standard Channel level, we don't really keep track of:
/// - the group channel's past jobs
/// - the group channel's stale jobs
/// - the group channel's share validation state
#[derive(Debug)]
pub struct GroupChannel<'a> {
    group_channel_id: u32,
    standard_channel_ids: HashSet<u32>,
    job_factory: JobFactory,
    job_store: Box<dyn JobStore<ExtendedJob<'a>>>,
    chain_tip: Option<ChainTip>,
}

impl<'a> GroupChannel<'a> {
    pub fn new(group_channel_id: u32, job_store: Box<dyn JobStore<ExtendedJob<'a>>>) -> Self {
        Self {
            group_channel_id,
            standard_channel_ids: HashSet::new(),
            job_factory: JobFactory::new(true),
            job_store,
            chain_tip: None,
        }
    }

    pub fn add_standard_channel_id(&mut self, standard_channel_id: u32) {
        self.standard_channel_ids.insert(standard_channel_id);
    }

    pub fn remove_standard_channel_id(&mut self, standard_channel_id: u32) {
        self.standard_channel_ids.remove(&standard_channel_id);
    }

    pub fn get_group_channel_id(&self) -> u32 {
        self.group_channel_id
    }

    pub fn get_standard_channel_ids(&self) -> &HashSet<u32> {
        &self.standard_channel_ids
    }

    pub fn get_chain_tip(&self) -> Option<&ChainTip> {
        self.chain_tip.as_ref()
    }

    /// Only for testing purposes, not meant to be used in real apps.
    #[cfg(test)]
    pub fn set_chain_tip(&mut self, chain_tip: ChainTip) {
        self.chain_tip = Some(chain_tip);
    }

    pub fn get_active_job(&self) -> Option<&ExtendedJob<'a>> {
        self.job_store.get_active_job()
    }

    pub fn get_future_template_to_job_id(&self) -> &HashMap<u64, u32> {
        self.job_store.get_future_template_to_job_id()
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, ExtendedJob<'a>> {
        self.job_store.get_future_jobs()
    }

    /// Updates the group channel state with a new template.
    ///
    /// If the template is a future template, the chain tip is not used.
    /// If the template is not a future template, the chain tip must be set.
    pub fn on_new_template(
        &mut self,
        template: NewTemplate<'a>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<(), GroupChannelError> {
        match template.future_template {
            true => {
                let new_job = self
                    .job_factory
                    .new_extended_job(
                        self.group_channel_id,
                        None,
                        vec![], /* empty extranonce prefix, as it will be replaced by the
                                 * standard channel's extranonce
                                 * prefix */
                        template.clone(),
                        coinbase_reward_outputs,
                    )
                    .map_err(GroupChannelError::JobFactoryError)?;
                self.job_store.add_future_job(template.template_id, new_job);
            }
            false => {
                match self.chain_tip.clone() {
                    // we can only create non-future jobs if we have a chain tip
                    None => return Err(GroupChannelError::ChainTipNotSet),
                    Some(chain_tip) => {
                        let new_job = self
                            .job_factory
                            .new_extended_job(
                                self.group_channel_id,
                                Some(chain_tip),
                                vec![], /* empty extranonce prefix, as it will be replaced by
                                         * the standard
                                         * channel's extranonce prefix */
                                template.clone(),
                                coinbase_reward_outputs,
                            )
                            .map_err(GroupChannelError::JobFactoryError)?;
                        self.job_store.set_active_job(new_job);
                    }
                }
            }
        }
        Ok(())
    }

    /// Updates the channel state with a new `SetNewPrevHash` message (Template Distribution
    /// Protocol variant).
    ///
    /// If there is some future job matching the `template_id`` that `SetNewPrevHash` points to,
    /// this future job is "activated" and set as the active job.
    ///
    /// The chain tip information is not kept in the channel state.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHashTdp<'a>,
    ) -> Result<(), GroupChannelError> {
        match self.job_store.get_future_jobs().is_empty() {
            true => {
                return Err(GroupChannelError::TemplateIdNotFound);
            }
            false => {
                self.job_store.activate_future_job(
                    set_new_prev_hash.template_id,
                    set_new_prev_hash.header_timestamp,
                );
            }
        }

        // update the chain tip
        let set_new_prev_hash_static = set_new_prev_hash.into_static();
        let new_chain_tip = ChainTip::new(
            set_new_prev_hash_static.prev_hash,
            set_new_prev_hash_static.n_bits,
            set_new_prev_hash_static.header_timestamp,
        );
        self.chain_tip = Some(new_chain_tip);

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use crate::channels::{
        chain_tip::ChainTip,
        server::{group::GroupChannel, jobs::job_store::DefaultJobStore},
    };
    use bitcoin::{transaction::TxOut, Amount, ScriptBuf};
    use codec_sv2::binary_sv2::Sv2Option;
    use mining_sv2::NewExtendedMiningJob;
    use std::convert::TryInto;
    use template_distribution_sv2::{NewTemplate, SetNewPrevHash};

    const SATS_AVAILABLE_IN_TEMPLATE: u64 = 5000000000;

    #[test]
    fn test_future_job_activation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation
        let group_channel_id = 1;
        let job_store = Box::new(DefaultJobStore::new());
        let mut group_channel = GroupChannel::new(group_channel_id, job_store);

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        assert!(group_channel.get_future_jobs().is_empty());
        group_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();
        assert!(group_channel.get_active_job().is_none());

        let future_job_id = group_channel
            .get_future_template_to_job_id()
            .get(&template.template_id)
            .unwrap();

        let future_job = group_channel
            .get_future_jobs()
            .get(future_job_id)
            .unwrap()
            .clone();

        // we know that the provided template + coinbase_reward_outputs should generate this future
        // job
        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        assert_eq!(future_job.get_job_message(), &expected_job);

        let ntime = 1746839905;

        let set_new_prev_hash = SetNewPrevHash {
            template_id: 1,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            header_timestamp: ntime,
            n_bits: 503543726,
            target: [
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                174, 119, 3, 0, 0,
            ]
            .into(),
        };

        group_channel
            .on_set_new_prev_hash(set_new_prev_hash)
            .unwrap();

        // we just activated the only future job
        assert!(group_channel.get_active_job().is_some());

        let mut previously_future_job = future_job.clone();
        previously_future_job.activate(ntime);

        let activated_job = group_channel.get_active_job().unwrap();

        // assert that the activated job is the same as the previously future job
        assert_eq!(
            activated_job.get_job_message(),
            previously_future_job.get_job_message()
        );
    }

    #[test]
    fn test_non_future_job_creation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation
        let group_channel_id = 1;

        let job_store = Box::new(DefaultJobStore::new());
        let mut group_channel = GroupChannel::new(group_channel_id, job_store);

        let ntime = 1746839905;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ]
        .into();
        let n_bits = 503543726;

        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);
        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        group_channel.set_chain_tip(chain_tip);
        group_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        let active_job = group_channel.get_active_job().unwrap();

        // we know that the provided template + coinbase_reward_outputs should generate this
        // non-future job
        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(Some(ntime)),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        assert_eq!(active_job.get_job_message(), &expected_job);
    }

    #[test]
    fn test_coinbase_reward_outputs_sum_above_template_value() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation
        let group_channel_id = 1;

        let job_store = Box::new(DefaultJobStore::new());
        let mut group_channel = GroupChannel::new(group_channel_id, job_store);

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);

        let invalid_coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE + 1), /* oops: one too many extra
                                                                      * sats */
            script_pubkey: script,
        }];

        assert!(group_channel
            .on_new_template(template.clone(), invalid_coinbase_reward_outputs)
            .is_err());

        assert!(group_channel.get_future_jobs().is_empty());
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/error.rs">
#[derive(Debug)]
pub enum ExtendedJobError {
    FailedToDeserializeCoinbase,
    FailedToDeserializeCoinbaseOutputs,
    CoinbaseInputCountMismatch,
    FailedToSerializeCoinbaseOutputs,
    FailedToSerializeCoinbasePrefix,
    FutureJobNotAllowed,
    InvalidMinNTime,
}

pub enum StandardJobError {
    FailedToDeserializeCoinbaseOutputs,
}

#[derive(Debug)]
pub enum JobFactoryError {
    InvalidTemplate(String),
    DeserializeCoinbaseOutputsError,
    CoinbaseTxPrefixError,
    CoinbaseTxSuffixError,
    CoinbaseOutputsSumOverflow,
    InvalidCoinbaseOutputsSum,
    ChainTipRequired,
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/extended.rs">
use super::Job;
use crate::{
    channels::{
        chain_tip::ChainTip,
        server::jobs::{error::ExtendedJobError, JobOrigin},
    },
    template_distribution_sv2::NewTemplate,
    utils::deserialize_template_outputs,
};
use bitcoin::{
    consensus::{deserialize, serialize},
    transaction::{Transaction, TxOut},
};
use codec_sv2::binary_sv2::{Seq0255, Sv2Option, B0255, B064K, U256};
use mining_sv2::{NewExtendedMiningJob, SetCustomMiningJob, MAX_EXTRANONCE_LEN};
use std::convert::TryInto;

/// Abstraction of an extended mining job with:
/// - the `NewTemplate` OR `SetCustomMiningJob` message that originated it
/// - the extranonce prefix associated with the channel at the time of job creation
/// - all coinbase outputs (spendable + unspendable) associated with the job
/// - the `NewExtendedMiningJob` message to be sent across the wire
#[derive(Debug, Clone)]
pub struct ExtendedJob<'a> {
    origin: JobOrigin<'a>,
    extranonce_prefix: Vec<u8>,
    coinbase_outputs: Vec<TxOut>,
    job_message: NewExtendedMiningJob<'a>,
}

impl Job for ExtendedJob<'_> {
    fn get_job_id(&self) -> u32 {
        self.job_message.job_id
    }

    fn activate(&mut self, min_ntime: u32) {
        self.activate(min_ntime);
    }
}

impl<'a> ExtendedJob<'a> {
    /// Creates a new job from a template.
    ///
    /// `additional_coinbase_outputs` are added to the coinbase outputs coming from the template.
    pub fn from_template(
        template: NewTemplate<'a>,
        extranonce_prefix: Vec<u8>,
        additional_coinbase_outputs: Vec<TxOut>,
        job_message: NewExtendedMiningJob<'a>,
    ) -> Result<Self, ExtendedJobError> {
        let template_coinbase_outputs = deserialize_template_outputs(
            template.coinbase_tx_outputs.to_vec(),
            template.coinbase_tx_outputs_count,
        )
        .map_err(|_| ExtendedJobError::FailedToDeserializeCoinbaseOutputs)?;

        let mut coinbase_outputs = vec![];
        coinbase_outputs.extend(additional_coinbase_outputs);
        coinbase_outputs.extend(template_coinbase_outputs);

        Ok(Self {
            origin: JobOrigin::NewTemplate(template),
            extranonce_prefix,
            coinbase_outputs,
            job_message,
        })
    }

    pub fn from_custom_job(
        custom_job: SetCustomMiningJob<'a>,
        extranonce_prefix: Vec<u8>,
        coinbase_outputs: Vec<TxOut>,
        job_message: NewExtendedMiningJob<'a>,
    ) -> Self {
        Self {
            origin: JobOrigin::SetCustomMiningJob(custom_job),
            extranonce_prefix,
            coinbase_outputs,
            job_message,
        }
    }

    /// Converts the `ExtendedJob` into a `SetCustomMiningJob` message.
    ///
    /// To be used by a Sv2 Job Declaration Client after:
    /// - a non-future `ExtendedJob` was created from a non-future `NewTemplate`
    /// - a future `ExtendedJob` was activated into a non-future `ExtendedJob`
    ///
    /// In other words, a future `ExtendedJob` cannot be converted into a `SetCustomMiningJob`.
    pub fn into_custom_job(
        self,
        request_id: u32,
        token: B0255<'a>,
        chain_tip: ChainTip,
    ) -> Result<SetCustomMiningJob<'a>, ExtendedJobError> {
        let coinbase_tx_prefix = self.get_coinbase_tx_prefix().inner_as_ref();
        let coinbase_tx_suffix = self.get_coinbase_tx_suffix().inner_as_ref();

        let mut serialized_coinbase: Vec<u8> = vec![];
        serialized_coinbase.extend(coinbase_tx_prefix);
        serialized_coinbase.extend(vec![0; MAX_EXTRANONCE_LEN]);
        serialized_coinbase.extend(coinbase_tx_suffix);

        let deserialized_coinbase: Transaction = deserialize(&serialized_coinbase)
            .map_err(|_| ExtendedJobError::FailedToDeserializeCoinbase)?;

        if deserialized_coinbase.input.len() != 1 {
            return Err(ExtendedJobError::CoinbaseInputCountMismatch);
        }

        let min_ntime = if let Some(job_min_ntime) = self.job_message.min_ntime.clone().into_inner()
        {
            // job min_ntime must be coherent with the provided chain tip
            // because chain_tip is where prev_hash and nbits are coming from
            if job_min_ntime < chain_tip.min_ntime() {
                return Err(ExtendedJobError::InvalidMinNTime);
            } else {
                job_min_ntime
            }
        } else {
            // future jobs are not allowed to be converted into `SetCustomMiningJob` messages
            return Err(ExtendedJobError::FutureJobNotAllowed);
        };

        let prev_hash = chain_tip.prev_hash();
        let nbits = chain_tip.nbits();

        let coinbase_prefix_start_index = 4 // tx version
            + 2 // segwit bytes
            + 1 // number of inputs
            + 32 // prev OutPoint
            + 4 // index
            + 1; // bytes in script
        let coinbase_prefix: B0255<'a> = coinbase_tx_prefix[coinbase_prefix_start_index..]
            .to_vec()
            .try_into()
            .map_err(|_| ExtendedJobError::FailedToSerializeCoinbasePrefix)?;
        let coinbase_tx_version = deserialized_coinbase.version.0 as u32;
        let coinbase_tx_locktime = deserialized_coinbase.lock_time.to_consensus_u32();
        let coinbase_tx_input_n_sequence = deserialized_coinbase.input[0].sequence.0 as u32;

        let serialized_outputs = serialize(&deserialized_coinbase.output);

        let coinbase_tx_outputs: B064K<'a> = serialized_outputs
            .try_into()
            .map_err(|_| ExtendedJobError::FailedToSerializeCoinbaseOutputs)?;

        Ok(SetCustomMiningJob {
            channel_id: self.job_message.channel_id,
            request_id,
            token,
            version: self.get_version(),
            prev_hash,
            min_ntime,
            nbits,
            coinbase_tx_version,
            coinbase_prefix,
            coinbase_tx_input_n_sequence,
            coinbase_tx_outputs,
            coinbase_tx_locktime,
            merkle_path: self.get_merkle_path().clone(),
        })
    }

    pub fn get_job_id(&self) -> u32 {
        self.job_message.job_id
    }

    pub fn get_origin(&self) -> &JobOrigin<'a> {
        &self.origin
    }

    pub fn get_coinbase_tx_prefix(&self) -> &B064K<'a> {
        &self.job_message.coinbase_tx_prefix
    }

    pub fn get_coinbase_tx_suffix(&self) -> &B064K<'a> {
        &self.job_message.coinbase_tx_suffix
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn get_coinbase_outputs(&self) -> &Vec<TxOut> {
        &self.coinbase_outputs
    }

    pub fn get_job_message(&self) -> &NewExtendedMiningJob<'a> {
        &self.job_message
    }

    pub fn get_merkle_path(&self) -> &Seq0255<'a, U256<'a>> {
        &self.job_message.merkle_path
    }

    pub fn get_min_ntime(&self) -> Sv2Option<'a, u32> {
        self.job_message.min_ntime.clone()
    }

    pub fn get_version(&self) -> u32 {
        self.job_message.version
    }

    pub fn version_rolling_allowed(&self) -> bool {
        self.job_message.version_rolling_allowed
    }

    /// Activates the job, setting the `min_ntime` field of the `NewExtendedMiningJob` message.
    ///
    /// To be used while activating future jobs upon updating channel `ChainTip` state.
    pub fn activate(&mut self, min_ntime: u32) {
        self.job_message.min_ntime = Sv2Option::new(Some(min_ntime));
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/factory.rs">
//! Abstraction of a factory for creating Sv2 Extended or Standard Jobs.
use crate::{
    channels::{
        chain_tip::ChainTip,
        server::jobs::{error::*, extended::ExtendedJob, standard::StandardJob},
    },
    template_distribution_sv2::NewTemplate,
    utils::{deserialize_template_outputs, merkle_root_from_path, Id as JobIdFactory},
};
use bitcoin::{
    absolute::LockTime,
    blockdata::witness::Witness,
    consensus::{serialize, Decodable},
    transaction::{OutPoint, Transaction, TxIn, TxOut, Version},
    Amount, Sequence,
};
use codec_sv2::binary_sv2::{Sv2Option, B064K};
use mining_sv2::{NewExtendedMiningJob, NewMiningJob, SetCustomMiningJob, MAX_EXTRANONCE_LEN};
use std::convert::TryInto;

/// A Factory for creating Extended or Standard Jobs.
///
/// Ensures unique job ids.
///
/// Enables creation of new Extended Jobs from NewTemplate and SetCustomMiningJob messages.
///
/// Enables creation of new Standard Jobs from NewTemplate messages.
#[derive(Debug, Clone)]
pub struct JobFactory {
    job_id_factory: JobIdFactory,
    version_rolling_allowed: bool,
}

impl JobFactory {
    pub fn new(version_rolling_allowed: bool) -> Self {
        Self {
            job_id_factory: JobIdFactory::new(),
            version_rolling_allowed,
        }
    }

    /// Creates a new job from a template.
    ///
    /// This job (and related shares) is fully committed to:
    /// - The template
    /// - The additional coinbase outputs (added to the outputs coming from the template)
    /// - The extranonce prefix of the channel at the time of job creation
    ///
    /// The optional `ChainTip` defines whether the job will be future or not.
    ///
    /// Note: version rolling is always allowed for standard jobs, so the `version_rolling_allowed`
    /// parameter is ignored.
    pub fn new_standard_job<'a>(
        &mut self,
        channel_id: u32,
        chain_tip: Option<ChainTip>,
        extranonce_prefix: Vec<u8>,
        template: NewTemplate<'a>,
        additional_coinbase_outputs: Vec<TxOut>,
    ) -> Result<StandardJob<'a>, JobFactoryError> {
        let job_id = self.job_id_factory.next();

        let version = template.version;

        let coinbase_tx_prefix =
            self.coinbase_tx_prefix(template.clone(), additional_coinbase_outputs.clone())?;
        let coinbase_tx_suffix =
            self.coinbase_tx_suffix(template.clone(), additional_coinbase_outputs.clone())?;
        let merkle_path = template.merkle_path.clone();
        let merkle_root = merkle_root_from_path(
            coinbase_tx_prefix.inner_as_ref(),
            coinbase_tx_suffix.inner_as_ref(),
            &extranonce_prefix,
            &merkle_path.inner_as_ref(),
        )
        .expect("merkle root must be valid")
        .try_into()
        .expect("merkle root must be 32 bytes");

        let job_message = match template.future_template {
            true => NewMiningJob {
                channel_id,
                job_id,
                min_ntime: Sv2Option::new(None),
                version,
                merkle_root,
            },
            false => {
                let min_ntime = match chain_tip {
                    Some(chain_tip) => Some(chain_tip.min_ntime()),
                    None => return Err(JobFactoryError::ChainTipRequired),
                };

                NewMiningJob {
                    channel_id,
                    job_id,
                    min_ntime: Sv2Option::new(min_ntime),
                    version,
                    merkle_root,
                }
            }
        };

        let job = StandardJob::from_template(
            template,
            extranonce_prefix,
            additional_coinbase_outputs,
            job_message,
        )
        .map_err(|_| JobFactoryError::DeserializeCoinbaseOutputsError)?;

        Ok(job)
    }

    /// Creates a new job from a template.
    ///
    /// This job (and related shares) is fully committed to:
    /// - The template
    /// - The additional coinbase outputs (added to the outputs coming from the template)
    /// - The extranonce prefix of the channel at the time of job creation
    ///
    /// The optional `ChainTip` defines whether the job will be future or not.
    pub fn new_extended_job<'a>(
        &mut self,
        channel_id: u32,
        chain_tip: Option<ChainTip>,
        extranonce_prefix: Vec<u8>,
        template: NewTemplate<'a>,
        additional_coinbase_outputs: Vec<TxOut>,
    ) -> Result<ExtendedJob<'a>, JobFactoryError> {
        let job_id = self.job_id_factory.next();

        let version = template.version;

        let coinbase_tx_prefix =
            self.coinbase_tx_prefix(template.clone(), additional_coinbase_outputs.clone())?;
        let coinbase_tx_suffix =
            self.coinbase_tx_suffix(template.clone(), additional_coinbase_outputs.clone())?;
        let merkle_path = template.merkle_path.clone();

        let job_message = match template.future_template {
            true => NewExtendedMiningJob {
                channel_id,
                job_id,
                min_ntime: Sv2Option::new(None),
                version,
                version_rolling_allowed: self.version_rolling_allowed,
                merkle_path,
                coinbase_tx_prefix,
                coinbase_tx_suffix,
            },
            false => {
                let min_ntime = match chain_tip {
                    Some(chain_tip) => Some(chain_tip.min_ntime()),
                    None => return Err(JobFactoryError::ChainTipRequired),
                };
                NewExtendedMiningJob {
                    channel_id,
                    job_id,
                    min_ntime: Sv2Option::new(min_ntime),
                    version,
                    version_rolling_allowed: self.version_rolling_allowed,
                    merkle_path,
                    coinbase_tx_prefix,
                    coinbase_tx_suffix,
                }
            }
        };

        let job = ExtendedJob::from_template(
            template,
            extranonce_prefix,
            additional_coinbase_outputs,
            job_message,
        )
        .map_err(|_| JobFactoryError::DeserializeCoinbaseOutputsError)?;

        Ok(job)
    }

    /// Creates a new job from a SetCustomMiningJob message.
    ///
    /// Assumes that the SetCustomMiningJob message has already been validated.
    pub fn new_custom_job<'a>(
        &mut self,
        set_custom_mining_job: SetCustomMiningJob<'a>,
        extranonce_prefix: Vec<u8>,
    ) -> Result<ExtendedJob<'a>, JobFactoryError> {
        let serialized_outputs = set_custom_mining_job
            .coinbase_tx_outputs
            .inner_as_ref()
            .to_vec();

        let coinbase_outputs = Vec::<TxOut>::consensus_decode(&mut serialized_outputs.as_slice())
            .map_err(|_| JobFactoryError::DeserializeCoinbaseOutputsError)?;

        let job_id = self.job_id_factory.next();

        let version = set_custom_mining_job.version;

        let coinbase_tx_prefix = self.custom_coinbase_tx_prefix(set_custom_mining_job.clone())?;
        let coinbase_tx_suffix = self.custom_coinbase_tx_suffix(set_custom_mining_job.clone())?;

        let merkle_path = set_custom_mining_job.merkle_path.clone().into_static();

        let job_message = NewExtendedMiningJob {
            channel_id: set_custom_mining_job.channel_id,
            job_id,
            min_ntime: Sv2Option::new(Some(set_custom_mining_job.min_ntime)),
            version,
            version_rolling_allowed: self.version_rolling_allowed,
            coinbase_tx_prefix,
            coinbase_tx_suffix,
            merkle_path,
        };

        let job = ExtendedJob::from_custom_job(
            set_custom_mining_job,
            extranonce_prefix,
            coinbase_outputs,
            job_message,
        );

        Ok(job)
    }
}

// impl block with private methods
impl JobFactory {
    // build a coinbase transaction from a SetCustomMiningJob
    // this is only used to extract coinbase_tx_prefix and coinbase_tx_suffix from the custom
    // coinbase
    fn custom_coinbase(&self, m: SetCustomMiningJob<'_>) -> Result<Transaction, JobFactoryError> {
        let deserialized_outputs = Vec::<TxOut>::consensus_decode(
            &mut m.coinbase_tx_outputs.inner_as_ref().to_vec().as_slice(),
        )
        .map_err(|_| JobFactoryError::DeserializeCoinbaseOutputsError)?;

        let mut script_sig = vec![];
        script_sig.extend_from_slice(m.coinbase_prefix.inner_as_ref());
        script_sig.extend_from_slice(&[0; MAX_EXTRANONCE_LEN]);

        // Create transaction input
        let tx_in = TxIn {
            previous_output: OutPoint::null(),
            script_sig: script_sig.into(),
            sequence: Sequence(m.coinbase_tx_input_n_sequence),
            witness: Witness::from(vec![vec![0; 32]]),
        };

        Ok(Transaction {
            version: Version::non_standard(m.coinbase_tx_version as i32),
            lock_time: LockTime::from_consensus(m.coinbase_tx_locktime),
            input: vec![tx_in],
            output: deserialized_outputs,
        })
    }

    fn custom_coinbase_tx_prefix(
        &self,
        m: SetCustomMiningJob<'_>,
    ) -> Result<B064K<'static>, JobFactoryError> {
        let coinbase = self.custom_coinbase(m.clone())?;
        let serialized_coinbase = serialize(&coinbase);

        let index = 4 // tx version
            + 2 // segwit
            + 1 // number of inputs
            + 32 // prev OutPoint
            + 4 // index
            + 1 // bytes in script
            + m.coinbase_prefix.inner_as_ref().len(); // script_sig_prefix

        let r = serialized_coinbase[0..index].to_vec();

        r.try_into()
            .map_err(|_| JobFactoryError::CoinbaseTxPrefixError)
    }

    fn custom_coinbase_tx_suffix(
        &self,
        m: SetCustomMiningJob<'_>,
    ) -> Result<B064K<'static>, JobFactoryError> {
        let coinbase = self.custom_coinbase(m.clone())?;
        let serialized_coinbase = serialize(&coinbase);

        // Calculate full extranonce size
        let full_extranonce_size = MAX_EXTRANONCE_LEN;

        let index = 4 // tx version
            + 2 // segwit
            + 1 // number of inputs
            + 32 // prev OutPoint
            + 4 // index
            + 1 // bytes in script
            + m.coinbase_prefix.inner_as_ref().len() // script_sig_prefix
            + full_extranonce_size;

        let r = serialized_coinbase[index..].to_vec();

        r.try_into()
            .map_err(|_| JobFactoryError::CoinbaseTxSuffixError)
    }

    // build a coinbase transaction from some template in the JobFactory
    fn coinbase(
        &self,
        template: NewTemplate<'_>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<Transaction, JobFactoryError> {
        // check that the sum of the additional coinbase outputs is equal to the value remaining in
        // the active template
        let mut coinbase_reward_outputs_sum = Amount::from_sat(0);
        for output in coinbase_reward_outputs.iter() {
            coinbase_reward_outputs_sum = coinbase_reward_outputs_sum
                .checked_add(output.value)
                .ok_or(JobFactoryError::CoinbaseOutputsSumOverflow)?;
        }

        if template.coinbase_tx_value_remaining < coinbase_reward_outputs_sum.to_sat() {
            return Err(JobFactoryError::InvalidCoinbaseOutputsSum);
        }

        let mut outputs = vec![];

        for output in coinbase_reward_outputs.iter() {
            outputs.push(output.clone());
        }

        let mut template_outputs = deserialize_template_outputs(
            template.coinbase_tx_outputs.to_vec(),
            template.coinbase_tx_outputs_count,
        )
        .map_err(|_| JobFactoryError::DeserializeCoinbaseOutputsError)?;

        outputs.append(&mut template_outputs);

        let mut script_sig = vec![];
        script_sig.extend_from_slice(&template.coinbase_prefix.to_vec());
        script_sig.extend_from_slice(&[0; MAX_EXTRANONCE_LEN]);

        let tx_in = TxIn {
            previous_output: OutPoint::null(),
            script_sig: script_sig.into(),
            sequence: Sequence(template.coinbase_tx_input_sequence),
            witness: Witness::from(vec![vec![0; 32]]),
        };

        Ok(Transaction {
            version: Version::non_standard(template.coinbase_tx_version as i32),
            lock_time: LockTime::from_consensus(template.coinbase_tx_locktime),
            input: vec![tx_in],
            output: outputs,
        })
    }

    fn coinbase_tx_prefix(
        &self,
        template: NewTemplate<'_>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<B064K<'static>, JobFactoryError> {
        let coinbase = self.coinbase(template.clone(), coinbase_reward_outputs)?;
        let serialized_coinbase = serialize(&coinbase);

        let index = 4 // tx version
            + 2 // segwit bytes
            + 1 // number of inputs
            + 32 // prev OutPoint
            + 4 // index
            + 1 // bytes in script
            + template.coinbase_prefix.len(); // script_sig_prefix

        let r = serialized_coinbase[0..index].to_vec();

        r.try_into()
            .map_err(|_| JobFactoryError::CoinbaseTxPrefixError)
    }

    fn coinbase_tx_suffix(
        &self,
        template: NewTemplate<'_>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<B064K<'static>, JobFactoryError> {
        let coinbase = self.coinbase(template.clone(), coinbase_reward_outputs)?;
        let serialized_coinbase = serialize(&coinbase);

        let full_extranonce_size = MAX_EXTRANONCE_LEN;

        let r = serialized_coinbase[4 // tx version
            + 2 // segwit bytes
            + 1 // number of inputs
            + 32 // prev OutPoint
            + 4 // index
            + 1 // bytes in script
            + template.coinbase_prefix.len() // script_sig_prefix
            + full_extranonce_size..]
            .to_vec();

        r.try_into()
            .map_err(|_| JobFactoryError::CoinbaseTxSuffixError)
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use bitcoin::ScriptBuf;
    use template_distribution_sv2::NewTemplate;

    #[test]
    fn test_new_job() {
        let mut job_factory = JobFactory::new(true);

        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967295,
            coinbase_tx_value_remaining: 5000000000,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(5000000000),
            script_pubkey: script,
        }];

        // match the original extranonce_prefix used to generate the expected job
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();

        let job = job_factory
            .new_extended_job(
                1,
                None,
                extranonce_prefix,
                template,
                coinbase_reward_outputs,
            )
            .unwrap();

        // we know that the provided template should generate this job
        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(None),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        assert_eq!(job.get_job_message(), &expected_job);
    }

    #[test]
    fn test_new_custom_job() {
        let mut job_factory = JobFactory::new(true);

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();

        let set_custom_mining_job = SetCustomMiningJob {
            channel_id: 1,
            request_id: 0,
            token: vec![0].try_into().unwrap(),
            version: 536870912,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            min_ntime: 1746839905,
            nbits: 503543726,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![82, 0].try_into().unwrap(),
            coinbase_tx_input_n_sequence: 4294967295,
            coinbase_tx_outputs: vec![
                2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220, 194, 147, 204, 170,
                14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0, 0, 0, 0, 0, 0, 38,
                106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222, 253, 63, 169, 153,
                223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139, 235, 216, 54, 151,
                78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 0,
            merkle_path: vec![].try_into().unwrap(),
        };

        let expected_job = NewExtendedMiningJob {
            channel_id: 1,
            job_id: 1,
            min_ntime: Sv2Option::new(Some(1746839905)),
            version: 536870912,
            version_rolling_allowed: true,
            coinbase_tx_prefix: vec![
                2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 34, 82, 0,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_suffix: vec![
                255, 255, 255, 255, 2, 0, 242, 5, 42, 1, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 0, 0, 0,
                0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
                253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
                235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            ]
            .try_into()
            .unwrap(),
            merkle_path: vec![].try_into().unwrap(),
        };

        let job = job_factory
            .new_custom_job(set_custom_mining_job, extranonce_prefix)
            .unwrap();

        assert_eq!(job.get_job_message(), &expected_job);
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/job_store.rs">
use std::{collections::HashMap, fmt::Debug};

use super::Job;

pub trait JobStore<T: Job>: Send + Sync + Debug {
    fn add_future_job(&mut self, template_id: u64, job: T) -> u32;
    fn add_active_job(&mut self, job: T);
    fn activate_future_job(&mut self, template_id: u64, prev_hash_header_timestamp: u32) -> bool;
    fn set_active_job(&mut self, job: T);
    fn get_future_template_to_job_id(&self) -> &HashMap<u64, u32>;
    fn get_active_job(&self) -> Option<&T>;
    fn get_future_jobs(&self) -> &HashMap<u32, T>;
    fn get_past_jobs(&self) -> &HashMap<u32, T>;
    fn get_stale_jobs(&self) -> &HashMap<u32, T>;
}

#[derive(Debug)]
pub struct DefaultJobStore<T: Job + Clone> {
    future_template_to_job_id: HashMap<u64, u32>,
    // future jobs are indexed with job_id (u32)
    future_jobs: HashMap<u32, T>,
    active_job: Option<T>,
    // past jobs are indexed with job_id (u32)
    past_jobs: HashMap<u32, T>,
    // stale jobs are indexed with job_id (u32)
    stale_jobs: HashMap<u32, T>,
}

impl<T: Job + Clone> DefaultJobStore<T> {
    pub fn new() -> Self {
        Self {
            future_template_to_job_id: HashMap::new(),
            future_jobs: HashMap::new(),
            active_job: None,
            past_jobs: HashMap::new(),
            stale_jobs: HashMap::new(),
        }
    }
}

impl<T: Job + Clone> Default for DefaultJobStore<T> {
    fn default() -> Self {
        Self::new()
    }
}

impl<T: Job + Clone + Debug> JobStore<T> for DefaultJobStore<T> {
    fn add_future_job(&mut self, template_id: u64, new_job: T) -> u32 {
        let new_job_id = new_job.get_job_id();
        self.future_jobs.insert(new_job_id, new_job);
        self.future_template_to_job_id
            .insert(template_id, new_job_id);
        new_job_id
    }

    fn add_active_job(&mut self, job: T) {
        // move currently active job to past jobs (so it can be marked as stale)
        if let Some(active_job) = self.active_job.take() {
            self.past_jobs.insert(active_job.get_job_id(), active_job);
        }
        // set the new active job
        self.active_job = Some(job);
    }

    fn set_active_job(&mut self, job: T) {
        self.active_job = Some(job);
    }

    fn activate_future_job(&mut self, template_id: u64, prev_hash_header_timestamp: u32) -> bool {
        let mut future_job =
            if let Some(job_id) = self.future_template_to_job_id.remove(&template_id) {
                if let Some(job) = self.future_jobs.remove(&job_id) {
                    job
                } else {
                    return false;
                }
            } else {
                return false;
            };

        // move currently active job to past jobs (so it can be marked as stale)
        if let Some(active_job) = self.active_job.take() {
            self.past_jobs.insert(active_job.get_job_id(), active_job);
        }

        // activate the future job
        future_job.activate(prev_hash_header_timestamp);
        self.active_job = Some(future_job);
        self.future_jobs.clear();
        self.future_template_to_job_id.clear();
        // mark all past jobs as stale, so that shares can be rejected with the appropriate error
        // code
        self.stale_jobs = self.past_jobs.clone();

        // clear past jobs, as we're no longer going to validate shares for them
        self.past_jobs.clear();
        true
    }

    fn get_future_template_to_job_id(&self) -> &HashMap<u64, u32> {
        &self.future_template_to_job_id
    }

    fn get_active_job(&self) -> Option<&T> {
        self.active_job.as_ref()
    }

    fn get_future_jobs(&self) -> &HashMap<u32, T> {
        &self.future_jobs
    }

    fn get_past_jobs(&self) -> &HashMap<u32, T> {
        &self.past_jobs
    }

    fn get_stale_jobs(&self) -> &HashMap<u32, T> {
        &self.stale_jobs
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/mod.rs">
pub mod error;
pub mod extended;
pub mod factory;
pub mod job_store;
pub mod standard;

use mining_sv2::SetCustomMiningJob;
use template_distribution_sv2::NewTemplate;

#[derive(Clone, Debug, PartialEq)]
pub enum JobOrigin<'a> {
    NewTemplate(NewTemplate<'a>),
    SetCustomMiningJob(SetCustomMiningJob<'a>),
}

pub trait Job: Send + Sync {
    fn get_job_id(&self) -> u32;
    fn activate(&mut self, prev_hash_header_timestamp: u32);
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/jobs/standard.rs">
use crate::{
    channels::server::jobs::{error::StandardJobError, Job},
    utils::deserialize_template_outputs,
};
use bitcoin::transaction::TxOut;
use codec_sv2::binary_sv2::{Sv2Option, U256};
use mining_sv2::NewMiningJob;
use template_distribution_sv2::NewTemplate;

/// Abstraction of a standard mining job with:
/// - the `NewTemplate` message that originated it
/// - the extranonce prefix associated with the channel at the time of job creation
/// - all coinbase outputs (spendable + unspendable) associated with the job
/// - the `NewMiningJob` message to be sent across the wire
#[derive(Debug, Clone)]
pub struct StandardJob<'a> {
    template: NewTemplate<'a>,
    extranonce_prefix: Vec<u8>,
    coinbase_outputs: Vec<TxOut>,
    job_message: NewMiningJob<'a>,
}

impl Job for StandardJob<'_> {
    fn get_job_id(&self) -> u32 {
        self.job_message.job_id
    }

    fn activate(&mut self, min_ntime: u32) {
        self.activate(min_ntime);
    }
}

impl<'a> StandardJob<'a> {
    pub fn from_template(
        template: NewTemplate<'a>,
        extranonce_prefix: Vec<u8>,
        additional_coinbase_outputs: Vec<TxOut>,
        job_message: NewMiningJob<'a>,
    ) -> Result<Self, StandardJobError> {
        let template_coinbase_outputs = deserialize_template_outputs(
            template.coinbase_tx_outputs.to_vec(),
            template.coinbase_tx_outputs_count,
        )
        .map_err(|_| StandardJobError::FailedToDeserializeCoinbaseOutputs)?;

        let mut coinbase_outputs = vec![];
        coinbase_outputs.extend(additional_coinbase_outputs);
        coinbase_outputs.extend(template_coinbase_outputs);

        Ok(Self {
            template,
            extranonce_prefix,
            coinbase_outputs,
            job_message,
        })
    }

    pub fn get_job_id(&self) -> u32 {
        self.job_message.job_id
    }

    pub fn get_coinbase_outputs(&self) -> &Vec<TxOut> {
        &self.coinbase_outputs
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn get_job_message(&self) -> &NewMiningJob<'a> {
        &self.job_message
    }

    pub fn get_template(&self) -> &NewTemplate<'a> {
        &self.template
    }

    pub fn get_merkle_root(&self) -> &U256<'a> {
        &self.job_message.merkle_root
    }

    pub fn is_future(&self) -> bool {
        self.job_message.min_ntime.clone().into_inner().is_none()
    }

    pub fn activate(&mut self, min_ntime: u32) {
        self.job_message.min_ntime = Sv2Option::new(Some(min_ntime));
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/mod.rs">
//! Abstractions for channels to be used by mining servers.

pub mod error;
pub mod extended;
pub mod group;
pub mod jobs;
pub mod share_accounting;
pub mod standard;
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/share_accounting.rs">
//! Abstractions for share validation for a Mining Server

use bitcoin::hashes::sha256d::Hash;
use std::collections::HashSet;

/// The outcome of share validation, from the perspective of a Mining Server.
///
/// The [`ShareValidationResult::ValidWithAcknowledgement`] variant carries:
/// - `last_sequence_number` (as `u32`)
/// - `new_submits_accepted_count` (as `u32`)
/// - `new_shares_sum` (as `u64`)
///
/// which are used to craft `SubmitShares.Success` Sv2 messages.
///
/// The [`ShareValidationResult::BlockFound`] variant carries:
/// - `template_id` (as `Option<u64>`)
/// - `coinbase` (as `Vec<u8>`)
///
/// where `template_id` is `None` if the share is for a custom job.
#[derive(Debug)]
pub enum ShareValidationResult {
    Valid,
    // last_sequence_number, new_submits_accepted_count, new_shares_sum
    ValidWithAcknowledgement(u32, u32, u64),
    // template_id, coinbase
    // template_id is None if custom job
    BlockFound(Option<u64>, Vec<u8>),
}

/// The error variants that can occur during share validation
#[derive(Debug)]
pub enum ShareValidationError {
    Invalid,
    Stale,
    InvalidJobId,
    DoesNotMeetTarget,
    VersionRollingNotAllowed,
    DuplicateShare,
    InvalidCoinbase,
    NoChainTip,
}

/// The state of share validation on the context of some specific channel (either Extended or
/// Standard)
///
/// Only meant for usage on Mining Servers.
#[derive(Clone, Debug)]
pub struct ShareAccounting {
    last_share_sequence_number: u32,
    shares_accepted: u32,
    share_work_sum: u64,
    share_batch_size: usize,
    seen_shares: HashSet<Hash>,
    best_diff: f64,
}

impl ShareAccounting {
    pub fn new(share_batch_size: usize) -> Self {
        Self {
            last_share_sequence_number: 0,
            shares_accepted: 0,
            share_work_sum: 0,
            share_batch_size,
            seen_shares: HashSet::new(),
            best_diff: 0.0,
        }
    }

    pub fn update_share_accounting(
        &mut self,
        share_work: u64,
        share_sequence_number: u32,
        share_hash: Hash,
    ) {
        self.last_share_sequence_number = share_sequence_number;
        self.shares_accepted += 1;
        self.share_work_sum += share_work;
        self.seen_shares.insert(share_hash);
    }

    /// clears the hashset of seen shares
    ///
    /// should be called on every chain tip update
    /// to avoid unbounded growth of memory
    pub fn flush_seen_shares(&mut self) {
        self.seen_shares.clear();
    }

    pub fn get_last_share_sequence_number(&self) -> u32 {
        self.last_share_sequence_number
    }

    pub fn get_shares_accepted(&self) -> u32 {
        self.shares_accepted
    }

    pub fn get_share_work_sum(&self) -> u64 {
        self.share_work_sum
    }

    pub fn get_share_batch_size(&self) -> usize {
        self.share_batch_size
    }

    pub fn should_acknowledge(&self) -> bool {
        self.shares_accepted % self.share_batch_size as u32 == 0
    }

    /// Checks if the share has been seen.
    /// Useful to avoid duplicate shares.
    pub fn is_share_seen(&self, share_hash: Hash) -> bool {
        self.seen_shares.contains(&share_hash)
    }

    pub fn get_best_diff(&self) -> f64 {
        self.best_diff
    }

    /// Updates the best diff if the new diff is higher.
    pub fn update_best_diff(&mut self, diff: f64) {
        if diff > self.best_diff {
            self.best_diff = diff;
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/channels/server/standard.rs">
//! Abstraction over the state of a Sv2 Standard Channel, as seen by a Mining Server
use crate::{
    channels::{
        chain_tip::ChainTip,
        server::{
            error::StandardChannelError,
            jobs::{factory::JobFactory, job_store::JobStore, standard::StandardJob},
            share_accounting::{ShareAccounting, ShareValidationError, ShareValidationResult},
        },
    },
    utils::{bytes_to_hex, hash_rate_to_target, target_to_difficulty, u256_to_block_hash},
};
use bitcoin::{
    absolute::LockTime,
    blockdata::{
        block::{Header, Version},
        witness::Witness,
    },
    consensus::Encodable,
    hashes::sha256d::Hash,
    transaction::{OutPoint, Transaction, TxIn, TxOut, Version as TxVersion},
    CompactTarget, Sequence, Target as BitcoinTarget,
};
use codec_sv2::binary_sv2;
use mining_sv2::{SubmitSharesStandard, Target, MAX_EXTRANONCE_LEN};
use std::{collections::HashMap, convert::TryInto};
use template_distribution_sv2::{NewTemplate, SetNewPrevHash};
use tracing::debug;

/// Abstraction of a Sv2 Standard Channel.
///
/// It keeps track of:
/// - the channel's unique `channel_id`
/// - the channel's `user_identity`
/// - the channel's unique `extranonce_prefix`
/// - the channel's requested max target (limit established by the client)
/// - the channel's target
/// - the channel's nominal hashrate
/// - the channel's active job
/// - the channel's future jobs (indexed by `template_id`, to be activated upon receipt of a
///   `SetNewPrevHash` message)
/// - the channel's past jobs (which were active jobs under the current chain tip, indexed by
///   `job_id`)
/// - the channel's stale jobs (which were past and active jobs under the previous chain tip,
///   indexed by `job_id`)
/// - the channel's job factory
/// - the channel's chain tip
#[derive(Debug)]
pub struct StandardChannel<'a> {
    pub channel_id: u32,
    user_identity: String,
    extranonce_prefix: Vec<u8>,
    requested_max_target: Target,
    target: Target,
    nominal_hashrate: f32,
    share_accounting: ShareAccounting,
    expected_share_per_minute: f32,
    job_store: Box<dyn JobStore<StandardJob<'a>>>,
    job_factory: JobFactory,
    chain_tip: Option<ChainTip>,
}

impl<'a> StandardChannel<'a> {
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        channel_id: u32,
        user_identity: String,
        extranonce_prefix: Vec<u8>,
        requested_max_target: Target,
        nominal_hashrate: f32,
        share_batch_size: usize,
        expected_share_per_minute: f32,
        job_store: Box<dyn JobStore<StandardJob<'a>>>,
    ) -> Result<Self, StandardChannelError> {
        let calculated_target =
            match hash_rate_to_target(nominal_hashrate.into(), expected_share_per_minute.into()) {
                Ok(target_u256) => target_u256,
                Err(_) => {
                    return Err(StandardChannelError::InvalidNominalHashrate);
                }
            };

        let target: Target = calculated_target.into();

        if target > requested_max_target {
            return Err(StandardChannelError::RequestedMaxTargetOutOfRange);
        }

        Ok(Self {
            channel_id,
            user_identity,
            extranonce_prefix,
            requested_max_target,
            target,
            nominal_hashrate,
            share_accounting: ShareAccounting::new(share_batch_size),
            expected_share_per_minute,
            job_factory: JobFactory::new(true),
            chain_tip: None,
            job_store,
        })
    }

    pub fn get_channel_id(&self) -> u32 {
        self.channel_id
    }

    pub fn get_user_identity(&self) -> &String {
        &self.user_identity
    }

    pub fn get_extranonce_prefix(&self) -> &Vec<u8> {
        &self.extranonce_prefix
    }

    pub fn set_extranonce_prefix(
        &mut self,
        extranonce_prefix: Vec<u8>,
    ) -> Result<(), StandardChannelError> {
        if extranonce_prefix.len() > MAX_EXTRANONCE_LEN {
            return Err(StandardChannelError::NewExtranoncePrefixTooLarge);
        }

        self.extranonce_prefix = extranonce_prefix;

        Ok(())
    }

    pub fn set_target(&mut self, target: Target) {
        self.target = target;
    }

    pub fn set_nominal_hashrate(&mut self, nominal_hashrate: f32) {
        self.nominal_hashrate = nominal_hashrate;
    }

    pub fn get_requested_max_target(&self) -> &Target {
        &self.requested_max_target
    }

    pub fn get_target(&self) -> &Target {
        &self.target
    }

    pub fn get_nominal_hashrate(&self) -> f32 {
        self.nominal_hashrate
    }

    /// Updates the channel's nominal hashrate and target.
    ///
    /// If requested_max_target is None, we use the cached value in the channel state.
    pub fn update_channel(
        &mut self,
        nominal_hashrate: f32,
        requested_max_target: Option<Target>,
    ) -> Result<(), StandardChannelError> {
        let target_u256 = match hash_rate_to_target(
            nominal_hashrate.into(),
            self.expected_share_per_minute.into(),
        ) {
            Ok(target_u256) => target_u256,
            Err(_) => {
                return Err(StandardChannelError::InvalidNominalHashrate);
            }
        };

        let requested_max_target = match requested_max_target {
            Some(ref requested_max_target) => requested_max_target.clone(),
            None => self.requested_max_target.clone(),
        };

        // debug hex of target_u256 and max_target
        // just like in share validation
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display
        let max_target_u256: binary_sv2::U256 = requested_max_target.clone().into();
        let mut max_target_bytes = max_target_u256.to_vec();
        max_target_bytes.reverse(); // Convert to big-endian for display

        // Get the old target for comparison on the debug log
        // Not really needed for the actual method functionality
        // But it's useful to have for debugging purposes
        let old_target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut old_target_bytes = old_target_u256.to_vec();
        old_target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "updating channel target \nold target:\t{}\nnew target:\t{}\nmax_target:\t{}",
            bytes_to_hex(&old_target_bytes),
            bytes_to_hex(&target_bytes),
            bytes_to_hex(&max_target_bytes)
        );

        let new_target: Target = target_u256.into();

        if new_target > requested_max_target {
            return Err(StandardChannelError::RequestedMaxTargetOutOfRange);
        }

        self.target = new_target;
        self.nominal_hashrate = nominal_hashrate;
        self.requested_max_target = requested_max_target;
        Ok(())
    }

    pub fn get_active_job(&self) -> Option<&StandardJob<'a>> {
        self.job_store.get_active_job()
    }

    pub fn get_future_template_to_job_id(&self) -> &HashMap<u64, u32> {
        self.job_store.get_future_template_to_job_id()
    }

    pub fn get_future_jobs(&self) -> &HashMap<u32, StandardJob<'a>> {
        self.job_store.get_future_jobs()
    }

    pub fn get_past_jobs(&self) -> &HashMap<u32, StandardJob<'a>> {
        self.job_store.get_past_jobs()
    }

    pub fn get_stale_jobs(&self) -> &HashMap<u32, StandardJob<'a>> {
        self.job_store.get_stale_jobs()
    }

    pub fn get_shares_per_minute(&self) -> f32 {
        self.expected_share_per_minute
    }

    pub fn get_chain_tip(&self) -> Option<&ChainTip> {
        self.chain_tip.as_ref()
    }

    /// Only for testing purposes, not meant to be used in real apps.
    #[cfg(test)]
    fn set_chain_tip(&mut self, chain_tip: ChainTip) {
        self.chain_tip = Some(chain_tip);
    }

    pub fn get_share_accounting(&self) -> &ShareAccounting {
        &self.share_accounting
    }

    /// Updates the channel state with a new job.
    ///
    /// If the template is a future template, the chain tip is not used.
    /// If the template is not a future template, the chain tip must be set.
    ///
    /// Only meant for usage on a Sv2 Pool Server or a Sv2 Job Declaration Client,
    /// but not on mining clients such as Mining Devices or Proxies.
    pub fn on_new_template(
        &mut self,
        template: NewTemplate<'a>,
        coinbase_reward_outputs: Vec<TxOut>,
    ) -> Result<(), StandardChannelError> {
        match template.future_template {
            true => {
                let new_job = self
                    .job_factory
                    .new_standard_job(
                        self.channel_id,
                        None,
                        self.extranonce_prefix.clone(),
                        template.clone(),
                        coinbase_reward_outputs,
                    )
                    .map_err(StandardChannelError::JobFactoryError)?;
                self.job_store.add_future_job(template.template_id, new_job);
            }
            false => {
                match self.chain_tip.clone() {
                    // we can only create non-future jobs if we have a chain tip
                    None => return Err(StandardChannelError::ChainTipNotSet),
                    Some(chain_tip) => {
                        let new_job = self
                            .job_factory
                            .new_standard_job(
                                self.channel_id,
                                Some(chain_tip),
                                self.extranonce_prefix.clone(),
                                template.clone(),
                                coinbase_reward_outputs,
                            )
                            .map_err(StandardChannelError::JobFactoryError)?;
                        self.job_store.add_active_job(new_job);
                    }
                }
            }
        }

        Ok(())
    }

    /// Updates the channel state with a new `SetNewPrevHash` message.
    ///
    /// If there are no future jobs, returns an error.
    /// If there are future jobs, the active job is set to the job with the given `template_id`.
    ///
    /// All past jobs are cleared.
    ///
    /// The chain tip information is not kept in the channel state.
    pub fn on_set_new_prev_hash(
        &mut self,
        set_new_prev_hash: SetNewPrevHash<'a>,
    ) -> Result<(), StandardChannelError> {
        match self.job_store.get_future_jobs().is_empty() {
            true => {
                return Err(StandardChannelError::TemplateIdNotFound);
            }
            false => {
                self.job_store.activate_future_job(
                    set_new_prev_hash.template_id,
                    set_new_prev_hash.header_timestamp,
                );
            }
        }

        // update the chain tip
        let set_new_prev_hash_static = set_new_prev_hash.into_static();
        let new_chain_tip = ChainTip::new(
            set_new_prev_hash_static.prev_hash,
            set_new_prev_hash_static.n_bits,
            set_new_prev_hash_static.header_timestamp,
        );
        self.chain_tip = Some(new_chain_tip);

        Ok(())
    }

    /// Validates a share.
    ///
    /// Updates the channel state with the result of the share validation.
    pub fn validate_share(
        &mut self,
        share: SubmitSharesStandard,
    ) -> Result<ShareValidationResult, ShareValidationError> {
        let job_id = share.job_id;

        // check if job_id is active job
        let is_active_job = self
            .job_store
            .get_active_job()
            .is_some_and(|job| job.get_job_id() == job_id);

        // check if job_id is past job
        let is_past_job = self.job_store.get_past_jobs().contains_key(&job_id);

        // check if job_id is stale job
        let is_stale_job = self.job_store.get_stale_jobs().contains_key(&job_id);

        if is_stale_job {
            return Err(ShareValidationError::Stale);
        }

        // if job_id is not active, past or stale, return error
        if !is_active_job && !is_past_job && !is_stale_job {
            return Err(ShareValidationError::InvalidJobId);
        }

        let job = if is_active_job {
            self.job_store
                .get_active_job()
                .expect("active job must exist")
        } else if is_past_job {
            self.job_store
                .get_past_jobs()
                .get(&job_id)
                .expect("past job must exist")
        } else {
            self.job_store
                .get_stale_jobs()
                .get(&job_id)
                .expect("stale job must exist")
        };

        let merkle_root: [u8; 32] = job
            .get_merkle_root()
            .inner_as_ref()
            .try_into()
            .expect("merkle root must be 32 bytes");

        let chain_tip = self
            .chain_tip
            .as_ref()
            .ok_or(ShareValidationError::NoChainTip)?;

        let prev_hash = chain_tip.prev_hash();
        let nbits = CompactTarget::from_consensus(chain_tip.nbits());

        // create the header for validation
        let header = Header {
            version: Version::from_consensus(share.version as i32),
            prev_blockhash: u256_to_block_hash(prev_hash.clone()),
            merkle_root: (*Hash::from_bytes_ref(&merkle_root)).into(),
            time: share.ntime,
            bits: nbits,
            nonce: share.nonce,
        };

        // convert the header hash to a target type for easy comparison
        let hash = header.block_hash();
        let raw_hash: [u8; 32] = *hash.to_raw_hash().as_ref();
        let hash_as_target: Target = raw_hash.into();
        let hash_as_diff = target_to_difficulty(hash_as_target.clone());
        let network_target = BitcoinTarget::from_compact(nbits);

        // print hash_as_target and self.target as human readable hex
        let hash_as_u256: binary_sv2::U256 = hash_as_target.clone().into();
        let mut hash_bytes = hash_as_u256.to_vec();
        hash_bytes.reverse(); // Convert to big-endian for display
        let target_u256: binary_sv2::U256 = self.target.clone().into();
        let mut target_bytes = target_u256.to_vec();
        target_bytes.reverse(); // Convert to big-endian for display

        debug!(
            "share validation \nshare:\t\t{}\nchannel target:\t{}\nnetwork target:\t{}",
            bytes_to_hex(&hash_bytes),
            bytes_to_hex(&target_bytes),
            format!("{:x}", network_target)
        );

        // check if a block was found
        if network_target.is_met_by(hash) {
            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            let mut script_sig = job.get_template().coinbase_prefix.to_vec();
            script_sig.extend(job.get_extranonce_prefix());

            let tx_in = TxIn {
                previous_output: OutPoint::null(),
                script_sig: script_sig.into(),
                sequence: Sequence(job.get_template().coinbase_tx_input_sequence),
                witness: Witness::from(vec![vec![0; 32]]),
            };

            let coinbase = Transaction {
                version: TxVersion::non_standard(job.get_template().coinbase_tx_version as i32),
                lock_time: LockTime::from_consensus(job.get_template().coinbase_tx_locktime),
                input: vec![tx_in],
                output: job.get_coinbase_outputs().to_vec(),
            };
            let mut serialized_coinbase = Vec::new();
            coinbase
                .consensus_encode(&mut serialized_coinbase)
                .map_err(|_| ShareValidationError::InvalidCoinbase)?;

            return Ok(ShareValidationResult::BlockFound(
                Some(job.get_template().template_id),
                serialized_coinbase,
            ));
        }

        // check if the share hash meets the channel target
        if hash_as_target <= self.target {
            if self.share_accounting.is_share_seen(hash.to_raw_hash()) {
                return Err(ShareValidationError::DuplicateShare);
            }

            self.share_accounting.update_share_accounting(
                target_to_difficulty(self.target.clone()) as u64,
                share.sequence_number,
                hash.to_raw_hash(),
            );

            // update the best diff
            self.share_accounting.update_best_diff(hash_as_diff);

            let last_sequence_number = self.share_accounting.get_last_share_sequence_number();
            let new_submits_accepted_count = self.share_accounting.get_shares_accepted();
            let new_shares_sum = self.share_accounting.get_share_work_sum();

            // if sequence number is a multiple of share_batch_size
            // it's time to send a SubmitShares.Success
            if self.share_accounting.should_acknowledge() {
                Ok(ShareValidationResult::ValidWithAcknowledgement(
                    last_sequence_number,
                    new_submits_accepted_count,
                    new_shares_sum,
                ))
            } else {
                Ok(ShareValidationResult::Valid)
            }
        } else {
            Err(ShareValidationError::DoesNotMeetTarget)
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::channels::{
        chain_tip::ChainTip,
        server::{
            error::StandardChannelError,
            jobs::{job_store::DefaultJobStore, standard::StandardJob},
            share_accounting::{ShareValidationError, ShareValidationResult},
            standard::StandardChannel,
        },
    };
    use bitcoin::{transaction::TxOut, Amount, ScriptBuf};
    use codec_sv2::binary_sv2::Sv2Option;
    use mining_sv2::{NewMiningJob, SubmitSharesStandard, Target};
    use std::convert::TryInto;
    use template_distribution_sv2::{NewTemplate, SetNewPrevHash as SetNewPrevHashTdp};

    const SATS_AVAILABLE_IN_TEMPLATE: u64 = 5000000000;

    #[test]
    fn test_future_job_activation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation
        let standard_channel_id = 1;
        let user_identity = "user_identity".to_string();

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();

        let max_target: Target = [0xff; 32].into();
        let nominal_hashrate = 10.0;
        let share_batch_size = 100;
        let expected_share_per_minute = 1.0;
        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut standard_channel = StandardChannel::new(
            standard_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let template = NewTemplate {
            template_id: 1,
            future_template: true,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![2, 159, 0, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967294,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 158,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        assert!(standard_channel.get_future_jobs().is_empty());

        standard_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        let expected_future_standard_job = NewMiningJob {
            channel_id: standard_channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(None),
        };

        let future_standard_job_from_channel =
            standard_channel.get_future_jobs().get(&1).unwrap().clone();
        assert_eq!(
            future_standard_job_from_channel.get_job_message(),
            &expected_future_standard_job
        );

        let ntime = 1747092633;
        let set_new_prev_hash = SetNewPrevHashTdp {
            template_id: template.template_id,
            prev_hash: [
                200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144,
                205, 88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
            ]
            .into(),
            header_timestamp: ntime,
            n_bits: 503543726,
            target: [
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                174, 119, 3, 0, 0,
            ]
            .into(),
        };

        standard_channel
            .on_set_new_prev_hash(set_new_prev_hash)
            .unwrap();
        let mut previously_future_job = future_standard_job_from_channel.clone();
        previously_future_job.activate(ntime);

        let activated_job = standard_channel.get_active_job().unwrap();

        // assert that the activated job is the same as the previously future job
        assert_eq!(
            activated_job.get_job_message(),
            previously_future_job.get_job_message()
        );
    }

    #[test]
    fn test_non_future_job_creation_flow() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let standard_channel_id = 1;
        let user_identity = "user_identity".to_string();

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();

        let max_target: Target = [0xff; 32].into();
        let nominal_hashrate = 10.0;
        let share_batch_size = 100;
        let expected_share_per_minute = 1.0;

        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut standard_channel = StandardChannel::new(
            standard_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let ntime = 1747092633;
        let prev_hash = [
            200, 53, 253, 129, 214, 31, 43, 84, 179, 58, 58, 76, 128, 213, 24, 53, 38, 144, 205,
            88, 172, 20, 251, 22, 217, 141, 21, 221, 21, 0, 0, 0,
        ]
        .into();
        let nbits = 503543726;

        let chain_tip = ChainTip::new(prev_hash, nbits, ntime);
        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![2, 159, 0, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967294,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 158,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        standard_channel.set_chain_tip(chain_tip);
        standard_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        let expected_active_standard_job = NewMiningJob {
            channel_id: standard_channel_id,
            job_id: 1,
            merkle_root: [
                189, 200, 25, 246, 119, 73, 34, 42, 209, 112, 237, 50, 169, 71, 163, 192, 24, 84,
                56, 86, 147, 71, 243, 44, 18, 107, 167, 169, 169, 66, 186, 98,
            ]
            .into(),
            version: 536870912,
            min_ntime: Sv2Option::new(Some(ntime)),
        };

        let active_standard_job_from_channel = standard_channel.get_active_job().unwrap().clone();

        assert_eq!(
            active_standard_job_from_channel.get_job_message(),
            &expected_active_standard_job
        );
    }

    #[test]
    fn test_share_validation_block_found() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let standard_channel_id = 1;
        let user_identity = "user_identity".to_string();

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target: Target = [0xff; 32].into();
        let nominal_hashrate = 1.0;
        let share_batch_size = 100;
        let expected_share_per_minute = 1.0;

        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut standard_channel = StandardChannel::new(
            standard_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target: 04325c53ef368eb04325c53ef368eb04325c53ef368eb04325c53ef368eb0431
        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![2, 159, 0, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967294,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 158,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network target: 7fffff0000000000000000000000000000000000000000000000000000000000
        let ntime = 1745596910;
        let prev_hash = [
            251, 175, 106, 40, 35, 87, 122, 90, 58, 51, 78, 32, 202, 236, 228, 36, 154, 174, 206,
            144, 147, 195, 21, 224, 195, 103, 214, 189, 51, 190, 24, 98,
        ]
        .into();
        let n_bits = 545259519;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);

        // prepare standard channel with non-future job
        standard_channel.set_chain_tip(chain_tip);
        standard_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        let active_standard_job = standard_channel.get_active_job().unwrap();

        // this share has hash 40b4c57b2c65052bbe1092e556146ad78cdd9e5ffaeff856a0eb54ee7b816da7
        // which satisfied the network target
        // 7fffff0000000000000000000000000000000000000000000000000000000000
        let share_valid_block = SubmitSharesStandard {
            channel_id: standard_channel_id,
            sequence_number: 0,
            job_id: active_standard_job.get_job_id(),
            nonce: 3,
            ntime: 1745596932,
            version: 536870912,
        };

        let res = standard_channel.validate_share(share_valid_block);

        assert!(matches!(res, Ok(ShareValidationResult::BlockFound(_, _))));
    }

    #[test]
    fn test_share_validation_does_not_meet_target() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let standard_channel_id = 1;
        let user_identity = "user_identity".to_string();

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target: Target = [0xff; 32].into();
        let nominal_hashrate = 100.0; // bigger hashrate to get higher difficulty
        let share_batch_size = 100;
        let expected_share_per_minute = 1.0;

        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut standard_channel = StandardChannel::new(
            standard_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target: 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435
        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![2, 159, 0, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967294,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 158,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let ntime = 1745596910;
        let prev_hash = [
            154, 124, 239, 231, 221, 122, 160, 173, 164, 175, 87, 33, 74, 214, 191, 107, 73, 34, 0,
            162, 227, 16, 44, 40, 33, 73, 0, 0, 0, 0, 0, 0,
        ]
        .into();
        let n_bits = 453040064;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);

        // prepare standard channel with non-future job
        standard_channel.set_chain_tip(chain_tip);
        standard_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        let active_standard_job = standard_channel.get_active_job().unwrap();

        // this share has hash a5b65006d89dab9de2b23ececd3b0435f163607f7da1ba2f0bcde62b29e8cd44
        // which does not meet the channel target
        // 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435
        let share_low_diff = SubmitSharesStandard {
            channel_id: standard_channel_id,
            sequence_number: 0,
            job_id: active_standard_job.get_job_id(),
            nonce: 3,
            ntime: 1745596932,
            version: 536870912,
        };

        let res = standard_channel.validate_share(share_low_diff);

        assert!(matches!(
            res.unwrap_err(),
            ShareValidationError::DoesNotMeetTarget
        ));
    }

    #[test]
    fn test_share_validation_valid_share() {
        // note:
        // the messages on this test were collected from a sane message flow
        // we use them as test vectors to assert correct behavior of job creation

        let standard_channel_id = 1;
        let user_identity = "user_identity".to_string();

        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target: Target = [0xff; 32].into();
        let nominal_hashrate = 1_000.0; // bigger hashrate to get higher difficulty
        let share_batch_size = 100;
        let expected_share_per_minute = 1.0;

        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut standard_channel = StandardChannel::new(
            standard_channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // channel target is:
        // 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205

        let template = NewTemplate {
            template_id: 1,
            future_template: false,
            version: 536870912,
            coinbase_tx_version: 2,
            coinbase_prefix: vec![2, 159, 0, 0].try_into().unwrap(),
            coinbase_tx_input_sequence: 4294967294,
            coinbase_tx_value_remaining: SATS_AVAILABLE_IN_TEMPLATE,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![
                0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209,
                222, 253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180,
                139, 235, 216, 54, 151, 78, 140, 249,
            ]
            .try_into()
            .unwrap(),
            coinbase_tx_locktime: 158,
            merkle_path: vec![].try_into().unwrap(),
        };

        // match the original script format used to generate the coinbase_reward_outputs for the
        // expected job
        let pubkey_hash = [
            235, 225, 183, 220, 194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194,
            8, 252,
        ];
        let mut script_bytes = vec![0]; // SegWit version 0
        script_bytes.push(20); // Push 20 bytes (length of pubkey hash)
        script_bytes.extend_from_slice(&pubkey_hash);
        let script = ScriptBuf::from(script_bytes);
        let coinbase_reward_outputs = vec![TxOut {
            value: Amount::from_sat(SATS_AVAILABLE_IN_TEMPLATE),
            script_pubkey: script,
        }];

        // network target: 000000000000d7c0000000000000000000000000000000000000000000000000
        let ntime = 1745596910;
        let prev_hash = [
            154, 124, 239, 231, 221, 122, 160, 173, 164, 175, 87, 33, 74, 214, 191, 107, 73, 34, 0,
            162, 227, 16, 44, 40, 33, 73, 0, 0, 0, 0, 0, 0,
        ]
        .into();
        let n_bits = 453040064;
        let chain_tip = ChainTip::new(prev_hash, n_bits, ntime);

        // prepare standard channel with non-future job
        standard_channel.set_chain_tip(chain_tip);
        standard_channel
            .on_new_template(template.clone(), coinbase_reward_outputs)
            .unwrap();

        // this share has hash 000010dcb838b589e5b0365350425ea82f368d330616f783d32dadf9b497bd02
        // which does meet the channel target
        // 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205
        // but does not meet network target
        // 000000000000d7c0000000000000000000000000000000000000000000000000
        let valid_share = SubmitSharesStandard {
            channel_id: standard_channel_id,
            sequence_number: 1,
            job_id: 1,
            nonce: 31978,
            ntime: 1745611105,
            version: 536870912,
        };
        let res = standard_channel.validate_share(valid_share);

        assert!(matches!(res, Ok(ShareValidationResult::Valid)));
    }

    #[test]
    fn test_update_channel() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let expected_share_per_minute = 1.0;
        let initial_hashrate = 10.0;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());
        // this is the most permissive possible max_target
        let max_target: Target = [0xff; 32].into();

        // Create a channel with initial hashrate
        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            max_target.clone(),
            initial_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        // Get the initial target
        let initial_target = channel.get_target().clone();

        // Update the channel with a new hashrate (higher)
        let new_hashrate = 100.0;
        channel
            .update_channel(new_hashrate, Some(max_target.clone()))
            .unwrap();

        // Get the new target after update
        let new_target = channel.get_target().clone();

        // The target should be different after updating with a different hashrate
        // old target: 006d0b803685c01b42e00da17006d0b803685c01b42e00da17006d0b803685bf
        // new target: 000aebbc990fff5144366f000aebbc990fff5144366f000aebbc990fff514435
        assert_ne!(initial_target, new_target);

        // The nominal hashrate should be updated
        assert_eq!(channel.get_nominal_hashrate(), new_hashrate);

        // Test invalid hashrate (negative)
        let result = channel.update_channel(-1.0, Some(max_target.clone()));
        assert!(result.is_err());
        assert!(matches!(
            result,
            Err(StandardChannelError::InvalidNominalHashrate)
        ));

        // Create a not so permissive max_target so we can test a target that exceeds it
        let not_so_permissive_max_target: Target = [
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
            0xff, 0xff, 0xff, 0x00,
        ]
        .into();

        // Try to update with a hashrate that would result in a target exceeding the max_target
        // new target: 2492492492492492492492492492492492492492492492492492492492492491
        // max target: 00ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
        let very_small_hashrate = 0.1;
        let result = channel.update_channel(
            very_small_hashrate,
            Some(not_so_permissive_max_target.clone()),
        );
        assert!(result.is_err());
        assert!(matches!(
            result,
            Err(StandardChannelError::RequestedMaxTargetOutOfRange)
        ));

        // Test successful update with not_so_permissive_max_target
        // new target: 0001179d9861a761ffdadd11c307c4fc04eea3a418f7d687584e4434af158205
        // max target: 00ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff
        let sufficiently_big_hashrate = 1000.0;
        let result = channel.update_channel(
            sufficiently_big_hashrate,
            Some(not_so_permissive_max_target),
        );
        assert!(result.is_ok());
    }

    #[test]
    fn test_update_extranonce_prefix() {
        let channel_id = 1;
        let user_identity = "user_identity".to_string();
        let extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,
        ]
        .to_vec();
        let max_target = [0xff; 32].into();
        let expected_share_per_minute = 1.0;
        let nominal_hashrate = 1_000.0;
        let share_batch_size = 100;
        let job_store = Box::new(DefaultJobStore::<StandardJob>::new());

        let mut channel = StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            max_target,
            nominal_hashrate,
            share_batch_size,
            expected_share_per_minute,
            job_store,
        )
        .unwrap();

        let current_extranonce_prefix = channel.get_extranonce_prefix();
        assert_eq!(current_extranonce_prefix, &extranonce_prefix);

        let new_extranonce_prefix = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,
        ]
        .to_vec();

        channel
            .set_extranonce_prefix(new_extranonce_prefix.clone())
            .unwrap();
        let current_extranonce_prefix = channel.get_extranonce_prefix();
        assert_eq!(current_extranonce_prefix, &new_extranonce_prefix);

        let new_extranonce_prefix_too_long = [
            83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111, 111, 108, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1,
        ]
        .to_vec();
        assert!(channel
            .set_extranonce_prefix(new_extranonce_prefix_too_long)
            .is_err());
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/errors.rs">
//! # Error Handling
//!
//! This module defines error types and utilities for handling errors in the `roles_logic_sv2`
//! module. It includes the [`Error`] enum for representing various errors.

use crate::{
    channels::server::error::{ExtendedChannelError, GroupChannelError, StandardChannelError},
    parsers::AnyMessage as AllMessages,
    utils::InputError,
    vardiff::error::VardiffError,
};
use bitcoin::hashes::FromSliceError;
use codec_sv2::binary_sv2::Error as BinarySv2Error;
use mining_sv2::ExtendedExtranonceError;
use std::{
    fmt::{self, Display, Formatter},
    sync::{MutexGuard, PoisonError},
};

/// Error enum
#[derive(Debug)]
pub enum Error {
    /// Payload size is too big to fit into a frame
    BadPayloadSize,
    /// Expected Length of 32, but received different length
    ExpectedLen32(usize),
    /// Error serializing/deserializing binary format
    BinarySv2Error(BinarySv2Error),
    /// Downstream is not connected anymore
    DownstreamDown,
    /// A channel was attempted to be added to an Upstream, but no groups are specified
    NoGroupsFound,
    /// Unexpected message received.
    UnexpectedMessage(u8),
    /// Extended channels do not have group IDs
    NoGroupIdOnExtendedChannel,
    /// No pairable upstream. Parameters are: (`min_v`, `max_v`, all flags supported)
    NoPairableUpstream((u16, u16, u32)),
    /// Error if the hashmap `future_jobs` field in the `GroupChannelJobDispatcher` is empty.
    NoFutureJobs,
    /// No Downstream's connected
    NoDownstreamsConnected,
    /// PrevHash requires non-existent Job Id
    PrevHashRequireNonExistentJobId(u32),
    /// Request Id not mapped
    RequestIdNotMapped(u32),
    /// There are no upstream connected
    NoUpstreamsConnected,
    /// Protocol has not been implemented, but should be
    UnimplementedProtocol,
    /// Unexpected `PoolMessage` type
    UnexpectedPoolMessage,
    /// Upstream is answering with a wrong request ID {} or
    /// `DownstreamMiningSelector::on_open_standard_channel_request` has not been called
    /// before relaying open channel request to upstream
    UnknownRequestId(u32),
    /// No more extranonces
    NoMoreExtranonces,
    /// A non future job always expect a previous new prev hash
    JobIsNotFutureButPrevHashNotPresent,
    /// If a channel is neither extended or part of a pool,
    /// the only thing to do when a OpenStandardChannel is received
    /// is to relay it upstream with and updated request id
    ChannelIsNeitherExtendedNeitherInAPool,
    /// No more available extranonces for downstream"
    ExtranonceSpaceEnded,
    /// Impossible to calculate merkle root
    ImpossibleToCalculateMerkleRoot,
    /// Group Id not found
    GroupIdNotFound,
    /// A share has been received but no job for it exist
    ShareDoNotMatchAnyJob,
    /// A share has been received but no channel for it exist
    ShareDoNotMatchAnyChannel,
    /// Coinbase prefix + extranonce + coinbase suffix is not a valid coinbase
    InvalidCoinbase,
    /// Value remaining in coinbase output was not correctly updated (it's equal to 0)
    ValueRemainingNotUpdated,
    /// Block header version cannot be bigger than `i32::MAX`
    VersionTooBig,
    /// Tx version cannot be bigger than `i32::MAX`
    TxVersionTooBig,
    /// Tx version cannot be lower than 1
    TxVersionTooLow,
    /// Impossible to decode tx
    TxDecodingError(String),
    /// No downstream has been registered for this channel id
    NotFoundChannelId,
    /// Impossible to create a standard job for channel
    /// because no valid job has been received from upstream yet
    NoValidJob,
    /// Impossible to create an extended job for channel
    /// because no valid job has been received from upstream yet
    NoValidTranslatorJob,
    /// Impossible to retrieve a template for the required job id
    NoTemplateForId,
    /// Impossible to retrieve a template for the required template id
    NoValidTemplate(String),
    /// Invalid extranonce size. Params: (required min, requested)
    InvalidExtranonceSize(u16, u16),
    /// Failed to create ExtendedExtranonce. Param: (error message)
    ExtendedExtranonceCreationFailed(String),
    /// Poison Lock
    PoisonLock(String),
    /// Channel Factory did not update job. Params: (downstream_job_id, upstream_job_id)
    JobNotUpdated(u32, u32),
    /// Impossible to get Target
    TargetError(InputError),
    /// Impossible to get Hashrate
    HashrateError(InputError),
    /// Message is well formatted but can not be handled
    LogicErrorMessage(std::boxed::Box<AllMessages<'static>>),
    /// JD server cannot propagate the block due to missing transactions
    JDSMissingTransactions,
    IoError(std::io::Error),
    FromSliceError(FromSliceError),
    /// Invalid user identity
    InvalidUserIdentity(String),
    ExtranoncePrefixFactoryError(ExtendedExtranonceError),
    FailedToCreateStandardChannel(StandardChannelError),
    Vardiff(VardiffError),
    FailedToCreateExtendedChannel(ExtendedChannelError),
    FailedToUpdateStandardChannel(StandardChannelError),
    FailedToUpdateExtendedChannel(ExtendedChannelError),
    FailedToProcessNewTemplateGroupChannel(GroupChannelError),
    FailedToProcessSetNewPrevHashGroupChannel(GroupChannelError),
    FailedToProcessNewTemplateExtendedChannel(ExtendedChannelError),
    FailedToProcessNewTemplateStandardChannel(StandardChannelError),
    FailedToProcessSetNewPrevHashExtendedChannel(ExtendedChannelError),
    FailedToProcessSetNewPrevHashStandardChannel(StandardChannelError),
    NoActiveJob,
    FailedToSendSolution,
    FailedToSetCustomMiningJob(ExtendedChannelError),
    FailedToDeserializeCoinbaseOutputs,
}

impl From<BinarySv2Error> for Error {
    fn from(v: BinarySv2Error) -> Error {
        Error::BinarySv2Error(v)
    }
}

impl From<std::io::Error> for Error {
    fn from(v: std::io::Error) -> Error {
        Error::IoError(v)
    }
}

impl From<FromSliceError> for Error {
    fn from(v: FromSliceError) -> Error {
        Error::FromSliceError(v)
    }
}

impl From<VardiffError> for Error {
    fn from(value: VardiffError) -> Self {
        Error::Vardiff(value)
    }
}

impl Display for Error {
    fn fmt(&self, f: &mut Formatter) -> fmt::Result {
        use Error::*;
        match self {
            BadPayloadSize => write!(f, "Payload is too big to fit into the frame"),
            BinarySv2Error(v) => write!(
                f,
                "BinarySv2Error: error in serializing/deserializing binary format {v:?}"
            ),
            DownstreamDown => {
                write!(
                    f,
                    "Downstream is not connected anymore"
                )
            }
            ExpectedLen32(l) => write!(f, "Expected length of 32, but received length of {l}"),
            NoGroupsFound => write!(
                f,
                "A channel was attempted to be added to an Upstream, but no groups are specified"
            ),
            UnexpectedMessage(type_) => write!(f, "Error: Unexpected message received. Recv m type: {type_:x}"),
            NoGroupIdOnExtendedChannel => write!(f, "Extended channels do not have group IDs"),
            NoPairableUpstream(a) => {
                write!(f, "No pairable upstream node: {a:?}")
            }
            NoFutureJobs => write!(f, "GroupChannelJobDispatcher does not have any future jobs"),
            NoDownstreamsConnected => write!(f, "NoDownstreamsConnected"),
            PrevHashRequireNonExistentJobId(id) => {
                write!(f, "PrevHashRequireNonExistentJobId {id}")
            }
            RequestIdNotMapped(id) => write!(f, "RequestIdNotMapped {id}"),
            NoUpstreamsConnected => write!(f, "There are no upstream connected"),
            UnexpectedPoolMessage => write!(f, "Unexpected `PoolMessage` type"),
            UnimplementedProtocol => write!(
                f,
                "TODO: `Protocol` has not been implemented, but should be"
            ),
            UnknownRequestId(id) => write!(
                f,
                "Upstream is answering with a wrong request ID {id} or
                DownstreamMiningSelector::on_open_standard_channel_request has not been called
                before relaying open channel request to upstream"
            ),
            InvalidExtranonceSize(required_min, requested) => {
                write!(
                    f,
                    "Invalid extranonce size: required min {required_min}, requested {requested}"
                )
            },
            NoMoreExtranonces => write!(f, "No more extranonces"),
            JobIsNotFutureButPrevHashNotPresent => write!(f, "A non future job always expect a previous new prev hash"),
            ChannelIsNeitherExtendedNeitherInAPool => write!(f, "If a channel is neither extended neither is part of a pool the only thing to do when a OpenStandardChannel is received is to relay it upstream with and updated request id"),
            ExtranonceSpaceEnded => write!(f, "No more available extranonces for downstream"),
            ImpossibleToCalculateMerkleRoot => write!(f, "Impossible to calculate merkle root"),
            GroupIdNotFound => write!(f, "Group id not found"),
            ShareDoNotMatchAnyJob => write!(f, "A share has been received but no job for it exist"),
            ShareDoNotMatchAnyChannel => write!(f, "A share has been received but no channel for it exist"),
            InvalidCoinbase => write!(f, "Coinbase prefix + extranonce + coinbase suffix is not a valid coinbase"),
            ValueRemainingNotUpdated => write!(f, "Value remaining in coinbase output was not correctly updated (it's equal to 0)"),
            VersionTooBig => write!(f, "We are trying to construct a block header with version bigger than i32::MAX"),
            TxVersionTooBig => write!(f, "Tx version can not be greater than i32::MAX"),
            TxVersionTooLow => write!(f, "Tx version can not be lower than 1"),
            TxDecodingError(e) => write!(f, "Impossible to decode tx: {e:?}"),
            NotFoundChannelId => write!(f, "No downstream has been registered for this channel id"),
            NoValidJob => write!(f, "Impossible to create a standard job for channelA cause no valid job has been received from upstream yet"),
            NoValidTranslatorJob => write!(f, "Impossible to create a extended job for channel cause no valid job has been received from upstream yet"),
            NoTemplateForId => write!(f, "Impossible to retrieve a template for the required job id"),
            NoValidTemplate(e) => write!(f, "Impossible to retrieve a template for the required template id: {e}"),
            PoisonLock(e) => write!(f, "Poison lock: {e}"),
            JobNotUpdated(ds_job_id, us_job_id) => write!(f, "Channel Factory did not update job: Downstream job id = {ds_job_id}, Upstream job id = {us_job_id}"),
            TargetError(e) => write!(f, "Impossible to get Target: {e:?}"),
            HashrateError(e) => write!(f, "Impossible to get Hashrate: {e:?}"),
            LogicErrorMessage(e) => write!(f, "Message is well formatted but can not be handled: {e:?}"),
            JDSMissingTransactions => write!(f, "JD server cannot propagate the block: missing transactions"),
            IoError(e) => write!(f, "IO error: {e:?}"),
            ExtendedExtranonceCreationFailed(e) => write!(f, "Failed to create ExtendedExtranonce: {e}"),
            FromSliceError(e) => write!(f, "Failed to hash from slice: {e}"),
            InvalidUserIdentity(e) => write!(f, "Invalid user identity: {e}"),
            ExtranoncePrefixFactoryError(e) => write!(f, "Failed to create ExtranoncePrefixFactory: {e:?}"),
            Vardiff(e) => write!(f, "Failed to adjust diff in vardiff module: {e:?}"),
            FailedToCreateStandardChannel(e) => write!(f, "Failed to create StandardChannel: {e:?}"),
            FailedToCreateExtendedChannel(e) => write!(f, "Failed to create ExtendedChannel: {e:?}"),
            FailedToProcessNewTemplateGroupChannel(e) => write!(f, "Failed to process NewTemplate: {e:?}"),
            FailedToProcessSetNewPrevHashGroupChannel(e) => write!(f, "Failed to process SetNewPrevHash: {e:?}"),
            NoActiveJob => write!(f, "No active job"),
            FailedToUpdateStandardChannel(e) => write!(f, "Failed to update StandardChannel: {e:?}"),
            FailedToUpdateExtendedChannel(e) => write!(f, "Failed to update ExtendedChannel: {e:?}"),
            FailedToSendSolution => write!(f, "Failed to send solution"),
            FailedToSetCustomMiningJob(e) => write!(f, "Failed to set custom mining job: {e:?}"),
            FailedToProcessNewTemplateExtendedChannel(e) => write!(f, "Failed to process NewTemplate: {e:?}"),
            FailedToProcessNewTemplateStandardChannel(e) => write!(f, "Failed to process NewTemplate: {e:?}"),
            FailedToProcessSetNewPrevHashExtendedChannel(e) => write!(f, "Failed to process SetNewPrevHash: {e:?}"),
            FailedToProcessSetNewPrevHashStandardChannel(e) => write!(f, "Failed to process SetNewPrevHash: {e:?}"),
            FailedToDeserializeCoinbaseOutputs => write!(f, "Failed to deserialize coinbase outputs"),
        }
    }
}

impl<T> From<PoisonError<MutexGuard<'_, T>>> for Error {
    fn from(value: PoisonError<MutexGuard<'_, T>>) -> Self {
        Error::PoisonLock(value.to_string())
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/common.rs">
//! # Common Handlers
//!
//! This module defines traits and implementations for handling common Stratum V2 messages exchanged
//! between upstream and downstream nodes.
//!
//! ## Message Handling
//!
//! Handlers in this module are responsible for:
//! - Parsing and deserializing common messages.
//! - Dispatching deserialized messages to appropriate handler functions based on message type, such
//!   as `SetupConnection` or `ChannelEndpointChanged`.
//! - Ensuring robust error handling for unexpected or malformed messages.
//!
//! ## Return Type
//!
//! Functions return `Result<SendTo, Error>`, where `SendTo` specifies the next action for the
//! message: whether to forward it, respond to it, or ignore it.
//!
//! ## Structure
//!
//! This module includes:
//! - Traits for upstream and downstream message parsing and handling.
//! - Functions to process common message types while maintaining clear separation of concerns.
//! - Error handling mechanisms to address edge cases and ensure reliable communication within
//!   Stratum V2 networks.

use super::SendTo_;
use crate::{errors::Error, parsers::CommonMessages, utils::Mutex};
use common_messages_sv2::{
    ChannelEndpointChanged, Reconnect, SetupConnection, SetupConnectionError,
    SetupConnectionSuccess, *,
};
use core::convert::TryInto;
use std::sync::Arc;

/// see [`SendTo_`]
pub type SendTo = SendTo_<CommonMessages<'static>, ()>;

/// A trait that is implemented by the downstream node, and is used to handle
/// common messages sent by the upstream to the downstream
pub trait ParseCommonMessagesFromUpstream
where
    Self: Sized,
{
    /// Takes a message type and a payload, and if the message type is a
    /// [`crate::parsers::CommonMessages`], it calls the appropriate handler function
    fn handle_message_common(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_common_deserialized(self_, (message_type, payload).try_into())
    }

    /// Takes a message and it calls the appropriate handler function
    fn handle_message_common_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<CommonMessages<'_>, Error>,
    ) -> Result<SendTo, Error> {
        match message {
            Ok(CommonMessages::SetupConnectionSuccess(m)) => {
                self_.safe_lock(|x| x.handle_setup_connection_success(m))?
            }
            Ok(CommonMessages::SetupConnectionError(m)) => {
                self_.safe_lock(|x| x.handle_setup_connection_error(m))?
            }
            Ok(CommonMessages::ChannelEndpointChanged(m)) => {
                self_.safe_lock(|x| x.handle_channel_endpoint_changed(m))?
            }
            Ok(CommonMessages::Reconnect(m)) => self_.safe_lock(|x| x.handle_reconnect(m))?,
            Ok(CommonMessages::SetupConnection(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_SETUP_CONNECTION))
            }
            Err(e) => Err(e),
        }
    }

    /// Handles a `SetupConnectionSuccess` message.
    ///
    /// This method processes a `SetupConnectionSuccess` message and handles it
    /// by delegating to the appropriate handler.
    fn handle_setup_connection_success(
        &mut self,
        m: SetupConnectionSuccess,
    ) -> Result<SendTo, Error>;

    /// Handles a `SetupConnectionError` message.
    ///
    /// This method processes a `SetupConnectionError` message and handles it
    /// by delegating to the appropriate handler.
    fn handle_setup_connection_error(&mut self, m: SetupConnectionError) -> Result<SendTo, Error>;

    /// Handles a `ChannelEndpointChanged` message.
    ///
    /// This method processes a `ChannelEndpointChanged` message and handles it
    /// by delegating to the appropriate handler.
    fn handle_channel_endpoint_changed(
        &mut self,
        m: ChannelEndpointChanged,
    ) -> Result<SendTo, Error>;

    /// Handles a `Reconnect` message.
    fn handle_reconnect(&mut self, m: Reconnect) -> Result<SendTo, Error>;
}

/// A trait that is implemented by the upstream node, and is used to handle
/// common messages sent by the downstream to the upstream
pub trait ParseCommonMessagesFromDownstream
where
    Self: Sized,
{
    /// It takes a message type and a payload, and if the message is a serialized setup connection
    /// message, it calls the `on_setup_connection` function on the routing logic, and then calls
    /// the `handle_setup_connection` function on the router
    fn handle_message_common(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_common_deserialized(self_, (message_type, payload).try_into())
    }

    /// It takes a message do setup connection message, it calls
    /// the `on_setup_connection` function on the routing logic, and then calls
    /// the `handle_setup_connection` function on the router
    fn handle_message_common_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<CommonMessages<'_>, Error>,
    ) -> Result<SendTo, Error> {
        match message {
            Ok(CommonMessages::SetupConnection(m)) => {
                self_.safe_lock(|x| x.handle_setup_connection(m))?
            }
            Ok(CommonMessages::SetupConnectionSuccess(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
            )),
            Ok(CommonMessages::SetupConnectionError(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
            )),
            Ok(CommonMessages::ChannelEndpointChanged(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
            )),
            Ok(CommonMessages::Reconnect(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_RECONNECT))
            }
            Err(e) => Err(e),
        }
    }

    /// Handles a `SetupConnection` message.
    ///
    /// This method processes a `SetupConnection` message and handles it
    /// by delegating to the appropriate handler in the routing logic.
    fn handle_setup_connection(&mut self, m: SetupConnection) -> Result<SendTo, Error>;
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/job_declaration.rs">
//! # Job Declaration Handlers
//!
//! This module defines traits and functions for handling job declaration messages within the
//! Stratum V2 protocol. The job declaration process is integral to managing mining tasks and
//! transactions between server and client components.
//!
//! ## Message Handling
//!
//! The handlers are responsible for the following tasks:
//! - Parsing and deserializing job declaration messages into appropriate types.
//! - Dispatching the deserialized messages to specific handler functions based on their type, such
//!   as handling job token allocation, job declaration success or error responses, and transaction
//!   data management.
//!
//! ## Return Type
//!
//! The functions return a `Result<SendTo, Error>`. The `SendTo` type determines the next action for
//! the message: whether the message should be relayed, responded to, or ignored. If an error occurs
//! during processing, the `Error` type is returned.
//!
//! ## Structure
//!
//! This module contains:
//! - Traits for processing job declaration messages, covering both server-side and client-side
//!   handling.
//! - Functions designed to parse, deserialize, and process messages related to job declarations,
//!   with robust error handling.
//! - Error handling mechanisms to address unexpected messages and ensure safe processing,
//!   particularly in the context of shared state.

use crate::{parsers::JobDeclaration, utils::Mutex};
use std::sync::Arc;

/// see [`SendTo_`]
pub type SendTo = SendTo_<JobDeclaration<'static>, ()>;
use super::SendTo_;
use crate::errors::Error;
use core::convert::TryInto;
use job_declaration_sv2::{
    MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN, MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
    MESSAGE_TYPE_DECLARE_MINING_JOB, MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR,
    MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS, MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
    MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS, MESSAGE_TYPE_PUSH_SOLUTION, *,
};

/// A trait for parsing and handling SV2 job declaration messages sent by a server.
///
/// This trait is designed to be implemented by downstream components that need to handle
/// various job declaration messages from an upstream SV2 server, such as job tokens allocation,
/// declaration success, and error messages.
pub trait ParseJobDeclarationMessagesFromUpstream
where
    Self: Sized,
{
    /// Routes an incoming job declaration message to the appropriate handler function.
    fn handle_message_job_declaration(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_job_declaration_deserialized(self_, (message_type, payload).try_into())
    }

    /// Routes a deserialized job declaration message to the appropriate handler function.
    fn handle_message_job_declaration_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<JobDeclaration<'_>, Error>,
    ) -> Result<SendTo, Error> {
        match message {
            Ok(JobDeclaration::AllocateMiningJobTokenSuccess(message)) => {
                self_.safe_lock(|x| x.handle_allocate_mining_job_token_success(message))?
            }
            Ok(JobDeclaration::DeclareMiningJobSuccess(message)) => {
                self_.safe_lock(|x| x.handle_declare_mining_job_success(message))?
            }
            Ok(JobDeclaration::DeclareMiningJobError(message)) => {
                self_.safe_lock(|x| x.handle_declare_mining_job_error(message))?
            }
            Ok(JobDeclaration::ProvideMissingTransactions(message)) => {
                self_.safe_lock(|x| x.handle_provide_missing_transactions(message))?
            }
            Ok(JobDeclaration::AllocateMiningJobToken(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
            )),
            Ok(JobDeclaration::DeclareMiningJob(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_DECLARE_MINING_JOB))
            }
            Ok(JobDeclaration::ProvideMissingTransactionsSuccess(_)) => Err(
                Error::UnexpectedMessage(MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS),
            ),
            Ok(JobDeclaration::PushSolution(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_PUSH_SOLUTION))
            }
            Err(e) => Err(e),
        }
    }

    /// Handles an `AllocateMiningJobTokenSuccess` message.
    ///
    /// This method processes a message indicating a successful job token allocation.
    fn handle_allocate_mining_job_token_success(
        &mut self,
        message: AllocateMiningJobTokenSuccess,
    ) -> Result<SendTo, Error>;

    /// Handles a `DeclareMiningJobSuccess` message.
    ///
    /// This method processes a message indicating a successful mining job declaration.
    fn handle_declare_mining_job_success(
        &mut self,
        message: DeclareMiningJobSuccess,
    ) -> Result<SendTo, Error>;

    /// Handles a `DeclareMiningJobError` message.
    ///
    /// This method processes a message indicating an error in the mining job declaration process.
    fn handle_declare_mining_job_error(
        &mut self,
        message: DeclareMiningJobError,
    ) -> Result<SendTo, Error>;

    /// Handles a `ProvideMissingTransactions` message.
    ///
    /// This method processes a message that supplies missing transaction data.
    fn handle_provide_missing_transactions(
        &mut self,
        message: ProvideMissingTransactions,
    ) -> Result<SendTo, Error>;
}

/// A trait responsible for handling job declaration messages sent by clients to upstream nodes.
/// The methods process messages like job declarations, solutions, and transaction success
/// indicators, ensuring proper routing and handling.
pub trait ParseJobDeclarationMessagesFromDownstream
where
    Self: Sized,
{
    /// Routes an incoming job declaration message to the appropriate handler function.
    fn handle_message_job_declaration(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_job_declaration_deserialized(self_, (message_type, payload).try_into())
    }

    /// Routes a deserialized job declaration message to the appropriate handler function.
    fn handle_message_job_declaration_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<JobDeclaration<'_>, Error>,
    ) -> Result<SendTo, Error> {
        match message {
            Ok(JobDeclaration::AllocateMiningJobToken(message)) => {
                self_.safe_lock(|x| x.handle_allocate_mining_job_token(message))?
            }
            Ok(JobDeclaration::DeclareMiningJob(message)) => {
                self_.safe_lock(|x| x.handle_declare_mining_job(message))?
            }
            Ok(JobDeclaration::ProvideMissingTransactionsSuccess(message)) => {
                self_.safe_lock(|x| x.handle_provide_missing_transactions_success(message))?
            }
            Ok(JobDeclaration::PushSolution(message)) => {
                self_.safe_lock(|x| x.handle_push_solution(message))?
            }
            Ok(JobDeclaration::AllocateMiningJobTokenSuccess(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
            )),
            Ok(JobDeclaration::DeclareMiningJobSuccess(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS,
            )),
            Ok(JobDeclaration::DeclareMiningJobError(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR,
            )),
            Ok(JobDeclaration::ProvideMissingTransactions(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
            )),
            Err(e) => Err(e),
        }
    }

    /// Handles an `AllocateMiningJobToken` message.
    fn handle_allocate_mining_job_token(
        &mut self,
        message: AllocateMiningJobToken,
    ) -> Result<SendTo, Error>;

    /// Handles a `DeclareMiningJob` message.
    ///
    /// This method processes a message that declares a new mining job.
    fn handle_declare_mining_job(&mut self, message: DeclareMiningJob) -> Result<SendTo, Error>;

    /// Handles a `ProvideMissingTransactionsSuccess` message.
    ///
    /// This method processes a message that confirms the receipt of missing transactions.
    fn handle_provide_missing_transactions_success(
        &mut self,
        message: ProvideMissingTransactionsSuccess,
    ) -> Result<SendTo, Error>;

    /// Handles a `PushSolution` message.
    ///
    /// This method is used to process a valid block found by the miner.
    fn handle_push_solution(&mut self, message: PushSolution) -> Result<SendTo, Error>;
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/mining.rs">
//! # Mining Handlers
//!
//! This module defines traits and functions for handling mining-related messages within the Stratum
//! V2 protocol.
//!
//! ## Message Handling
//!
//! Handlers in this module are responsible for:
//! - Parsing and deserializing mining-related messages into the appropriate types.
//! - Dispatching the deserialized messages to specific handler functions based on message type,
//!   such as handling new mining jobs, share submissions, and extranonce updates.
//! - Ensuring the integrity and validity of received messages, while interacting with downstream
//!   mining systems to ensure proper communication and task execution.
//!
//! ## Return Type
//!
//! Functions return `Result<SendTo<Down>, Error>`, where `SendTo<Down>` specifies the next action
//! for the message: whether it should be sent to the downstream node, an error response should be
//! generated, or the message should be ignored.
//!
//! ## Structure
//!
//! This module includes:
//! - Traits for processing mining-related messages for both upstream and downstream communication.
//! - Functions to parse, deserialize, and process messages related to mining, ensuring robust error
//!   handling for unexpected conditions.
//! - Support for managing mining channels, extranonce prefixes, and share submissions, while
//!   handling edge cases and ensuring the correctness of the mining process.

use crate::{errors::Error, parsers::Mining};
use codec_sv2::binary_sv2;
use core::convert::TryInto;
use mining_sv2::{
    CloseChannel, NewExtendedMiningJob, NewMiningJob, OpenExtendedMiningChannel,
    OpenExtendedMiningChannelSuccess, OpenMiningChannelError, OpenStandardMiningChannel,
    OpenStandardMiningChannelSuccess, SetCustomMiningJob, SetCustomMiningJobError,
    SetCustomMiningJobSuccess, SetExtranoncePrefix, SetGroupChannel, SetNewPrevHash, SetTarget,
    SubmitSharesError, SubmitSharesExtended, SubmitSharesStandard, SubmitSharesSuccess,
    UpdateChannel, UpdateChannelError,
};

use super::SendTo_;

use crate::utils::Mutex;
use mining_sv2::*;
use std::{fmt::Debug as D, sync::Arc};

/// see [`SendTo_`]
pub type SendTo<Remote> = SendTo_<Mining<'static>, Remote>;

/// Represents supported channel types in a mining connection.
#[derive(Debug, Copy, Clone, PartialEq, Eq)]
pub enum SupportedChannelTypes {
    Standard,
    Extended,
    Group,
    /// Represents a connection that supports both group and extended channels.
    GroupAndExtended,
}

/// Trait for parsing downstream mining messages in a Stratum V2 connection.
///
/// This trait defines methods for parsing and routing downstream messages
/// related to mining operations.
pub trait ParseMiningMessagesFromDownstream<Up: D>
where
    Self: Sized + D,
{
    /// Returns the type of channel supported by the downstream connection.
    fn get_channel_type(&self) -> SupportedChannelTypes;

    /// Handles a mining message from the downstream, given its type and payload.
    fn handle_message_mining(
        self_mutex: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo<Up>, Error>
    where
        Self: Sized,
    {
        match Self::handle_message_mining_deserialized(
            self_mutex,
            (message_type, payload).try_into(),
        ) {
            Err(Error::UnexpectedMessage(0)) => Err(Error::UnexpectedMessage(message_type)),
            result => result,
        }
    }

    /// Deserializes and processes a mining message from the downstream.
    fn handle_message_mining_deserialized(
        self_mutex: Arc<Mutex<Self>>,
        message: Result<Mining<'_>, Error>,
    ) -> Result<SendTo<Up>, Error>
    where
        Self: Sized,
    {
        let (channel_type, is_work_selection_enabled) = self_mutex
            .safe_lock(|self_| (self_.get_channel_type(), self_.is_work_selection_enabled()))?;
        match message {
            Ok(Mining::OpenStandardMiningChannel(m)) => {
                // check user auth
                if !Self::is_downstream_authorized(self_mutex.clone(), &m.user_identity)? {
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        OpenMiningChannelError::new_unknown_user(m.get_request_id_as_u32()),
                    )));
                }
                match channel_type {
                    SupportedChannelTypes::Standard => self_mutex
                        .safe_lock(|self_| self_.handle_open_standard_mining_channel(m))?,
                    SupportedChannelTypes::Extended => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL,
                    )),
                    SupportedChannelTypes::Group => self_mutex
                        .safe_lock(|self_| self_.handle_open_standard_mining_channel(m))?,
                    SupportedChannelTypes::GroupAndExtended => self_mutex
                        .safe_lock(|self_| self_.handle_open_standard_mining_channel(m))?,
                }
            }
            Ok(Mining::OpenExtendedMiningChannel(m)) => {
                // check user auth
                if !Self::is_downstream_authorized(self_mutex.clone(), &m.user_identity)? {
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        OpenMiningChannelError::new_unknown_user(m.get_request_id_as_u32()),
                    )));
                };
                match channel_type {
                    SupportedChannelTypes::Standard => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL,
                    )),
                    SupportedChannelTypes::Extended => self_mutex
                        .safe_lock(|self_| self_.handle_open_extended_mining_channel(m))?,
                    SupportedChannelTypes::Group => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL,
                    )),
                    SupportedChannelTypes::GroupAndExtended => self_mutex
                        .safe_lock(|self_| self_.handle_open_extended_mining_channel(m))?,
                }
            }
            Ok(Mining::UpdateChannel(m)) => {
                self_mutex.safe_lock(|self_| self_.handle_update_channel(m))?
            }
            Ok(Mining::SubmitSharesStandard(m)) => match channel_type {
                SupportedChannelTypes::Standard => {
                    self_mutex.safe_lock(|self_| self_.handle_submit_shares_standard(m))?
                }
                SupportedChannelTypes::Extended => Err(Error::UnexpectedMessage(
                    MESSAGE_TYPE_SUBMIT_SHARES_STANDARD,
                )),
                SupportedChannelTypes::Group => {
                    self_mutex.safe_lock(|self_| self_.handle_submit_shares_standard(m))?
                }
                SupportedChannelTypes::GroupAndExtended => {
                    self_mutex.safe_lock(|self_| self_.handle_submit_shares_standard(m))?
                }
            },
            Ok(Mining::SubmitSharesExtended(m)) => match channel_type {
                SupportedChannelTypes::Standard => Err(Error::UnexpectedMessage(
                    MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
                )),
                SupportedChannelTypes::Extended => {
                    self_mutex.safe_lock(|self_| self_.handle_submit_shares_extended(m))?
                }
                SupportedChannelTypes::Group => Err(Error::UnexpectedMessage(
                    MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
                )),
                SupportedChannelTypes::GroupAndExtended => {
                    self_mutex.safe_lock(|self_| self_.handle_submit_shares_extended(m))?
                }
            },
            Ok(Mining::SetCustomMiningJob(m)) => match (channel_type, is_work_selection_enabled) {
                (SupportedChannelTypes::Extended, true) => {
                    self_mutex.safe_lock(|self_| self_.handle_set_custom_mining_job(m))?
                }
                (SupportedChannelTypes::GroupAndExtended, true) => {
                    self_mutex.safe_lock(|self_| self_.handle_set_custom_mining_job(m))?
                }
                _ => Err(Error::UnexpectedMessage(MESSAGE_TYPE_SET_CUSTOM_MINING_JOB)),
            },
            Ok(_) => Err(Error::UnexpectedMessage(0)),
            Err(e) => Err(e),
        }
    }

    /// Checks if work selection is enabled for the downstream connection.
    fn is_work_selection_enabled(&self) -> bool;

    /// Checks if the downstream user is authorized.
    fn is_downstream_authorized(
        _self_mutex: Arc<Mutex<Self>>,
        _user_identity: &binary_sv2::Str0255,
    ) -> Result<bool, Error>;

    /// Handles an `OpenStandardMiningChannel` message.
    fn handle_open_standard_mining_channel(
        &mut self,
        m: OpenStandardMiningChannel,
    ) -> Result<SendTo<Up>, Error>;

    /// Handles an `OpenExtendedMiningChannel` message.
    fn handle_open_extended_mining_channel(
        &mut self,
        m: OpenExtendedMiningChannel,
    ) -> Result<SendTo<Up>, Error>;

    /// Handles an `UpdateChannel` message.
    ///
    /// This method processes an `UpdateChannel` message and updates the channel settings.
    fn handle_update_channel(&mut self, m: UpdateChannel) -> Result<SendTo<Up>, Error>;

    /// Handles a `SubmitSharesStandard` message.
    ///
    /// This method processes a `SubmitSharesStandard` message and validates the submitted shares.
    fn handle_submit_shares_standard(
        &mut self,
        m: SubmitSharesStandard,
    ) -> Result<SendTo<Up>, Error>;

    /// Handles a `SubmitSharesExtended` message.
    ///
    /// This method processes a `SubmitSharesExtended` message and validates the submitted shares.
    fn handle_submit_shares_extended(
        &mut self,
        m: SubmitSharesExtended,
    ) -> Result<SendTo<Up>, Error>;

    /// Handles a `SetCustomMiningJob` message.
    ///
    /// This method processes a `SetCustomMiningJob` message and applies the custom mining job
    /// settings.
    fn handle_set_custom_mining_job(&mut self, m: SetCustomMiningJob) -> Result<SendTo<Up>, Error>;
}

/// A trait defining the parser for upstream mining messages used by a downstream.
///
/// This trait provides the functionality to handle and route various types of mining messages
/// from the upstream based on the message type and payload.
pub trait ParseMiningMessagesFromUpstream<Down: D>
where
    Self: Sized + D,
{
    /// Retrieves the type of the channel supported by this upstream parser.
    fn get_channel_type(&self) -> SupportedChannelTypes;

    /// Parses and routes SV2 mining messages from the upstream based on the message type and
    /// payload. The implementor of DownstreamMining needs to pass a RequestIdMapper if changing
    /// the request ID. Proxies typically need this to ensure the request ID is unique across
    /// the connection.
    fn handle_message_mining(
        self_mutex: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo<Down>, Error> {
        match Self::handle_message_mining_deserialized(
            self_mutex,
            (message_type, payload).try_into(),
        ) {
            Err(Error::UnexpectedMessage(0)) => Err(Error::UnexpectedMessage(message_type)),
            result => result,
        }
    }

    /// Handles the deserialized mining message from the upstream, processing it according to the
    /// routing logic.
    fn handle_message_mining_deserialized(
        self_mutex: Arc<Mutex<Self>>,
        message: Result<Mining, Error>,
    ) -> Result<SendTo<Down>, Error> {
        let (channel_type, is_work_selection_enabled) =
            self_mutex.safe_lock(|s| (s.get_channel_type(), s.is_work_selection_enabled()))?;

        match message {
            Ok(Mining::OpenStandardMiningChannelSuccess(m)) => {
                match channel_type {
                    SupportedChannelTypes::Standard => self_mutex
                        .safe_lock(|s| s.handle_open_standard_mining_channel_success(m))?,
                    SupportedChannelTypes::Extended => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS,
                    )),
                    SupportedChannelTypes::Group => self_mutex
                        .safe_lock(|s| s.handle_open_standard_mining_channel_success(m))?,
                    SupportedChannelTypes::GroupAndExtended => self_mutex
                        .safe_lock(|s| s.handle_open_standard_mining_channel_success(m))?,
                }
            }
            Ok(Mining::OpenExtendedMiningChannelSuccess(m)) => {
                match channel_type {
                    SupportedChannelTypes::Standard => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS,
                    )),
                    SupportedChannelTypes::Extended => self_mutex
                        .safe_lock(|s| s.handle_open_extended_mining_channel_success(m))?,
                    SupportedChannelTypes::Group => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS,
                    )),
                    SupportedChannelTypes::GroupAndExtended => self_mutex
                        .safe_lock(|s| s.handle_open_extended_mining_channel_success(m))?,
                }
            }
            Ok(Mining::OpenMiningChannelError(m)) => {
                self_mutex.safe_lock(|x| x.handle_open_mining_channel_error(m))?
            }
            Ok(Mining::UpdateChannelError(m)) => {
                self_mutex.safe_lock(|x| x.handle_update_channel_error(m))?
            }
            Ok(Mining::CloseChannel(m)) => self_mutex.safe_lock(|x| x.handle_close_channel(m))?,

            Ok(Mining::SetExtranoncePrefix(m)) => {
                self_mutex.safe_lock(|x| x.handle_set_extranonce_prefix(m))?
            }
            Ok(Mining::SubmitSharesSuccess(m)) => {
                self_mutex.safe_lock(|x| x.handle_submit_shares_success(m))?
            }
            Ok(Mining::SubmitSharesError(m)) => {
                self_mutex.safe_lock(|x| x.handle_submit_shares_error(m))?
            }
            Ok(Mining::NewMiningJob(m)) => match channel_type {
                SupportedChannelTypes::Standard => {
                    self_mutex.safe_lock(|x| x.handle_new_mining_job(m))?
                }
                SupportedChannelTypes::Extended => {
                    Err(Error::UnexpectedMessage(MESSAGE_TYPE_NEW_MINING_JOB))
                }
                SupportedChannelTypes::Group => {
                    Err(Error::UnexpectedMessage(MESSAGE_TYPE_NEW_MINING_JOB))
                }
                SupportedChannelTypes::GroupAndExtended => {
                    Err(Error::UnexpectedMessage(MESSAGE_TYPE_NEW_MINING_JOB))
                }
            },
            Ok(Mining::NewExtendedMiningJob(m)) => match channel_type {
                SupportedChannelTypes::Standard => Err(Error::UnexpectedMessage(
                    MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB,
                )),
                SupportedChannelTypes::Extended => {
                    self_mutex.safe_lock(|x| x.handle_new_extended_mining_job(m))?
                }
                SupportedChannelTypes::Group => {
                    self_mutex.safe_lock(|x| x.handle_new_extended_mining_job(m))?
                }
                SupportedChannelTypes::GroupAndExtended => {
                    self_mutex.safe_lock(|x| x.handle_new_extended_mining_job(m))?
                }
            },
            Ok(Mining::SetNewPrevHash(m)) => {
                self_mutex.safe_lock(|x| x.handle_set_new_prev_hash(m))?
            }
            Ok(Mining::SetCustomMiningJobSuccess(m)) => {
                match (channel_type, is_work_selection_enabled) {
                    (SupportedChannelTypes::Extended, true) => {
                        self_mutex.safe_lock(|x| x.handle_set_custom_mining_job_success(m))?
                    }
                    (SupportedChannelTypes::GroupAndExtended, true) => {
                        self_mutex.safe_lock(|x| x.handle_set_custom_mining_job_success(m))?
                    }
                    _ => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS,
                    )),
                }
            }

            Ok(Mining::SetCustomMiningJobError(m)) => {
                match (channel_type, is_work_selection_enabled) {
                    (SupportedChannelTypes::Extended, true) => {
                        self_mutex.safe_lock(|x| x.handle_set_custom_mining_job_error(m))?
                    }
                    (SupportedChannelTypes::Group, true) => {
                        self_mutex.safe_lock(|x| x.handle_set_custom_mining_job_error(m))?
                    }
                    (SupportedChannelTypes::GroupAndExtended, true) => {
                        self_mutex.safe_lock(|x| x.handle_set_custom_mining_job_error(m))?
                    }
                    _ => Err(Error::UnexpectedMessage(
                        MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR,
                    )),
                }
            }
            Ok(Mining::SetTarget(m)) => self_mutex.safe_lock(|x| x.handle_set_target(m))?,
            Ok(Mining::SetGroupChannel(m)) => match channel_type {
                SupportedChannelTypes::Standard => {
                    Err(Error::UnexpectedMessage(MESSAGE_TYPE_SET_GROUP_CHANNEL))
                }
                SupportedChannelTypes::Extended => {
                    Err(Error::UnexpectedMessage(MESSAGE_TYPE_SET_GROUP_CHANNEL))
                }
                SupportedChannelTypes::Group => {
                    self_mutex.safe_lock(|x| x.handle_set_group_channel(m))?
                }
                SupportedChannelTypes::GroupAndExtended => {
                    self_mutex.safe_lock(|x| x.handle_set_group_channel(m))?
                }
            },
            Ok(_) => Err(Error::UnexpectedMessage(0)),
            Err(e) => Err(e),
        }
    }

    /// Determines whether work selection is enabled for this upstream.
    fn is_work_selection_enabled(&self) -> bool;

    /// Handles a successful response for opening a standard mining channel.
    fn handle_open_standard_mining_channel_success(
        &mut self,
        m: OpenStandardMiningChannelSuccess,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles a successful response for opening an extended mining channel.
    fn handle_open_extended_mining_channel_success(
        &mut self,
        m: OpenExtendedMiningChannelSuccess,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles an error when opening a mining channel.
    fn handle_open_mining_channel_error(
        &mut self,
        m: OpenMiningChannelError,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles an error when updating a mining channel.
    fn handle_update_channel_error(&mut self, m: UpdateChannelError)
        -> Result<SendTo<Down>, Error>;

    /// Handles a request to close a mining channel.
    fn handle_close_channel(&mut self, m: CloseChannel) -> Result<SendTo<Down>, Error>;

    /// Handles a request to set the extranonce prefix for mining.
    fn handle_set_extranonce_prefix(
        &mut self,
        m: SetExtranoncePrefix,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles a successful submission of shares.
    fn handle_submit_shares_success(
        &mut self,
        m: SubmitSharesSuccess,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles an error when submitting shares.
    fn handle_submit_shares_error(&mut self, m: SubmitSharesError) -> Result<SendTo<Down>, Error>;

    /// Handles a new mining job.
    fn handle_new_mining_job(&mut self, m: NewMiningJob) -> Result<SendTo<Down>, Error>;

    /// Handles a new extended mining job.
    fn handle_new_extended_mining_job(
        &mut self,
        m: NewExtendedMiningJob,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles a request to set the new previous hash.
    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo<Down>, Error>;

    /// Handles a successful response for setting a custom mining job.
    fn handle_set_custom_mining_job_success(
        &mut self,
        m: SetCustomMiningJobSuccess,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles an error when setting a custom mining job.
    fn handle_set_custom_mining_job_error(
        &mut self,
        m: SetCustomMiningJobError,
    ) -> Result<SendTo<Down>, Error>;

    /// Handles a request to set the target for mining.
    fn handle_set_target(&mut self, m: SetTarget) -> Result<SendTo<Down>, Error>;

    /// Handles a request to set the group channel for mining.
    fn handle_set_group_channel(&mut self, _m: SetGroupChannel) -> Result<SendTo<Down>, Error>;
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/mod.rs">
//! # Handlers
//!
//! This module centralizes the logic for processing and routing Stratum V2 protocol messages,
//! defining traits and utilities to handle messages for both Downstream and Upstream roles.
//!
//! ## Purpose
//!
//! - Standardize the handling of protocol-specific messages.
//! - Enable efficient routing, transformation, and relaying of messages between nodes.
//! - Support modularity and scalability across Stratum V2 subprotocols.
//!
//! ## Structure
//!
//! The module is organized by subprotocol and role, with handler traits for:
//! - `ParseDownstream[Protocol]`: Handles messages from Downstream nodes.
//! - `ParseUpstream[Protocol]`: Handles messages from Upstream nodes.
//!
//! Supported subprotocols include:
//! - `common`: Shared messages across all Sv2 roles.
//! - `job_declaration`: Manages custom mining job declarations, transactions, and solutions.
//! - `mining`: Manages standard mining communication (e.g., job dispatch, shares submission).
//! - `template_distribution`: Handles block templates updates and transaction data.
//! - `Common Messages`: Shared across all Sv2 roles.
//!
//! ## Return Values
//!
//! Handlers return `Result<SendTo_, Error>`, where:
//! - `SendTo_` specifies the action (relay, respond, or no action).
//! - `Error` indicates processing issues.
pub mod common;
pub mod job_declaration;
pub mod mining;
pub mod template_distribution;
use crate::utils::Mutex;
use std::sync::Arc;

#[derive(Debug)]
/// Represents a serializable entity used for communication between Remotes.
/// The `SendTo_` enum adds context to the message, specifying the intended action.
pub enum SendTo_<Message, Remote> {
    /// Relay a new message to a specific remote.
    RelayNewMessageToRemote(Arc<Mutex<Remote>>, Message),
    /// Relay the same received message to a specific remote to avoid extra allocations.
    RelaySameMessageToRemote(Arc<Mutex<Remote>>),
    /// Relay a new message without specifying a specific remote.
    ///
    /// This is common in proxies that translate between SV1 and SV2 protocols, where messages are
    /// often broadcasted via extended channels.
    RelayNewMessage(Message),
    /// Directly respond to a received message.
    Respond(Message),
    /// Relay multiple messages to various destinations.
    Multiple(Vec<SendTo_<Message, Remote>>),
    /// Indicates that no immediate action is required for the message.
    ///
    /// This variant allows for cases where the message is still needed for later processing
    /// (e.g., transformations or when two roles are implemented in the same software).
    None(Option<Message>),
}

impl<SubProtocol, Remote> SendTo_<SubProtocol, Remote> {
    /// Extracts the message, if available.
    pub fn into_message(self) -> Option<SubProtocol> {
        match self {
            Self::RelayNewMessageToRemote(_, m) => Some(m),
            Self::RelaySameMessageToRemote(_) => None,
            Self::RelayNewMessage(m) => Some(m),
            Self::Respond(m) => Some(m),
            Self::Multiple(_) => None,
            Self::None(m) => m,
        }
    }

    /// Extracts the remote, if available.
    pub fn into_remote(self) -> Option<Arc<Mutex<Remote>>> {
        match self {
            Self::RelayNewMessageToRemote(r, _) => Some(r),
            Self::RelaySameMessageToRemote(r) => Some(r),
            Self::RelayNewMessage(_) => None,
            Self::Respond(_) => None,
            Self::Multiple(_) => None,
            Self::None(_) => None,
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/handlers/template_distribution.rs">
//! # Template Distribution Handlers
//!
//! This module defines traits and functions for handling template distribution messages within the
//! Stratum V2 protocol.
//!
//! ## Message Handling
//!
//! Handlers are responsible for:
//! - Parsing and deserializing template distribution messages into appropriate types.
//! - Dispatching the deserialized messages to specific handler functions based on message type,
//!   such as handling new templates, transaction data requests, and coinbase output data.
//!
//! ## Return Type
//!
//! Functions return `Result<SendTo, Error>`, where `SendTo` determines the next action for the
//! message: whether it should be relayed, responded to, or ignored.
//!
//! ## Structure
//!
//! This module includes:
//! - Traits for processing template distribution messages, including server-side and client-side
//!   handling.
//! - Functions to parse, deserialize, and process messages related to template distribution,
//!   ensuring robust error handling.
//! - Error handling mechanisms to address unexpected messages and ensure safe processing,
//!   especially in the context of shared state.

use super::SendTo_;
use crate::{errors::Error, parsers::TemplateDistribution, utils::Mutex};
use template_distribution_sv2::{
    CoinbaseOutputConstraints, NewTemplate, RequestTransactionData, RequestTransactionDataError,
    RequestTransactionDataSuccess, SetNewPrevHash, SubmitSolution,
};

/// see [`SendTo_`]
pub type SendTo = SendTo_<TemplateDistribution<'static>, ()>;
use core::convert::TryInto;
use std::sync::Arc;
use template_distribution_sv2::*;

/// Trait for handling template distribution messages received from server (Template Provider).
/// Includes functions to handle messages such as new templates, previous hash updates, and
/// transaction data requests.
pub trait ParseTemplateDistributionMessagesFromServer
where
    Self: Sized,
{
    /// Handles incoming template distribution messages.
    ///
    /// This function is responsible for parsing and dispatching the appropriate handler based on
    /// the message type. It first deserializes the payload and then routes it to the
    /// corresponding handler function.
    fn handle_message_template_distribution(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_template_distribution_deserialized(
            self_,
            (message_type, payload).try_into(),
        )
    }

    /// Handles deserialized template distribution messages.
    ///
    /// This function takes the deserialized message and processes it according to the specific
    /// message type, invoking the appropriate handler function.
    fn handle_message_template_distribution_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<TemplateDistribution<'_>, Error>,
    ) -> Result<SendTo, Error> {
        // Is ok to unwrap a safe_lock result
        match message {
            Ok(TemplateDistribution::NewTemplate(m)) => {
                self_.safe_lock(|x| x.handle_new_template(m))?
            }
            Ok(TemplateDistribution::SetNewPrevHash(m)) => {
                self_.safe_lock(|x| x.handle_set_new_prev_hash(m))?
            }
            Ok(TemplateDistribution::RequestTransactionDataSuccess(m)) => {
                self_.safe_lock(|x| x.handle_request_tx_data_success(m))?
            }
            Ok(TemplateDistribution::RequestTransactionDataError(m)) => {
                self_.safe_lock(|x| x.handle_request_tx_data_error(m))?
            }
            Ok(TemplateDistribution::CoinbaseOutputConstraints(_)) => Err(
                Error::UnexpectedMessage(MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS),
            ),
            Ok(TemplateDistribution::RequestTransactionData(_)) => Err(Error::UnexpectedMessage(
                MESSAGE_TYPE_REQUEST_TRANSACTION_DATA,
            )),
            Ok(TemplateDistribution::SubmitSolution(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_SUBMIT_SOLUTION))
            }
            Err(e) => Err(e),
        }
    }

    /// Handles a `NewTemplate` message.
    ///
    /// This method processes the `NewTemplate` message, which contains information about a newly
    /// generated template.
    fn handle_new_template(&mut self, m: NewTemplate) -> Result<SendTo, Error>;

    /// Handles a `SetNewPrevHash` message.
    ///
    /// This method processes the `SetNewPrevHash` message, which updates the previous hash for a
    /// template.
    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo, Error>;

    /// Handles a `RequestTransactionDataSuccess` message.
    ///
    /// This method processes the success response for a requested transaction data message.
    fn handle_request_tx_data_success(
        &mut self,
        m: RequestTransactionDataSuccess,
    ) -> Result<SendTo, Error>;

    /// Handles a `RequestTransactionDataError` message.
    ///
    /// This method processes an error response for a requested transaction data message.
    fn handle_request_tx_data_error(
        &mut self,
        m: RequestTransactionDataError,
    ) -> Result<SendTo, Error>;
}

/// Trait for handling template distribution messages received from downstream nodes (client side).
/// Includes functions to handle messages such as coinbase output data size, transaction data
/// requests, and solution submissions.
pub trait ParseTemplateDistributionMessagesFromClient
where
    Self: Sized,
{
    /// Handles incoming template distribution messages.
    ///
    /// This function is responsible for parsing and dispatching the appropriate handler based on
    /// the message type. It first deserializes the payload and then routes it to the
    /// corresponding handler function.
    fn handle_message_template_distribution(
        self_: Arc<Mutex<Self>>,
        message_type: u8,
        payload: &mut [u8],
    ) -> Result<SendTo, Error> {
        Self::handle_message_template_distribution_deserialized(
            self_,
            (message_type, payload).try_into(),
        )
    }

    /// Handles deserialized template distribution messages.
    ///
    /// This function takes the deserialized message and processes it according to the specific
    /// message type, invoking the appropriate handler function.
    fn handle_message_template_distribution_deserialized(
        self_: Arc<Mutex<Self>>,
        message: Result<TemplateDistribution<'_>, Error>,
    ) -> Result<SendTo, Error> {
        match message {
            Ok(TemplateDistribution::CoinbaseOutputConstraints(m)) => {
                self_.safe_lock(|x| x.handle_coinbase_out_data_size(m))?
            }
            Ok(TemplateDistribution::RequestTransactionData(m)) => {
                self_.safe_lock(|x| x.handle_request_tx_data(m))?
            }
            Ok(TemplateDistribution::SubmitSolution(m)) => {
                self_.safe_lock(|x| x.handle_request_submit_solution(m))?
            }
            Ok(TemplateDistribution::NewTemplate(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_NEW_TEMPLATE))
            }
            Ok(TemplateDistribution::SetNewPrevHash(_)) => {
                Err(Error::UnexpectedMessage(MESSAGE_TYPE_SET_NEW_PREV_HASH))
            }
            Ok(TemplateDistribution::RequestTransactionDataSuccess(_)) => Err(
                Error::UnexpectedMessage(MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS),
            ),
            Ok(TemplateDistribution::RequestTransactionDataError(_)) => Err(
                Error::UnexpectedMessage(MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR),
            ),
            Err(e) => Err(e),
        }
    }

    /// Handles a `CoinbaseOutputConstraints` message.
    ///
    /// This method processes a message that includes the coinbase output data size.
    fn handle_coinbase_out_data_size(
        &mut self,
        m: CoinbaseOutputConstraints,
    ) -> Result<SendTo, Error>;

    /// Handles a `RequestTransactionData` message.
    ///
    /// This method processes a message requesting transaction data.
    fn handle_request_tx_data(&mut self, m: RequestTransactionData) -> Result<SendTo, Error>;

    /// Handles a `SubmitSolution` message.
    ///
    /// This method processes a solution submission message.
    fn handle_request_submit_solution(&mut self, m: SubmitSolution) -> Result<SendTo, Error>;
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/job_creator.rs">
//! # Job Creator
//!
//! This module provides logic to create extended mining jobs given a template from
//! a template provider as well as logic to clean up old templates when new blocks are mined.
use crate::{errors, utils::Id, Error};
use bitcoin::{
    absolute::LockTime,
    blockdata::{
        transaction::{OutPoint, Transaction, TxIn, TxOut, Version},
        witness::Witness,
    },
    consensus,
    consensus::Decodable,
    Amount,
};
use codec_sv2::binary_sv2::{self, B064K};
use mining_sv2::NewExtendedMiningJob;
use nohash_hasher::BuildNoHashHasher;
use std::{collections::HashMap, convert::TryInto};
use template_distribution_sv2::{NewTemplate, SetNewPrevHash};
use tracing::debug;

#[derive(Debug)]
pub struct JobsCreators {
    lasts_new_template: Vec<NewTemplate<'static>>,
    job_to_template_id: HashMap<u32, u64, BuildNoHashHasher<u32>>,
    templte_to_job_id: HashMap<u64, u32, BuildNoHashHasher<u64>>,
    ids: Id,
    last_target: mining_sv2::Target,
    last_ntime: Option<u32>,
    extranonce_len: u8,
}

/// Transforms the byte array `coinbase_outputs` in a vector of TxOut
/// It assumes the data to be valid data and does not do any kind of check
pub fn tx_outputs_to_costum_scripts(tx_outputs: &[u8]) -> Vec<TxOut> {
    let mut txs = vec![];
    let mut cursor = 0;
    let mut txouts = &tx_outputs[cursor..];
    while let Ok(out) = TxOut::consensus_decode(&mut txouts) {
        let len = match out.script_pubkey.len() {
            a @ 0..=252 => 8 + 1 + a,
            a @ 253..=10000 => 8 + 3 + a,
            _ => break,
        };
        cursor += len;
        txs.push(out)
    }
    txs
}

impl JobsCreators {
    /// Constructor
    pub fn new(extranonce_len: u8) -> Self {
        Self {
            lasts_new_template: Vec::new(),
            job_to_template_id: HashMap::with_hasher(BuildNoHashHasher::default()),
            templte_to_job_id: HashMap::with_hasher(BuildNoHashHasher::default()),
            ids: Id::new(),
            last_target: mining_sv2::Target::new(0, 0),
            last_ntime: None,
            extranonce_len,
        }
    }

    /// Get template id from job
    pub fn get_template_id_from_job(&self, job_id: u32) -> Option<u64> {
        self.job_to_template_id.get(&job_id).map(|x| x - 1)
    }

    /// Used to create new jobs when a new template arrives
    pub fn on_new_template(
        &mut self,
        template: &mut NewTemplate,
        version_rolling_allowed: bool,
        mut pool_coinbase_outputs: Vec<TxOut>,
    ) -> Result<NewExtendedMiningJob<'static>, Error> {
        let server_tx_outputs = template.coinbase_tx_outputs.to_vec();
        let mut outputs = tx_outputs_to_costum_scripts(&server_tx_outputs);
        pool_coinbase_outputs.append(&mut outputs);

        // This is to make sure that 0 is never used, so we can use 0 for
        // set_new_prev_hashes that do not refer to any future job/template if needed
        // Then we will do the inverse (-1) where needed
        let template_id = template.template_id + 1;
        self.lasts_new_template.push(template.as_static());
        let next_job_id = self.ids.next();
        self.job_to_template_id.insert(next_job_id, template_id);
        self.templte_to_job_id.insert(template_id, next_job_id);
        new_extended_job(
            template,
            &mut pool_coinbase_outputs,
            next_job_id,
            version_rolling_allowed,
            self.extranonce_len,
            self.last_ntime,
        )
    }

    pub(crate) fn reset_new_templates(&mut self, template: Option<NewTemplate<'static>>) {
        match template {
            Some(t) => self.lasts_new_template = vec![t],
            None => self.lasts_new_template = vec![],
        }
    }

    /// When we get a new `SetNewPrevHash` we need to clear all the other templates and only
    /// keep the one that matches the template_id of the new prev hash. If none match then
    /// we clear all the saved templates.
    pub fn on_new_prev_hash(&mut self, prev_hash: &SetNewPrevHash<'static>) -> Option<u32> {
        self.last_target = prev_hash.target.clone().into();
        self.last_ntime = prev_hash.header_timestamp.into(); // set correct ntime
        let template: Vec<NewTemplate<'static>> = self
            .lasts_new_template
            .clone()
            .into_iter()
            .filter(|a| a.template_id == prev_hash.template_id)
            .collect();
        match template.len() {
            0 => {
                self.reset_new_templates(None);
                None
            }
            1 => {
                self.reset_new_templates(Some(template[0].clone()));

                self.templte_to_job_id
                    .get(&(prev_hash.template_id + 1))
                    .copied()
            }
            // TODO how many templates can we have at max
            _ => todo!("{:#?}", template.len()),
        }
    }

    /// Returns the latest mining target
    pub fn last_target(&self) -> mining_sv2::Target {
        self.last_target.clone()
    }
}

/// Converts custom job into extended job
pub fn extended_job_from_custom_job(
    referenced_job: &mining_sv2::SetCustomMiningJob,
    extranonce_len: u8,
) -> Result<NewExtendedMiningJob<'static>, Error> {
    let mut outputs =
        tx_outputs_to_costum_scripts(referenced_job.coinbase_tx_outputs.clone().as_ref());

    let mut template_value = 0;
    for output in &outputs {
        template_value += output.value.to_sat();
    }

    let mut template = NewTemplate {
        template_id: 0,
        future_template: false,
        version: referenced_job.version,
        coinbase_tx_version: referenced_job.coinbase_tx_version,
        coinbase_prefix: referenced_job.coinbase_prefix.clone(),
        coinbase_tx_input_sequence: referenced_job.coinbase_tx_input_n_sequence,
        coinbase_tx_value_remaining: template_value,
        coinbase_tx_outputs_count: outputs.len() as u32,
        coinbase_tx_outputs: referenced_job.coinbase_tx_outputs.clone(),
        coinbase_tx_locktime: referenced_job.coinbase_tx_locktime,
        merkle_path: referenced_job.merkle_path.clone(),
    };
    new_extended_job(
        &mut template,
        &mut outputs,
        0,
        true,
        extranonce_len,
        Some(referenced_job.min_ntime),
    )
}

// Returns an extended job given the provided template from the Template Provider and other
// Pool role related fields.
//
// Pool related arguments:
//
// * `coinbase_outputs`: coinbase output transactions specified by the pool.
// * `job_id`: incremented job identifier specified by the pool.
// * `version_rolling_allowed`: boolean specified by the channel.
// * `extranonce_len`: extranonce length specified by the channel.
fn new_extended_job(
    new_template: &mut NewTemplate,
    coinbase_outputs: &mut [TxOut],
    job_id: u32,
    version_rolling_allowed: bool,
    extranonce_len: u8,
    ntime: Option<u32>,
) -> Result<NewExtendedMiningJob<'static>, Error> {
    coinbase_outputs[0].value = match new_template.coinbase_tx_value_remaining.checked_mul(1) {
        //check that value_remaining is updated by TP
        Some(result) => Amount::from_sat(result),
        None => return Err(Error::ValueRemainingNotUpdated),
    };
    let tx_version = new_template
        .coinbase_tx_version
        .try_into()
        .map_err(|_| Error::TxVersionTooBig)?;

    let script_sig_prefix = new_template.coinbase_prefix.to_vec();
    let script_sig_prefix_len = script_sig_prefix.len();

    let coinbase = coinbase(
        script_sig_prefix,
        tx_version,
        new_template.coinbase_tx_locktime,
        new_template.coinbase_tx_input_sequence,
        coinbase_outputs,
        extranonce_len,
    )?;

    let min_ntime = binary_sv2::Sv2Option::new(if new_template.future_template {
        None
    } else {
        ntime
    });

    let new_extended_mining_job: NewExtendedMiningJob<'static> = NewExtendedMiningJob {
        channel_id: 0,
        job_id,
        min_ntime,
        version: new_template.version,
        version_rolling_allowed,
        merkle_path: new_template.merkle_path.clone().into_static(),
        coinbase_tx_prefix: coinbase_tx_prefix(&coinbase, script_sig_prefix_len)?,
        coinbase_tx_suffix: coinbase_tx_suffix(&coinbase, extranonce_len, script_sig_prefix_len)?,
    };

    debug!(
        "New extended mining job created: {:?}",
        new_extended_mining_job
    );
    Ok(new_extended_mining_job)
}

// Used to extract the coinbase transaction prefix for extended jobs
// so the extranonce search space can be introduced
fn coinbase_tx_prefix(
    coinbase: &Transaction,
    script_sig_prefix_len: usize,
) -> Result<B064K<'static>, Error> {
    let encoded = consensus::serialize(coinbase);
    // If script_prefix_len is not 0 we are not in a test environment and the coinbase will have the
    // 0 witness
    let segwit_bytes = match script_sig_prefix_len {
        0 => 0,
        _ => 2,
    };
    let index = 4    // tx version
        + segwit_bytes
        + 1  // number of inputs TODO can be also 3
        + 32 // prev OutPoint
        + 4  // index
        + 1  // bytes in script TODO can be also 3
        + script_sig_prefix_len; // script_sig_prefix
    let r = encoded[0..index].to_vec();
    r.try_into().map_err(Error::BinarySv2Error)
}

// Used to extract the coinbase transaction suffix for extended jobs
// so the extranonce search space can be introduced
fn coinbase_tx_suffix(
    coinbase: &Transaction,
    extranonce_len: u8,
    script_sig_prefix_len: usize,
) -> Result<B064K<'static>, Error> {
    let encoded = consensus::serialize(coinbase);
    // If script_sig_prefix_len is not 0 we are not in a test environment and the coinbase have the
    // 0 witness
    let segwit_bytes = match script_sig_prefix_len {
        0 => 0,
        _ => 2,
    };
    let r = encoded[4    // tx version
        + segwit_bytes
        + 1  // number of inputs TODO can be also 3
        + 32 // prev OutPoint
        + 4  // index
        + 1  // bytes in script TODO can be also 3
        + script_sig_prefix_len  // script_sig_prefix
        + (extranonce_len as usize)..]
        .to_vec();
    r.try_into().map_err(Error::BinarySv2Error)
}

// try to build a Transaction coinbase
fn coinbase(
    script_sig_prefix: Vec<u8>,
    version: i32,
    lock_time: u32,
    sequence: u32,
    coinbase_outputs: &[TxOut],
    extranonce_len: u8,
) -> Result<Transaction, Error> {
    // If script_sig_prefix_len is not 0 we are not in a test environment and the coinbase have the
    // 0 witness
    let witness = match script_sig_prefix.len() {
        0 => Witness::from(vec![] as Vec<Vec<u8>>),
        _ => Witness::from(vec![vec![0; 32]]),
    };
    let mut script_sig = script_sig_prefix;
    script_sig.extend_from_slice(&vec![0; extranonce_len as usize]);
    let tx_in = TxIn {
        previous_output: OutPoint::null(),
        script_sig: script_sig.into(),
        sequence: bitcoin::Sequence(sequence),
        witness,
    };
    Ok(Transaction {
        version: Version::non_standard(version),
        lock_time: LockTime::from_consensus(lock_time),
        input: vec![tx_in],
        output: coinbase_outputs.to_vec(),
    })
}

/// Helper type to strip a segwit data from the coinbase_tx_prefix and coinbase_tx_suffix
/// to ensure miners are hashing with the correct coinbase
pub fn extended_job_to_non_segwit(
    job: NewExtendedMiningJob<'static>,
    full_extranonce_len: usize,
) -> Result<NewExtendedMiningJob<'static>, Error> {
    let mut encoded = job.coinbase_tx_prefix.to_vec();
    // just add empty extranonce space so it can be deserialized. The real extranonce
    // should be inserted based on the miner's shares
    let extranonce = vec![0_u8; full_extranonce_len];
    encoded.extend_from_slice(&extranonce[..]);
    encoded.extend_from_slice(job.coinbase_tx_suffix.inner_as_ref());
    let coinbase = consensus::deserialize(&encoded).map_err(|_| Error::InvalidCoinbase)?;
    let stripped_tx = StrippedCoinbaseTx::from_coinbase(coinbase, full_extranonce_len)?;

    Ok(NewExtendedMiningJob {
        channel_id: job.channel_id,
        job_id: job.job_id,
        min_ntime: job.min_ntime,
        version: job.version,
        version_rolling_allowed: job.version_rolling_allowed,
        merkle_path: job.merkle_path,
        coinbase_tx_prefix: stripped_tx.into_coinbase_tx_prefix()?,
        coinbase_tx_suffix: stripped_tx.into_coinbase_tx_suffix()?,
    })
}
// Helper type to strip a segwit data from the coinbase_tx_prefix and coinbase_tx_suffix
// to ensure miners are hashing with the correct coinbase
struct StrippedCoinbaseTx {
    version: u32,
    inputs: Vec<Vec<u8>>,
    outputs: Vec<Vec<u8>>,
    lock_time: u32,
    // helper field
    bip141_bytes_len: usize,
}

impl StrippedCoinbaseTx {
    // create
    fn from_coinbase(tx: Transaction, full_extranonce_len: usize) -> Result<Self, Error> {
        let bip141_bytes_len = tx
            .input
            .last()
            .ok_or(Error::BadPayloadSize)?
            .script_sig
            .len()
            - full_extranonce_len;
        Ok(Self {
            version: tx.version.0 as u32,
            inputs: tx
                .input
                .iter()
                .map(|txin| {
                    let mut ser: Vec<u8> = vec![];
                    ser.extend_from_slice(txin.previous_output.txid.as_ref());
                    ser.extend_from_slice(&txin.previous_output.vout.to_le_bytes());
                    ser.push(txin.script_sig.len() as u8);
                    ser.extend_from_slice(txin.script_sig.as_bytes());
                    ser.extend_from_slice(&txin.sequence.0.to_le_bytes());
                    ser
                })
                .collect(),
            outputs: tx.output.iter().map(consensus::serialize).collect(),
            lock_time: tx.lock_time.to_consensus_u32(),
            bip141_bytes_len,
        })
    }

    // The coinbase tx prefix is the LE bytes concatenation of the tx version and all
    // of the tx inputs minus the 32 bytes after the script_sig_prefix bytes
    // and the last input's sequence (used as the first entry in the coinbase tx suffix).
    // The last 32 bytes after the bip34 bytes in the script will be used to allow extranonce
    // space for the miner. We remove the bip141 marker and flag since it is only used for
    // computing the `wtxid` and the legacy `txid` is what is used for computing the merkle root
    // clippy allow because we don't want to consume self
    #[allow(clippy::wrong_self_convention)]
    fn into_coinbase_tx_prefix(&self) -> Result<B064K<'static>, errors::Error> {
        let mut inputs = self.inputs.clone();
        let last_input = inputs.last_mut().ok_or(Error::BadPayloadSize)?;
        let new_last_input_len =
            32 // outpoint
                + 4 // vout
                + 1 // script length byte -> TODO can be also 3 (based on TODO in `coinbase_tx_prefix()`)
                + self.bip141_bytes_len // space for bip34 bytes
            ;
        last_input.truncate(new_last_input_len);
        let mut prefix: Vec<u8> = vec![];
        prefix.extend_from_slice(&self.version.to_le_bytes());
        prefix.push(self.inputs.len() as u8);
        prefix.extend_from_slice(&inputs.concat());
        prefix.try_into().map_err(Error::BinarySv2Error)
    }

    // This coinbase tx suffix is the sequence of the last tx input plus
    // the serialized tx outputs and the lock time. Note we do not use the witnesses
    // (placed between txouts and lock time) since it is only used for
    // computing the `wtxid` and the legacy `txid` is what is used for computing the merkle root
    // clippy allow because we don't want to consume self
    #[allow(clippy::wrong_self_convention)]
    fn into_coinbase_tx_suffix(&self) -> Result<B064K<'static>, errors::Error> {
        let mut suffix: Vec<u8> = vec![];
        let last_input = self.inputs.last().ok_or(Error::BadPayloadSize)?;
        // only take the last intput's sequence u32 (bytes after the extranonce space)
        let last_input_sequence = &last_input[last_input.len() - 4..];
        suffix.extend_from_slice(last_input_sequence);
        suffix.push(self.outputs.len() as u8);
        suffix.extend_from_slice(&self.outputs.concat());
        suffix.extend_from_slice(&self.lock_time.to_le_bytes());
        suffix.try_into().map_err(Error::BinarySv2Error)
    }
}

// Test
#[cfg(test)]

pub mod tests {
    use super::*;
    use crate::utils::merkle_root_from_path;
    #[cfg(feature = "prop_test")]
    use codec_sv2::binary_sv2::u256_from_int;
    use quickcheck::{Arbitrary, Gen};
    use std::{cmp, vec};

    #[cfg(feature = "prop_test")]
    use std::borrow::BorrowMut;

    use bitcoin::{consensus::Encodable, secp256k1::Secp256k1, Network, PrivateKey, PublicKey};

    pub fn template_from_gen(g: &mut Gen) -> NewTemplate<'static> {
        let mut coinbase_prefix_gen = Gen::new(255);
        let mut coinbase_prefix: vec::Vec<u8> = vec::Vec::new();

        let max_num_for_script_sig_prefix = 253;
        let prefix_len = cmp::min(u8::arbitrary(&mut coinbase_prefix_gen), 6);
        coinbase_prefix.push(prefix_len);
        coinbase_prefix.resize_with(prefix_len as usize + 2, || {
            cmp::min(
                u8::arbitrary(&mut coinbase_prefix_gen),
                max_num_for_script_sig_prefix,
            )
        });
        let coinbase_prefix: binary_sv2::B0255 = coinbase_prefix.try_into().unwrap();

        let mut coinbase_tx_outputs_gen = Gen::new(32);
        let mut coinbase_tx_outputs_inner: vec::Vec<u8> = vec::Vec::new();
        coinbase_tx_outputs_inner.resize_with(32, || u8::arbitrary(&mut coinbase_tx_outputs_gen));
        let coinbase_tx_outputs: binary_sv2::B064K = coinbase_tx_outputs_inner.try_into().unwrap();

        let mut merkle_path_inner_gen = Gen::new(32);
        let mut merkle_path_inner: vec::Vec<u8> = vec::Vec::new();
        merkle_path_inner.resize_with(32, || u8::arbitrary(&mut merkle_path_inner_gen));
        let merkle_path_inner: binary_sv2::U256 = merkle_path_inner.try_into().unwrap();
        let merkle_path: binary_sv2::Seq0255<binary_sv2::U256> = vec![merkle_path_inner].into();

        NewTemplate {
            template_id: u64::arbitrary(g),
            future_template: bool::arbitrary(g),
            version: u32::arbitrary(g),
            coinbase_tx_version: 2,
            coinbase_prefix,
            coinbase_tx_input_sequence: u32::arbitrary(g),
            coinbase_tx_value_remaining: u64::arbitrary(g),
            coinbase_tx_outputs_count: 0,
            coinbase_tx_outputs,
            coinbase_tx_locktime: u32::arbitrary(g),
            merkle_path,
        }
    }

    const PRIVATE_KEY_BTC: [u8; 32] = [34; 32];
    const NETWORK: Network = Network::Testnet;

    #[cfg(feature = "prop_test")]
    const BLOCK_REWARD: u64 = 625_000_000_000;

    pub fn new_pub_key() -> PublicKey {
        let priv_k = PrivateKey::from_slice(&PRIVATE_KEY_BTC, NETWORK).unwrap();
        let secp = Secp256k1::default();

        PublicKey::from_private_key(&secp, &priv_k)
    }

    #[cfg(feature = "prop_test")]
    use bitcoin::ScriptBuf;

    // Test job_id_from_template
    #[cfg(feature = "prop_test")]
    #[quickcheck_macros::quickcheck]
    fn test_job_id_from_template(mut template: NewTemplate<'static>) {
        let mut prefix = template.coinbase_prefix.to_vec();
        if prefix.len() > 0 {
            let len = u8::min(prefix[0], 6);
            prefix[0] = len;
            prefix.resize(len as usize + 2, 0);
            template.coinbase_prefix = prefix.try_into().unwrap();
        };
        let out = TxOut {
            value: Amount::from_sat(BLOCK_REWARD),
            script_pubkey: ScriptBuf::new_p2pk(&new_pub_key()),
        };
        let mut jobs_creators = JobsCreators::new(32);

        let job = jobs_creators
            .on_new_template(template.borrow_mut(), false, vec![out])
            .unwrap();

        assert_eq!(
            jobs_creators.get_template_id_from_job(job.job_id),
            Some(template.template_id)
        );

        // Assert returns non if no match
        assert_eq!(jobs_creators.get_template_id_from_job(70), None);
    }

    // Test reset new template
    #[cfg(feature = "prop_test")]
    #[quickcheck_macros::quickcheck]
    fn test_reset_new_template(mut template: NewTemplate<'static>) {
        let out = TxOut {
            value: Amount::from_sat(BLOCK_REWARD),
            script_pubkey: ScriptBuf::new_p2pk(&new_pub_key()),
        };
        let mut jobs_creators = JobsCreators::new(32);

        assert_eq!(jobs_creators.lasts_new_template.len(), 0);

        let _ = jobs_creators.on_new_template(template.borrow_mut(), false, vec![out]);

        assert_eq!(jobs_creators.lasts_new_template.len(), 1);
        assert_eq!(jobs_creators.lasts_new_template[0], template);

        //Create a 2nd template
        let mut template2 = template_from_gen(&mut Gen::new(255));
        template2.template_id = template.template_id.checked_sub(1).unwrap_or(0);

        // Reset new template
        jobs_creators.reset_new_templates(Some(template2.clone()));

        // Should be pointing at new template
        assert_eq!(jobs_creators.lasts_new_template.len(), 1);
        assert_eq!(jobs_creators.lasts_new_template[0], template2);

        // Reset new template
        jobs_creators.reset_new_templates(None);

        // Should be pointing at new template
        assert_eq!(jobs_creators.lasts_new_template.len(), 0);
    }

    // Test on_new_prev_hash
    #[cfg(feature = "prop_test")]
    #[quickcheck_macros::quickcheck]
    fn test_on_new_prev_hash(mut template: NewTemplate<'static>) {
        let out = TxOut {
            value: Amount::from_sat(BLOCK_REWARD),
            script_pubkey: ScriptBuf::new_p2pk(&new_pub_key()),
        };
        let mut jobs_creators = JobsCreators::new(32);

        //Create a template
        let _ = jobs_creators.on_new_template(template.borrow_mut(), false, vec![out]);
        let test_id = template.template_id;

        // Create a SetNewPrevHash with matching template_id
        let prev_hash = SetNewPrevHash {
            template_id: test_id,
            prev_hash: u256_from_int(45_u32),
            header_timestamp: 0,
            n_bits: 0,
            target: ([0_u8; 32]).try_into().unwrap(),
        };

        jobs_creators.on_new_prev_hash(&prev_hash);

        //Validate that we still have the same template loaded as there were matching templateIds
        assert_eq!(jobs_creators.lasts_new_template.len(), 1);
        assert_eq!(jobs_creators.lasts_new_template[0], template);

        // Create a SetNewPrevHash with matching template_id
        let test_id_2 = test_id.wrapping_add(1);
        let prev_hash2 = SetNewPrevHash {
            template_id: test_id_2,
            prev_hash: u256_from_int(45_u32),
            header_timestamp: 0,
            n_bits: 0,
            target: ([0_u8; 32]).try_into().unwrap(),
        };

        jobs_creators.on_new_prev_hash(&prev_hash2);

        //Validate that templates were cleared as we got a new templateId in setNewPrevHash
        assert_eq!(jobs_creators.lasts_new_template.len(), 0);
    }

    #[quickcheck_macros::quickcheck]
    fn it_parse_valid_tx_outs(
        mut hash1: Vec<u8>,
        mut hash2: Vec<u8>,
        value1: u64,
        value2: u64,
        size1: u8,
        size2: u8,
    ) {
        hash1.resize(size1 as usize + 2, 0);
        hash2.resize(size2 as usize + 2, 0);
        let tx1 = TxOut {
            value: Amount::from_sat(value1),
            script_pubkey: hash1.into(),
        };
        let tx2 = TxOut {
            value: Amount::from_sat(value2),
            script_pubkey: hash2.into(),
        };
        let mut encoded1 = vec![];
        let mut encoded2 = vec![];
        tx1.consensus_encode(&mut encoded1).unwrap();
        tx2.consensus_encode(&mut encoded2).unwrap();
        let mut encoded = vec![];
        encoded.append(&mut encoded1.clone());
        encoded.append(&mut encoded2.clone());
        let outs = tx_outputs_to_costum_scripts(&encoded[..]);
        assert!(outs[0] == tx1);
        assert!(outs[1] == tx2);
    }

    // test that witness stripped tx id matches that of the txid of the coinbase
    #[test]
    fn stripped_tx_id() {
        let encoded: &[u8] = &[
            2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 36, 2, 107, 22, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
            255, 255, 2, 0, 0, 0, 0, 0, 0, 0, 0, 67, 65, 4, 70, 109, 127, 202, 229, 99, 229, 203,
            9, 160, 209, 135, 11, 181, 128, 52, 72, 4, 97, 120, 121, 161, 73, 73, 207, 34, 40, 95,
            27, 174, 63, 39, 103, 40, 23, 108, 60, 100, 49, 248, 238, 218, 69, 56, 220, 55, 200,
            101, 226, 120, 79, 58, 158, 119, 208, 68, 243, 62, 64, 119, 151, 225, 39, 138, 172, 0,
            0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 226, 246, 28, 63, 113, 209, 222,
            253, 63, 169, 153, 223, 163, 105, 83, 117, 92, 105, 6, 137, 121, 153, 98, 180, 139,
            235, 216, 54, 151, 78, 140, 249, 1, 32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        ];
        let coinbase: Transaction = consensus::deserialize(encoded).unwrap();
        let stripped = StrippedCoinbaseTx::from_coinbase(coinbase.clone(), 32).unwrap();
        let prefix = stripped.into_coinbase_tx_prefix().unwrap().to_vec();
        let suffix = stripped.into_coinbase_tx_suffix().unwrap().to_vec();
        let extranonce = &[0_u8; 32];
        let path: &[binary_sv2::U256] = &[];
        let stripped_merkle_root =
            merkle_root_from_path(&prefix[..], &suffix[..], extranonce, path).unwrap();
        let txid = coinbase.compute_txid();
        let txid_bytes: &[u8; 32] = txid.as_ref();
        let og_merkle_root = txid_bytes.to_vec();
        assert!(
            stripped_merkle_root == og_merkle_root,
            "stripped tx hash is not the same as bitcoin crate"
        );
    }
    #[test]
    fn stripped_tx_id_braiins_example() {
        let mut encoded = vec![];
        let coinbase_prefix = &[
            1_u8, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 75, 3, 176, 235, 11, 250, 190, 109, 109,
            50, 247, 22, 140, 225, 176, 1, 231, 78, 225, 50, 226, 181, 165, 55, 145, 137, 154, 46,
            9, 44, 65, 72, 231, 173, 111, 131, 26, 81, 223, 179, 225, 1, 0, 0, 0, 0, 0, 0, 0,
        ];
        let coinbase_suffix = &[
            245_u8, 192, 42, 69, 19, 47, 115, 108, 117, 115, 104, 47, 0, 0, 0, 0, 3, 78, 213, 148,
            39, 0, 0, 0, 0, 25, 118, 169, 20, 124, 21, 78, 209, 220, 89, 96, 158, 61, 38, 171, 178,
            223, 46, 163, 213, 135, 205, 140, 65, 136, 172, 0, 0, 0, 0, 0, 0, 0, 0, 44, 106, 76,
            41, 82, 83, 75, 66, 76, 79, 67, 75, 58, 214, 9, 239, 96, 221, 25, 108, 87, 155, 50, 55,
            47, 91, 115, 172, 168, 0, 12, 86, 195, 26, 241, 10, 22, 190, 151, 254, 24, 0, 78, 106,
            26, 0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 103, 66, 68, 105, 2, 55,
            65, 241, 216, 46, 82, 223, 150, 0, 97, 103, 2, 82, 186, 233, 145, 90, 210, 231, 35,
            100, 107, 52, 171, 233, 50, 200, 0, 0, 0, 0,
        ];
        let extranonce = [0_u8; 15]; // braiins pool requires 15 bytes for extranonce
        encoded.extend_from_slice(coinbase_prefix);
        let mut encoded_clone = encoded.clone();
        encoded_clone.extend_from_slice(&extranonce);
        encoded_clone.extend_from_slice(coinbase_suffix);
        // let mut i = 1;
        // while let Err(_) = Transaction::deserialize(&encoded_clone) {
        //     encoded_clone = encoded.clone();
        //     extranonce.push(0);
        //     encoded_clone.extend_from_slice(&extranonce[..]);
        //     encoded_clone.extend_from_slice(coinbase_suffix);
        //     i+=1;
        // }
        // println!("SIZE: {:?}", i);
        let _tx: Transaction = consensus::deserialize(&encoded_clone).unwrap();
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/lib.rs">
//! # Stratum V2 Roles-Logic Library
//!
//! roles_logic_sv2 provides the core logic and utilities for implementing roles in the Stratum V2
//! (Sv2) protocol, such as miners, pools, and proxies. It abstracts message handling, channel
//! management, job creation, and routing logic, enabling efficient and secure communication across
//! upstream and downstream connections.
//!
//! ## Usage
//!
//! To include this crate in your project, run:
//! ```bash
//! $ cargo add roles_logic_sv2
//! ```
//!
//! ## Build Options
//!
//! This crate can be built with the following features:
//!
//! - `prop_test`: Enables support for property testing in [`template_distribution_sv2`] crate.
pub mod channel_logic;
pub mod channels;
pub mod errors;
pub mod handlers;
pub mod job_creator;
pub mod parsers;
pub mod utils;
pub mod vardiff;
pub use bitcoin;
pub use codec_sv2;
pub use common_messages_sv2;
pub use errors::Error;
pub use job_declaration_sv2;
pub use mining_sv2;
pub use template_distribution_sv2;
pub use vardiff::{classic::VardiffState, Vardiff};
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/parsers.rs">
//! # Parsing, Serializing, and Message Type Identification
//!
//! Provides logic to convert raw Stratum V2 (Sv2) message data into Rust types, as well as logic
//! to handle conversions among Sv2 rust types.
//!
//! Most of the logic on this module is tightly coupled with the [`binary_sv2`] crate.
//!
//! ## Responsibilities
//! - **Parsing**: Converts raw Sv2 message bytes into Rust enums ([`CommonMessages`], [`Mining`],
//!   etc.).
//! - **Serialization**: Converts Rust enums back into binary format for transmission.
//! - **Protocol Abstraction**: Separates logic for different Sv2 subprotocols, ensuring modular and
//!   extensible design.
//! - **Message Metadata**: Identifies message types and channel bits for routing and processing.
//!
//! ## Supported Subprotocols
//! - **Common Messages**: Shared across all Sv2 roles.
//! - **Template Distribution**: Handles block templates updates and transaction data.
//! - **Job Declaration**: Manages custom mining job declarations, transactions, and solutions.
//! - **Mining Protocol**: Manages standard mining communication (e.g., job dispatch, shares
//!   submission).

use crate::Error;
use codec_sv2::{
    binary_sv2::{
        self,
        decodable::{DecodableField, FieldMarker},
        encodable::EncodableField,
        from_bytes, Deserialize, GetSize,
    },
    framing_sv2::framing::Sv2Frame,
};
use common_messages_sv2::*;
use core::{
    convert::{TryFrom, TryInto},
    fmt,
};
use job_declaration_sv2::*;
use mining_sv2::*;
use template_distribution_sv2::*;

use common_messages_sv2::{
    ChannelEndpointChanged, Reconnect, SetupConnection, SetupConnectionError,
    SetupConnectionSuccess,
};
use job_declaration_sv2::{
    AllocateMiningJobToken, AllocateMiningJobTokenSuccess, DeclareMiningJob, DeclareMiningJobError,
    DeclareMiningJobSuccess, ProvideMissingTransactions, ProvideMissingTransactionsSuccess,
    PushSolution,
};
use mining_sv2::{
    CloseChannel, NewExtendedMiningJob, NewMiningJob, OpenExtendedMiningChannel,
    OpenExtendedMiningChannelSuccess, OpenMiningChannelError, OpenStandardMiningChannel,
    OpenStandardMiningChannelSuccess, SetCustomMiningJob, SetCustomMiningJobError,
    SetCustomMiningJobSuccess, SetExtranoncePrefix, SetGroupChannel,
    SetNewPrevHash as MiningSetNewPrevHash, SetTarget, SubmitSharesError, SubmitSharesExtended,
    SubmitSharesStandard, SubmitSharesSuccess, UpdateChannel, UpdateChannelError,
};
use template_distribution_sv2::{
    CoinbaseOutputConstraints, NewTemplate, RequestTransactionData, RequestTransactionDataError,
    RequestTransactionDataSuccess, SetNewPrevHash, SubmitSolution,
};

/// Converts a message type number to a human-readable name
pub fn message_type_to_name(msg_type: u8) -> &'static str {
    match msg_type {
        // Common messages (0x00-0x0F)
        0x00 => "SetupConnection",
        0x01 => "SetupConnectionSuccess",
        0x02 => "SetupConnectionError",
        0x03 => "ChannelEndpointChanged",

        // Mining messages (0x10-0x2F)
        0x10 => "OpenStandardMiningChannel",
        0x11 => "OpenStandardMiningChannelSuccess",
        0x12 => "OpenMiningChannelError",
        0x13 => "OpenExtendedMiningChannel",
        0x14 => "OpenExtendedMiningChannelSuccess",
        0x15 => "NewMiningJob",
        0x16 => "UpdateChannel",
        0x17 => "UpdateChannelError",
        0x18 => "CloseChannel",
        0x19 => "SetExtranoncePrefix",
        0x1a => "SubmitSharesStandard",
        0x1b => "SubmitSharesExtended",
        0x1c => "SubmitSharesSuccess",
        0x1d => "SubmitSharesError",
        0x1f => "NewExtendedMiningJob",
        0x20 => "SetNewPrevHash",
        0x21 => "SetTarget",
        0x22 => "SetCustomMiningJob",
        0x23 => "SetCustomMiningJobSuccess",
        0x24 => "SetCustomMiningJobError",
        0x25 => "Reconnect", // todo: fix this like listed on `const_sv2`
        0x26 => "SetGroupChannel",

        // Job Declaration messages (0x50-0x6F)
        0x50 => "AllocateMiningJobToken",
        0x51 => "AllocateMiningJobTokenSuccess",
        0x55 => "ProvideMissingTransactions",
        0x56 => "ProvideMissingTransactionsSuccess",
        0x57 => "DeclareMiningJob",
        0x58 => "DeclareMiningJobSuccess",
        0x59 => "DeclareMiningJobError",
        0x60 => "PushSolution",

        // Template Distribution messages (0x70-0x7F)
        0x70 => "CoinbaseOutputDataSize",
        0x71 => "NewTemplate",
        0x72 => "SetNewPrevHash",
        0x73 => "RequestTransactionData",
        0x74 => "RequestTransactionDataSuccess",
        0x75 => "RequestTransactionDataError",
        0x76 => "SubmitSolution",

        // Unknown message type
        _ => "Unknown Message",
    }
}

/// Common Sv2 protocol messages used across all subprotocols.
///
/// These messages are essential
/// for initializing connections and managing endpoints.
/// A parser of messages that are common to all Sv2 subprotocols, to be used for parsing raw
/// messages
#[derive(Clone, Debug, PartialEq)]
pub enum CommonMessages<'a> {
    /// Notifies about changes in channel endpoint configuration.
    ChannelEndpointChanged(ChannelEndpointChanged),
    /// Reconnects a client to a new server
    Reconnect(Reconnect<'a>),
    /// Initiates a connection between a client and server.
    SetupConnection(SetupConnection<'a>),
    /// Indicates an error during connection setup.
    SetupConnectionError(SetupConnectionError<'a>),
    /// Acknowledges successful connection setup.
    SetupConnectionSuccess(SetupConnectionSuccess),
}

impl fmt::Display for CommonMessages<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            CommonMessages::ChannelEndpointChanged(m) => write!(f, "{m}"),
            CommonMessages::Reconnect(m) => write!(f, "{m}"),
            CommonMessages::SetupConnection(m) => write!(f, "{m}"),
            CommonMessages::SetupConnectionError(m) => write!(f, "{m}"),
            CommonMessages::SetupConnectionSuccess(m) => write!(f, "{m}"),
        }
    }
}

/// A parser of messages of Template Distribution subprotocol, to be used for parsing raw messages
#[derive(Clone, Debug)]
pub enum TemplateDistribution<'a> {
    CoinbaseOutputConstraints(CoinbaseOutputConstraints),
    NewTemplate(NewTemplate<'a>),
    RequestTransactionData(RequestTransactionData),
    RequestTransactionDataError(RequestTransactionDataError<'a>),
    RequestTransactionDataSuccess(RequestTransactionDataSuccess<'a>),
    SetNewPrevHash(SetNewPrevHash<'a>),
    SubmitSolution(SubmitSolution<'a>),
}

impl fmt::Display for TemplateDistribution<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            TemplateDistribution::CoinbaseOutputConstraints(m) => {
                write!(f, "CoinbaseOutputConstraints: {m}")
            }
            TemplateDistribution::NewTemplate(m) => write!(f, "{m}"),
            TemplateDistribution::RequestTransactionData(m) => {
                write!(f, "{m}")
            }
            TemplateDistribution::RequestTransactionDataError(m) => {
                write!(f, "{m}")
            }
            TemplateDistribution::RequestTransactionDataSuccess(m) => {
                write!(f, "{m}")
            }
            TemplateDistribution::SetNewPrevHash(m) => write!(f, "{m}"),
            TemplateDistribution::SubmitSolution(m) => write!(f, "{m}"),
        }
    }
}

/// A parser of messages of Job Declaration subprotocol, to be used for parsing raw messages
#[derive(Clone, Debug)]
pub enum JobDeclaration<'a> {
    AllocateMiningJobToken(AllocateMiningJobToken<'a>),
    AllocateMiningJobTokenSuccess(AllocateMiningJobTokenSuccess<'a>),
    DeclareMiningJob(DeclareMiningJob<'a>),
    DeclareMiningJobError(DeclareMiningJobError<'a>),
    DeclareMiningJobSuccess(DeclareMiningJobSuccess<'a>),
    ProvideMissingTransactions(ProvideMissingTransactions<'a>),
    ProvideMissingTransactionsSuccess(ProvideMissingTransactionsSuccess<'a>),
    PushSolution(PushSolution<'a>),
}

impl fmt::Display for JobDeclaration<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            JobDeclaration::AllocateMiningJobToken(m) => write!(f, "AllocateMiningJobToken: {m}"),
            JobDeclaration::AllocateMiningJobTokenSuccess(m) => {
                write!(f, "AllocateMiningJobTokenSuccess: {m}")
            }
            JobDeclaration::DeclareMiningJob(m) => write!(f, "DeclareMiningJob: {m}"),
            JobDeclaration::DeclareMiningJobError(m) => write!(f, "DeclareMiningJobError: {m}"),
            JobDeclaration::DeclareMiningJobSuccess(m) => {
                write!(f, "DeclareMiningJobSuccess: {m}")
            }
            JobDeclaration::ProvideMissingTransactions(m) => {
                write!(f, "ProvideMissingTransactions: {m}")
            }
            JobDeclaration::ProvideMissingTransactionsSuccess(m) => {
                write!(f, "ProvideMissingTransactionsSuccess: {m}")
            }
            JobDeclaration::PushSolution(m) => write!(f, "PushSolution: {m}"),
        }
    }
}

/// Mining subprotocol messages: categorization, encapsulation, and parsing.
///
/// Encapsulates mining-related Sv2 protocol messages, providing both a structured representation
/// of parsed messages and an abstraction for communication between mining-related roles. These
/// messages are essential for managing mining channels, distributing jobs, and processing shares.
///
/// ## Purpose
/// - **Parsing Raw Messages**:
///   - Converts raw binary Sv2 mining subprotocol messages into strongly-typed Rust
///     representations.
///   - Simplifies deserialization by mapping raw data directly to the appropriate enum variant.
///   - Once parsed, the [`Mining`] enum provides a structured interface that can be passed through
///     routing and processing layers in roles like proxies or pools.
/// - **Encapsulation**:
///   - Groups mining-related messages into a unified type, abstracting away low-level subprotocol
///     details and making it easier to interact with Sv2 protocol messages.
/// - **Facilitating Modular Handling**:
///   - Categorizes mining messages under a single enum, enabling roles (e.g., proxies or pools) to
///     route and process messages more efficiently using pattern matching and centralized logic.
/// - **Bridging Parsed Messages and Role Logic**:
///   - Acts as a bridge between parsed subprotocol messages and role-specific logic, providing a
///     unified interface for handling mining-related communication. This reduces complexity and
///     ensures consistency across roles.
#[derive(Clone, Debug)]
pub enum Mining<'a> {
    CloseChannel(CloseChannel<'a>),
    NewExtendedMiningJob(NewExtendedMiningJob<'a>),
    NewMiningJob(NewMiningJob<'a>),
    OpenExtendedMiningChannel(OpenExtendedMiningChannel<'a>),
    OpenExtendedMiningChannelSuccess(OpenExtendedMiningChannelSuccess<'a>),
    OpenMiningChannelError(OpenMiningChannelError<'a>),
    OpenStandardMiningChannel(OpenStandardMiningChannel<'a>),
    OpenStandardMiningChannelSuccess(OpenStandardMiningChannelSuccess<'a>),
    SetCustomMiningJob(SetCustomMiningJob<'a>),
    SetCustomMiningJobError(SetCustomMiningJobError<'a>),
    SetCustomMiningJobSuccess(SetCustomMiningJobSuccess),
    SetExtranoncePrefix(SetExtranoncePrefix<'a>),
    SetGroupChannel(SetGroupChannel<'a>),
    SetNewPrevHash(MiningSetNewPrevHash<'a>),
    SetTarget(SetTarget<'a>),
    SubmitSharesError(SubmitSharesError<'a>),
    SubmitSharesExtended(SubmitSharesExtended<'a>),
    SubmitSharesStandard(SubmitSharesStandard),
    SubmitSharesSuccess(SubmitSharesSuccess),
    UpdateChannel(UpdateChannel<'a>),
    UpdateChannelError(UpdateChannelError<'a>),
}

impl fmt::Display for Mining<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            Mining::CloseChannel(m) => write!(f, "{m}"),
            Mining::NewExtendedMiningJob(m) => write!(f, "{m}"),
            Mining::NewMiningJob(m) => write!(f, "{m}"),
            Mining::OpenExtendedMiningChannel(m) => write!(f, "{m}"),
            Mining::OpenExtendedMiningChannelSuccess(m) => write!(f, "{m}"),
            Mining::OpenMiningChannelError(m) => write!(f, "{m}"),
            Mining::OpenStandardMiningChannel(m) => write!(f, "{m}"),
            Mining::OpenStandardMiningChannelSuccess(m) => write!(f, "{m}"),
            Mining::SetCustomMiningJob(m) => write!(f, "{m}"),
            Mining::SetCustomMiningJobError(m) => write!(f, "{m}"),
            Mining::SetCustomMiningJobSuccess(m) => write!(f, "{m}"),
            Mining::SetExtranoncePrefix(m) => write!(f, "{m}"),
            Mining::SetGroupChannel(m) => write!(f, "{m}"),
            Mining::SetNewPrevHash(m) => write!(f, "{m}"),
            Mining::SetTarget(m) => write!(f, "{m}"),
            Mining::SubmitSharesError(m) => write!(f, "{m}"),
            Mining::SubmitSharesExtended(m) => write!(f, "{m}"),
            Mining::SubmitSharesStandard(m) => write!(f, "{m}"),
            Mining::SubmitSharesSuccess(m) => write!(f, "{m}"),
            Mining::UpdateChannel(m) => write!(f, "{m}"),
            Mining::UpdateChannelError(m) => write!(f, "{m}"),
        }
    }
}

impl Mining<'_> {
    /// converter into static lifetime
    pub fn into_static(self) -> Mining<'static> {
        match self {
            Mining::CloseChannel(m) => Mining::CloseChannel(m.into_static()),
            Mining::NewExtendedMiningJob(m) => Mining::NewExtendedMiningJob(m.into_static()),
            Mining::NewMiningJob(m) => Mining::NewMiningJob(m.into_static()),
            Mining::OpenExtendedMiningChannel(m) => {
                Mining::OpenExtendedMiningChannel(m.into_static())
            }
            Mining::OpenExtendedMiningChannelSuccess(m) => {
                Mining::OpenExtendedMiningChannelSuccess(m.into_static())
            }
            Mining::OpenMiningChannelError(m) => Mining::OpenMiningChannelError(m.into_static()),
            Mining::OpenStandardMiningChannel(m) => {
                Mining::OpenStandardMiningChannel(m.into_static())
            }
            Mining::OpenStandardMiningChannelSuccess(m) => {
                Mining::OpenStandardMiningChannelSuccess(m.into_static())
            }
            Mining::SetCustomMiningJob(m) => Mining::SetCustomMiningJob(m.into_static()),
            Mining::SetCustomMiningJobError(m) => Mining::SetCustomMiningJobError(m.into_static()),
            Mining::SetCustomMiningJobSuccess(m) => {
                Mining::SetCustomMiningJobSuccess(m.into_static())
            }
            Mining::SetExtranoncePrefix(m) => Mining::SetExtranoncePrefix(m.into_static()),
            Mining::SetGroupChannel(m) => Mining::SetGroupChannel(m.into_static()),
            Mining::SetNewPrevHash(m) => Mining::SetNewPrevHash(m.into_static()),
            Mining::SetTarget(m) => Mining::SetTarget(m.into_static()),
            Mining::SubmitSharesError(m) => Mining::SubmitSharesError(m.into_static()),
            Mining::SubmitSharesExtended(m) => Mining::SubmitSharesExtended(m.into_static()),
            Mining::SubmitSharesStandard(m) => Mining::SubmitSharesStandard(m),
            Mining::SubmitSharesSuccess(m) => Mining::SubmitSharesSuccess(m),
            Mining::UpdateChannel(m) => Mining::UpdateChannel(m.into_static()),
            Mining::UpdateChannelError(m) => Mining::UpdateChannelError(m.into_static()),
        }
    }
}

impl CommonMessages<'_> {
    /// converter into static lifetime
    pub fn into_static(self) -> CommonMessages<'static> {
        match self {
            CommonMessages::ChannelEndpointChanged(m) => CommonMessages::ChannelEndpointChanged(m),
            CommonMessages::Reconnect(m) => CommonMessages::Reconnect(m.into_static()),
            CommonMessages::SetupConnection(m) => CommonMessages::SetupConnection(m.into_static()),
            CommonMessages::SetupConnectionError(m) => {
                CommonMessages::SetupConnectionError(m.into_static())
            }
            CommonMessages::SetupConnectionSuccess(m) => CommonMessages::SetupConnectionSuccess(m),
        }
    }
}

impl TemplateDistribution<'_> {
    /// converter into static lifetime
    pub fn into_static(self) -> TemplateDistribution<'static> {
        match self {
            TemplateDistribution::CoinbaseOutputConstraints(m) => {
                TemplateDistribution::CoinbaseOutputConstraints(m)
            }
            TemplateDistribution::NewTemplate(m) => {
                TemplateDistribution::NewTemplate(m.into_static())
            }
            TemplateDistribution::RequestTransactionData(m) => {
                TemplateDistribution::RequestTransactionData(m)
            }
            TemplateDistribution::RequestTransactionDataError(m) => {
                TemplateDistribution::RequestTransactionDataError(m.into_static())
            }
            TemplateDistribution::RequestTransactionDataSuccess(m) => {
                TemplateDistribution::RequestTransactionDataSuccess(m.into_static())
            }
            TemplateDistribution::SetNewPrevHash(m) => {
                TemplateDistribution::SetNewPrevHash(m.into_static())
            }
            TemplateDistribution::SubmitSolution(m) => {
                TemplateDistribution::SubmitSolution(m.into_static())
            }
        }
    }
}

impl JobDeclaration<'_> {
    /// converter into static lifetime
    pub fn into_static(self) -> JobDeclaration<'static> {
        match self {
            JobDeclaration::AllocateMiningJobToken(m) => {
                JobDeclaration::AllocateMiningJobToken(m.into_static())
            }
            JobDeclaration::AllocateMiningJobTokenSuccess(m) => {
                JobDeclaration::AllocateMiningJobTokenSuccess(m.into_static())
            }
            JobDeclaration::DeclareMiningJob(m) => {
                JobDeclaration::DeclareMiningJob(m.into_static())
            }
            JobDeclaration::DeclareMiningJobError(m) => {
                JobDeclaration::DeclareMiningJobError(m.into_static())
            }
            JobDeclaration::DeclareMiningJobSuccess(m) => {
                JobDeclaration::DeclareMiningJobSuccess(m.into_static())
            }
            JobDeclaration::ProvideMissingTransactions(m) => {
                JobDeclaration::ProvideMissingTransactions(m.into_static())
            }
            JobDeclaration::ProvideMissingTransactionsSuccess(m) => {
                JobDeclaration::ProvideMissingTransactionsSuccess(m.into_static())
            }
            JobDeclaration::PushSolution(m) => JobDeclaration::PushSolution(m.into_static()),
        }
    }
}

impl AnyMessage<'_> {
    /// converter into static lifetime
    pub fn into_static(self) -> AnyMessage<'static> {
        match self {
            AnyMessage::Common(m) => AnyMessage::Common(m.into_static()),
            AnyMessage::Mining(m) => AnyMessage::Mining(m.into_static()),
            AnyMessage::JobDeclaration(m) => AnyMessage::JobDeclaration(m.into_static()),
            AnyMessage::TemplateDistribution(m) => {
                AnyMessage::TemplateDistribution(m.into_static())
            }
        }
    }
}

/// A trait that every Sv2 message parser must implement.
/// It helps parsing from Rust types to raw messages.
pub trait IsSv2Message {
    /// get message type
    fn message_type(&self) -> u8;
    /// get channel bit
    fn channel_bit(&self) -> bool;
}

impl IsSv2Message for CommonMessages<'_> {
    fn message_type(&self) -> u8 {
        match self {
            Self::ChannelEndpointChanged(_) => MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
            Self::Reconnect(_) => MESSAGE_TYPE_RECONNECT,
            Self::SetupConnection(_) => MESSAGE_TYPE_SETUP_CONNECTION,
            Self::SetupConnectionError(_) => MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
            Self::SetupConnectionSuccess(_) => MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        }
    }

    fn channel_bit(&self) -> bool {
        match self {
            Self::ChannelEndpointChanged(_) => CHANNEL_BIT_CHANNEL_ENDPOINT_CHANGED,
            Self::Reconnect(_) => CHANNEL_BIT_RECONNECT,
            Self::SetupConnection(_) => CHANNEL_BIT_SETUP_CONNECTION,
            Self::SetupConnectionError(_) => CHANNEL_BIT_SETUP_CONNECTION_ERROR,
            Self::SetupConnectionSuccess(_) => CHANNEL_BIT_SETUP_CONNECTION_SUCCESS,
        }
    }
}

impl IsSv2Message for TemplateDistribution<'_> {
    fn message_type(&self) -> u8 {
        match self {
            Self::CoinbaseOutputConstraints(_) => MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS,
            Self::NewTemplate(_) => MESSAGE_TYPE_NEW_TEMPLATE,
            Self::RequestTransactionData(_) => MESSAGE_TYPE_REQUEST_TRANSACTION_DATA,
            Self::RequestTransactionDataError(_) => MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR,
            Self::RequestTransactionDataSuccess(_) => MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS,
            Self::SetNewPrevHash(_) => MESSAGE_TYPE_SET_NEW_PREV_HASH,
            Self::SubmitSolution(_) => MESSAGE_TYPE_SUBMIT_SOLUTION,
        }
    }
    fn channel_bit(&self) -> bool {
        match self {
            Self::CoinbaseOutputConstraints(_) => CHANNEL_BIT_COINBASE_OUTPUT_CONSTRAINTS,
            Self::NewTemplate(_) => CHANNEL_BIT_NEW_TEMPLATE,
            Self::RequestTransactionData(_) => CHANNEL_BIT_REQUEST_TRANSACTION_DATA,
            Self::RequestTransactionDataError(_) => CHANNEL_BIT_REQUEST_TRANSACTION_DATA_ERROR,
            Self::RequestTransactionDataSuccess(_) => CHANNEL_BIT_REQUEST_TRANSACTION_DATA_SUCCESS,
            Self::SetNewPrevHash(_) => CHANNEL_BIT_SET_NEW_PREV_HASH,
            Self::SubmitSolution(_) => CHANNEL_BIT_SUBMIT_SOLUTION,
        }
    }
}
impl IsSv2Message for JobDeclaration<'_> {
    fn message_type(&self) -> u8 {
        match self {
            Self::AllocateMiningJobToken(_) => MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
            Self::AllocateMiningJobTokenSuccess(_) => {
                MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS
            }
            Self::DeclareMiningJob(_) => MESSAGE_TYPE_DECLARE_MINING_JOB,
            Self::DeclareMiningJobSuccess(_) => MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS,
            Self::DeclareMiningJobError(_) => MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR,
            Self::ProvideMissingTransactions(_) => MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
            Self::ProvideMissingTransactionsSuccess(_) => {
                MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS
            }
            Self::PushSolution(_) => MESSAGE_TYPE_PUSH_SOLUTION,
        }
    }
    fn channel_bit(&self) -> bool {
        match self {
            Self::AllocateMiningJobToken(_) => CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN,
            Self::AllocateMiningJobTokenSuccess(_) => CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
            Self::DeclareMiningJob(_) => CHANNEL_BIT_DECLARE_MINING_JOB,
            Self::DeclareMiningJobSuccess(_) => CHANNEL_BIT_DECLARE_MINING_JOB_SUCCESS,
            Self::DeclareMiningJobError(_) => CHANNEL_BIT_DECLARE_MINING_JOB_ERROR,
            Self::ProvideMissingTransactions(_) => CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS,
            Self::ProvideMissingTransactionsSuccess(_) => {
                CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS_SUCCESS
            }
            Self::PushSolution(_) => CHANNEL_BIT_SUBMIT_SOLUTION_JD,
        }
    }
}
impl IsSv2Message for Mining<'_> {
    fn message_type(&self) -> u8 {
        match self {
            Self::CloseChannel(_) => MESSAGE_TYPE_CLOSE_CHANNEL,
            Self::NewExtendedMiningJob(_) => MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB,
            Self::NewMiningJob(_) => MESSAGE_TYPE_NEW_MINING_JOB,
            Self::OpenExtendedMiningChannel(_) => MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL,
            Self::OpenExtendedMiningChannelSuccess(_) => {
                MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS
            }
            Self::OpenMiningChannelError(_) => MESSAGE_TYPE_OPEN_MINING_CHANNEL_ERROR,
            Self::OpenStandardMiningChannel(_) => MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL,
            Self::OpenStandardMiningChannelSuccess(_) => {
                MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS
            }
            Self::SetCustomMiningJob(_) => MESSAGE_TYPE_SET_CUSTOM_MINING_JOB,
            Self::SetCustomMiningJobError(_) => MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR,
            Self::SetCustomMiningJobSuccess(_) => MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS,
            Self::SetExtranoncePrefix(_) => MESSAGE_TYPE_SET_EXTRANONCE_PREFIX,
            Self::SetGroupChannel(_) => MESSAGE_TYPE_SET_GROUP_CHANNEL,
            Self::SetNewPrevHash(_) => MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH,
            Self::SetTarget(_) => MESSAGE_TYPE_SET_TARGET,
            Self::SubmitSharesError(_) => MESSAGE_TYPE_SUBMIT_SHARES_ERROR,
            Self::SubmitSharesExtended(_) => MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
            Self::SubmitSharesStandard(_) => MESSAGE_TYPE_SUBMIT_SHARES_STANDARD,
            Self::SubmitSharesSuccess(_) => MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
            Self::UpdateChannel(_) => MESSAGE_TYPE_UPDATE_CHANNEL,
            Self::UpdateChannelError(_) => MESSAGE_TYPE_UPDATE_CHANNEL_ERROR,
        }
    }

    fn channel_bit(&self) -> bool {
        match self {
            Self::CloseChannel(_) => CHANNEL_BIT_CLOSE_CHANNEL,
            Self::NewExtendedMiningJob(_) => CHANNEL_BIT_NEW_EXTENDED_MINING_JOB,
            Self::NewMiningJob(_) => CHANNEL_BIT_NEW_MINING_JOB,
            Self::OpenExtendedMiningChannel(_) => CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL,
            Self::OpenExtendedMiningChannelSuccess(_) => {
                CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS
            }
            Self::OpenMiningChannelError(_) => CHANNEL_BIT_OPEN_MINING_CHANNEL_ERROR,
            Self::OpenStandardMiningChannel(_) => CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL,
            Self::OpenStandardMiningChannelSuccess(_) => {
                CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL_SUCCESS
            }
            Self::SetCustomMiningJob(_) => CHANNEL_BIT_SET_CUSTOM_MINING_JOB,
            Self::SetCustomMiningJobError(_) => CHANNEL_BIT_SET_CUSTOM_MINING_JOB_ERROR,
            Self::SetCustomMiningJobSuccess(_) => CHANNEL_BIT_SET_CUSTOM_MINING_JOB_SUCCESS,
            Self::SetExtranoncePrefix(_) => CHANNEL_BIT_SET_EXTRANONCE_PREFIX,
            Self::SetGroupChannel(_) => CHANNEL_BIT_SET_GROUP_CHANNEL,
            Self::SetNewPrevHash(_) => CHANNEL_BIT_MINING_SET_NEW_PREV_HASH,
            Self::SetTarget(_) => CHANNEL_BIT_SET_TARGET,
            Self::SubmitSharesError(_) => CHANNEL_BIT_SUBMIT_SHARES_ERROR,
            Self::SubmitSharesExtended(_) => CHANNEL_BIT_SUBMIT_SHARES_EXTENDED,
            Self::SubmitSharesStandard(_) => CHANNEL_BIT_SUBMIT_SHARES_STANDARD,
            Self::SubmitSharesSuccess(_) => CHANNEL_BIT_SUBMIT_SHARES_SUCCESS,
            Self::UpdateChannel(_) => CHANNEL_BIT_UPDATE_CHANNEL,
            Self::UpdateChannelError(_) => CHANNEL_BIT_UPDATE_CHANNEL_ERROR,
        }
    }
}

impl<'decoder> From<CommonMessages<'decoder>> for EncodableField<'decoder> {
    fn from(m: CommonMessages<'decoder>) -> Self {
        match m {
            CommonMessages::ChannelEndpointChanged(a) => a.into(),
            CommonMessages::Reconnect(a) => a.into(),
            CommonMessages::SetupConnection(a) => a.into(),
            CommonMessages::SetupConnectionError(a) => a.into(),
            CommonMessages::SetupConnectionSuccess(a) => a.into(),
        }
    }
}
impl<'decoder> From<TemplateDistribution<'decoder>> for EncodableField<'decoder> {
    fn from(m: TemplateDistribution<'decoder>) -> Self {
        match m {
            TemplateDistribution::CoinbaseOutputConstraints(a) => a.into(),
            TemplateDistribution::NewTemplate(a) => a.into(),
            TemplateDistribution::RequestTransactionData(a) => a.into(),
            TemplateDistribution::RequestTransactionDataError(a) => a.into(),
            TemplateDistribution::RequestTransactionDataSuccess(a) => a.into(),
            TemplateDistribution::SetNewPrevHash(a) => a.into(),
            TemplateDistribution::SubmitSolution(a) => a.into(),
        }
    }
}
impl<'decoder> From<JobDeclaration<'decoder>> for EncodableField<'decoder> {
    fn from(m: JobDeclaration<'decoder>) -> Self {
        match m {
            JobDeclaration::AllocateMiningJobToken(a) => a.into(),
            JobDeclaration::AllocateMiningJobTokenSuccess(a) => a.into(),
            JobDeclaration::DeclareMiningJob(a) => a.into(),
            JobDeclaration::DeclareMiningJobSuccess(a) => a.into(),
            JobDeclaration::DeclareMiningJobError(a) => a.into(),
            JobDeclaration::ProvideMissingTransactions(a) => a.into(),
            JobDeclaration::ProvideMissingTransactionsSuccess(a) => a.into(),
            JobDeclaration::PushSolution(a) => a.into(),
        }
    }
}

impl<'decoder> From<Mining<'decoder>> for EncodableField<'decoder> {
    fn from(m: Mining<'decoder>) -> Self {
        match m {
            Mining::CloseChannel(a) => a.into(),
            Mining::NewExtendedMiningJob(a) => a.into(),
            Mining::NewMiningJob(a) => a.into(),
            Mining::OpenExtendedMiningChannel(a) => a.into(),
            Mining::OpenExtendedMiningChannelSuccess(a) => a.into(),
            Mining::OpenMiningChannelError(a) => a.into(),
            Mining::OpenStandardMiningChannel(a) => a.into(),
            Mining::OpenStandardMiningChannelSuccess(a) => a.into(),
            Mining::SetCustomMiningJob(a) => a.into(),
            Mining::SetCustomMiningJobError(a) => a.into(),
            Mining::SetCustomMiningJobSuccess(a) => a.into(),
            Mining::SetExtranoncePrefix(a) => a.into(),
            Mining::SetGroupChannel(a) => a.into(),
            Mining::SetNewPrevHash(a) => a.into(),
            Mining::SetTarget(a) => a.into(),
            Mining::SubmitSharesError(a) => a.into(),
            Mining::SubmitSharesExtended(a) => a.into(),
            Mining::SubmitSharesStandard(a) => a.into(),
            Mining::SubmitSharesSuccess(a) => a.into(),
            Mining::UpdateChannel(a) => a.into(),
            Mining::UpdateChannelError(a) => a.into(),
        }
    }
}

impl GetSize for CommonMessages<'_> {
    fn get_size(&self) -> usize {
        match self {
            CommonMessages::ChannelEndpointChanged(a) => a.get_size(),
            CommonMessages::Reconnect(a) => a.get_size(),
            CommonMessages::SetupConnection(a) => a.get_size(),
            CommonMessages::SetupConnectionError(a) => a.get_size(),
            CommonMessages::SetupConnectionSuccess(a) => a.get_size(),
        }
    }
}
impl GetSize for TemplateDistribution<'_> {
    fn get_size(&self) -> usize {
        match self {
            TemplateDistribution::CoinbaseOutputConstraints(a) => a.get_size(),
            TemplateDistribution::NewTemplate(a) => a.get_size(),
            TemplateDistribution::RequestTransactionData(a) => a.get_size(),
            TemplateDistribution::RequestTransactionDataError(a) => a.get_size(),
            TemplateDistribution::RequestTransactionDataSuccess(a) => a.get_size(),
            TemplateDistribution::SetNewPrevHash(a) => a.get_size(),
            TemplateDistribution::SubmitSolution(a) => a.get_size(),
        }
    }
}
impl GetSize for JobDeclaration<'_> {
    fn get_size(&self) -> usize {
        match self {
            JobDeclaration::AllocateMiningJobToken(a) => a.get_size(),
            JobDeclaration::AllocateMiningJobTokenSuccess(a) => a.get_size(),
            JobDeclaration::DeclareMiningJob(a) => a.get_size(),
            JobDeclaration::DeclareMiningJobSuccess(a) => a.get_size(),
            JobDeclaration::DeclareMiningJobError(a) => a.get_size(),
            JobDeclaration::ProvideMissingTransactions(a) => a.get_size(),
            JobDeclaration::ProvideMissingTransactionsSuccess(a) => a.get_size(),
            JobDeclaration::PushSolution(a) => a.get_size(),
        }
    }
}
impl GetSize for Mining<'_> {
    fn get_size(&self) -> usize {
        match self {
            Mining::CloseChannel(a) => a.get_size(),
            Mining::NewExtendedMiningJob(a) => a.get_size(),
            Mining::NewMiningJob(a) => a.get_size(),
            Mining::OpenExtendedMiningChannel(a) => a.get_size(),
            Mining::OpenExtendedMiningChannelSuccess(a) => a.get_size(),
            Mining::OpenMiningChannelError(a) => a.get_size(),
            Mining::OpenStandardMiningChannel(a) => a.get_size(),
            Mining::OpenStandardMiningChannelSuccess(a) => a.get_size(),
            Mining::SetCustomMiningJob(a) => a.get_size(),
            Mining::SetCustomMiningJobError(a) => a.get_size(),
            Mining::SetCustomMiningJobSuccess(a) => a.get_size(),
            Mining::SetExtranoncePrefix(a) => a.get_size(),
            Mining::SetGroupChannel(a) => a.get_size(),
            Mining::SetNewPrevHash(a) => a.get_size(),
            Mining::SetTarget(a) => a.get_size(),
            Mining::SubmitSharesError(a) => a.get_size(),
            Mining::SubmitSharesExtended(a) => a.get_size(),
            Mining::SubmitSharesStandard(a) => a.get_size(),
            Mining::SubmitSharesSuccess(a) => a.get_size(),
            Mining::UpdateChannel(a) => a.get_size(),
            Mining::UpdateChannelError(a) => a.get_size(),
        }
    }
}

impl<'decoder> Deserialize<'decoder> for CommonMessages<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}
impl<'decoder> Deserialize<'decoder> for TemplateDistribution<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}
impl<'decoder> Deserialize<'decoder> for JobDeclaration<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}
impl<'decoder> Deserialize<'decoder> for Mining<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}

impl<'decoder> Deserialize<'decoder> for AnyMessage<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}

impl<'decoder> Deserialize<'decoder> for MiningDeviceMessages<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}

/// A list of 8-bit message type variants that are common to all Sv2 subprotocols
#[derive(Debug, Clone, Copy)]
#[repr(u8)]
#[allow(clippy::enum_variant_names)]
pub enum CommonMessageTypes {
    SetupConnection = MESSAGE_TYPE_SETUP_CONNECTION,
    SetupConnectionSuccess = MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
    SetupConnectionError = MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
    ChannelEndpointChanged = MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
    Reconnect = MESSAGE_TYPE_RECONNECT,
}

impl TryFrom<u8> for CommonMessageTypes {
    type Error = Error;

    fn try_from(v: u8) -> Result<CommonMessageTypes, Error> {
        match v {
            MESSAGE_TYPE_SETUP_CONNECTION => Ok(CommonMessageTypes::SetupConnection),
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS => Ok(CommonMessageTypes::SetupConnectionSuccess),
            MESSAGE_TYPE_SETUP_CONNECTION_ERROR => Ok(CommonMessageTypes::SetupConnectionError),
            MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED => Ok(CommonMessageTypes::ChannelEndpointChanged),
            MESSAGE_TYPE_RECONNECT => Ok(CommonMessageTypes::Reconnect),
            _ => Err(Error::UnexpectedMessage(v)),
        }
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for CommonMessages<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let msg_type: CommonMessageTypes = v.0.try_into()?;
        match msg_type {
            CommonMessageTypes::SetupConnection => {
                let message: SetupConnection<'a> = from_bytes(v.1)?;
                Ok(CommonMessages::SetupConnection(message))
            }
            CommonMessageTypes::SetupConnectionSuccess => {
                let message: SetupConnectionSuccess = from_bytes(v.1)?;
                Ok(CommonMessages::SetupConnectionSuccess(message))
            }
            CommonMessageTypes::SetupConnectionError => {
                let message: SetupConnectionError<'a> = from_bytes(v.1)?;
                Ok(CommonMessages::SetupConnectionError(message))
            }
            CommonMessageTypes::ChannelEndpointChanged => {
                let message: ChannelEndpointChanged = from_bytes(v.1)?;
                Ok(CommonMessages::ChannelEndpointChanged(message))
            }
            CommonMessageTypes::Reconnect => {
                let message: Reconnect = from_bytes(v.1)?;
                Ok(CommonMessages::Reconnect(message))
            }
        }
    }
}

/// A list of 8-bit message type variants under Template Distribution subprotocol
#[derive(Debug, Clone, Copy)]
#[repr(u8)]
#[allow(clippy::enum_variant_names)]
pub enum TemplateDistributionTypes {
    CoinbaseOutputConstraints = MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS,
    NewTemplate = MESSAGE_TYPE_NEW_TEMPLATE,
    SetNewPrevHash = MESSAGE_TYPE_SET_NEW_PREV_HASH,
    RequestTransactionData = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA,
    RequestTransactionDataSuccess = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS,
    RequestTransactionDataError = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR,
    SubmitSolution = MESSAGE_TYPE_SUBMIT_SOLUTION,
}

impl TryFrom<u8> for TemplateDistributionTypes {
    type Error = Error;

    fn try_from(v: u8) -> Result<TemplateDistributionTypes, Error> {
        match v {
            MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS => {
                Ok(TemplateDistributionTypes::CoinbaseOutputConstraints)
            }
            MESSAGE_TYPE_NEW_TEMPLATE => Ok(TemplateDistributionTypes::NewTemplate),
            MESSAGE_TYPE_SET_NEW_PREV_HASH => Ok(TemplateDistributionTypes::SetNewPrevHash),
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA => {
                Ok(TemplateDistributionTypes::RequestTransactionData)
            }
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS => {
                Ok(TemplateDistributionTypes::RequestTransactionDataSuccess)
            }
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR => {
                Ok(TemplateDistributionTypes::RequestTransactionDataError)
            }
            MESSAGE_TYPE_SUBMIT_SOLUTION => Ok(TemplateDistributionTypes::SubmitSolution),
            _ => Err(Error::UnexpectedMessage(v)),
        }
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for TemplateDistribution<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let msg_type: TemplateDistributionTypes = v.0.try_into()?;
        match msg_type {
            TemplateDistributionTypes::CoinbaseOutputConstraints => {
                let message: CoinbaseOutputConstraints = from_bytes(v.1)?;
                Ok(TemplateDistribution::CoinbaseOutputConstraints(message))
            }
            TemplateDistributionTypes::NewTemplate => {
                let message: NewTemplate<'a> = from_bytes(v.1)?;
                Ok(TemplateDistribution::NewTemplate(message))
            }
            TemplateDistributionTypes::SetNewPrevHash => {
                let message: SetNewPrevHash<'a> = from_bytes(v.1)?;
                Ok(TemplateDistribution::SetNewPrevHash(message))
            }
            TemplateDistributionTypes::RequestTransactionData => {
                let message: RequestTransactionData = from_bytes(v.1)?;
                Ok(TemplateDistribution::RequestTransactionData(message))
            }
            TemplateDistributionTypes::RequestTransactionDataSuccess => {
                let message: RequestTransactionDataSuccess = from_bytes(v.1)?;
                Ok(TemplateDistribution::RequestTransactionDataSuccess(message))
            }
            TemplateDistributionTypes::RequestTransactionDataError => {
                let message: RequestTransactionDataError = from_bytes(v.1)?;
                Ok(TemplateDistribution::RequestTransactionDataError(message))
            }
            TemplateDistributionTypes::SubmitSolution => {
                let message: SubmitSolution = from_bytes(v.1)?;
                Ok(TemplateDistribution::SubmitSolution(message))
            }
        }
    }
}

/// A list of 8-bit message type variants under Job Declaration subprotocol
#[derive(Debug, Clone, Copy)]
#[repr(u8)]
#[allow(clippy::enum_variant_names)]
pub enum JobDeclarationTypes {
    AllocateMiningJobToken = MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
    AllocateMiningJobTokenSuccess = MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
    DeclareMiningJob = MESSAGE_TYPE_DECLARE_MINING_JOB,
    DeclareMiningJobSuccess = MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS,
    DeclareMiningJobError = MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR,
    ProvideMissingTransactions = MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
    ProvideMissingTransactionsSuccess = MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS,
    PushSolution = MESSAGE_TYPE_PUSH_SOLUTION,
}

impl TryFrom<u8> for JobDeclarationTypes {
    type Error = Error;

    fn try_from(v: u8) -> Result<JobDeclarationTypes, Error> {
        match v {
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN => {
                Ok(JobDeclarationTypes::AllocateMiningJobToken)
            }
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS => {
                Ok(JobDeclarationTypes::AllocateMiningJobTokenSuccess)
            }
            MESSAGE_TYPE_DECLARE_MINING_JOB => Ok(JobDeclarationTypes::DeclareMiningJob),
            MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS => {
                Ok(JobDeclarationTypes::DeclareMiningJobSuccess)
            }
            MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR => Ok(JobDeclarationTypes::DeclareMiningJobError),
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS => {
                Ok(JobDeclarationTypes::ProvideMissingTransactions)
            }
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS => {
                Ok(JobDeclarationTypes::ProvideMissingTransactionsSuccess)
            }
            MESSAGE_TYPE_PUSH_SOLUTION => Ok(JobDeclarationTypes::PushSolution),
            _ => Err(Error::UnexpectedMessage(v)),
        }
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for JobDeclaration<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let msg_type: JobDeclarationTypes = v.0.try_into()?;
        match msg_type {
            JobDeclarationTypes::AllocateMiningJobToken => {
                let message: AllocateMiningJobToken = from_bytes(v.1)?;
                Ok(JobDeclaration::AllocateMiningJobToken(message))
            }
            JobDeclarationTypes::AllocateMiningJobTokenSuccess => {
                let message: AllocateMiningJobTokenSuccess = from_bytes(v.1)?;
                Ok(JobDeclaration::AllocateMiningJobTokenSuccess(message))
            }
            JobDeclarationTypes::DeclareMiningJob => {
                let message: DeclareMiningJob = from_bytes(v.1)?;
                Ok(JobDeclaration::DeclareMiningJob(message))
            }
            JobDeclarationTypes::DeclareMiningJobSuccess => {
                let message: DeclareMiningJobSuccess = from_bytes(v.1)?;
                Ok(JobDeclaration::DeclareMiningJobSuccess(message))
            }
            JobDeclarationTypes::DeclareMiningJobError => {
                let message: DeclareMiningJobError = from_bytes(v.1)?;
                Ok(JobDeclaration::DeclareMiningJobError(message))
            }
            JobDeclarationTypes::ProvideMissingTransactions => {
                let message: ProvideMissingTransactions = from_bytes(v.1)?;
                Ok(JobDeclaration::ProvideMissingTransactions(message))
            }
            JobDeclarationTypes::ProvideMissingTransactionsSuccess => {
                let message: ProvideMissingTransactionsSuccess = from_bytes(v.1)?;
                Ok(JobDeclaration::ProvideMissingTransactionsSuccess(message))
            }
            JobDeclarationTypes::PushSolution => {
                let message: PushSolution = from_bytes(v.1)?;
                Ok(JobDeclaration::PushSolution(message))
            }
        }
    }
}

/// A list of 8-bit message type variants under Mining subprotocol
#[derive(Debug, Clone, Copy)]
#[repr(u8)]
#[allow(clippy::enum_variant_names)]
pub enum MiningTypes {
    CloseChannel = MESSAGE_TYPE_CLOSE_CHANNEL,
    NewExtendedMiningJob = MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB,
    NewMiningJob = MESSAGE_TYPE_NEW_MINING_JOB,
    OpenExtendedMiningChannel = MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL,
    OpenExtendedMiningChannelSuccess = MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS,
    OpenMiningChannelError = MESSAGE_TYPE_OPEN_MINING_CHANNEL_ERROR,
    OpenStandardMiningChannel = MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL,
    OpenStandardMiningChannelSuccess = MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS,
    SetCustomMiningJob = MESSAGE_TYPE_SET_CUSTOM_MINING_JOB,
    SetCustomMiningJobError = MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR,
    SetCustomMiningJobSuccess = MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS,
    SetExtranoncePrefix = MESSAGE_TYPE_SET_EXTRANONCE_PREFIX,
    SetGroupChannel = MESSAGE_TYPE_SET_GROUP_CHANNEL,
    SetNewPrevHash = MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH,
    SetTarget = MESSAGE_TYPE_SET_TARGET,
    SubmitSharesError = MESSAGE_TYPE_SUBMIT_SHARES_ERROR,
    SubmitSharesExtended = MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
    SubmitSharesStandard = MESSAGE_TYPE_SUBMIT_SHARES_STANDARD,
    SubmitSharesSuccess = MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
    UpdateChannel = MESSAGE_TYPE_UPDATE_CHANNEL,
    UpdateChannelError = MESSAGE_TYPE_UPDATE_CHANNEL_ERROR,
}

impl TryFrom<u8> for MiningTypes {
    type Error = Error;

    fn try_from(v: u8) -> Result<MiningTypes, Error> {
        match v {
            MESSAGE_TYPE_CLOSE_CHANNEL => Ok(MiningTypes::CloseChannel),
            MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB => Ok(MiningTypes::NewExtendedMiningJob),
            MESSAGE_TYPE_NEW_MINING_JOB => Ok(MiningTypes::NewMiningJob),
            MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL => Ok(MiningTypes::OpenExtendedMiningChannel),
            MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS => {
                Ok(MiningTypes::OpenExtendedMiningChannelSuccess)
            }
            MESSAGE_TYPE_OPEN_MINING_CHANNEL_ERROR => Ok(MiningTypes::OpenMiningChannelError),
            MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL => Ok(MiningTypes::OpenStandardMiningChannel),
            MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS => {
                Ok(MiningTypes::OpenStandardMiningChannelSuccess)
            }
            MESSAGE_TYPE_SET_CUSTOM_MINING_JOB => Ok(MiningTypes::SetCustomMiningJob),
            MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR => Ok(MiningTypes::SetCustomMiningJobError),
            MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS => {
                Ok(MiningTypes::SetCustomMiningJobSuccess)
            }
            MESSAGE_TYPE_SET_EXTRANONCE_PREFIX => Ok(MiningTypes::SetExtranoncePrefix),
            MESSAGE_TYPE_SET_GROUP_CHANNEL => Ok(MiningTypes::SetGroupChannel),
            MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH => Ok(MiningTypes::SetNewPrevHash),
            MESSAGE_TYPE_SET_TARGET => Ok(MiningTypes::SetTarget),
            MESSAGE_TYPE_SUBMIT_SHARES_ERROR => Ok(MiningTypes::SubmitSharesError),
            MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED => Ok(MiningTypes::SubmitSharesExtended),
            MESSAGE_TYPE_SUBMIT_SHARES_STANDARD => Ok(MiningTypes::SubmitSharesStandard),
            MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS => Ok(MiningTypes::SubmitSharesSuccess),
            MESSAGE_TYPE_UPDATE_CHANNEL => Ok(MiningTypes::UpdateChannel),
            MESSAGE_TYPE_UPDATE_CHANNEL_ERROR => Ok(MiningTypes::UpdateChannelError),
            MESSAGE_TYPE_SETUP_CONNECTION => Err(Error::UnexpectedMessage(v)),
            _ => Err(Error::UnexpectedMessage(v)),
        }
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for Mining<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let msg_type: MiningTypes = v.0.try_into()?;
        match msg_type {
            MiningTypes::CloseChannel => {
                let message: CloseChannel = from_bytes(v.1)?;
                Ok(Mining::CloseChannel(message))
            }
            MiningTypes::NewExtendedMiningJob => {
                let message: NewExtendedMiningJob = from_bytes(v.1)?;
                Ok(Mining::NewExtendedMiningJob(message))
            }
            MiningTypes::NewMiningJob => {
                let message: NewMiningJob = from_bytes(v.1)?;
                Ok(Mining::NewMiningJob(message))
            }
            MiningTypes::OpenExtendedMiningChannel => {
                let message: OpenExtendedMiningChannel = from_bytes(v.1)?;
                Ok(Mining::OpenExtendedMiningChannel(message))
            }
            MiningTypes::OpenExtendedMiningChannelSuccess => {
                let message: OpenExtendedMiningChannelSuccess = from_bytes(v.1)?;
                Ok(Mining::OpenExtendedMiningChannelSuccess(message))
            }
            MiningTypes::OpenMiningChannelError => {
                let message: OpenMiningChannelError = from_bytes(v.1)?;
                Ok(Mining::OpenMiningChannelError(message))
            }
            MiningTypes::OpenStandardMiningChannel => {
                let message: OpenStandardMiningChannel = from_bytes(v.1)?;
                Ok(Mining::OpenStandardMiningChannel(message))
            }
            MiningTypes::OpenStandardMiningChannelSuccess => {
                let message: OpenStandardMiningChannelSuccess = from_bytes(v.1)?;
                Ok(Mining::OpenStandardMiningChannelSuccess(message))
            }
            MiningTypes::SetCustomMiningJob => {
                let message: SetCustomMiningJob = from_bytes(v.1)?;
                Ok(Mining::SetCustomMiningJob(message))
            }
            MiningTypes::SetCustomMiningJobError => {
                let message: SetCustomMiningJobError = from_bytes(v.1)?;
                Ok(Mining::SetCustomMiningJobError(message))
            }
            MiningTypes::SetCustomMiningJobSuccess => {
                let message: SetCustomMiningJobSuccess = from_bytes(v.1)?;
                Ok(Mining::SetCustomMiningJobSuccess(message))
            }
            MiningTypes::SetExtranoncePrefix => {
                let message: SetExtranoncePrefix = from_bytes(v.1)?;
                Ok(Mining::SetExtranoncePrefix(message))
            }
            MiningTypes::SetGroupChannel => {
                let message: SetGroupChannel = from_bytes(v.1)?;
                Ok(Mining::SetGroupChannel(message))
            }
            MiningTypes::SetNewPrevHash => {
                let message: MiningSetNewPrevHash = from_bytes(v.1)?;
                Ok(Mining::SetNewPrevHash(message))
            }
            MiningTypes::SetTarget => {
                let message: SetTarget = from_bytes(v.1)?;
                Ok(Mining::SetTarget(message))
            }
            MiningTypes::SubmitSharesError => {
                let message: SubmitSharesError = from_bytes(v.1)?;
                Ok(Mining::SubmitSharesError(message))
            }
            MiningTypes::SubmitSharesExtended => {
                let message: SubmitSharesExtended = from_bytes(v.1)?;
                Ok(Mining::SubmitSharesExtended(message))
            }
            MiningTypes::SubmitSharesStandard => {
                let message: SubmitSharesStandard = from_bytes(v.1)?;
                Ok(Mining::SubmitSharesStandard(message))
            }
            MiningTypes::SubmitSharesSuccess => {
                let message: SubmitSharesSuccess = from_bytes(v.1)?;
                Ok(Mining::SubmitSharesSuccess(message))
            }
            MiningTypes::UpdateChannel => {
                let message: UpdateChannel = from_bytes(v.1)?;
                Ok(Mining::UpdateChannel(message))
            }
            MiningTypes::UpdateChannelError => {
                let message: UpdateChannelError = from_bytes(v.1)?;
                Ok(Mining::UpdateChannelError(message))
            }
        }
    }
}

/// A parser of messages that a Mining Device could send
#[derive(Clone, Debug)]
pub enum MiningDeviceMessages<'a> {
    Common(CommonMessages<'a>),
    Mining(Mining<'a>),
}
impl<'decoder> From<MiningDeviceMessages<'decoder>> for EncodableField<'decoder> {
    fn from(m: MiningDeviceMessages<'decoder>) -> Self {
        match m {
            MiningDeviceMessages::Common(a) => a.into(),
            MiningDeviceMessages::Mining(a) => a.into(),
        }
    }
}
impl GetSize for MiningDeviceMessages<'_> {
    fn get_size(&self) -> usize {
        match self {
            MiningDeviceMessages::Common(a) => a.get_size(),
            MiningDeviceMessages::Mining(a) => a.get_size(),
        }
    }
}
impl<'a> TryFrom<(u8, &'a mut [u8])> for MiningDeviceMessages<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let is_common: Result<CommonMessageTypes, Error> = v.0.try_into();
        let is_mining: Result<MiningTypes, Error> = v.0.try_into();
        match (is_common, is_mining) {
            (Ok(_), Err(_)) => Ok(Self::Common(v.try_into()?)),
            (Err(_), Ok(_)) => Ok(Self::Mining(v.try_into()?)),
            (Err(e), Err(_)) => Err(e),
            // this is an impossible state is safe to panic here
            (Ok(_), Ok(_)) => panic!(),
        }
    }
}

/// A parser of all possible SV2 messages
#[derive(Clone, Debug)]
pub enum AnyMessage<'a> {
    Common(CommonMessages<'a>),
    Mining(Mining<'a>),
    JobDeclaration(JobDeclaration<'a>),
    TemplateDistribution(TemplateDistribution<'a>),
}

impl fmt::Display for AnyMessage<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        match self {
            AnyMessage::Common(m) => write!(f, "CommonMessage: {m}"),
            AnyMessage::Mining(m) => write!(f, "MiningMessage: {m}"),
            AnyMessage::JobDeclaration(m) => write!(f, "JobDeclarationMessage: {m}"),
            AnyMessage::TemplateDistribution(m) => write!(f, "TemplateDistributionMessage: {m}"),
        }
    }
}

impl<'a> TryFrom<MiningDeviceMessages<'a>> for AnyMessage<'a> {
    type Error = Error;

    fn try_from(value: MiningDeviceMessages<'a>) -> Result<Self, Self::Error> {
        match value {
            MiningDeviceMessages::Common(m) => Ok(AnyMessage::Common(m)),
            MiningDeviceMessages::Mining(m) => Ok(AnyMessage::Mining(m)),
        }
    }
}

impl<'decoder> From<AnyMessage<'decoder>> for EncodableField<'decoder> {
    fn from(m: AnyMessage<'decoder>) -> Self {
        match m {
            AnyMessage::Common(a) => a.into(),
            AnyMessage::Mining(a) => a.into(),
            AnyMessage::JobDeclaration(a) => a.into(),
            AnyMessage::TemplateDistribution(a) => a.into(),
        }
    }
}
impl GetSize for AnyMessage<'_> {
    fn get_size(&self) -> usize {
        match self {
            AnyMessage::Common(a) => a.get_size(),
            AnyMessage::Mining(a) => a.get_size(),
            AnyMessage::JobDeclaration(a) => a.get_size(),
            AnyMessage::TemplateDistribution(a) => a.get_size(),
        }
    }
}

impl IsSv2Message for AnyMessage<'_> {
    fn message_type(&self) -> u8 {
        match self {
            AnyMessage::Common(a) => a.message_type(),
            AnyMessage::Mining(a) => a.message_type(),
            AnyMessage::JobDeclaration(a) => a.message_type(),
            AnyMessage::TemplateDistribution(a) => a.message_type(),
        }
    }

    fn channel_bit(&self) -> bool {
        match self {
            AnyMessage::Common(a) => a.channel_bit(),
            AnyMessage::Mining(a) => a.channel_bit(),
            AnyMessage::JobDeclaration(a) => a.channel_bit(),
            AnyMessage::TemplateDistribution(a) => a.channel_bit(),
        }
    }
}

impl IsSv2Message for MiningDeviceMessages<'_> {
    fn message_type(&self) -> u8 {
        match self {
            MiningDeviceMessages::Common(a) => a.message_type(),
            MiningDeviceMessages::Mining(a) => a.message_type(),
        }
    }

    fn channel_bit(&self) -> bool {
        match self {
            MiningDeviceMessages::Common(a) => a.channel_bit(),
            MiningDeviceMessages::Mining(a) => a.channel_bit(),
        }
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for AnyMessage<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let is_common: Result<CommonMessageTypes, Error> = v.0.try_into();
        let is_mining: Result<MiningTypes, Error> = v.0.try_into();
        let is_job_declaration: Result<JobDeclarationTypes, Error> = v.0.try_into();
        let is_template_distribution: Result<TemplateDistributionTypes, Error> = v.0.try_into();
        match (
            is_common,
            is_mining,
            is_job_declaration,
            is_template_distribution,
        ) {
            (Ok(_), Err(_), Err(_), Err(_)) => Ok(Self::Common(v.try_into()?)),
            (Err(_), Ok(_), Err(_), Err(_)) => Ok(Self::Mining(v.try_into()?)),
            (Err(_), Err(_), Ok(_), Err(_)) => Ok(Self::JobDeclaration(v.try_into()?)),
            (Err(_), Err(_), Err(_), Ok(_)) => Ok(Self::TemplateDistribution(v.try_into()?)),
            (Err(e), Err(_), Err(_), Err(_)) => Err(e),
            // This is an impossible state is safe to panic here
            _ => panic!(),
        }
    }
}

impl<'a> From<SetupConnection<'a>> for CommonMessages<'a> {
    fn from(v: SetupConnection<'a>) -> Self {
        CommonMessages::SetupConnection(v)
    }
}

impl From<SetupConnectionSuccess> for CommonMessages<'_> {
    fn from(v: SetupConnectionSuccess) -> Self {
        CommonMessages::SetupConnectionSuccess(v)
    }
}

impl<'a> From<SetupConnectionError<'a>> for CommonMessages<'a> {
    fn from(v: SetupConnectionError<'a>) -> Self {
        CommonMessages::SetupConnectionError(v)
    }
}

impl<'a> From<OpenStandardMiningChannel<'a>> for Mining<'a> {
    fn from(v: OpenStandardMiningChannel<'a>) -> Self {
        Mining::OpenStandardMiningChannel(v)
    }
}
impl<'a> From<UpdateChannel<'a>> for Mining<'a> {
    fn from(v: UpdateChannel<'a>) -> Self {
        Mining::UpdateChannel(v)
    }
}
impl<'a> From<OpenStandardMiningChannelSuccess<'a>> for Mining<'a> {
    fn from(v: OpenStandardMiningChannelSuccess<'a>) -> Self {
        Mining::OpenStandardMiningChannelSuccess(v)
    }
}

impl<'a, T: Into<CommonMessages<'a>>> From<T> for AnyMessage<'a> {
    fn from(v: T) -> Self {
        AnyMessage::Common(v.into())
    }
}

impl<'a, T: Into<CommonMessages<'a>>> From<T> for MiningDeviceMessages<'a> {
    fn from(v: T) -> Self {
        MiningDeviceMessages::Common(v.into())
    }
}

impl<'decoder, B: AsMut<[u8]> + AsRef<[u8]>> TryFrom<AnyMessage<'decoder>>
    for Sv2Frame<AnyMessage<'decoder>, B>
{
    type Error = Error;

    fn try_from(v: AnyMessage<'decoder>) -> Result<Self, Error> {
        let extension_type = 0;
        let channel_bit = v.channel_bit();
        let message_type = v.message_type();
        Sv2Frame::from_message(v, message_type, extension_type, channel_bit)
            .ok_or(Error::BadPayloadSize)
    }
}

impl<'decoder, B: AsMut<[u8]> + AsRef<[u8]>> TryFrom<MiningDeviceMessages<'decoder>>
    for Sv2Frame<MiningDeviceMessages<'decoder>, B>
{
    type Error = Error;

    fn try_from(v: MiningDeviceMessages<'decoder>) -> Result<Self, Error> {
        let extension_type = 0;
        let channel_bit = v.channel_bit();
        let message_type = v.message_type();
        Sv2Frame::from_message(v, message_type, extension_type, channel_bit)
            .ok_or(Error::BadPayloadSize)
    }
}

impl<'decoder, B: AsMut<[u8]> + AsRef<[u8]>> TryFrom<TemplateDistribution<'decoder>>
    for Sv2Frame<TemplateDistribution<'decoder>, B>
{
    type Error = Error;

    fn try_from(v: TemplateDistribution<'decoder>) -> Result<Self, Error> {
        let extension_type = 0;
        let channel_bit = v.channel_bit();
        let message_type = v.message_type();
        Sv2Frame::from_message(v, message_type, extension_type, channel_bit)
            .ok_or(Error::BadPayloadSize)
    }
}

impl<'a> TryFrom<AnyMessage<'a>> for MiningDeviceMessages<'a> {
    type Error = Error;

    fn try_from(value: AnyMessage<'a>) -> Result<Self, Error> {
        match value {
            AnyMessage::Common(message) => Ok(Self::Common(message)),
            AnyMessage::Mining(message) => Ok(Self::Mining(message)),
            AnyMessage::JobDeclaration(_) => Err(Error::UnexpectedPoolMessage),
            AnyMessage::TemplateDistribution(_) => Err(Error::UnexpectedPoolMessage),
        }
    }
}

#[cfg(test)]
mod test {
    use crate::{
        mining_sv2::NewMiningJob,
        parsers::{AnyMessage, Mining},
    };
    use codec_sv2::{
        binary_sv2::{Sv2Option, U256},
        StandardSv2Frame,
    };
    use std::convert::{TryFrom, TryInto};

    pub type Message = AnyMessage<'static>;
    pub type StdFrame = StandardSv2Frame<Message>;

    #[test]
    fn new_mining_job_serialization() {
        const CORRECTLY_SERIALIZED_MSG: &[u8] = &[
            0, 128, 21, 49, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 1, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,
            19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,
            41, 42, 43, 44, 45, 46, 47, 48,
        ];
        let mining_message = AnyMessage::Mining(Mining::NewMiningJob(NewMiningJob {
            channel_id: u32::from_le_bytes([1, 2, 3, 4]),
            job_id: u32::from_le_bytes([5, 6, 7, 8]),
            min_ntime: Sv2Option::new(Some(u32::from_le_bytes([9, 10, 11, 12]))),
            version: u32::from_le_bytes([13, 14, 15, 16]),
            merkle_root: U256::try_from((17_u8..(17 + 32)).collect::<Vec<u8>>()).unwrap(),
        }));
        message_serialization_check(mining_message, CORRECTLY_SERIALIZED_MSG);
    }

    fn message_serialization_check(message: AnyMessage<'static>, expected_result: &[u8]) {
        let frame = StdFrame::try_from(message).unwrap();
        let encoded_frame_length = frame.encoded_length();

        let mut buffer = [0; 0xffff];
        frame.serialize(&mut buffer).unwrap();
        check_length_consistency(&buffer[..encoded_frame_length]);
        check_length_consistency(expected_result);
        assert_eq!(
            is_channel_msg(&buffer),
            is_channel_msg(expected_result),
            "Unexpected channel_message flag",
        );
        assert_eq!(
            extract_extension_type(&buffer),
            extract_extension_type(expected_result),
            "Unexpected extension type",
        );
        assert_eq!(
            extract_message_type(&buffer),
            extract_message_type(expected_result),
            "Unexpected message type",
        );
        assert_eq!(
            extract_payload_length(&buffer),
            extract_payload_length(expected_result),
            "Unexpected message length",
        );
        assert_eq!(
            encoded_frame_length as u32,
            expected_result.len() as u32,
            "Method encoded_length() returned different length than the actual message length",
        );
        assert_eq!(
            extract_payload(&buffer[..encoded_frame_length]),
            extract_payload(expected_result),
            "Unexpected payload",
        )
    }

    fn is_channel_msg(serialized_frame: &[u8]) -> bool {
        let array_repre = serialized_frame[..2].try_into().unwrap();
        let decoded_extension_type = u16::from_le_bytes(array_repre);
        (decoded_extension_type & (1 << 15)) != 0
    }
    fn extract_extension_type(serialized_frame: &[u8]) -> u16 {
        let array_repre = serialized_frame[..2].try_into().unwrap();
        let decoded_extension_type = u16::from_le_bytes(array_repre);
        decoded_extension_type & (u16::MAX >> 1)
    }
    fn extract_message_type(serialized_frame: &[u8]) -> u8 {
        serialized_frame[2]
    }
    fn extract_payload_length(serialized_frame: &[u8]) -> u32 {
        let mut array_repre = [0; 4];
        array_repre[..3].copy_from_slice(&serialized_frame[3..6]);
        u32::from_le_bytes(array_repre)
    }
    fn extract_payload(serialized_frame: &[u8]) -> &[u8] {
        assert!(serialized_frame.len() > 6);
        &serialized_frame[6..]
    }

    fn check_length_consistency(serialized_frame: &[u8]) {
        let message_length = extract_payload_length(serialized_frame) as usize;
        let payload_length = extract_payload(serialized_frame).len();
        assert_eq!(
            message_length, payload_length,
            "Header declared length [{message_length} bytes] differs from the actual payload length [{payload_length} bytes]",
        );
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/utils.rs">
//! # Collection of Helper Primitives
//!
//! Provides a collection of utilities and helper structures used throughout the Stratum V2
//! protocol implementation. These utilities simplify common tasks, such as ID generation and
//! management, mutex management, difficulty target calculations, merkle root calculations, and
//! more.

use bitcoin::{
    blockdata::block::{Header, Version},
    consensus,
    consensus::Decodable,
    hash_types::{BlockHash, TxMerkleNode},
    hashes::{sha256d::Hash as DHash, Hash},
    transaction::TxOut,
    Block, CompactTarget, Transaction,
};
use codec_sv2::binary_sv2::U256;
use job_declaration_sv2::{DeclareMiningJob, PushSolution};
use mining_sv2::Target;
use primitive_types::U256 as U256Primitive;
use std::{
    cmp::max,
    convert::TryInto,
    fmt::Write,
    io::Cursor,
    ops::Div,
    sync::{Mutex as Mutex_, MutexGuard, PoisonError},
};
use tracing::error;

use crate::errors::Error;

/// Generator of unique IDs for channels and groups.
///
/// It keeps an internal counter, which is incremented every time a new unique id is requested.
#[derive(Debug, PartialEq, Eq, Clone)]
pub struct Id {
    state: u32,
}

impl Id {
    /// Creates a new [`Id`] instance initialized to `0`.
    pub fn new() -> Self {
        Self { state: 0 }
    }

    /// Increments then returns the internal state on a new ID.
    #[allow(clippy::should_implement_trait)]
    pub fn next(&mut self) -> u32 {
        self.state += 1;
        self.state
    }
}

impl Default for Id {
    fn default() -> Self {
        Self::new()
    }
}

/// Deserializes a vector of serialized outputs into a vector of TxOuts.
///
/// Only to be used for deserializing outputs from a NewTemplate message.
///
/// Not suitable for deserializing outputs from a SetCustomMiningJob message or
/// AllocateMiningJobToken.Success.
pub fn deserialize_template_outputs(
    serialized_outputs: Vec<u8>,
    coinbase_tx_outputs_count: u32,
) -> Result<Vec<TxOut>, Error> {
    let mut cursor = Cursor::new(serialized_outputs);

    (0..coinbase_tx_outputs_count)
        .map(|_| {
            TxOut::consensus_decode(&mut cursor)
                .map_err(|_| Error::FailedToDeserializeCoinbaseOutputs)
        })
        .collect()
}

/// Custom synchronization primitive for managing shared mutable state.
///
/// This custom mutex implementation builds on [`std::sync::Mutex`] to enhance usability and safety
/// in concurrent environments. It provides ergonomic methods to safely access and modify inner
/// values while reducing the risk of deadlocks and panics. It is used throughout SRI applications
/// to managed shared state across multiple threads, such as tracking active mining sessions,
/// routing jobs, and managing connections safely and efficiently.
///
/// ## Advantages
/// - **Closure-Based Locking:** The `safe_lock` method encapsulates the locking process, ensuring
///   the lock is automatically released after the closure completes.
/// - **Error Handling:** `safe_lock` enforces explicit handling of potential [`PoisonError`]
///   conditions, reducing the risk of panics caused by poisoned locks.
/// - **Panic-Safe Option:** The `super_safe_lock` method provides an alternative that unwraps the
///   result of `safe_lock`, with optional runtime safeguards against panics.
/// - **Extensibility:** Includes feature-gated functionality to customize behavior, such as
///   stricter runtime checks using external tools like
///   [`no-panic`](https://github.com/dtolnay/no-panic).
#[derive(Debug)]
pub struct Mutex<T: ?Sized>(Mutex_<T>);

impl<T> Mutex<T> {
    /// Mutex safe lock.
    ///
    /// Safely locks the `Mutex` and executes a closer (`thunk`) with a mutable reference to the
    /// inner value. This ensures that the lock is automatically released after the closure
    /// completes, preventing deadlocks. It explicitly returns a [`PoisonError`] containing a
    /// [`MutexGuard`] to the inner value in cases where the lock is poisoned.
    ///
    /// To prevent poison lock errors, unwraps should never be used within the closure. The result
    /// should always be returned and handled outside of the sage lock.
    pub fn safe_lock<F, Ret>(&self, thunk: F) -> Result<Ret, PoisonError<MutexGuard<'_, T>>>
    where
        F: FnOnce(&mut T) -> Ret,
    {
        let mut lock = self.0.lock()?;
        let return_value = thunk(&mut *lock);
        drop(lock);
        Ok(return_value)
    }

    /// Mutex super safe lock.
    ///
    /// Locks the `Mutex` and executes a closure (`thunk`) with a mutable reference to the inner
    /// value, panicking if the lock is poisoned.
    ///
    /// This is a convenience wrapper around `safe_lock` for cases where explicit error handling is
    /// unnecessary or undesirable. Use with caution in production code.
    pub fn super_safe_lock<F, Ret>(&self, thunk: F) -> Ret
    where
        F: FnOnce(&mut T) -> Ret,
    {
        //#[cfg(feature = "disable_nopanic")]
        {
            self.safe_lock(thunk).unwrap()
        }
        //#[cfg(not(feature = "disable_nopanic"))]
        //{
        //    // based on https://github.com/dtolnay/no-panic
        //    struct __NoPanic;
        //    extern "C" {
        //        #[link_name = "super_safe_lock called on a function that may panic"]
        //        fn trigger() -> !;
        //    }
        //    impl core::ops::Drop for __NoPanic {
        //        fn drop(&mut self) {
        //            unsafe {
        //                trigger();
        //            }
        //        }
        //    }
        //    let mut lock = self.0.lock().expect("threads to never panic");
        //    let __guard = __NoPanic;
        //    let return_value = thunk(&mut *lock);
        //    core::mem::forget(__guard);
        //    drop(lock);
        //    return_value
        //}
    }

    /// Creates a new [`Mutex`] instance, storing the initial value inside.
    pub fn new(v: T) -> Self {
        Mutex(Mutex_::new(v))
    }

    /// Removes lock for direct access.
    ///
    /// Acquires a lock on the [`Mutex`] and returns a [`MutexGuard`] for direct access to the
    /// inner value. Allows for manual lock handling and is useful in scenarios where closures are
    /// not convenient.
    pub fn to_remove(&self) -> Result<MutexGuard<'_, T>, PoisonError<MutexGuard<'_, T>>> {
        self.0.lock()
    }
}

/// Computes the Merkle root from coinbase transaction components and a path of transaction hashes.
///
/// Validates and deserializes a coinbase transaction before building the 32-byte Merkle root.
/// Returns [`None`] is the arguments are invalid.
///
/// ## Components
/// * `coinbase_tx_prefix`: First part of the coinbase transaction (the part before the extranonce).
///   Should be converted from [`binary_sv2::B064K`].
/// * `coinbase_tx_suffix`: Coinbase transaction suffix (the part after the extranonce). Should be
///   converted from [`binary_sv2::B064K`].
/// * `extranonce`: Extra nonce space. Should be converted from [`binary_sv2::B032`] and padded with
///   zeros if not `32` bytes long.
/// * `path`: List of transaction hashes. Should be converted from [`binary_sv2::U256`].
pub fn merkle_root_from_path<T: AsRef<[u8]>>(
    coinbase_tx_prefix: &[u8],
    coinbase_tx_suffix: &[u8],
    extranonce: &[u8],
    path: &[T],
) -> Option<Vec<u8>> {
    let mut coinbase =
        Vec::with_capacity(coinbase_tx_prefix.len() + coinbase_tx_suffix.len() + extranonce.len());
    coinbase.extend_from_slice(coinbase_tx_prefix);
    coinbase.extend_from_slice(extranonce);
    coinbase.extend_from_slice(coinbase_tx_suffix);
    let coinbase: Transaction = match consensus::deserialize(&coinbase[..]) {
        Ok(trans) => trans,
        Err(e) => {
            error!("ERROR: {}", e);
            dbg!(e);
            return None;
        }
    };

    let coinbase_id: [u8; 32] = *coinbase.compute_txid().as_ref();

    Some(merkle_root_from_path_(coinbase_id, path).to_vec())
}

/// Computes the Merkle root from a validated coinbase transaction and a path of transaction
/// hashes.
///
/// If the `path` is empty, the coinbase transaction hash (`coinbase_id`) is returned as the root.
///
/// ## Components
/// * `coinbase_id`: Coinbase transaction hash.
/// * `path`: List of transaction hashes. Should be converted from [`binary_sv2::U256`].
pub fn merkle_root_from_path_<T: AsRef<[u8]>>(coinbase_id: [u8; 32], path: &[T]) -> [u8; 32] {
    match path.len() {
        0 => coinbase_id,
        _ => reduce_path(coinbase_id, path),
    }
}

// Helper function to format bytes as hex string
// useful for visualizing targets
pub fn bytes_to_hex(bytes: &[u8]) -> String {
    let mut s = String::with_capacity(bytes.len() * 2);
    for &b in bytes {
        write!(&mut s, "{b:02x}")
            .expect("Writing hex bytes to pre-allocated string should never fail");
    }
    s
}

// Computes the Merkle root by iteratively combining the coinbase transaction hash with each
// transaction hash in the `path`.
//
// Handles the core logic of combining hashes using the Bitcoin double-SHA256 hashing algorithm.
fn reduce_path<T: AsRef<[u8]>>(coinbase_id: [u8; 32], path: &[T]) -> [u8; 32] {
    let mut root = coinbase_id;
    for node in path {
        let to_hash = [&root[..], node.as_ref()].concat();
        let hash = DHash::hash(&to_hash);
        root = *hash.as_ref();
    }
    root
}

/// A list of potential errors during conversion between hashrate and target
#[derive(Debug)]
pub enum InputError {
    NegativeInput,
    DivisionByZero,
    ArithmeticOverflow,
}

/// Calculates the mining target threshold for a mining device based on its hashrate (H/s) and
/// desired share frequency (shares/min).
///
/// Determines the maximum hash value (target), in big endian, that a mining device can produce to
/// find a valid share. The target is derived from the miner's hashrate and the expected number of
/// shares per minute, aligning the miner's workload with the upstream's (e.g. pool's) share
/// frequency requirements.
///
/// Typically used during connection setup to assign a starting target based on the mining device's
/// reported hashrate and to recalculate during runtime when a mining device's hashrate changes,
/// ensuring they submit shares at the desired rate.
///
/// ## Formula
/// ```text
/// t = (2^256 - sh) / (sh + 1)
/// ```
///
/// Where:
/// - `h`: Mining device hashrate (H/s).
/// - `s`: Shares per second `60 / shares/min` (s).
/// - `sh`: `h * s`, the mining device's work over `s` seconds.
///
/// According to \[1] and \[2], it is possible to model the probability of finding a block with
/// a random variable X whose distribution is negative hypergeometric \[3]. Such a variable is
/// characterized as follows:
///
/// Say that there are `n` (`2^256`) elements (possible hash values), of which `t` (values <=
/// target) are defined as success and the remaining as failures. The variable `X` has co-domain
/// the positive integers, and `X=k` is the event where element are drawn one after the other,
/// without replacement, and only the `k`th element is successful. The expected value of this
/// variable is `(n-t)/(t+1)`. So, on average, a miner has to perform `(2^256-t)/(t+1)` hashes
/// before finding hash whose value is below the target `t`.
///
/// If the pool wants, on average, a share every `s` seconds, then, on average, the miner has to
/// perform `h*s` hashes before finding one that is smaller than the target, where `h` is the
/// miner's hashrate. Therefore, `s*h= (2^256-t)/(t+1)`. If we consider `h` the global Bitcoin's
/// hashrate, `s = 600` seconds and `t` the Bitcoin global target, then, for all the blocks we
/// tried, the two members of the equations have the same order of magnitude and, most of the
/// cases, they coincide with the first two digits.
///
/// We take this as evidence of the correctness of our calculations. Thus, if the pool wants on
/// average a share every `s` seconds from a miner with hashrate `h`, then the target `t` for the
/// miner is `t = (2^256-sh)/(sh+1)`.
///
/// \[1] [https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3399742](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3399742)
///
/// \[2] [https://www.zora.uzh.ch/id/eprint/173483/1/SSRN-id3399742-2.pdf](https://www.zora.uzh.ch/id/eprint/173483/1/SSRN-id3399742-2.pdf)
///
/// \[3] [https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution](https://en.wikipedia.org/wiki/Negative_hypergeometric_distribution)
pub fn hash_rate_to_target(
    hashrate: f64,
    share_per_min: f64,
) -> Result<U256<'static>, crate::Error> {
    // checks that we are not dividing by zero
    if share_per_min == 0.0 {
        return Err(Error::TargetError(InputError::DivisionByZero));
    }
    if share_per_min.is_sign_negative() {
        return Err(Error::TargetError(InputError::NegativeInput));
    };
    if hashrate.is_sign_negative() {
        return Err(Error::TargetError(InputError::NegativeInput));
    };

    // if we want 5 shares per minute, this means that s=60/5=12 seconds interval between shares
    // this quantity will be at the numerator, so we multiply the result by 100 again later
    let shares_occurrency_frequence = 60_f64 / share_per_min;

    let h_times_s = hashrate * shares_occurrency_frequence;
    let h_times_s = h_times_s as u128;

    // We calculate the denominator: h*s+1
    // the denominator is h*s+1, where h*s is an u128, so always positive.
    // this means that the denominator can never be zero
    // we add 100 in place of 1 because h*s is actually h*s*100, we in order to simplify later we
    // must calculate (h*s+1)*100
    let h_times_s_plus_one = max(h_times_s, h_times_s + 1);

    let h_times_s_plus_one = from_u128_to_u256(h_times_s_plus_one);
    let denominator = h_times_s_plus_one;

    // We calculate the numerator: 2^256-sh
    let two_to_256_minus_one = [255_u8; 32];
    let two_to_256_minus_one = U256Primitive::from_big_endian(two_to_256_minus_one.as_ref());

    let mut h_times_s_array = [0u8; 32];
    h_times_s_array[16..].copy_from_slice(&h_times_s.to_be_bytes());
    let numerator = two_to_256_minus_one - U256Primitive::from_big_endian(h_times_s_array.as_ref());

    let mut target = numerator.div(denominator).to_big_endian();
    target.reverse();
    Ok(U256::<'static>::from(target))
}

/// Calculates the hashrate (H/s) required to produce a specific number of shares per minute for a
/// given mining target (big endian).
///
/// It is the inverse of [`hash_rate_to_target`], enabling backward calculations to estimate a
/// mining device's performance from its submitted shares.
///
/// Typically used to calculate the mining device's effective hashrate during runtime based on the
/// submitted shares and the assigned target, also helps detect changes in miner performance and
/// recalibrate the target (using [`hash_rate_to_target`]) if necessary.
///
/// ## Formula
/// ```text
/// h = (2^256 - t) / (s * (t + 1))
/// ```
///
/// Where:
/// - `h`: Mining device hashrate (H/s).
/// - `t`: Target threshold.
/// - `s`: Shares per minute.
pub fn hash_rate_from_target(target: U256<'static>, share_per_min: f64) -> Result<f64, Error> {
    // checks that we are not dividing by zero
    if share_per_min == 0.0 {
        return Err(Error::HashrateError(InputError::DivisionByZero));
    }
    if share_per_min.is_sign_negative() {
        return Err(Error::HashrateError(InputError::NegativeInput));
    }
    let mut target_arr: [u8; 32] = [0; 32];
    let slice: &mut [u8] = &mut target_arr;
    slice.copy_from_slice(target.inner_as_ref());
    target_arr.reverse();
    let target = U256Primitive::from_big_endian(target_arr.as_ref());
    // we calculate the numerator 2^256-t
    // note that [255_u8,;32] actually is 2^256 -1, but 2^256 -t = (2^256-1) - (t-1)
    let max_target = [255_u8; 32];
    let max_target = U256Primitive::from_big_endian(max_target.as_ref());
    let numerator = max_target - (target - U256Primitive::one());
    // now we calculate the denominator s(t+1)
    // *100 here to move the fractional bit up so we can make this an int later
    let shares_occurrency_frequence = 60_f64 / (share_per_min) * 100.0;
    // note that t+1 cannot be zero because t unsigned. Therefore the denominator is zero if and
    // only if s is zero.
    let shares_occurrency_frequence = shares_occurrency_frequence as u128;
    if shares_occurrency_frequence == 0_u128 {
        return Err(Error::HashrateError(InputError::DivisionByZero));
    }
    let shares_occurrency_frequence = from_u128_to_u256(shares_occurrency_frequence);
    let target_plus_one = U256Primitive::from_big_endian(target_arr.as_ref())
        .checked_add(U256Primitive::one())
        .ok_or(Error::HashrateError(InputError::ArithmeticOverflow))?;
    let denominator = target_plus_one
        .checked_mul(shares_occurrency_frequence)
        .and_then(|e| e.checked_div(U256Primitive::from(100)))
        .ok_or(Error::HashrateError(InputError::ArithmeticOverflow))?;
    let result = numerator.div(denominator).low_u128();
    // we multiply back by 100 so that it cancels with the same factor at the denominator
    Ok(result as f64)
}

/// Converts a `Target` to a `f64` difficulty.
pub fn target_to_difficulty(target: Target) -> f64 {
    // Genesis block target: 0x00000000ffff0000000000000000000000000000000000000000000000000000
    // (in little endian)
    let max_target_bytes = [
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
        0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0xff, 0x00, 0x00,
        0x00, 0x00,
    ];
    let max_target = U256Primitive::from_little_endian(&max_target_bytes);

    // Convert input target to U256Primitive
    let target_u256: U256<'static> = target.into();
    let mut target_bytes = [0u8; 32];
    target_bytes.copy_from_slice(target_u256.inner_as_ref());
    let target = U256Primitive::from_little_endian(&target_bytes);

    // Calculate difficulty = max_target / target
    // We need to handle the full 256-bit values properly
    // Convert to f64 by taking the ratio of the most significant bits
    let max_target_high = (max_target >> 128).low_u128() as f64;
    let max_target_low = max_target.low_u128() as f64;
    let target_high = (target >> 128).low_u128() as f64;
    let target_low = target.low_u128() as f64;

    // Combine high and low parts with appropriate scaling
    let max_target_f64 = max_target_high * (2.0f64.powi(128)) + max_target_low;
    let target_f64 = target_high * (2.0f64.powi(128)) + target_low;

    max_target_f64 / target_f64
}

/// Converts a `u128` to a [`U256`].
pub fn from_u128_to_u256(input: u128) -> U256Primitive {
    let input: [u8; 16] = input.to_be_bytes();
    let mut be_bytes = [0_u8; 32];
    for (i, b) in input.iter().enumerate() {
        be_bytes[16 + i] = *b;
    }
    U256Primitive::from_big_endian(be_bytes.as_ref())
}

/// Generates and manages unique IDs for groups and channels.
///
/// [`GroupId`] allows combining the group and channel [`Id`]s into a single 64-bit value, enabling
/// efficient tracking and referencing of group-channel relationships.
///
/// This is specifically used for packaging multiple channels into a single group, such that
/// multiple mining or communication channels can be managed as a cohesive unit. This is
/// particularly useful in scenarios where multiple downstreams share common properties or need to
/// be treated collectively for routing or load balancing.
///
/// A group acts as a container for multiple channels. Each channel represents a distinct
/// communication pathway between a downstream (e.g. a mining device) and an upstream (e.g. a proxy
/// or pool). Channels within a group might share common configurations, such as difficulty
/// settings or work templates. Operations like broadcasting job updates or handling difficulty
/// adjustments can be efficiently applied to all channels in a group. By treating a group as a
/// single entity, the protocol reduces overhead of managing individual channels, especially in
/// large mining farms.
#[derive(Debug, Default)]
pub struct GroupId {
    group_ids: Id,
    channel_ids: Id,
}

impl GroupId {
    /// Creates a new [`GroupId`] instance.
    ///
    /// New GroupId it starts with groups 0, since 0 is reserved for hom downstream's.
    pub fn new() -> Self {
        Self {
            group_ids: Id::new(),
            channel_ids: Id::new(),
        }
    }

    /// Generates a new unique group ID.
    ///
    /// Increments the internal group ID counter and returns the next available group ID.
    pub fn new_group_id(&mut self) -> u32 {
        self.group_ids.next()
    }

    /// Generates a new unique channel ID for a given group.
    ///
    /// Increments the internal channel ID counter and returns the next available channel ID.
    ///
    /// **Note**: The `_group_id` parameter is reserved for future use to create a hierarchical
    /// structure of IDs without breaking compatibility with older versions.
    pub fn new_channel_id(&mut self, _group_id: u32) -> u32 {
        self.channel_ids.next()
    }

    /// Combines a group ID and channel ID into a single 64-bit unique ID.
    ///
    /// Concatenates the group ID and channel ID, storing the group ID in the higher 32 bits and
    /// the channel ID in the lower 32 bits. This combined identifier is useful for efficiently
    /// tracking and referencing unique group-channel pairs.
    pub fn into_complete_id(group_id: u32, channel_id: u32) -> u64 {
        let part_1 = channel_id.to_le_bytes();
        let part_2 = group_id.to_le_bytes();
        u64::from_be_bytes([
            part_2[3], part_2[2], part_2[1], part_2[0], part_1[3], part_1[2], part_1[1], part_1[0],
        ])
    }

    /// Extracts the group ID from a complete group-channel 64-bit unique ID.
    ///
    /// The group ID is the higher 32 bits.
    pub fn into_group_id(complete_id: u64) -> u32 {
        let complete = complete_id.to_le_bytes();
        u32::from_le_bytes([complete[4], complete[5], complete[6], complete[7]])
    }

    /// Extracts the channel ID from a complete group-channel 64-bit unique ID.
    ///
    /// The channel ID is the lower 32 bits.
    pub fn into_channel_id(complete_id: u64) -> u32 {
        let complete = complete_id.to_le_bytes();
        u32::from_le_bytes([complete[0], complete[1], complete[2], complete[3]])
    }
}

#[test]
fn test_group_id_new_group_id() {
    let mut group_ids = GroupId::new();
    let _ = group_ids.new_group_id();
    let id = group_ids.new_group_id();
    assert!(id == 2);
}
#[test]
fn test_group_id_new_channel_id() {
    let mut group_ids = GroupId::new();
    let _ = group_ids.new_group_id();
    let id = group_ids.new_group_id();
    let channel_id = group_ids.new_channel_id(id);
    assert!(channel_id == 1);
}
#[test]
fn test_group_id_new_into_complete_id() {
    let group_id = u32::from_le_bytes([0, 1, 2, 3]);
    let channel_id = u32::from_le_bytes([10, 11, 12, 13]);
    let complete_id = GroupId::into_complete_id(group_id, channel_id);
    assert!([10, 11, 12, 13, 0, 1, 2, 3] == complete_id.to_le_bytes());
}

#[test]
fn test_group_id_new_into_group_id() {
    let group_id = u32::from_le_bytes([0, 1, 2, 3]);
    let channel_id = u32::from_le_bytes([10, 11, 12, 13]);
    let complete_id = GroupId::into_complete_id(group_id, channel_id);
    let channel_from_complete = GroupId::into_channel_id(complete_id);
    assert!(channel_id == channel_from_complete);
}

#[test]
fn test_merkle_root_independent_vector() {
    const REFERENCE_MERKLE_ROOT: [u8; 32] = [
        28, 204, 213, 73, 250, 160, 146, 15, 5, 127, 9, 214, 204, 20, 164, 199, 20, 181, 26, 190,
        236, 91, 40, 225, 128, 239, 213, 148, 232, 77, 4, 36,
    ];
    const BRANCH: &[[u8; 32]] = &[
        [
            224, 195, 140, 86, 17, 172, 9, 61, 54, 73, 215, 202, 109, 83, 124, 163, 215, 78, 143,
            204, 44, 242, 242, 122, 37, 106, 55, 81, 58, 234, 27, 210,
        ],
        [
            35, 10, 232, 246, 235, 117, 56, 190, 87, 77, 81, 11, 159, 79, 90, 62, 91, 52, 41, 49,
            57, 245, 219, 122, 115, 223, 199, 229, 238, 60, 47, 144,
        ],
        [
            95, 18, 132, 87, 213, 76, 188, 74, 245, 106, 18, 149, 106, 32, 209, 158, 239, 3, 17,
            26, 207, 230, 118, 149, 120, 48, 96, 66, 214, 150, 137, 220,
        ],
        [
            205, 167, 106, 179, 82, 50, 157, 76, 91, 36, 54, 226, 34, 183, 162, 179, 109, 64, 185,
            207, 103, 192, 63, 31, 141, 126, 34, 30, 68, 69, 154, 176,
        ],
        [
            251, 236, 76, 1, 218, 98, 98, 236, 144, 52, 151, 246, 95, 13, 109, 240, 240, 195, 64,
            157, 7, 142, 28, 242, 29, 123, 51, 93, 51, 36, 143, 148,
        ],
        [
            35, 146, 105, 130, 188, 39, 97, 252, 75, 229, 185, 148, 242, 106, 164, 112, 123, 66,
            34, 95, 218, 203, 50, 203, 129, 208, 109, 220, 112, 228, 121, 160,
        ],
        [
            44, 55, 125, 47, 249, 213, 175, 143, 140, 50, 219, 72, 111, 71, 125, 54, 85, 70, 4, 85,
            60, 92, 208, 35, 113, 245, 128, 139, 228, 4, 230, 177,
        ],
        [
            169, 119, 48, 178, 205, 188, 19, 220, 85, 29, 174, 45, 158, 172, 222, 238, 170, 144,
            79, 140, 56, 90, 105, 187, 204, 145, 241, 96, 75, 88, 6, 133,
        ],
        [
            72, 202, 11, 90, 167, 140, 253, 12, 58, 85, 223, 17, 82, 112, 24, 129, 186, 39, 224,
            171, 227, 192, 14, 167, 154, 248, 150, 55, 114, 169, 43, 17,
        ],
    ];
    const CB_PREFIX: &[u8] = &[
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 75, 3, 139, 133, 11, 250, 190, 109, 109, 43, 220,
        215, 96, 154, 211, 18, 14, 53, 53, 0, 95, 132, 159, 127, 54, 197, 70, 135, 74, 17, 149, 12,
        104, 133, 16, 182, 152, 109, 207, 13, 9, 1, 0, 0, 0, 0, 0, 0, 0,
    ];
    const CB_SUFFIX: &[u8] = &[
        89, 236, 29, 54, 20, 47, 115, 108, 117, 115, 104, 47, 0, 0, 0, 0, 3, 236, 42, 86, 37, 0, 0,
        0, 0, 25, 118, 169, 20, 124, 21, 78, 209, 220, 89, 96, 158, 61, 38, 171, 178, 223, 46, 163,
        213, 135, 205, 140, 65, 136, 172, 0, 0, 0, 0, 0, 0, 0, 0, 44, 106, 76, 41, 82, 83, 75, 66,
        76, 79, 67, 75, 58, 155, 83, 3, 23, 69, 4, 30, 18, 212, 34, 33, 76, 167, 101, 132, 91, 1,
        127, 124, 85, 238, 57, 118, 135, 107, 35, 25, 33, 0, 71, 6, 88, 0, 0, 0, 0, 0, 0, 0, 0, 38,
        106, 36, 170, 33, 169, 237, 123, 170, 130, 253, 191, 130, 150, 16, 0, 18, 157, 2, 231, 33,
        177, 230, 137, 182, 134, 51, 32, 216, 181, 6, 73, 60, 103, 211, 194, 61, 77, 64, 0, 0, 0,
        0,
    ];
    const EXTRANONCE_PREFIX: &[u8] = &[41, 101, 8, 3, 39, 21, 251];
    const EXTRANONCE: &[u8] = &[165, 6, 238, 7, 139, 252, 22, 7];

    let full_extranonce = {
        let mut xn = EXTRANONCE_PREFIX.to_vec();
        xn.extend_from_slice(EXTRANONCE);
        xn
    };

    let calculated_merkle_root =
        merkle_root_from_path(CB_PREFIX, CB_SUFFIX, &full_extranonce, BRANCH)
            .expect("Ultimate failure. Merkle root calculator returned None");
    assert_eq!(
        calculated_merkle_root, REFERENCE_MERKLE_ROOT,
        "Merkle root does not match reference"
    )
}

#[test]
fn test_merkle_root_from_path() {
    let coinbase_bytes = vec![
        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 75, 3, 63, 146, 11, 250, 190, 109, 109, 86, 6,
        110, 64, 228, 218, 247, 203, 127, 75, 141, 53, 51, 197, 180, 38, 117, 115, 221, 103, 2, 11,
        85, 213, 65, 221, 74, 90, 97, 128, 91, 182, 1, 0, 0, 0, 0, 0, 0, 0, 49, 101, 7, 7, 139,
        168, 76, 0, 1, 0, 0, 0, 0, 0, 0, 70, 84, 183, 110, 24, 47, 115, 108, 117, 115, 104, 47, 0,
        0, 0, 0, 3, 120, 55, 179, 37, 0, 0, 0, 0, 25, 118, 169, 20, 124, 21, 78, 209, 220, 89, 96,
        158, 61, 38, 171, 178, 223, 46, 163, 213, 135, 205, 140, 65, 136, 172, 0, 0, 0, 0, 0, 0, 0,
        0, 44, 106, 76, 41, 82, 83, 75, 66, 76, 79, 67, 75, 58, 216, 82, 49, 182, 148, 133, 228,
        178, 20, 248, 55, 219, 145, 83, 227, 86, 32, 97, 240, 182, 3, 175, 116, 196, 69, 114, 83,
        46, 0, 71, 230, 205, 0, 0, 0, 0, 0, 0, 0, 0, 38, 106, 36, 170, 33, 169, 237, 179, 75, 32,
        206, 223, 111, 113, 150, 112, 248, 21, 36, 163, 123, 107, 168, 153, 76, 233, 86, 77, 218,
        162, 59, 48, 26, 180, 38, 62, 34, 3, 185, 0, 0, 0, 0,
    ];
    let a = [
        122, 97, 64, 124, 164, 158, 164, 14, 87, 119, 226, 169, 34, 196, 251, 51, 31, 131, 109,
        250, 13, 54, 94, 6, 177, 27, 156, 154, 101, 30, 123, 159,
    ];
    let b = [
        180, 113, 121, 253, 215, 85, 129, 38, 108, 2, 86, 66, 46, 12, 131, 139, 130, 87, 29, 92,
        59, 164, 247, 114, 251, 140, 129, 88, 127, 196, 125, 116,
    ];
    let c = [
        171, 77, 225, 148, 80, 32, 41, 157, 246, 77, 161, 49, 87, 139, 214, 236, 149, 164, 192,
        128, 195, 9, 5, 168, 131, 27, 250, 9, 60, 179, 206, 94,
    ];
    let d = [
        6, 187, 202, 75, 155, 220, 255, 166, 199, 35, 182, 220, 20, 96, 123, 41, 109, 40, 186, 142,
        13, 139, 230, 164, 116, 177, 217, 23, 16, 123, 135, 202,
    ];
    let e = [
        109, 45, 171, 89, 223, 39, 132, 14, 150, 128, 241, 113, 136, 227, 105, 123, 224, 48, 66,
        240, 189, 186, 222, 49, 173, 143, 80, 90, 110, 219, 192, 235,
    ];
    let f = [
        196, 7, 21, 180, 228, 161, 182, 132, 28, 153, 242, 12, 210, 127, 157, 86, 62, 123, 181, 33,
        84, 3, 105, 129, 148, 162, 5, 152, 64, 7, 196, 156,
    ];
    let g = [
        22, 16, 18, 180, 109, 237, 68, 167, 197, 10, 195, 134, 11, 119, 219, 184, 49, 140, 239, 45,
        27, 210, 212, 120, 186, 60, 155, 105, 106, 219, 218, 32,
    ];
    let h = [
        83, 228, 21, 241, 42, 240, 8, 254, 109, 156, 59, 171, 167, 46, 183, 60, 27, 63, 241, 211,
        235, 179, 147, 99, 46, 3, 22, 166, 159, 169, 183, 159,
    ];
    let i = [
        230, 81, 3, 190, 66, 73, 200, 55, 94, 135, 209, 50, 92, 193, 114, 202, 141, 170, 124, 142,
        206, 29, 88, 9, 22, 110, 203, 145, 238, 66, 166, 35,
    ];
    let l = [
        43, 106, 86, 239, 237, 74, 208, 202, 247, 133, 88, 42, 15, 77, 163, 186, 85, 26, 89, 151,
        5, 19, 30, 122, 108, 220, 215, 104, 152, 226, 113, 55,
    ];
    let m = [
        148, 76, 200, 221, 206, 54, 56, 45, 252, 60, 123, 202, 195, 73, 144, 65, 168, 184, 59, 130,
        145, 229, 250, 44, 213, 70, 175, 128, 34, 31, 102, 80,
    ];
    let n = [
        203, 112, 102, 31, 49, 147, 24, 25, 245, 61, 179, 146, 205, 127, 126, 100, 78, 204, 228,
        146, 209, 154, 89, 194, 209, 81, 57, 167, 88, 251, 44, 76,
    ];
    let mut path = vec![a, b, c, d, e, f, g, h, i, l, m, n];
    let expected_root = vec![
        73, 100, 41, 247, 106, 44, 1, 242, 3, 64, 100, 1, 98, 155, 40, 91, 170, 255, 170, 29, 193,
        255, 244, 71, 236, 29, 134, 218, 94, 45, 78, 77,
    ];
    let root = merkle_root_from_path(
        &coinbase_bytes[..20],
        &coinbase_bytes[30..],
        &coinbase_bytes[20..30],
        &path,
    )
    .unwrap();
    assert_eq!(expected_root, root);

    //Target coinbase_id return path
    path.clear();
    let coinbase_id = vec![
        10, 66, 217, 241, 152, 86, 5, 234, 225, 85, 251, 215, 105, 1, 21, 126, 222, 69, 40, 157,
        23, 177, 157, 106, 234, 164, 243, 206, 23, 241, 250, 166,
    ];

    let root = merkle_root_from_path(
        &coinbase_bytes[..20],
        &coinbase_bytes[30..],
        &coinbase_bytes[20..30],
        &path,
    )
    .unwrap();
    assert_eq!(coinbase_id, root);

    //Target None return path on serialization
    assert_eq!(
        merkle_root_from_path(&coinbase_bytes, &coinbase_bytes, &coinbase_bytes, &path),
        None
    );
}

/// Converts a `u256` to a [`BlockHash`] type.
pub fn u256_to_block_hash(v: U256<'static>) -> BlockHash {
    let hash: [u8; 32] = v.to_vec().try_into().unwrap();
    let hash = Hash::from_slice(&hash).unwrap();
    BlockHash::from_raw_hash(hash)
}

// Returns a new `Header`.
//
// Expected endianness inputs:
// `version`     LE
// `prev_hash`   BE
// `merkle_root` BE
// `time`        BE
// `bits`        BE
// `nonce`       BE
#[allow(dead_code)]
pub(crate) fn new_header(
    version: i32,
    prev_hash: &[u8],
    merkle_root: &[u8],
    time: u32,
    bits: u32,
    nonce: u32,
) -> Result<Header, Error> {
    if prev_hash.len() != 32 {
        return Err(Error::ExpectedLen32(prev_hash.len()));
    }
    if merkle_root.len() != 32 {
        return Err(Error::ExpectedLen32(merkle_root.len()));
    }
    let mut prev_hash_arr = [0u8; 32];
    prev_hash_arr.copy_from_slice(prev_hash);
    let prev_hash = DHash::from_bytes_ref(&prev_hash_arr);

    let mut merkle_root_arr = [0u8; 32];
    merkle_root_arr.copy_from_slice(merkle_root);
    let merkle_root = DHash::from_bytes_ref(&merkle_root_arr);

    Ok(Header {
        version: Version::from_consensus(version),
        prev_blockhash: BlockHash::from_raw_hash(*prev_hash),
        merkle_root: TxMerkleNode::from_raw_hash(*merkle_root),
        time,
        bits: CompactTarget::from_consensus(bits),
        nonce,
    })
}

/// Creates a block from a solution submission.
///
/// Facilitates the creation of valid Bitcoin blocks by combining a declared mining job, a list of
/// transactions, and a solution message from the mining device. It encapsulates the necessary data
/// (the coinbase, a list of transactions, and a miner-provided solution) to assemble a complete
/// and valid block that can be submitted to the Bitcoin network.
///
/// It is used in the Job Declarator server to handle the final step in processing the mining job
/// solutions.
pub struct BlockCreator<'a> {
    last_declare: DeclareMiningJob<'a>,
    tx_list: Vec<bitcoin::Transaction>,
    message: PushSolution<'a>,
}

impl<'a> BlockCreator<'a> {
    /// Creates a new [`BlockCreator`] instance.
    pub fn new(
        last_declare: DeclareMiningJob<'a>,
        tx_list: Vec<bitcoin::Transaction>,
        message: PushSolution<'a>,
    ) -> BlockCreator<'a> {
        BlockCreator {
            last_declare,
            tx_list,
            message,
        }
    }
}

// TODO write a test for this function that takes an already mined block, and test if the new
// block created with the hash of the new block created with the block creator coincides with the
// hash of the mined block
impl<'a> From<BlockCreator<'a>> for bitcoin::Block {
    fn from(block_creator: BlockCreator<'a>) -> bitcoin::Block {
        let last_declare = block_creator.last_declare;
        let mut tx_list = block_creator.tx_list;
        let message = block_creator.message;

        let coinbase_pre = last_declare.coinbase_prefix.to_vec();
        let extranonce = message.extranonce.to_vec();
        let coinbase_suf = last_declare.coinbase_suffix.to_vec();
        let mut path: Vec<Vec<u8>> = vec![];
        for tx in &tx_list {
            let id = tx.compute_txid();
            let id_bytes: &[u8; 32] = id.as_ref();
            path.push(id_bytes.to_vec());
        }
        let merkle_root =
            merkle_root_from_path(&coinbase_pre[..], &coinbase_suf[..], &extranonce[..], &path)
                .expect("Invalid coinbase");
        let merkle_root = Hash::from_slice(merkle_root.as_slice()).unwrap();

        let prev_blockhash = u256_to_block_hash(message.prev_hash.into_static());
        let header = Header {
            version: Version::from_consensus(message.version as i32),
            prev_blockhash,
            merkle_root,
            time: message.ntime,
            bits: CompactTarget::from_consensus(message.nbits),
            nonce: message.nonce,
        };

        let coinbase = [coinbase_pre, extranonce, coinbase_suf].concat();
        let coinbase = consensus::deserialize(&coinbase[..]).unwrap();
        tx_list.insert(0, coinbase);

        let mut block = Block {
            header,
            txdata: tx_list.clone(),
        };

        block.header.merkle_root = block.compute_merkle_root().unwrap();
        block
    }
}

#[cfg(test)]
mod tests {

    use super::{hash_rate_from_target, hash_rate_to_target, *};
    use codec_sv2::binary_sv2::{Seq0255, B064K, U256};
    use rand::Rng;
    use serde::Deserialize;
    use std::{convert::TryInto, num::ParseIntError};

    fn decode_hex(s: &str) -> Result<Vec<u8>, ParseIntError> {
        (0..s.len())
            .step_by(2)
            .map(|i| u8::from_str_radix(&s[i..i + 2], 16))
            .collect()
    }

    #[derive(Debug, Deserialize)]
    struct TestBlockToml {
        block_hash: String,
        version: u32,
        prev_hash: String,
        time: u32,
        merkle_root: String,
        nbits: u32,
        nonce: u32,
        coinbase_tx_prefix: String,
        coinbase_script: String,
        coinbase_tx_suffix: String,
        path: Vec<String>,
    }

    #[derive(Debug)]
    struct TestBlock<'decoder> {
        #[allow(dead_code)]
        block_hash: U256<'decoder>,
        version: u32,
        prev_hash: Vec<u8>,
        time: u32,
        merkle_root: Vec<u8>,
        nbits: u32,
        nonce: u32,
        coinbase_tx_prefix: B064K<'decoder>,
        coinbase_script: Vec<u8>,
        coinbase_tx_suffix: B064K<'decoder>,
        path: Seq0255<'decoder, U256<'decoder>>,
    }

    fn get_test_block<'decoder>() -> TestBlock<'decoder> {
        let test_file = std::fs::read_to_string("reg-test-block.toml")
            .expect("Could not read file from string");
        let block: TestBlockToml =
            toml::from_str(&test_file).expect("Could not parse toml file as `TestBlockToml`");

        // Get block hash
        let block_hash_vec =
            decode_hex(&block.block_hash).expect("Could not decode hex string to `Vec<u8>`");
        let mut block_hash_vec: [u8; 32] = block_hash_vec
            .try_into()
            .expect("Slice is incorrect length");
        block_hash_vec.reverse();
        let block_hash: U256 = block_hash_vec.into();

        // Get prev hash
        let mut prev_hash: Vec<u8> =
            decode_hex(&block.prev_hash).expect("Could not convert `String` to `&[u8]`");
        prev_hash.reverse();

        // Get Merkle root
        let mut merkle_root =
            decode_hex(&block.merkle_root).expect("Could not decode hex string to `Vec<u8>`");
        // Swap endianness to LE
        merkle_root.reverse();

        // Get Merkle path
        let mut path_vec = Vec::<U256>::new();
        for p in block.path {
            let p_vec = decode_hex(&p).expect("Could not decode hex string to `Vec<u8>`");
            let p_arr: [u8; 32] = p_vec.try_into().expect("Slice is incorrect length");
            let p_u256: U256 = (p_arr).into();
            path_vec.push(p_u256);
        }

        let path = Seq0255::new(path_vec).expect("Could not convert `Vec<U256>` to `Seq0255`");

        // Pass in coinbase as three pieces:
        //   coinbase_tx_prefix + coinbase script + coinbase_tx_suffix
        let coinbase_tx_prefix_vec = decode_hex(&block.coinbase_tx_prefix)
            .expect("Could not decode hex string to `Vec<u8>`");
        let coinbase_tx_prefix: B064K = coinbase_tx_prefix_vec
            .try_into()
            .expect("Could not convert `Vec<u8>` into `B064K`");

        let coinbase_script =
            decode_hex(&block.coinbase_script).expect("Could not decode hex `String` to `Vec<u8>`");

        let coinbase_tx_suffix_vec = decode_hex(&block.coinbase_tx_suffix)
            .expect("Could not decode hex `String` to `Vec<u8>`");
        let coinbase_tx_suffix: B064K = coinbase_tx_suffix_vec
            .try_into()
            .expect("Could not convert `Vec<u8>` to `B064K`");

        TestBlock {
            block_hash,
            version: block.version,
            prev_hash,
            time: block.time,
            merkle_root,
            nbits: block.nbits,
            nonce: block.nonce,
            coinbase_tx_prefix,
            coinbase_script,
            coinbase_tx_suffix,
            path,
        }
    }

    #[test]
    fn gets_merkle_root_from_path() {
        let block = get_test_block();
        let expect: Vec<u8> = block.merkle_root;

        let actual = merkle_root_from_path(
            block.coinbase_tx_prefix.inner_as_ref(),
            block.coinbase_tx_suffix.inner_as_ref(),
            &block.coinbase_script,
            &block.path.inner_as_ref(),
        )
        .unwrap();

        assert_eq!(expect, actual);
    }

    #[test]

    fn gets_new_header() -> Result<(), Error> {
        let block = get_test_block();

        if !block.prev_hash.len() == 32 {
            return Err(Error::ExpectedLen32(block.prev_hash.len()));
        }
        if !block.merkle_root.len() == 32 {
            return Err(Error::ExpectedLen32(block.merkle_root.len()));
        }
        let mut prev_hash_arr = [0u8; 32];
        prev_hash_arr.copy_from_slice(&block.prev_hash);
        let prev_hash = DHash::from_bytes_ref(&prev_hash_arr);

        let mut merkle_root_arr = [0u8; 32];
        merkle_root_arr.copy_from_slice(&block.merkle_root);
        let merkle_root = DHash::from_bytes_ref(&merkle_root_arr);

        let expect = Header {
            version: Version::from_consensus(block.version as i32),
            prev_blockhash: BlockHash::from_raw_hash(*prev_hash),
            merkle_root: TxMerkleNode::from_raw_hash(*merkle_root),
            time: block.time,
            bits: CompactTarget::from_consensus(block.nbits),
            nonce: block.nonce,
        };

        let actual_block = get_test_block();
        let actual = new_header(
            block.version as i32,
            &actual_block.prev_hash,
            &actual_block.merkle_root,
            block.time,
            block.nbits,
            block.nonce,
        )?;
        assert_eq!(actual, expect);
        Ok(())
    }

    #[test]
    fn test_hash_rate_to_target() {
        let mut rng = rand::thread_rng();
        let mut successes = 0;

        let hr = 10.0; // 10 h/s
        let hrs = hr * 60.0; // number of hashes in 1 minute
        let mut target = hash_rate_to_target(hr, 1.0).unwrap().to_vec();
        target.reverse();
        let target = U256Primitive::from_big_endian(&target[..]);

        let mut i: i64 = 0;
        let mut results = vec![];
        let attempts = 1000;
        while successes < attempts {
            let a: u128 = rng.gen();
            let b: u128 = rng.gen();
            let a = a.to_be_bytes();
            let b = b.to_be_bytes();
            let concat = [&a[..], &b[..]].concat().to_vec();
            i += 1;
            if U256Primitive::from_big_endian(&concat[..]) <= target {
                results.push(i);
                i = 0;
                successes += 1;
            }
        }

        let mut average: f64 = 0.0;
        for i in &results {
            average += (*i as f64) / attempts as f64;
        }
        let delta = (hrs - average) as i64;
        assert!(delta.abs() < 100);
    }

    #[test]
    fn test_hash_rate_from_target() {
        let hr = 202470.828;
        let expected_share_per_min = 1.0;
        let target = hash_rate_to_target(hr, expected_share_per_min).unwrap();
        let realized_share_per_min = expected_share_per_min * 10.0; // increase SPM by 10x
        let hash_rate = hash_rate_from_target(target.clone(), realized_share_per_min).unwrap();
        let new_hr = (hr * 10.0).trunc();

        assert!(
            hash_rate == new_hr,
            "hash_rate_from_target equation was not properly transformed"
        )
    }

    #[test]
    fn test_hash_rate_from_target_zero_share_per_min() {
        // Test division by zero error handling when share_per_min is 0.
        let hr = 202470.828;
        let expected_share_per_min = 1.0;
        let target = hash_rate_to_target(hr, expected_share_per_min).unwrap();
        let share_per_min = 0.0;
        let result = hash_rate_from_target(target, share_per_min);

        assert!(
            matches!(
                result,
                Err(Error::HashrateError(InputError::DivisionByZero))
            ),
            "Should return division by zero error"
        );
    }

    #[test]
    fn test_hash_rate_from_target_arithmetic_overflow() {
        // Test arithmetic overflow error handling with extremely low share_per_min and maximal
        // target.
        let mut target_bytes = [0xff; 32];
        target_bytes[0] = 0x7f; // Reduce the magnitude to avoid direct overflow
        let target_sv2 = U256::from(target_bytes);
        let share_per_min = 0.001;
        let result = hash_rate_from_target(target_sv2, share_per_min);

        assert!(
            matches!(
                result,
                Err(Error::HashrateError(InputError::ArithmeticOverflow))
            ),
            "Should return arithmetic overflow error"
        );
    }

    #[test]
    fn test_super_safe_lock() {
        let m = super::Mutex::new(1u32);
        m.safe_lock(|i| *i += 1).unwrap();
        // m.super_safe_lock(|i| *i = (*i).checked_add(1).unwrap()); // will not compile
        m.super_safe_lock(|i| *i = (*i).checked_add(1).unwrap_or_default()); // compiles
    }

    #[test]
    fn test_target_to_difficulty() {
        // Test target: 0x000000000004864c000000000000000000000000000000000000000000000000
        let target_bytes = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x4c, 0x86, 0x04, 0x00,
            0x00, 0x00, 0x00, 0x00,
        ];
        let target = Target::from(target_bytes);
        let difficulty = target_to_difficulty(target);

        // Expected difficulty: 14484.162361
        let expected_difficulty = 14484.162361;
        let epsilon = 0.000001; // Small value for floating point comparison

        assert!(
            (difficulty - expected_difficulty).abs() < epsilon,
            "Expected difficulty {}, got {}",
            expected_difficulty,
            difficulty
        );

        let max_target_bytes = [
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
            0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xff, 0xff,
            0x00, 0x00, 0x00, 0x00,
        ];
        let max_target = Target::from(max_target_bytes);
        let max_difficulty = target_to_difficulty(max_target);

        let expected_max_difficulty = 1.0;
        let epsilon = 0.000001; // Small value for floating point comparison

        assert!(
            (max_difficulty - expected_max_difficulty).abs() < epsilon,
            "Expected difficulty {}, got {}",
            expected_max_difficulty,
            max_difficulty
        );
    }

    #[test]
    fn test_hash_rate_from_target_with_max_target() {
        use codec_sv2::binary_sv2::U256;
        // This is the maximum value for a 256-bit unsigned integer
        let max_u128 = 340282366920938463463374607431768211455u128;
        // Compose the bytes for U256::MAX
        let mut max_bytes = [0u8; 32];
        max_bytes[..16].copy_from_slice(&max_u128.to_be_bytes());
        max_bytes[16..].copy_from_slice(&max_u128.to_be_bytes());
        let target = U256::from(max_bytes);
        let share_per_min = 4.0;
        let result = hash_rate_from_target(target, share_per_min);
        assert!(
            matches!(
                result,
                Err(Error::HashrateError(InputError::ArithmeticOverflow))
            ),
            "Expected ArithmeticOverflow error, got: {:?}",
            result
        );
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/classic.rs">
use crate::utils::hash_rate_from_target;
use mining_sv2::Target;
use tracing::debug;

/// Default minimum hashrate (H/s) if not specified.
const DEFAULT_MIN_HASHRATE: f32 = 1.0;

use super::{error::VardiffError, Vardiff};

/// Represents the dynamic state for a variable difficulty (Vardiff) connection.
///
/// Tracks performance and adjusts the mining target to achieve a desired share rate.
#[derive(Debug)]
pub struct VardiffState {
    /// Count of shares received since the last difficulty adjustment.
    pub shares_since_last_update: u32,
    /// Unix timestamp (seconds) of the last difficulty adjustment.
    pub timestamp_of_last_update: u64,
    /// The lowest hashrate (H/s) the system will allow; values below this are clamped.
    pub min_allowed_hashrate: f32,
}

impl VardiffState {
    /// Creates a new `VardiffState` with the default minimum hashrate.
    ///
    /// # Arguments
    /// * `estimated_hashrate` - The initial hashrate estimate.
    pub fn new() -> Result<Self, VardiffError> {
        Self::new_with_min(DEFAULT_MIN_HASHRATE)
    }

    /// Creates a new `VardiffState` with a specific minimum hashrate.
    ///
    /// # Arguments
    /// * `min_allowed_hashrate` - The minimum hashrate to enforce.
    pub fn new_with_min(min_allowed_hashrate: f32) -> Result<Self, VardiffError> {
        let timestamp_secs = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs();

        Ok(VardiffState {
            shares_since_last_update: 0,
            timestamp_of_last_update: timestamp_secs,
            min_allowed_hashrate,
        })
    }

    /// Sets the count of shares since the last update.
    pub fn set_shares_since_last_update(&mut self, shares_since_last_update: u32) {
        self.shares_since_last_update = shares_since_last_update;
    }
}

impl Vardiff for VardiffState {
    fn last_update_timestamp(&self) -> u64 {
        self.timestamp_of_last_update
    }

    fn shares_since_last_update(&self) -> u32 {
        self.shares_since_last_update
    }

    fn min_allowed_hashrate(&self) -> f32 {
        self.min_allowed_hashrate
    }

    /// Sets the timestamp of the last update.
    fn set_timestamp_of_last_update(&mut self, timestamp_of_last_update: u64) {
        self.timestamp_of_last_update = timestamp_of_last_update;
    }

    /// Increments the share counter by one.
    fn increment_shares_since_last_update(&mut self) {
        self.shares_since_last_update += 1;
    }

    /// Resets the share counter and updates the timestamp to now.
    fn reset_counter(&mut self) -> Result<(), VardiffError> {
        let timestamp_secs = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)?
            .as_secs();
        self.set_timestamp_of_last_update(timestamp_secs);
        self.set_shares_since_last_update(0);
        Ok(())
    }

    /// Checks channel performance and potentially updates the hashrate and target.
    ///
    /// It calculates the realized share rate since the last update. If the
    /// deviation from the target rate is significant enough (based on internal,
    /// time-sensitive thresholds), it estimates a new hashrate and applies it.
    ///
    /// It returns `Ok(Some(new_hashrate))` when an update occurs,
    /// `Ok(None)` when conditions don't warrant an update, and
    /// `Err` for actual processing errors.
    fn try_vardiff(
        &mut self,
        hashrate: f32,
        target: &Target,
        shares_per_minute: f32,
    ) -> Result<Option<f32>, VardiffError> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map_err(VardiffError::TimeError)?
            .as_secs();

        let delta_time = now - self.timestamp_of_last_update;

        if delta_time <= 15 {
            return Ok(None);
        }

        let realized_share_per_min =
            self.shares_since_last_update as f64 / (delta_time as f64 / 60.0);

        debug!(
            target: "vardiff",
            "Hashrate update check triggered:
            - Elapsed time: {}s
            - Shares since last update: {}
            - Realized shares per minute: {:.4}
            - Current miner target: {:?}",
            delta_time,
            self.shares_since_last_update,
            realized_share_per_min,
            target
        );

        let mut new_hashrate = match hash_rate_from_target(
            target.clone().into(),
            realized_share_per_min,
        ) {
            Ok(hashrate) => hashrate as f32,
            Err(e) => {
                debug!(
                    target: "vardiff",
                    "Target->Hashrate conversion failed: {:?}. Falling back using previous hashrate and realized_shares_per_minute", e
                );
                hashrate * realized_share_per_min as f32 / shares_per_minute
            }
        };

        let hashrate_delta = new_hashrate - hashrate;
        let hashrate_delta_percentage = (hashrate_delta.abs() / hashrate) * 100.0;

        debug!(
            target: "vardiff",
            "Calculated new hashrate: {:.2} H/s ( {:.2}%, previous {:.2} H/s)",
            new_hashrate,
            hashrate_delta_percentage,
            hashrate,
        );

        let should_update = match hashrate_delta_percentage {
            pct if pct >= 100.0 => true,
            pct if pct >= 60.0 && delta_time >= 60 => true,
            pct if pct >= 50.0 && delta_time >= 120 => true,
            pct if pct >= 45.0 && delta_time >= 180 => true,
            pct if pct >= 30.0 && delta_time >= 240 => true,
            pct if pct >= 15.0 && delta_time >= 300 => true,
            _ => false,
        };

        if !should_update {
            return Ok(None);
        }

        // realized_share_per_min is 0.0 when d.difficulty_mgmt.shares_since_last_update is 0
        // so it's safe to compare realized_share_per_min with == 0.0
        if realized_share_per_min == 0.0 {
            new_hashrate = match delta_time {
                dt if dt <= 30 => hashrate / 1.5,
                dt if dt < 60 => hashrate / 2.0,
                _ => hashrate / 3.0,
            };
        } else if hashrate_delta_percentage > 1000.0 {
            new_hashrate = match delta_time {
                dt if dt <= 30 => hashrate * 10.0,
                dt if dt < 60 => hashrate * 5.0,
                _ => hashrate * 3.0,
            };
        }
        if new_hashrate < self.min_allowed_hashrate {
            debug!(
                target: "vardiff",
                "New hashrate {:.2} H/s below minimum threshold {:.2} H/s  clamping",
                new_hashrate,
                self.min_allowed_hashrate
            );
            new_hashrate = self.min_allowed_hashrate;
        }
        self.reset_counter()?;

        Ok(Some(new_hashrate))
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/error.rs">
/// Defines errors encountered during Vardiff operations.
#[derive(Debug)]
pub enum VardiffError {
    /// Failed to convert hashrate to a target.
    HashrateToTargetError(String),
    /// Failed to convert target to a hashrate.
    TargetToHashrateError(String),
    /// System time error occurred.
    TimeError(std::time::SystemTimeError),
}

impl From<std::time::SystemTimeError> for VardiffError {
    fn from(value: std::time::SystemTimeError) -> Self {
        VardiffError::TimeError(value)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/mod.rs">
use error::VardiffError;
use mining_sv2::Target;
use std::fmt::Debug;

pub mod classic;
pub mod error;
#[cfg(test)]
pub mod test;

/// Trait defining the interface for a Vardiff implementation.
pub trait Vardiff: Debug + Send + Sync {
    /// Gets the timestamp of the last update.
    fn last_update_timestamp(&self) -> u64;

    /// Gets the share count since the last update.
    fn shares_since_last_update(&self) -> u32;

    /// Sets timestamp since last update.
    fn set_timestamp_of_last_update(&mut self, timestamp: u64);

    /// Increments the share count.
    fn increment_shares_since_last_update(&mut self);

    /// Resets share count and timestamp for a new cycle.
    fn reset_counter(&mut self) -> Result<(), VardiffError>;

    /// Checks performance and potentially adjusts difficulty, returning the new
    /// hashrate if an update occurred.
    fn try_vardiff(
        &mut self,
        hashrate: f32,
        target: &Target,
        shares_per_minute: f32,
    ) -> Result<Option<f32>, VardiffError>;

    /// Gets the minimum allowed hashrate (H/s).
    fn min_allowed_hashrate(&self) -> f32;
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/test/classic.rs">
/// Classic implementation test suite
use crate::vardiff::test::{
    simulate_shares_and_wait, TEST_MIN_ALLOWED_HASHRATE, TEST_SHARES_PER_MINUTE,
};
use crate::{utils::hash_rate_to_target, vardiff::VardiffError, VardiffState};

use super::{
    test_increment_and_reset_shares, test_try_vardiff_low_hashrate_decrease_target,
    test_try_vardiff_no_shares_30_to_60s_decrease,
    test_try_vardiff_no_shares_less_than_30s_decrease,
    test_try_vardiff_no_shares_more_than_60s_decrease,
    test_try_vardiff_stable_hashrate_minimal_change_or_no_change,
    test_try_vardiff_with_less_spm_than_expected, test_try_vardiff_with_shares_30_to_60s,
    test_try_vardiff_with_shares_less_than_30, test_try_vardiff_with_shares_more_than_60s, Vardiff,
};

fn new_test_vardiff_state() -> Result<VardiffState, VardiffError> {
    VardiffState::new_with_min(TEST_MIN_ALLOWED_HASHRATE)
}

#[test]
fn test_initialization_and_getters() {
    let vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");

    assert_eq!(vardiff.min_allowed_hashrate(), TEST_MIN_ALLOWED_HASHRATE);
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

#[test]
fn test_increment_and_reset_shares_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_increment_and_reset_shares(&mut vardiff)
}

#[test]
fn test_try_vardiff_stable_hashrate_minimal_change_or_no_change_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_stable_hashrate_minimal_change_or_no_change(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_low_hashrate_decrease_target_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_low_hashrate_decrease_target(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_with_shares_less_than_30_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_with_shares_less_than_30(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_with_shares_30_to_60s_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_with_shares_30_to_60s(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_with_shares_more_than_60s_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_with_shares_more_than_60s(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_no_shares_30_to_60s_decrease_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_no_shares_30_to_60s_decrease(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_no_shares_more_than_60s_decrease_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_no_shares_more_than_60s_decrease(&mut vardiff);
}

#[test]
pub fn test_try_vardiff_no_shares_less_than_30s_decrease_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_no_shares_less_than_30s_decrease(&mut vardiff);
}

#[test]
fn test_try_vardiff_with_less_spm_than_expected_classic() {
    let mut vardiff = new_test_vardiff_state().expect("Failed to create VardiffState");
    test_try_vardiff_with_less_spm_than_expected(&mut vardiff);
}

#[test]
fn test_try_vardiff_hashrate_clamps_to_minimum() {
    let hashrate = TEST_MIN_ALLOWED_HASHRATE * 1.5;
    let target = hash_rate_to_target(hashrate.into(), TEST_SHARES_PER_MINUTE.into())
        .unwrap()
        .into();

    let mut vardiff = VardiffState::new_with_min(TEST_MIN_ALLOWED_HASHRATE)
        .expect("Failed to create VardiffState");

    let simulation_duration_secs = 16;
    simulate_shares_and_wait(&mut vardiff, 0, simulation_duration_secs);

    let result = vardiff
        .try_vardiff(hashrate, &target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(result.is_some(), "Hashrate should update");
    let new_hashrate = result.unwrap();

    assert_eq!(
        new_hashrate, TEST_MIN_ALLOWED_HASHRATE,
        "Hashrate should be clamped to minimum"
    );
    assert_eq!(
        new_hashrate, TEST_MIN_ALLOWED_HASHRATE,
        "Stored hashrate should be clamped"
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}
</file>

<file path="stratum-1.4.0/protocols/v2/roles-logic-sv2/src/vardiff/test/mod.rs">
/// Contains a generic test implementation that is agnostic to the Vardiff implementation,
/// providing methods to verify the correctness of any specific implementation.
use std::{thread, time::Duration};

mod classic;

use super::Vardiff;
use crate::utils::hash_rate_to_target;
use mining_sv2::Target;

pub const TEST_INITIAL_HASHRATE: f32 = 1000.0;
pub const TEST_SHARES_PER_MINUTE: f32 = 10.0;
pub const TEST_MIN_ALLOWED_HASHRATE: f32 = 10.0;

// Helper function to simulate a number of shares being found over a given duration.
pub fn simulate_shares_and_wait<V: Vardiff>(
    vardiff: &mut V,
    num_shares: u32,
    wait_duration_secs: u64,
) {
    for _ in 0..num_shares {
        vardiff.increment_shares_since_last_update();
    }

    // Rather than waiting for wait_duration,
    // we are performing time magic and going
    // back in time.
    let now = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs()
        - wait_duration_secs;

    vardiff.set_timestamp_of_last_update(now);
}

// Verifies that the share counter can be incremented and reset correctly.
pub fn test_increment_and_reset_shares<V: Vardiff>(vardiff: &mut V) {
    let initial_timestamp = vardiff.last_update_timestamp();

    vardiff.increment_shares_since_last_update();
    assert_eq!(vardiff.shares_since_last_update(), 1);

    vardiff.increment_shares_since_last_update();
    assert_eq!(vardiff.shares_since_last_update(), 2);

    thread::sleep(Duration::from_secs(1));

    vardiff.reset_counter().expect("Failed to reset counter");
    assert_eq!(vardiff.shares_since_last_update(), 0);

    assert!(
        vardiff.last_update_timestamp() > initial_timestamp,
        "Timestamp should update on reset"
    );
}

// Ensures that `try_vardiff` results in a minimal or no change when the hashrate is stable.
pub fn test_try_vardiff_stable_hashrate_minimal_change_or_no_change<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let iniital_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration_secs = 5;
    let expected_shares_for_duration = 1;

    simulate_shares_and_wait(
        vardiff,
        expected_shares_for_duration,
        simulation_duration_secs,
    );

    let result = vardiff
        .try_vardiff(initial_hashrate, &iniital_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");

    if let Some(new_hashrate) = result {
        let diff_percentage = ((new_hashrate - initial_hashrate).abs() / initial_hashrate) * 100.0;
        println!(
            "Stable hashrate test: new hashrate {new_hashrate}, initial {initial_hashrate}, diff_pct {diff_percentage}"
        );
        assert!(
            diff_percentage < 20.0,
            "Change should be minimal for stable rate if any"
        );
        assert_eq!(vardiff.shares_since_last_update(), 0)
    } else {
        assert_eq!(None, result);
    }
}

// Tests if a high share submission rate correctly increases the difficulty (lowers the target).
pub fn test_try_vardiff_low_hashrate_decrease_target<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 16;
    simulate_shares_and_wait(vardiff, 16, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(
        result.is_some(),
        "Hashrate should update due to low share count"
    );
    let new_hashrate = result.unwrap();

    // As estimated shares per minute is 10
    // with current setup realized shares per minute is 60
    // comes under no special case
    assert_eq!(new_hashrate, 6.0 * initial_hashrate);
    let target: Target = hash_rate_to_target(new_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
        .unwrap()
        .into();
    println!("target: {target:?}");
    assert!(
        target < initial_target,
        "Target should become harder (larger value)"
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Checks the difficulty adjustment logic for a high share rate within a 30-second window.
pub fn test_try_vardiff_with_shares_less_than_30<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 16;
    simulate_shares_and_wait(vardiff, 500, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(
        result.is_some(),
        "Hashrate should update due to low share count"
    );
    let new_hashrate = result.unwrap();

    // This logic checks the `dt <= 30` case, which multiple by 10
    assert_eq!(new_hashrate, 10.0 * initial_hashrate);

    let target: Target = hash_rate_to_target(new_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
        .unwrap()
        .into();
    assert!(
        target < initial_target,
        "Target should become harder (larger value)"
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Checks the difficulty adjustment logic for a high share rate within a 30 to 60-second window.
pub fn test_try_vardiff_with_shares_30_to_60s<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 31;
    simulate_shares_and_wait(vardiff, 5000, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(
        result.is_some(),
        "Hashrate should update due to low share count"
    );
    let new_hashrate = result.unwrap();

    // This logic checks the `dt < 60` case, which multiple by 5
    assert_eq!(new_hashrate, 5.0 * initial_hashrate);
    let target: Target = hash_rate_to_target(new_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
        .unwrap()
        .into();
    assert!(
        target < initial_target,
        "Target should become harder (larger value)"
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Checks the difficulty adjustment logic for a high share rate over a 60-second window.
pub fn test_try_vardiff_with_shares_more_than_60s<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 60;
    simulate_shares_and_wait(vardiff, 1000, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(
        result.is_some(),
        "Hashrate should update due to low share count"
    );
    let new_hashrate = result.unwrap();

    // This logic checks the `dt >= 60` case, which multiple by 3
    assert_eq!(new_hashrate, 3.0 * initial_hashrate);
    let target: Target = hash_rate_to_target(new_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
        .unwrap()
        .into();
    assert!(
        target < initial_target,
        "Target should become harder (larger value)"
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Verifies that difficulty decreases when no shares are found within a 30-second window.
fn test_try_vardiff_no_shares_less_than_30s_decrease<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 16;
    simulate_shares_and_wait(vardiff, 0, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    assert!(result.is_some(), "Hashrate should update");
    let new_hashrate = result.unwrap();

    // This logic checks the `dt < 30` case, which divides by 1.5
    let expected_hashrate = initial_hashrate / 1.5;
    assert!(
        (new_hashrate - expected_hashrate).abs() < 0.01,
        "Hashrate should be initial / 1.5. Got: {}, Expected: {}",
        new_hashrate,
        expected_hashrate
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Verifies that difficulty decreases when no shares are found within a 30 to 60-second window.
fn test_try_vardiff_no_shares_30_to_60s_decrease<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 31;
    simulate_shares_and_wait(vardiff, 0, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    let new_hashrate = result.expect("Hashrate should have updated");

    // This logic checks the `dt < 60` case, which divides by 2.0
    let expected_hashrate = initial_hashrate / 2.0;
    assert!(
        (new_hashrate - expected_hashrate).abs() < 0.01,
        "Hashrate should be initial / 2. Got: {}, Expected: {}",
        new_hashrate,
        expected_hashrate
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

// Verifies that difficulty decreases when no shares are found over a 60-second window.
fn test_try_vardiff_no_shares_more_than_60s_decrease<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    let simulation_duration = 60;
    simulate_shares_and_wait(vardiff, 0, simulation_duration);

    let result = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed");
    let new_hashrate = result.expect("Hashrate should have updated");

    // This logic checks the `dt >= 60` case, which divides by 3.0
    let expected_hashrate = initial_hashrate / 3.0;
    assert!(
        (new_hashrate - expected_hashrate).abs() < 0.01,
        "Hashrate should be initial / 3. Got: {}, Expected: {}",
        new_hashrate,
        expected_hashrate
    );
    assert_eq!(vardiff.shares_since_last_update(), 0);
}

fn test_try_vardiff_with_less_spm_than_expected<V: Vardiff>(vardiff: &mut V) {
    let initial_hashrate = TEST_INITIAL_HASHRATE;
    let initial_target =
        hash_rate_to_target(initial_hashrate.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    assert_eq!(initial_hashrate, 1000.0);

    let simulation_duration = 60;
    // testing case when realized_shares_per_minute / shares_per_minute = 0.4
    simulate_shares_and_wait(vardiff, 4, simulation_duration);

    let hashrate_after_60s = vardiff
        .try_vardiff(initial_hashrate, &initial_target, TEST_SHARES_PER_MINUTE)
        .expect("try_vardiff failed")
        .unwrap();
    let target_after_60s: Target =
        hash_rate_to_target(hashrate_after_60s.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    assert_eq!(hashrate_after_60s, 400.0);

    let simulation_duration = 120;
    // testing case when realized_shares_per_minute / shares_per_minute = 0.5
    simulate_shares_and_wait(vardiff, 10, simulation_duration);

    let hashrate_after_120s = vardiff
        .try_vardiff(
            hashrate_after_60s,
            &target_after_60s,
            TEST_SHARES_PER_MINUTE,
        )
        .expect("try_vardiff failed")
        .unwrap();
    let target_after_120s: Target =
        hash_rate_to_target(hashrate_after_120s.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    assert_eq!(hashrate_after_120s, 200.0);

    let simulation_duration = 180;
    // testing case when realized_shares_per_minute / shares_per_minute = 0.55
    simulate_shares_and_wait(vardiff, 16, simulation_duration);

    let hashrate_after_180s = vardiff
        .try_vardiff(
            hashrate_after_120s,
            &target_after_120s,
            TEST_SHARES_PER_MINUTE,
        )
        .expect("try_vardiff failed")
        .unwrap();
    let target_after_180s: Target =
        hash_rate_to_target(hashrate_after_180s.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    assert_eq!(hashrate_after_180s, 106.0);

    let simulation_duration = 240;
    // testing case when realized_shares_per_minute / shares_per_minute = 0.7
    simulate_shares_and_wait(vardiff, 28, simulation_duration);

    let hashrate_after_240s = vardiff
        .try_vardiff(
            hashrate_after_180s,
            &target_after_180s,
            TEST_SHARES_PER_MINUTE,
        )
        .expect("try_vardiff failed")
        .unwrap();
    let target_after_240s: Target =
        hash_rate_to_target(hashrate_after_240s.into(), TEST_SHARES_PER_MINUTE.into())
            .unwrap()
            .into();

    assert_eq!(hashrate_after_240s, 74.2);

    let simulation_duration = 300;
    // testing case when realized_shares_per_minute / shares_per_minute = 0.85
    simulate_shares_and_wait(vardiff, 42, simulation_duration);

    let hashrate_after_300s = vardiff
        .try_vardiff(
            hashrate_after_240s,
            &target_after_240s,
            TEST_SHARES_PER_MINUTE,
        )
        .expect("try_vardiff failed")
        .unwrap();

    assert_eq!(hashrate_after_300s, 62.327995);
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/Cargo.toml">
[package]
name = "common_messages_sv2"
version = "5.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 subprotocol common messages"
documentation = "https://docs.rs/common_messages_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../binary-sv2", version = "^3.0.0" }
quickcheck = { version = "1.0.3", optional = true }
quickcheck_macros = { version = "1", optional = true }

[features]
prop_test = ["quickcheck"]
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/README.md">
# common_messages_sv2

[![crates.io](https://img.shields.io/crates/v/common_messages_sv2.svg)](https://crates.io/crates/common_messages_sv2)
[![docs.rs](https://docs.rs/common_messages_sv2/badge.svg)](https://docs.rs/common_messages_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg)](https://app.codecov.io/gh/stratum-mining/stratum/tree/main/protocols%2Fv2%2Fcommon_messages_sv2)

`common_messages_sv2` is a Rust `#![no-std]` crate that implements a set of messages shared across all Stratum V2 subprotocols.

For further information, please refer to [Stratum V2 documentation - Common Messages](https://stratumprotocol.org/specification/03-Protocol-Overview/#36-common-protocol-messages).

## Build Options

This crate can be built with the following features:
 - `quickcheck`: Enables support for property-based testing using QuickCheck.

## Usage

To include this crate in your project, run:

```bash
$ cargo add common_messages_sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/channel_endpoint_changed.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};
use core::convert::TryInto;

/// Message used by an upstream role for announcing a mining channel endpoint change.
///
/// This message should be sent when a mining channels upstream or downstream endpoint changes and
/// that channel had previously exchanged message(s) with `channel_msg` bitset of unknown
/// `extension_type`.
///
/// When a downstream receives such a message, any extension state (including version and extension
/// support) must be reset and renegotiated.
#[repr(C)]
#[derive(Serialize, Deserialize, Debug, Copy, Clone, PartialEq, Eq)]
pub struct ChannelEndpointChanged {
    /// Unique identifier of the channel that has changed its endpoint.
    pub channel_id: u32,
}

impl fmt::Display for ChannelEndpointChanged {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "ChannelEndpointChanged(channel_id: {})", self.channel_id)
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/lib.rs">
//! # Stratum V2 Common Messages Crate.
//!
//! This crate defines a set of shared messages used across all Stratum V2 subprotocols.
//!
//! ## Build Options
//! This crate can be built with the following features:
//! - `std`: Enables support for standard library features.
//! - `quickcheck`: Enables support for property-based testing using QuickCheck.
//!
//!
//! For further information about the messages, please refer to [Stratum V2
//! documentation - Common Messages](https://stratumprotocol.org/specification/03-Protocol-Overview/#36-common-protocol-messages).

#![no_std]

extern crate alloc;
mod channel_endpoint_changed;
mod reconnect;
mod setup_connection;

#[cfg(feature = "prop_test")]
use alloc::vec;
#[cfg(feature = "prop_test")]
use core::convert::TryInto;
#[cfg(feature = "prop_test")]
use quickcheck::{Arbitrary, Gen};

pub use channel_endpoint_changed::ChannelEndpointChanged;
pub use reconnect::Reconnect;
pub use setup_connection::{
    has_requires_std_job, has_version_rolling, has_work_selection, Protocol, SetupConnection,
    SetupConnectionError, SetupConnectionSuccess,
};

pub use setup_connection::{CSetupConnection, CSetupConnectionError};

// Discriminants for Stratum V2 (sub)protocols
//
// Discriminants are unique identifiers used to distinguish between different
// Stratum V2 (sub)protocols. Each protocol within the SV2 ecosystem has a
// specific discriminant value that indicates its type, enabling the correct
// interpretation and handling of messages. These discriminants ensure that
// messages are processed by the appropriate protocol handlers,
// thereby facilitating seamless communication across different components of
// the SV2 architecture.
//
// More info can be found [on Chapter 03 of the Stratum V2 specs](https://github.com/stratum-mining/sv2-spec/blob/main/03-Protocol-Overview.md#3-protocol-overview).
pub const SV2_MINING_PROTOCOL_DISCRIMINANT: u8 = 0;
pub const SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT: u8 = 1;
pub const SV2_TEMPLATE_DISTRIBUTION_PROTOCOL_DISCRIMINANT: u8 = 2;

// Common message types used across all Stratum V2 (sub)protocols.
pub const MESSAGE_TYPE_SETUP_CONNECTION: u8 = 0x0;
pub const MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS: u8 = 0x1;
pub const MESSAGE_TYPE_SETUP_CONNECTION_ERROR: u8 = 0x2;
pub const MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED: u8 = 0x3;
pub const MESSAGE_TYPE_RECONNECT: u8 = 0x04;

pub const CHANNEL_BIT_SETUP_CONNECTION: bool = false;
pub const CHANNEL_BIT_SETUP_CONNECTION_SUCCESS: bool = false;
pub const CHANNEL_BIT_SETUP_CONNECTION_ERROR: bool = false;
pub const CHANNEL_BIT_CHANNEL_ENDPOINT_CHANGED: bool = true;

#[no_mangle]
/// A C-compatible function that exports the [`ChannelEndpointChanged`] struct.
pub extern "C" fn _c_export_channel_endpoint_changed(_a: ChannelEndpointChanged) {}

#[no_mangle]
/// A C-compatible function that exports the `SetupConnection` struct.
pub extern "C" fn _c_export_setup_conn_succ(_a: SetupConnectionSuccess) {}

#[cfg(feature = "prop_test")]
impl ChannelEndpointChanged {
    pub fn from_gen(g: &mut Gen) -> Self {
        ChannelEndpointChanged {
            channel_id: u32::arbitrary(g),
        }
    }
}

#[cfg(feature = "prop_test")]
impl SetupConnection<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let protocol = setup_connection::Protocol::MiningProtocol;

        let mut endpoint_host_gen = Gen::new(255);
        let mut endpoint_host: vec::Vec<u8> = vec::Vec::new();
        endpoint_host.resize_with(255, || u8::arbitrary(&mut endpoint_host_gen));
        let endpoint_host: binary_sv2::Str0255 = endpoint_host.try_into().unwrap();

        let mut vendor_gen = Gen::new(255);
        let mut vendor: vec::Vec<u8> = vec::Vec::new();
        vendor.resize_with(255, || u8::arbitrary(&mut vendor_gen));
        let vendor: binary_sv2::Str0255 = vendor.try_into().unwrap();

        let mut hardware_version_gen = Gen::new(255);
        let mut hardware_version: vec::Vec<u8> = vec::Vec::new();
        hardware_version.resize_with(255, || u8::arbitrary(&mut hardware_version_gen));
        let hardware_version: binary_sv2::Str0255 = hardware_version.try_into().unwrap();

        let mut firmware_gen = Gen::new(255);
        let mut firmware: vec::Vec<u8> = vec::Vec::new();
        firmware.resize_with(255, || u8::arbitrary(&mut firmware_gen));
        let firmware: binary_sv2::Str0255 = firmware.try_into().unwrap();

        let mut device_id_gen = Gen::new(255);
        let mut device_id: vec::Vec<u8> = vec::Vec::new();
        device_id.resize_with(255, || u8::arbitrary(&mut device_id_gen));
        let device_id: binary_sv2::Str0255 = device_id.try_into().unwrap();

        SetupConnection {
            protocol,
            min_version: u16::arbitrary(g),
            max_version: u16::arbitrary(g),
            flags: u32::arbitrary(g),
            endpoint_host,
            endpoint_port: u16::arbitrary(g),
            vendor,
            hardware_version,
            firmware,
            device_id,
        }
    }
}

#[cfg(feature = "prop_test")]
impl SetupConnectionError<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let mut error_code_gen = Gen::new(255);
        let mut error_code: vec::Vec<u8> = vec::Vec::new();
        error_code.resize_with(255, || u8::arbitrary(&mut error_code_gen));
        let error_code: binary_sv2::Str0255 = error_code.try_into().unwrap();

        SetupConnectionError {
            flags: u32::arbitrary(g),
            error_code,
        }
    }
}

#[cfg(feature = "prop_test")]
impl SetupConnectionSuccess {
    pub fn from_gen(g: &mut Gen) -> Self {
        SetupConnectionSuccess {
            used_version: u16::arbitrary(g),
            flags: u32::arbitrary(g),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/reconnect.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255};
use core::convert::TryInto;

/// Message used by upstream to redirect downstream connection(s) to a new host.
///
/// Upon receiving the message, the downstream re-initiates the Noise Handshake process and uses
/// the pools authority public key to verify the certificate presented by the new server.
///
/// For security reasons, it is not possible to reconnect to an upstream with a certificate signed
/// by a different pool authority key. The message intentionally does not contain a pool public key
/// and thus cannot be used to reconnect to a different pool. This ensures that an attacker will
/// not be able to redirect hashrate to an arbitrary server in case the pool server get compromised
/// and instructed to send reconnects to a new location.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct Reconnect<'decoder> {
    /// When empty, downstream node should attempt to reconnect to current pool host.
    pub new_host: Str0255<'decoder>,
    /// When 0, downstream node should attempt to reconnect to current pool host.
    pub new_port: u16,
}

impl fmt::Display for Reconnect<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "Reconnect(new_host: {}, new_port: {})",
            self.new_host.as_utf8_or_hex(),
            self.new_port
        )
    }
}

impl PartialEq for Reconnect<'_> {
    fn eq(&self, other: &Self) -> bool {
        self.new_host.as_ref() == other.new_host.as_ref() && self.new_port == other.new_port
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/common-messages/src/setup_connection.rs">
use crate::{
    SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT, SV2_MINING_PROTOCOL_DISCRIMINANT,
    SV2_TEMPLATE_DISTRIBUTION_PROTOCOL_DISCRIMINANT,
};
use alloc::{fmt, vec::Vec};
use binary_sv2::{
    binary_codec_sv2,
    binary_codec_sv2::CVec,
    decodable::{DecodableField, FieldMarker},
    free_vec, Deserialize, Error, GetSize, Serialize, Str0255,
};
use core::convert::{TryFrom, TryInto};

/// Used by downstream to initiate a Stratum V2 connection with an upstream role.
///
/// This is usually the first message sent by a downstream role on a newly opened connection,
/// after completing the handshake process.
///
/// Downstreams that do not wish to provide telemetry data to the upstream role **should** set
/// [`SetupConnection::device_id`] to an empty string. However, they **must** set
/// [`SetupConnection::vendor`] to a string describing the manufacturer/developer and firmware
/// version and **should** set [`SetupConnection::hardware_version`] to a string describing, at
/// least, the particular hardware/software package in use.
///
/// A valid response to this message from the upstream role can either be [`SetupConnectionSuccess`]
/// or [`SetupConnectionError`] message.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct SetupConnection<'decoder> {
    /// Protocol to be used for the connection.
    pub protocol: Protocol,
    /// The minimum protocol version supported.
    ///
    /// Currently must be set to 2.
    pub min_version: u16,
    /// The maximum protocol version supported.
    ///
    /// Currently must be set to 2.
    pub max_version: u16,
    /// Flags indicating optional protocol features supported by the downstream.
    ///
    /// Each [`SetupConnection::protocol`] value has it's own flags.
    pub flags: u32,
    /// ASCII representation of the connection hostname or IP address.
    pub endpoint_host: Str0255<'decoder>,
    /// Connection port value.
    pub endpoint_port: u16,
    /// Device vendor name.
    pub vendor: Str0255<'decoder>,
    /// Device hardware version.
    pub hardware_version: Str0255<'decoder>,
    /// Device firmware version.
    pub firmware: Str0255<'decoder>,
    /// Device identifier.
    pub device_id: Str0255<'decoder>,
}

impl fmt::Display for SetupConnection<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetupConnection(protocol: {}, min_version: {}, max_version: {}, flags: {}, endpoint_host: {}, endpoint_port: {}, vendor: {}, hardware_version: {}, firmware: {}, device_id: {})",
            self.protocol as u8,
            self.min_version,
            self.max_version,
            self.flags,
            self.endpoint_host.as_utf8_or_hex(),
            self.endpoint_port,
            self.vendor.as_utf8_or_hex(),
            self.hardware_version.as_utf8_or_hex(),
            self.firmware.as_utf8_or_hex(),
            self.device_id.as_utf8_or_hex()
        )
    }
}

impl SetupConnection<'_> {
    /// Set the flag to indicate that the downstream requires a standard job
    pub fn set_requires_standard_job(&mut self) {
        self.flags |= 0b_0000_0000_0000_0000_0000_0000_0000_0001;
    }

    /// Set a flag to indicate that the JDC allows [`Full Template`] mode.
    ///
    /// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
    pub fn allow_full_template_mode(&mut self) {
        self.flags |= 0b_0000_0000_0000_0000_0000_0000_0000_0001;
    }

    /// Check if passed flags support self flag
    pub fn check_flags(protocol: Protocol, available_flags: u32, required_flags: u32) -> bool {
        match protocol {
            // [0] [0] -> true
            // [0] [1] -> false
            // [1] [1] -> true
            // [0] [1] -> false
            Protocol::MiningProtocol => {
                // Evaluates protocol requirements based on flag bits.
                //
                // Checks if the current protocol meets the required flags for work selection and
                // version rolling by reversing the bits of `available_flags` and
                // `required_flags`. It extracts the 30th and 29th bits to determine
                // if work selection and version rolling are needed.
                //
                // Returns `true` if:
                // - The work selection requirement is satisfied or not needed.
                // - The version rolling requirement is satisfied or not needed.
                //
                // Otherwise, returns `false`.
                let available = available_flags.reverse_bits();
                let required_flags = required_flags.reverse_bits();
                let requires_work_selection_passed = required_flags >> 30 > 0;
                let requires_version_rolling_passed = required_flags >> 29 > 0;

                let requires_work_selection_self = available >> 30 > 0;
                let requires_version_rolling_self = available >> 29 > 0;

                let work_selection =
                    !requires_work_selection_self || requires_work_selection_passed;
                let version_rolling =
                    !requires_version_rolling_self || requires_version_rolling_passed;

                work_selection && version_rolling
            }
            Protocol::JobDeclarationProtocol => {
                // Determines if asynchronous job mining is required based on flag bits.
                //
                // Reverses the bits of `available_flags` and `required_flags`, extracts the 31st
                // bit from each, and evaluates if the condition is met using these
                // bits. Returns `true` or `false` based on:
                // - False if `full_template_mode_required_self` is true and
                //   `allow_full_teamplate_mode_passed` is false.
                // - True otherwise.
                //
                // TODO: Simplify this code by removing the `reverse_bits` function and
                // using bitwise operations directly. i.e., try to use `&` and `|` to
                // combine the bits instead of reversing them.
                let available = available_flags.reverse_bits();
                let required = required_flags.reverse_bits();

                let allow_full_teamplate_mode_passed = (required >> 31) & 1 > 0;
                let full_template_mode_required_self = (available >> 31) & 1 > 0;

                match (
                    full_template_mode_required_self,
                    allow_full_teamplate_mode_passed,
                ) {
                    (true, true) => true,
                    (true, false) => false,
                    (false, true) => true,
                    (false, false) => true,
                }
            }
            Protocol::TemplateDistributionProtocol => {
                // These protocols do not define flags for setting up a connection.
                false
            }
        }
    }

    /// Check whether received versions are supported.
    ///
    /// If the versions are not supported, return `None` otherwise return the biggest version
    /// available
    pub fn get_version(&self, min_version: u16, max_version: u16) -> Option<u16> {
        if self.min_version > max_version || min_version > self.max_version {
            None
        } else {
            Some(self.max_version.min(max_version))
        }
    }

    /// Checks whether passed flags indicate that the downstream requires standard job.
    pub fn requires_standard_job(&self) -> bool {
        has_requires_std_job(self.flags)
    }
}

/// Helper function to check if `REQUIRES_STANDARD_JOBS` bit flag present.
pub fn has_requires_std_job(flags: u32) -> bool {
    let flags = flags.reverse_bits();
    let flag = flags >> 31;
    flag != 0
}

/// Helper function to check if `REQUIRES_VERSION_ROLLING` bit flag present.
pub fn has_version_rolling(flags: u32) -> bool {
    let flags = flags.reverse_bits();
    let flags = flags << 1;
    let flag = flags >> 31;
    flag != 0
}

/// Helper function to check if `REQUIRES_WORK_SELECTION` bit flag present.
pub fn has_work_selection(flags: u32) -> bool {
    let flags = flags.reverse_bits();
    let flags = flags << 2;
    let flag = flags >> 31;
    flag != 0
}

/// C representation of [`SetupConnection`]
#[repr(C)]
#[derive(Debug, Clone)]
pub struct CSetupConnection {
    /// Protocol to be used for the connection.
    pub protocol: Protocol,
    /// The minimum protocol version supported.
    ///
    /// Currently must be set to 2.
    pub min_version: u16,
    /// The maximum protocol version supported.
    ///
    /// Currently must be set to 2.
    pub max_version: u16,
    /// Flags indicating optional protocol features supported by the downstream.
    ///
    /// Each [`SetupConnection::protocol`] value has it's own flags.
    pub flags: u32,
    /// ASCII representation of the connection hostname or IP address.
    pub endpoint_host: CVec,
    /// Connection port value.
    pub endpoint_port: u16,
    /// Device vendor name.
    pub vendor: CVec,
    /// Device hardware version.
    pub hardware_version: CVec,
    /// Device firmware version.
    pub firmware: CVec,
    /// Device identifier.
    pub device_id: CVec,
}

impl<'a> CSetupConnection {
    #[allow(clippy::wrong_self_convention)]
    /// Convert C representation to Rust representation
    pub fn to_rust_rep_mut(&'a mut self) -> Result<SetupConnection<'a>, Error> {
        let endpoint_host: Str0255 = self.endpoint_host.as_mut_slice().try_into()?;
        let vendor: Str0255 = self.vendor.as_mut_slice().try_into()?;
        let hardware_version: Str0255 = self.hardware_version.as_mut_slice().try_into()?;
        let firmware: Str0255 = self.firmware.as_mut_slice().try_into()?;
        let device_id: Str0255 = self.device_id.as_mut_slice().try_into()?;

        Ok(SetupConnection {
            protocol: self.protocol,
            min_version: self.min_version,
            max_version: self.max_version,
            flags: self.flags,
            endpoint_host,
            endpoint_port: self.endpoint_port,
            vendor,
            hardware_version,
            firmware,
            device_id,
        })
    }
}

#[no_mangle]
pub extern "C" fn free_setup_connection(s: CSetupConnection) {
    drop(s)
}

impl Drop for CSetupConnection {
    fn drop(&mut self) {
        free_vec(&mut self.endpoint_host);
        free_vec(&mut self.vendor);
        free_vec(&mut self.hardware_version);
        free_vec(&mut self.firmware);
        free_vec(&mut self.device_id);
    }
}

impl From<SetupConnection<'_>> for CSetupConnection {
    fn from(v: SetupConnection) -> Self {
        Self {
            protocol: v.protocol,
            min_version: v.min_version,
            max_version: v.max_version,
            flags: v.flags,
            endpoint_host: v.endpoint_host.into(),
            endpoint_port: v.endpoint_port,
            vendor: v.vendor.into(),
            hardware_version: v.hardware_version.into(),
            firmware: v.firmware.into(),
            device_id: v.device_id.into(),
        }
    }
}

/// Message used by an upstream role to accept a connection setup request from a downstream role.
///
/// This message is sent in response to a [`SetupConnection`] message.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Copy)]
#[repr(C)]
pub struct SetupConnectionSuccess {
    /// Selected version based on the [`SetupConnection::min_version`] and
    /// [`SetupConnection::max_version`] sent by the downstream role.
    ///
    /// This version will be used on the connection for the rest of its life.
    pub used_version: u16,
    /// Flags indicating optional protocol features supported by the upstream.
    ///
    /// The downstream is required to verify this set of flags and act accordingly.
    ///
    /// Each [`SetupConnection::protocol`] field has its own values/flags.
    pub flags: u32,
}

impl fmt::Display for SetupConnectionSuccess {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetupConnectionSuccess(used_version: {}, flags: {})",
            self.used_version, self.flags
        )
    }
}

/// Message used by an upstream role to reject a connection setup request from a downstream role.
///
/// This message is sent in response to a [`SetupConnection`] message.
///
/// The connection setup process could fail because of protocol version negotiation. In order to
/// allow a downstream to determine the set of available features for a given upstream (e.g. for
/// proxies which dynamically switch between different pools and need to be aware of supported
/// options), downstream should send a [`SetupConnection`] message with all flags set and examine
/// the (potentially) resulting [`SetupConnectionError`] messages flags field.
///
/// The upstream must provide the full set of flags which it does not support in each
/// [`SetupConnectionError`] message and must consistently support the same set of flags across all
/// servers on the same hostname and port number.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct SetupConnectionError<'decoder> {
    /// Unsupported feature flags.
    ///
    /// In case `error_code` is `unsupported-feature-flags`, this field is used to indicate which
    /// flag is causing an error, otherwise it will be set to 0.
    pub flags: u32,
    /// Reason for setup connection error.
    ///
    /// Possible error codes:
    /// - unsupported-feature-flags
    /// - unsupported-protocol
    /// - protocol-version-mismatch
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for SetupConnectionError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetupConnectionError(flags: {}, error_code: {})",
            self.flags,
            self.error_code.as_utf8_or_hex()
        )
    }
}

#[repr(C)]
#[derive(Debug, Clone)]
/// C representation of [`SetupConnectionError`]
pub struct CSetupConnectionError {
    flags: u32,
    error_code: CVec,
}

impl<'a> CSetupConnectionError {
    #[allow(clippy::wrong_self_convention)]
    /// Convert C representation to Rust representation
    pub fn to_rust_rep_mut(&'a mut self) -> Result<SetupConnectionError<'a>, Error> {
        let error_code: Str0255 = self.error_code.as_mut_slice().try_into()?;

        Ok(SetupConnectionError {
            flags: self.flags,
            error_code,
        })
    }
}

#[no_mangle]
pub extern "C" fn free_setup_connection_error(s: CSetupConnectionError) {
    drop(s)
}

impl Drop for CSetupConnectionError {
    fn drop(&mut self) {
        free_vec(&mut self.error_code);
    }
}

impl<'a> From<SetupConnectionError<'a>> for CSetupConnectionError {
    fn from(v: SetupConnectionError<'a>) -> Self {
        Self {
            flags: v.flags,
            error_code: v.error_code.into(),
        }
    }
}

/// This enum has a list of the different Stratum V2 subprotocols.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
#[repr(u8)]
#[allow(clippy::enum_variant_names)]
pub enum Protocol {
    /// Mining protocol.
    MiningProtocol = SV2_MINING_PROTOCOL_DISCRIMINANT,
    /// Job declaration protocol.
    JobDeclarationProtocol = SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT,
    /// Template distribution protocol.
    TemplateDistributionProtocol = SV2_TEMPLATE_DISTRIBUTION_PROTOCOL_DISCRIMINANT,
}

impl From<Protocol> for binary_sv2::encodable::EncodableField<'_> {
    fn from(v: Protocol) -> Self {
        let val = v as u8;
        val.into()
    }
}

impl<'decoder> binary_sv2::Decodable<'decoder> for Protocol {
    fn get_structure(
        _: &[u8],
    ) -> core::result::Result<alloc::vec::Vec<FieldMarker>, binary_sv2::Error> {
        let field: FieldMarker = (0_u8).into();
        Ok(alloc::vec![field])
    }
    fn from_decoded_fields(
        mut v: alloc::vec::Vec<DecodableField<'decoder>>,
    ) -> core::result::Result<Self, binary_sv2::Error> {
        let val = v.pop().ok_or(binary_sv2::Error::NoDecodableFieldPassed)?;
        let val: u8 = val.try_into()?;
        val.try_into()
            .map_err(|_| binary_sv2::Error::ValueIsNotAValidProtocol(val))
    }
}

impl TryFrom<u8> for Protocol {
    type Error = ();

    fn try_from(value: u8) -> Result<Self, Self::Error> {
        match value {
            SV2_MINING_PROTOCOL_DISCRIMINANT => Ok(Protocol::MiningProtocol),
            SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT => Ok(Protocol::JobDeclarationProtocol),
            SV2_TEMPLATE_DISTRIBUTION_PROTOCOL_DISCRIMINANT => {
                Ok(Protocol::TemplateDistributionProtocol)
            }
            _ => Err(()),
        }
    }
}

impl GetSize for Protocol {
    fn get_size(&self) -> usize {
        1
    }
}

#[cfg(test)]
mod test {
    use super::*;
    use crate::alloc::string::ToString;
    use core::convert::TryInto;

    #[test]
    fn test_check_flag() {
        let protocol = crate::Protocol::MiningProtocol;
        let flag_available = 0b_0000_0000_0000_0000_0000_0000_0000_0000;
        let flag_required = 0b_0000_0000_0000_0000_0000_0000_0000_0001;
        assert!(SetupConnection::check_flags(
            protocol,
            flag_available,
            flag_required
        ));

        let protocol = crate::Protocol::JobDeclarationProtocol;

        let available_flags = 0b_1000_0000_0000_0000_0000_0000_0000_0000;
        let required_flags = 0b_1000_0000_0000_0000_0000_0000_0000_0000;
        assert!(SetupConnection::check_flags(
            protocol,
            available_flags,
            required_flags
        ));
    }

    #[test]
    fn test_has_requires_std_job() {
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0001;
        assert!(has_requires_std_job(flags));
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0010;
        assert!(!has_requires_std_job(flags));
    }

    #[test]
    fn test_has_version_rolling() {
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0010;
        assert!(has_version_rolling(flags));
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0001;
        assert!(!has_version_rolling(flags));
    }

    #[test]
    fn test_has_work_selection() {
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0100;
        assert!(has_work_selection(flags));
        let flags = 0b_0000_0000_0000_0000_0000_0000_0000_0001;
        assert!(!has_work_selection(flags));
    }

    fn create_setup_connection() -> SetupConnection<'static> {
        SetupConnection {
            protocol: Protocol::MiningProtocol,
            min_version: 1,
            max_version: 4,
            flags: 0,
            endpoint_host: "0.0.0.0".to_string().into_bytes().try_into().unwrap(),
            endpoint_port: 0,
            vendor: "vendor".to_string().into_bytes().try_into().unwrap(),
            hardware_version: "hw_version".to_string().into_bytes().try_into().unwrap(),
            firmware: "firmware".to_string().into_bytes().try_into().unwrap(),
            device_id: "device_id".to_string().into_bytes().try_into().unwrap(),
        }
    }

    #[test]
    fn test_get_version() {
        let setup_conn = create_setup_connection();
        assert_eq!(setup_conn.get_version(1, 5).unwrap(), 4);
        assert_eq!(setup_conn.get_version(6, 6), None);
    }

    // Test SetupConnection::set_requires_std_job
    #[test]
    fn test_set_requires_std_job() {
        let mut setup_conn = create_setup_connection();
        assert!(!setup_conn.requires_standard_job());
        setup_conn.set_requires_standard_job();
        assert!(setup_conn.requires_standard_job());
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/Cargo.toml">
[package]
name = "job_declaration_sv2"
version = "4.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "SV2 job declaration protocol types"
documentation = "https://docs.rs/job_declaration_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


[dependencies]
binary_sv2 = { path = "../../binary-sv2", version = "^3.0.0" }
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/README.md">
# job_declaration_sv2

[![crates.io](https://img.shields.io/crates/v/job_declaration_sv2.svg)](https://crates.io/crates/job_declaration_sv2)
[![docs.rs](https://docs.rs/job_declaration_sv2/badge.svg)](https://docs.rs/job_declaration_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg)](https://app.codecov.io/gh/stratum-mining/stratum/tree/main/protocols%2Fv2%2Fjob_declaration_sv2)

`job_declaration_sv2` is a Rust `#![no-std]` crate that contains the messages defined in the Job Declaration Protocol of Stratum V2.
This protocol runs between the Job Declarator Server(JDS) and Job Declarator Client(JDC). and can be
provided as a trusted 3rd party service for mining farms.

For further information about the messages, please refer to [Stratum V2 documentation - Job Distribution](https://stratumprotocol.org/specification/06-Job-Declaration-Protocol/).

## Usage

To include this crate in your project, run:

```bash
$ cargo add job_declaration_sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/allocate_mining_job_token.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255, B0255, B064K};
use core::convert::TryInto;

/// Message used by JDC to request an identifier for a future mining job from JDS.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct AllocateMiningJobToken<'decoder> {
    /// Unconstrained sequence of bytes. Whatever is needed by the JDS to
    /// identify/authenticate the client. Additional restrictions can be imposed by the
    /// JDS. It is highly recommended that UTF-8 encoding is used.
    pub user_identifier: Str0255<'decoder>,
    /// A unique identifier for pairing the response/request.
    pub request_id: u32,
}

impl fmt::Display for AllocateMiningJobToken<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "AllocateMiningJobToken(user_identifier: {}, request_id: {})",
            self.user_identifier.as_utf8_or_hex(),
            self.request_id
        )
    }
}

/// Message used by JDS to accept [`AllocateMiningJobToken`] message.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct AllocateMiningJobTokenSuccess<'decoder> {
    /// A unique identifier for pairing the response/request.
    ///
    /// This **must** be the same as the received [`AllocateMiningJobToken::request_id`].
    pub request_id: u32,
    /// A token that makes the JDC eligible for committing a mining job for approval/transactions
    /// declaration or for identifying custom mining job on mining connection.
    pub mining_job_token: B0255<'decoder>,
    /// Bitcoin transaction outputs added by JDS.
    pub coinbase_outputs: B064K<'decoder>,
}

impl fmt::Display for AllocateMiningJobTokenSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "AllocateMiningJobTokenSuccess(request_id: {}, mining_job_token: {}, coinbase_outputs: {})",
            self.request_id,
            self.mining_job_token,
            self.coinbase_outputs
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/declare_mining_job.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Seq064K, Serialize, Str0255, B0255, B064K, U256};
use core::convert::TryInto;

/// Message used by JDC to proposes a selected set of transactions to JDS they wish to
/// mine on.
///
/// Used only under [`Full Template`] mode.
///
/// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct DeclareMiningJob<'decoder> {
    /// A unique identifier for this request.
    ///
    /// Used for pairing request/response.
    pub request_id: u32,
    /// Token received previously through [`crate::AllocateMiningJobTokenSuccess`] message.
    pub mining_job_token: B0255<'decoder>,
    /// Header version field.
    pub version: u32,
    /// The coinbase transaction nVersion field
    pub coinbase_prefix: B064K<'decoder>,
    /// Up to 8 bytes (not including the length byte) which are to be placed at the beginning of
    /// the coinbase field in the coinbase transaction.
    pub coinbase_suffix: B064K<'decoder>,
    /// List of the transaction ids contained in the template. JDS checks the list against its
    /// mempool and requests missing txs via [`crate::ProvideMissingTransactions`].
    ///
    /// This list Does not include the coinbase transaction (as there is no corresponding full data
    /// for it yet).
    pub tx_ids_list: Seq064K<'decoder, U256<'decoder>>,
    /// Extra data which the JDS may require to validate the work.
    pub excess_data: B064K<'decoder>,
}

impl fmt::Display for DeclareMiningJob<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "DeclareMiningJob(request_id: {}, mining_job_token: {}, version: {}, coinbase_prefix: {}, coinbase_suffix: {}, tx_ids_list: {}, excess_data: {})",
            self.request_id,
            self.mining_job_token,
            self.version,
            self.coinbase_prefix,
            self.coinbase_suffix,
            self.tx_ids_list,
            self.excess_data
        )
    }
}

/// Messaged used by JDS to accept [`DeclareMiningJob`] message.
///
/// If [`Full Template`] mode is used, JDS MAY request txdata via `ProvideMissingTransactions`
/// before making this commitment.
///
/// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct DeclareMiningJobSuccess<'decoder> {
    /// A unique identifier for this request.
    ///
    /// Must be the same as the received [`DeclareMiningJob::request_id`].
    pub request_id: u32,
    /// This **may** be the same token as [DeclareMiningJob::mining_job_token] if the pool allows
    /// to start mining on a non declared job. If the token is different (irrespective of if the
    /// downstream is already mining using it), the downstream **must** send a `SetCustomMiningJob`
    /// message on each connection which wishes to mine using the declared job.
    pub new_mining_job_token: B0255<'decoder>,
}

impl fmt::Display for DeclareMiningJobSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "DeclareMiningJobSuccess(request_id: {}, new_mining_job_token: {})",
            self.request_id, self.new_mining_job_token
        )
    }
}

/// Messaged used by JDS to reject [`DeclareMiningJob`] message.
///
/// Downstream should consider this as a trigger to fallback into some other Pool/JDS or solo
/// mining.
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct DeclareMiningJobError<'decoder> {
    /// The unique identifier of the request.
    ///
    /// Must be the same as the received [`DeclareMiningJob::request_id`].
    pub request_id: u32,
    /// Possible values:
    ///
    /// - invalid-mining-job-token
    /// - invalid-job-param-value-{DeclareMiningJob::field}
    pub error_code: Str0255<'decoder>,
    /// Optional details about the error.
    pub error_details: B064K<'decoder>,
}

impl fmt::Display for DeclareMiningJobError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "DeclareMiningJobError(request_id: {}, error_code: {}, error_details: {})",
            self.request_id,
            self.error_code.as_utf8_or_hex(),
            self.error_details
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/lib.rs">
//! # Job Declaration Protocol
//!
//! `job_declaration_sv2` is a Rust crate that implements a set of messages defined in the Job
//! Declaration Protocol of Stratum V2.  This protocol runs between the Job Declarator Server
//! (JDS) and Job Declarator Client (JDC).
//!
//! ## Build Options
//! This crate can be built with the following features:
//! - `std`: Enables support for standard library features.
//!
//! For further information about the messages, please refer to [Stratum V2 documentation - Job
//! Declaration](https://stratumprotocol.org/specification/06-Job-Declaration-Protocol/).

#![no_std]

extern crate alloc;
mod allocate_mining_job_token;
mod declare_mining_job;
mod provide_missing_transactions;
mod push_solution;

pub use allocate_mining_job_token::{AllocateMiningJobToken, AllocateMiningJobTokenSuccess};
pub use declare_mining_job::{DeclareMiningJob, DeclareMiningJobError, DeclareMiningJobSuccess};
pub use provide_missing_transactions::{
    ProvideMissingTransactions, ProvideMissingTransactionsSuccess,
};
pub use push_solution::PushSolution;

// Job Declaration Protocol message types.
pub const MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN: u8 = 0x50;
pub const MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS: u8 = 0x51;
pub const MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS: u8 = 0x55;
pub const MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS: u8 = 0x56;
pub const MESSAGE_TYPE_DECLARE_MINING_JOB: u8 = 0x57;
pub const MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS: u8 = 0x58;
pub const MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR: u8 = 0x59;
pub const MESSAGE_TYPE_PUSH_SOLUTION: u8 = 0x60;

// In the Job Declaration protocol, the `channel_msg` bit is always unset,
// except for `SUBMIT_SOLUTION_JD`, which requires a specific channel reference.
pub const CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN: bool = false;
pub const CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN_SUCCESS: bool = false;
pub const CHANNEL_BIT_DECLARE_MINING_JOB: bool = false;
pub const CHANNEL_BIT_DECLARE_MINING_JOB_SUCCESS: bool = false;
pub const CHANNEL_BIT_DECLARE_MINING_JOB_ERROR: bool = false;
pub const CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS: bool = false;
pub const CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS_SUCCESS: bool = false;
pub const CHANNEL_BIT_SUBMIT_SOLUTION_JD: bool = true;
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/provide_missing_transactions.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Seq064K, Serialize, B016M};
use core::convert::TryInto;

/// Message used by the JDS to ask for transactions that it did not recognize from
/// [`crate::DeclareMiningJob`] message.
///
/// In order to do block propagation, JDS must know all the transactions within the current block
/// template. These transactions are provided by the JDC to JDS as a sequence of transaction ids in
/// the [`crate::DeclareMiningJob`] message. If JDS is unable to recognize any of the transactions
/// through its mempool, it sends this message to ask for them. They are specified by their
/// position in the original [`crate::DeclareMiningJob`] message, 0-indexed not including the
/// coinbase transaction.
///
/// Used only under [`Full Template`] mode.
///
/// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct ProvideMissingTransactions<'decoder> {
    /// Unique Identifier.
    ///
    /// Must be the same as the received [`crate::DeclareMiningJob::request_id`].
    pub request_id: u32,
    /// A list of unrecognized transactions that need to be supplied by the JDC in full. They are
    /// specified by their position in the original [`crate::DeclareMiningJob`] message, 0-indexed
    /// not including the coinbase transaction transaction.
    pub unknown_tx_position_list: Seq064K<'decoder, u16>,
}

impl fmt::Display for ProvideMissingTransactions<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "ProvideMissingTransactions(request_id: {}, unknown_tx_position_list: {})",
            self.request_id, self.unknown_tx_position_list
        )
    }
}

/// Message used by JDC to accept [`ProvideMissingTransactions`] message and provide the full
/// list of transactions in the order they were requested by [`ProvideMissingTransactions`].
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct ProvideMissingTransactionsSuccess<'decoder> {
    /// Unique Identifier.
    ///
    /// Must be the same as the received [`ProvideMissingTransactions::request_id`].
    pub request_id: u32,
    /// List of full transactions as requested by [`ProvideMissingTransactions`].
    pub transaction_list: Seq064K<'decoder, B016M<'decoder>>,
}
impl fmt::Display for ProvideMissingTransactionsSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "ProvideMissingTransactionsSuccess(request_id: {}, transaction_list: {})",
            self.request_id, self.transaction_list
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/job-declaration/src/push_solution.rs">
use alloc::vec::Vec;
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, B032, U256};
use core::{convert::TryInto, fmt};

/// Message used by JDC to push a solution to JDS as soon as it finds a new valid block.
///
/// Upon receiving this message, JDS should propagate the new block as soon as possible.
///
/// Note that JDC is also expected to share the new block data through `SubmitSolution` message
/// under the Template Distribution Protocol.
///
/// Used only under [`Full Template`] mode.
///
/// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct PushSolution<'decoder> {
    /// Full extranonce that forms a valid submission.
    pub extranonce: B032<'decoder>,
    /// Previous block hash.
    pub prev_hash: U256<'decoder>,
    /// Contains the time the block was constructed as a Unix timestamp.
    pub ntime: u32,
    /// Nonce of the block.
    pub nonce: u32,
    /// The bits field is compact representation of the target at the time the block was mined.
    pub nbits: u32,
    /// The version field in a Bitcoin header initially indicated protocol rule changes. [`BIP9`]
    /// altered its use by turning it into a bit vector for coordinated soft fork signaling.
    /// [`BIP320`] further refined its purpose by dedicating 16 bits(starting from 13 to 28) of the
    /// version field for general-purpose miner use, ensuring that such usage doesn't interfere
    /// with the soft fork signaling mechanism defined by [`BIP9`].
    ///
    /// [`BIP9`]: https://en.bitcoin.it/wiki/BIP_0009
    /// [`BIP320`]: https://en.bitcoin.it/wiki/BIP_0320
    pub version: u32,
}

impl fmt::Display for PushSolution<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "PushSolution(extranonce: {}, prev_hash: {}, ntime: {}, nonce: {}, nbits: {}, version: {})",
            self.extranonce,
            self.prev_hash,
            self.ntime,
            self.nonce,
            self.nbits,
            self.version
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/Cargo.toml">
[package]
name = "mining_sv2"
version = "4.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "SV2 mining protocol types"
documentation = "https://docs.rs/mining_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../binary-sv2", version = "^3.0.0" }

[dev-dependencies]
quickcheck = "1.0.3"
quickcheck_macros = "1"
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/README.md">
# mining_sv2

[![crates.io](https://img.shields.io/crates/v/mining_sv2.svg)](https://crates.io/crates/mining_sv2)
[![docs.rs](https://docs.rs/mining_sv2/badge.svg)](https://docs.rs/mining_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg)](https://app.codecov.io/gh/stratum-mining/stratum/tree/main/protocols%2Fv2%2Fmining_sv2)

`mining_sv2` is a Rust `#![no_std]` crate that implements a set of  messages defined in the Mining protocol of Stratum V2.
The Mining protocol enables:
- distribution of work to mining devices
- submission of proof of work from mining devices
- notification of custom work to pool (in conjunction with Job Declaration Subprotocol) 

For further information about the messages, please refer to [Stratum V2 documentation - Mining](https://stratumprotocol.org/specification/05-Mining-Protocol/).

## Usage

To include this crate in your project, run:

```bash
$ cargo add mining_sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/close_channel.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255};
use core::convert::TryInto;

/// Message used by a downstream to close a mining channel.
///
/// If you are sending this message through a proxy on behalf of multiple downstreams, you must send
/// it for each open channel separately.
///
/// Upon receiving this message, upstream **must** stop sending messages for the channel.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct CloseChannel<'decoder> {
    /// Channel id of the channel to be closed.
    pub channel_id: u32,
    /// Reason for closing the channel.
    pub reason_code: Str0255<'decoder>,
}

impl fmt::Display for CloseChannel<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "CloseChannel(channel_id: {}, reason_code: {})",
            self.channel_id,
            self.reason_code.as_utf8_or_hex()
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/lib.rs">
//! # Stratum V2 Mining Protocol Messages Crate
//!
//! `mining_sv2` is a Rust crate that implements a set of messages defined in the Mining protocol
//! of Stratum V2.
//!
//! The Mining protocol enables the distribution of work to mining devices and the submission of
//! proof-of-work results.
//!
//! For further information about the messages, please refer to [Stratum V2 documentation - Mining](https://stratumprotocol.org/specification/05-Mining-Protocol/).
//!
//! ## Build Options
//!
//! This crate can be built with the following features:
//!
//! ## Usage
//!
//! To include this crate in your project, run:
//! ```bash
//! $ cargo add mining_sv2
//! ```
//!
//! For further information about the mining protocol, please refer to [Stratum V2 documentation -
//! Mining Protocol](https://stratumprotocol.org/specification/05-Mining-Protocol/).

#![no_std]

use binary_sv2::{B032, U256};
use core::{
    cmp::{Ord, PartialOrd},
    convert::TryInto,
};

#[macro_use]
extern crate alloc;

mod close_channel;
mod new_mining_job;
mod open_channel;
mod set_custom_mining_job;
mod set_extranonce_prefix;
mod set_group_channel;
mod set_new_prev_hash;
mod set_target;
mod submit_shares;
mod update_channel;

pub use close_channel::CloseChannel;
use core::ops::Range;
pub use new_mining_job::{NewExtendedMiningJob, NewMiningJob};
pub use open_channel::{
    OpenExtendedMiningChannel, OpenExtendedMiningChannelSuccess, OpenMiningChannelError,
    OpenStandardMiningChannel, OpenStandardMiningChannelSuccess,
};
pub use set_custom_mining_job::{
    SetCustomMiningJob, SetCustomMiningJobError, SetCustomMiningJobSuccess,
};
pub use set_extranonce_prefix::SetExtranoncePrefix;
pub use set_group_channel::SetGroupChannel;
pub use set_new_prev_hash::SetNewPrevHash;
pub use set_target::SetTarget;
pub use submit_shares::{
    SubmitSharesError, SubmitSharesExtended, SubmitSharesStandard, SubmitSharesSuccess,
};
pub use update_channel::{UpdateChannel, UpdateChannelError};

// Mining Protocol message types.
pub const MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL: u8 = 0x10;
pub const MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS: u8 = 0x11;
pub const MESSAGE_TYPE_OPEN_MINING_CHANNEL_ERROR: u8 = 0x12;
pub const MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL: u8 = 0x13;
pub const MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS: u8 = 0x14;
pub const MESSAGE_TYPE_NEW_MINING_JOB: u8 = 0x15;
pub const MESSAGE_TYPE_UPDATE_CHANNEL: u8 = 0x16;
pub const MESSAGE_TYPE_UPDATE_CHANNEL_ERROR: u8 = 0x17;
pub const MESSAGE_TYPE_CLOSE_CHANNEL: u8 = 0x18;
pub const MESSAGE_TYPE_SET_EXTRANONCE_PREFIX: u8 = 0x19;
pub const MESSAGE_TYPE_SUBMIT_SHARES_STANDARD: u8 = 0x1a;
pub const MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED: u8 = 0x1b;
pub const MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS: u8 = 0x1c;
pub const MESSAGE_TYPE_SUBMIT_SHARES_ERROR: u8 = 0x1d;
pub const MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB: u8 = 0x1f;
pub const MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH: u8 = 0x20;
pub const MESSAGE_TYPE_SET_TARGET: u8 = 0x21;
pub const MESSAGE_TYPE_SET_CUSTOM_MINING_JOB: u8 = 0x22;
pub const MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS: u8 = 0x23;
pub const MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR: u8 = 0x24;
pub const MESSAGE_TYPE_SET_GROUP_CHANNEL: u8 = 0x25;

// Channel bits in the Mining protocol vary depending on the message.
pub const CHANNEL_BIT_CLOSE_CHANNEL: bool = true;
pub const CHANNEL_BIT_NEW_EXTENDED_MINING_JOB: bool = true;
pub const CHANNEL_BIT_NEW_MINING_JOB: bool = true;
pub const CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL: bool = false;
pub const CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS: bool = false;
pub const CHANNEL_BIT_OPEN_MINING_CHANNEL_ERROR: bool = false;
pub const CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL: bool = false;
pub const CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL_SUCCESS: bool = false;
pub const CHANNEL_BIT_RECONNECT: bool = false;
pub const CHANNEL_BIT_SET_CUSTOM_MINING_JOB: bool = false;
pub const CHANNEL_BIT_SET_CUSTOM_MINING_JOB_ERROR: bool = false;
pub const CHANNEL_BIT_SET_CUSTOM_MINING_JOB_SUCCESS: bool = false;
pub const CHANNEL_BIT_SET_EXTRANONCE_PREFIX: bool = true;
pub const CHANNEL_BIT_SET_GROUP_CHANNEL: bool = false;
pub const CHANNEL_BIT_MINING_SET_NEW_PREV_HASH: bool = true;
pub const CHANNEL_BIT_SET_TARGET: bool = true;
pub const CHANNEL_BIT_SUBMIT_SHARES_ERROR: bool = true;
pub const CHANNEL_BIT_SUBMIT_SHARES_EXTENDED: bool = true;
pub const CHANNEL_BIT_SUBMIT_SHARES_STANDARD: bool = true;
pub const CHANNEL_BIT_SUBMIT_SHARES_SUCCESS: bool = true;
pub const CHANNEL_BIT_UPDATE_CHANNEL: bool = true;
pub const CHANNEL_BIT_UPDATE_CHANNEL_ERROR: bool = true;

pub const MAX_EXTRANONCE_LEN: usize = 32;

/// Target is a 256-bit unsigned integer in little-endian
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Target {
    head: u128, // least significant bits
    tail: u128, // most significant bits
}

impl Target {
    pub fn new(head: u128, tail: u128) -> Self {
        Self { head, tail }
    }
}

impl From<[u8; 32]> for Target {
    fn from(v: [u8; 32]) -> Self {
        // below unwraps never panics
        let head = u128::from_le_bytes(v[0..16].try_into().unwrap());
        let tail = u128::from_le_bytes(v[16..32].try_into().unwrap());
        Self { head, tail }
    }
}

impl From<Extranonce> for alloc::vec::Vec<u8> {
    fn from(v: Extranonce) -> Self {
        v.extranonce
    }
}

impl<'a> From<U256<'a>> for Target {
    fn from(v: U256<'a>) -> Self {
        let inner = v.inner_as_ref();
        // below unwraps never panics
        let head = u128::from_le_bytes(inner[0..16].try_into().unwrap());
        let tail = u128::from_le_bytes(inner[16..32].try_into().unwrap());
        Self { head, tail }
    }
}

impl From<Target> for U256<'static> {
    fn from(v: Target) -> Self {
        let mut inner = v.head.to_le_bytes().to_vec();
        inner.extend_from_slice(&v.tail.to_le_bytes());
        // below unwraps never panics
        inner.try_into().unwrap()
    }
}

impl PartialOrd for Target {
    fn partial_cmp(&self, other: &Self) -> Option<core::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for Target {
    fn cmp(&self, other: &Self) -> core::cmp::Ordering {
        if self.tail == other.tail && self.head == other.head {
            core::cmp::Ordering::Equal
        } else if self.tail != other.tail {
            self.tail.cmp(&other.tail)
        } else {
            self.head.cmp(&other.head)
        }
    }
}

// WARNING: do not derive Copy on this type. Some operations performed to a copy of an extranonce
// do not affect the original, and this may lead to different extranonce inconsistency
/// Extranonce bytes which need to be added to the coinbase to form a fully valid submission.
///
/// Representation is in big endian, so tail is for the digits relative to smaller powers
///
/// `full coinbase = coinbase_tx_prefix + extranonce + coinbase_tx_suffix`.
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct Extranonce {
    extranonce: alloc::vec::Vec<u8>,
}

// this function converts a U256 type in little endian to Extranonce type
impl<'a> From<U256<'a>> for Extranonce {
    fn from(v: U256<'a>) -> Self {
        let extranonce: alloc::vec::Vec<u8> = v.inner_as_ref().into();
        Self { extranonce }
    }
}

// This function converts an Extranonce type to U256n little endian
impl From<Extranonce> for U256<'_> {
    fn from(v: Extranonce) -> Self {
        let inner = v.extranonce;
        debug_assert!(inner.len() <= 32);
        // below unwraps never panics
        inner.try_into().unwrap()
    }
}

// this function converts an extranonce to the type B032
impl<'a> From<B032<'a>> for Extranonce {
    fn from(v: B032<'a>) -> Self {
        let extranonce: alloc::vec::Vec<u8> = v.inner_as_ref().into();
        Self { extranonce }
    }
}

// this function converts an Extranonce type in B032 in little endian
impl From<Extranonce> for B032<'_> {
    fn from(v: Extranonce) -> Self {
        let inner = v.extranonce.to_vec();
        // below unwraps never panics
        inner.try_into().unwrap()
    }
}

impl Default for Extranonce {
    fn default() -> Self {
        Self {
            extranonce: vec![0; 32],
        }
    }
}

impl core::convert::TryFrom<alloc::vec::Vec<u8>> for Extranonce {
    type Error = ();

    fn try_from(v: alloc::vec::Vec<u8>) -> Result<Self, Self::Error> {
        if v.len() > MAX_EXTRANONCE_LEN {
            Err(())
        } else {
            Ok(Extranonce { extranonce: v })
        }
    }
}

impl Extranonce {
    pub fn new(len: usize) -> Option<Self> {
        if len > MAX_EXTRANONCE_LEN {
            None
        } else {
            let extranonce = vec![0; len];
            Some(Self { extranonce })
        }
    }

    /// this function converts a Extranonce type to b032 type
    pub fn from_vec_with_len(mut extranonce: alloc::vec::Vec<u8>, len: usize) -> Self {
        extranonce.resize(len, 0);
        Self { extranonce }
    }

    pub fn into_b032(self) -> B032<'static> {
        self.into()
    }
    // B032 type is more used, this is why the output signature is not ExtendedExtranoncee the B032
    // type is more used, this is why the output signature is not ExtendedExtranoncee
    #[allow(clippy::should_implement_trait)]
    pub fn next(&mut self) -> Option<B032> {
        increment_bytes_be(&mut self.extranonce).ok()?;
        // below unwraps never panics
        Some(self.extranonce.clone().try_into().unwrap())
    }

    pub fn to_vec(self) -> alloc::vec::Vec<u8> {
        self.extranonce
    }
}

impl From<&mut ExtendedExtranonce> for Extranonce {
    fn from(v: &mut ExtendedExtranonce) -> Self {
        let mut extranonce = v.inner.to_vec();
        extranonce.truncate(v.range_2.end);
        Self { extranonce }
    }
}

#[derive(Debug, Clone)]
/// Downstream and upstream are relative to user P. In simple terms, upstream is
/// the part of the protocol that a user P sees when looking above, and downstream is what they see
/// when looking below.
///
/// An `ExtendedExtranonce` is defined by 3 ranges:
///
/// - `range_0`: Represents the extended extranonce part reserved by upstream relative to P (for
///   most upstream nodes, e.g., a pool, this is `[0..0]`) and it is fixed for P.
///
/// - `range_1`: Represents the extended extranonce part reserved for P. P assigns to every relative
///   downstream a unique extranonce with different values in range_1 in the following way: if D_i
///   is the (i+1)-th downstream that connected to P, then D_i gets from P an extranonce with
///   range_1=i (note that the concatenation of range_0 and range_1 is the range_0 relative to D_i,
///   and range_2 of P is the range_1 of D_i).
///
/// - `range_2`: Represents the range that P reserves for the downstreams.
///
///
/// In the following scenarios, we examine the extended extranonce in some cases:
///
/// **Scenario 1: P is a pool**
/// - `range_0`  `0..0`: There is no upstream relative to the pool P, so no space is reserved by
///   the upstream.
/// - `range_1`  `0..16`: The pool P increments these bytes to ensure each downstream gets a unique
///   extended extranonce search space. The pool could optionally choose to set some fixed bytes as
///   `additional_coinbase_script_data` (smaller than 16 bytes), which are set on the beginning of
///   this range and will not be incremented. Usually, these bytes are used to add an identifier for
///   the pool.
/// - `range_2`  `16..32`: These bytes are not changed by the pool but are changed by the pool's
///   downstream.
///
/// **Scenario 2: P is a translator proxy**
/// - `range_0`  `0..16`: These bytes are set by the upstream and P shouldn't change them.
/// - `range_1`  `16..24`: These bytes are modified by P each time an Sv1 mining device connects,
///   ensuring each connected Sv1 mining device gets a different extended extranonce search space.
/// - `range_2`  `24..32`: These bytes are left free for the Sv1 mining device.
///
/// **Scenario 3: P is an Sv1 mining device**
/// - `range_0`  `0..24`: These bytes are set by the device's upstreams.
/// - `range_1`  `24..32`: These bytes are changed by P (if capable) to increment the search space.
/// - `range_2`  `32..32`: No more downstream.
///
/// # Examples
///
/// Basic usage without additional coinbase script data:
///
/// ```
/// use mining_sv2::*;
/// use core::convert::TryInto;
///
/// // Create an extended extranonce of len 32, reserving the first 7 bytes for the pool
/// let mut pool_extended_extranonce = ExtendedExtranonce::new(0..0, 0..7, 7..32, None).unwrap();
///
/// // On open extended channel (requesting to use a range of 4 bytes), the pool allocates this extranonce_prefix:
/// let new_extended_channel_extranonce_prefix = pool_extended_extranonce.next_prefix_extended(4).unwrap();
/// let expected_extranonce_prefix = vec![0, 0, 0, 0, 0, 0, 1];
/// assert_eq!(new_extended_channel_extranonce_prefix.clone().to_vec(), expected_extranonce_prefix);
///
/// // On open extended channel (requesting to use a range of 20 bytes), the pool allocates this extranonce_prefix:
/// let new_extended_channel_extranonce_prefix = pool_extended_extranonce.next_prefix_extended(20).unwrap();
/// let expected_extranonce_prefix = vec![0, 0, 0, 0, 0, 0, 2];
/// assert_eq!(new_extended_channel_extranonce_prefix.clone().to_vec(), expected_extranonce_prefix);
///
/// // On open extended channel (requesting to use a range of 26 bytes, which is too much), we get an error:
/// let new_extended_channel_extranonce_prefix_error = pool_extended_extranonce.next_prefix_extended(26);
/// assert!(new_extended_channel_extranonce_prefix_error.is_err());
///
/// // Then the pool receives a request to open a standard channel
/// let new_standard_channel_extranonce = pool_extended_extranonce.next_prefix_standard().unwrap();
/// // For standard channels, only the bytes in range_2 are incremented
/// let expected_standard_extranonce = vec![0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1];
/// assert_eq!(new_standard_channel_extranonce.to_vec(), expected_standard_extranonce);
///
/// // Now the proxy receives the ExtendedExtranonce previously created
/// // The proxy knows the extranonce space reserved to the pool is 7 bytes and that the total
/// // extranonce len is 32 bytes and decides to reserve 4 bytes for itself and leave the remaining 21 for
/// // further downstreams.
/// let range_0 = 0..7;
/// let range_1 = 7..11;
/// let range_2 = 11..32;
/// let mut proxy_extended_extranonce = ExtendedExtranonce::from_upstream_extranonce(
///     new_extended_channel_extranonce_prefix,
///     range_0,
///     range_1,
///     range_2
/// ).unwrap();
///
/// // The proxy generates an extended extranonce for downstream (allowing it to use a range of 3 bytes)
/// let new_extended_channel_extranonce_prefix = proxy_extended_extranonce.next_prefix_extended(3).unwrap();
/// let expected_extranonce_prefix = vec![0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1];
/// assert_eq!(new_extended_channel_extranonce_prefix.clone().to_vec(), expected_extranonce_prefix);
///
/// // When the proxy receives a share from downstream and wants to recreate the full extranonce
/// // e.g., because it wants to check the share's work
/// let received_extranonce: Extranonce = vec![0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0].try_into().unwrap();
/// let share_complete_extranonce = proxy_extended_extranonce.extranonce_from_downstream_extranonce(received_extranonce.clone()).unwrap();
/// let expected_complete_extranonce = vec![0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
/// assert_eq!(share_complete_extranonce.to_vec(), expected_complete_extranonce);
///
/// // Now the proxy wants to send the extranonce received from downstream and the part of extranonce
/// // owned by itself to the pool
/// let extranonce_to_send = proxy_extended_extranonce.without_upstream_part(Some(received_extranonce)).unwrap();
/// let expected_extranonce_to_send = vec![0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
/// assert_eq!(extranonce_to_send.to_vec(), expected_extranonce_to_send);
/// ```
///
/// Using additional coinbase script data:
///
/// ```
/// use mining_sv2::*;
/// use core::convert::TryInto;
///
/// // Create an extended extranonce with additional coinbase script data
/// let additional_data = vec![0x42, 0x43]; // Example additional coinbase script data
/// let mut pool_extended_extranonce = ExtendedExtranonce::new(
///     0..0,
///     0..7,
///     7..32,
///     Some(additional_data.clone())
/// ).unwrap();
///
/// // When using additional coinbase script data, only bytes after the data are incremented
/// let new_extended_channel_extranonce = pool_extended_extranonce.next_prefix_extended(3).unwrap();
/// let expected_extranonce = vec![0x42, 0x43, 0, 0, 0, 0, 1];
/// assert_eq!(new_extended_channel_extranonce.clone().to_vec(), expected_extranonce);
///
/// // For standard channels, only range_2 is incremented while range_1 (including additional data) is preserved
/// let new_standard_channel_extranonce = pool_extended_extranonce.next_prefix_standard().unwrap();
/// // Note that the additional data (0x42, 0x43) and the incremented bytes in range_1 are preserved
/// let expected_standard_extranonce = vec![0x42, 0x43, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1];
/// assert_eq!(new_standard_channel_extranonce.to_vec(), expected_standard_extranonce);
///
/// // Now the proxy receives the ExtendedExtranonce previously created
/// // The proxy knows the extranonce space reserved to the pool is 7 bytes and that the total
/// // extranonce len is 32 bytes and decides to reserve 4 bytes for itself and leave the remaining 21 for
/// // further downstreams.
/// let range_0 = 0..7;
/// let range_1 = 7..11;
/// let range_2 = 11..32;
/// let mut proxy_extended_extranonce = ExtendedExtranonce::from_upstream_extranonce(
///     new_extended_channel_extranonce,
///     range_0,
///     range_1,
///     range_2
/// ).unwrap();
///
/// // The proxy generates an extended extranonce for downstream
/// let new_extended_channel_extranonce_prefix = proxy_extended_extranonce.next_prefix_extended(3).unwrap();
/// let expected_extranonce_prefix = vec![0x42, 0x43, 0, 0, 0, 0, 1, 0, 0, 0, 1];
/// assert_eq!(new_extended_channel_extranonce_prefix.clone().to_vec(), expected_extranonce_prefix);
///
/// // When the proxy receives a share from downstream and wants to recreate the full extranonce
/// // e.g., because it wants to check the share's work
/// let received_extranonce: Extranonce = vec![0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0].try_into().unwrap();
/// let share_complete_extranonce = proxy_extended_extranonce.extranonce_from_downstream_extranonce(received_extranonce.clone()).unwrap();
/// let expected_complete_extranonce = vec![0x42, 0x43, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
/// assert_eq!(share_complete_extranonce.to_vec(), expected_complete_extranonce);
///
/// // Now the proxy wants to send the extranonce received from downstream and the part of extranonce
/// // owned by itself to the pool
/// let extranonce_to_send = proxy_extended_extranonce.without_upstream_part(Some(received_extranonce)).unwrap();
/// let expected_extranonce_to_send = vec![0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 8, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];
pub struct ExtendedExtranonce {
    inner: alloc::vec::Vec<u8>,
    range_0: core::ops::Range<usize>,
    range_1: core::ops::Range<usize>,
    range_2: core::ops::Range<usize>,
    additional_coinbase_script_data: Option<alloc::vec::Vec<u8>>,
}

/// Error type for ExtendedExtranonce operations
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum ExtendedExtranonceError {
    /// The range_2.end is greater than MAX_EXTRANONCE_LEN
    ExceedsMaxLength,
    /// The ranges are invalid (e.g. range_0.end != range_1.start)
    InvalidRanges,
    /// The downstream extranonce length doesn't match the expected length
    InvalidDownstreamLength,
    /// The extranonce bytes in range_1 are at maximum value and can't be incremented
    MaxValueReached,
    /// The additional coinbase script data length is invalid
    InvalidAdditionalCoinbaseScriptDataLength,
}

/// the trait PartialEq is implemented in such a way that only the relevant bytes are compared.
/// If range_2.end is set to 20, then the following ExtendedExtranonces are equal
/// ExtendedExtranonce {
///     inner: [0000 0000 0000 0000      0000 0000 0000 0000],
///     range_0: [0..5],
///     range_1: [5..10],
///     range_2: [10, 20],
/// }
/// ExtendedExtranonce {
///     inner: [0000 0000 0000 0000      0000 1111 1111 1111],
///     range_0: [0..5],
///     range_1: [5..10],
///     range_2: [10, 20],
/// }
impl PartialEq for ExtendedExtranonce {
    fn eq(&self, other: &Self) -> bool {
        let len = self.range_2.end;
        self.inner[0..len] == other.inner[0..len]
            && self.range_0 == other.range_0
            && self.range_1 == other.range_1
            && self.range_2 == other.range_2
    }
}

impl ExtendedExtranonce {
    /// every extranonce start from zero.
    pub fn new(
        range_0: Range<usize>,
        range_1: Range<usize>,
        range_2: Range<usize>,
        additional_coinbase_script_data: Option<alloc::vec::Vec<u8>>,
    ) -> Result<Self, ExtendedExtranonceError> {
        // Validate ranges
        if range_0.start != 0
            || range_0.end != range_1.start
            || range_1.end != range_2.start
            || range_1.end < range_1.start
            || range_2.end < range_2.start
        {
            return Err(ExtendedExtranonceError::InvalidRanges);
        }

        if let Some(additional_coinbase_script_data) = additional_coinbase_script_data.clone() {
            if additional_coinbase_script_data.len() > range_1.end - range_1.start {
                return Err(ExtendedExtranonceError::InvalidAdditionalCoinbaseScriptDataLength);
            }
        }

        // Check if range_2.end exceeds MAX_EXTRANONCE_LEN
        if range_2.end > MAX_EXTRANONCE_LEN {
            return Err(ExtendedExtranonceError::ExceedsMaxLength);
        }

        let mut inner = vec![0; range_2.end];
        if let Some(additional_coinbase_script_data) = additional_coinbase_script_data.clone() {
            inner[range_1.start..range_1.start + additional_coinbase_script_data.len()]
                .copy_from_slice(&additional_coinbase_script_data);
        }

        Ok(Self {
            inner,
            range_0,
            range_1,
            range_2,
            additional_coinbase_script_data,
        })
    }

    pub fn new_with_inner_only_test(
        range_0: Range<usize>,
        range_1: Range<usize>,
        range_2: Range<usize>,
        mut inner: alloc::vec::Vec<u8>,
    ) -> Result<Self, ExtendedExtranonceError> {
        // Validate ranges
        if range_0.start != 0
            || range_0.end != range_1.start
            || range_1.end != range_2.start
            || range_1.end < range_1.start
            || range_2.end < range_2.start
        {
            return Err(ExtendedExtranonceError::InvalidRanges);
        }

        // Check if range_2.end exceeds MAX_EXTRANONCE_LEN
        if range_2.end > MAX_EXTRANONCE_LEN {
            return Err(ExtendedExtranonceError::ExceedsMaxLength);
        }

        inner.resize(MAX_EXTRANONCE_LEN, 0);
        Ok(Self {
            inner,
            range_0,
            range_1,
            range_2,
            additional_coinbase_script_data: None,
        })
    }

    pub fn get_len(&self) -> usize {
        self.range_2.end
    }

    pub fn get_range2_len(&self) -> usize {
        self.range_2.end - self.range_2.start
    }

    pub fn get_range0_len(&self) -> usize {
        self.range_0.end - self.range_0.start
    }

    pub fn get_prefix_len(&self) -> usize {
        self.range_1.end - self.range_0.start
    }

    /// Suppose that P receives from the upstream an extranonce that needs to be converted into any
    /// ExtendedExtranonce, eg when an extended channel is opened. Then range_0 (that should
    /// be provided along the Extranonce) is reserved for the upstream and can't be modiefied by
    /// P. If the bytes recerved to P (range_1 and range_2) are not set to zero, returns None,
    /// otherwise returns Some(ExtendedExtranonce). If the range_2.end field is greater than 32,
    /// returns None.
    pub fn from_upstream_extranonce(
        v: Extranonce,
        range_0: Range<usize>,
        range_1: Range<usize>,
        range_2: Range<usize>,
    ) -> Result<Self, ExtendedExtranonceError> {
        // Validate ranges
        if range_0.start != 0
            || range_0.end != range_1.start
            || range_1.end != range_2.start
            || range_1.end < range_1.start
            || range_2.end < range_2.start
        {
            return Err(ExtendedExtranonceError::InvalidRanges);
        }

        // Check if range_2.end exceeds MAX_EXTRANONCE_LEN
        if range_2.end > MAX_EXTRANONCE_LEN {
            return Err(ExtendedExtranonceError::ExceedsMaxLength);
        }

        let mut inner = v.extranonce;
        inner.resize(range_2.end, 0);
        let rest = vec![0; range_2.end - inner.len()];
        let inner = [inner, rest].concat();
        Ok(Self {
            inner,
            range_0,
            range_1,
            range_2,
            additional_coinbase_script_data: None,
        })
    }

    /// Specular of [Self::from_upstream_extranonce]
    pub fn extranonce_from_downstream_extranonce(
        &self,
        dowstream_extranonce: Extranonce,
    ) -> Result<Extranonce, ExtendedExtranonceError> {
        if dowstream_extranonce.extranonce.len() != self.range_2.end - self.range_2.start {
            return Err(ExtendedExtranonceError::InvalidDownstreamLength);
        }
        let mut res = self.inner[self.range_0.start..self.range_1.end].to_vec();
        for b in dowstream_extranonce.extranonce {
            res.push(b)
        }
        res.try_into()
            .map_err(|_| ExtendedExtranonceError::ExceedsMaxLength)
    }

    /// Calculates the next extranonce for standard channels.
    pub fn next_prefix_standard(&mut self) -> Result<Extranonce, ExtendedExtranonceError> {
        let non_reserved_extranonces_bytes = &mut self.inner[self.range_2.start..self.range_2.end];
        match increment_bytes_be(non_reserved_extranonces_bytes) {
            Ok(_) => Ok(self.into()),
            Err(_) => Err(ExtendedExtranonceError::MaxValueReached),
        }
    }

    /// Calculates the next extranonce for extended channels.
    /// The required_len variable represents the range requested by the downstream to use.
    /// The part that is incremented is range_1, as every downstream must have different jobs.
    pub fn next_prefix_extended(
        &mut self,
        required_len: usize,
    ) -> Result<Extranonce, ExtendedExtranonceError> {
        if required_len > self.range_2.end - self.range_2.start {
            return Err(ExtendedExtranonceError::InvalidDownstreamLength);
        };

        // Determine the start position for extended_part based on additional_coinbase_script_data
        // If additional_coinbase_script_data is Some, some bytes are meant to be fixed and not
        // incremented
        let extended_part_start =
            if let Some(additional_data) = &self.additional_coinbase_script_data {
                self.range_1.start + additional_data.len()
            } else {
                self.range_1.start
            };

        let extended_part = &mut self.inner[extended_part_start..self.range_1.end];
        match increment_bytes_be(extended_part) {
            Ok(_) => {
                let result = self.inner[..self.range_1.end].to_vec();
                // Safe unwrap result will be always less the MAX_EXTRANONCE_LEN
                result
                    .try_into()
                    .map_err(|_| ExtendedExtranonceError::ExceedsMaxLength)
            }
            Err(_) => Err(ExtendedExtranonceError::MaxValueReached),
        }
    }

    /// Return a vec with the extranonce bytes that belong to self and downstream removing the
    /// ones owned by upstream (using Sv1 terms the extranonce1 is removed)
    /// If dowstream_extranonce is Some(v) it replace the downstream extranonce part with v
    pub fn without_upstream_part(
        &self,
        downstream_extranonce: Option<Extranonce>,
    ) -> Result<Extranonce, ExtendedExtranonceError> {
        match downstream_extranonce {
            Some(downstream_extranonce) => {
                if downstream_extranonce.extranonce.len() != self.range_2.end - self.range_2.start {
                    return Err(ExtendedExtranonceError::InvalidDownstreamLength);
                }
                let mut res = self.inner[self.range_1.start..self.range_1.end].to_vec();
                for b in downstream_extranonce.extranonce {
                    res.push(b)
                }
                res.try_into()
                    .map_err(|_| ExtendedExtranonceError::ExceedsMaxLength)
            }
            None => self.inner[self.range_1.start..self.range_2.end]
                .to_vec()
                .try_into()
                .map_err(|_| ExtendedExtranonceError::ExceedsMaxLength),
        }
    }

    pub fn upstream_part(&self) -> Extranonce {
        self.inner[self.range_0.start..self.range_1.end]
            .to_vec()
            .try_into()
            .unwrap()
    }
}
/// This function is used to increment extranonces, and it is used in next_standard and in
/// next_extended methods. If the input consists of an array of 255 as u8 (the maximum value) then
/// the input cannot be incremented. In this case, the input is not changed and the function returns
/// Err(()). In every other case, the function increments the input and returns Ok(())
fn increment_bytes_be(bs: &mut [u8]) -> Result<(), ()> {
    for b in bs.iter_mut().rev() {
        if *b != u8::MAX {
            *b += 1;
            return Ok(());
        } else {
            *b = 0;
        }
    }
    for b in bs.iter_mut() {
        *b = u8::MAX
    }
    Err(())
}

#[cfg(test)]
pub mod tests {
    use super::*;
    use alloc::vec::Vec;
    use quickcheck_macros;

    #[test]
    fn test_extranonce_errors() {
        let extranonce = Extranonce::try_from(vec![0; MAX_EXTRANONCE_LEN + 1]);
        assert!(extranonce.is_err());

        assert!(Extranonce::new(MAX_EXTRANONCE_LEN + 1).is_none());
    }

    #[test]
    fn test_from_upstream_extranonce_error() {
        let range_0 = 0..0;
        let range_1 = 0..0;
        let range_2 = 0..MAX_EXTRANONCE_LEN + 1;
        let extranonce = Extranonce::new(10).unwrap();

        let extended_extranonce =
            ExtendedExtranonce::from_upstream_extranonce(extranonce, range_0, range_1, range_2);
        assert!(extended_extranonce.is_err());
        assert_eq!(
            extended_extranonce.unwrap_err(),
            ExtendedExtranonceError::ExceedsMaxLength
        );
    }

    #[test]
    fn test_invalid_ranges() {
        // Test with range_0.start != 0
        let result = ExtendedExtranonce::new(1..2, 2..3, 3..10, None);
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), ExtendedExtranonceError::InvalidRanges);

        // Test with range_0.end != range_1.start
        let result = ExtendedExtranonce::new(0..2, 3..4, 4..10, None);
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), ExtendedExtranonceError::InvalidRanges);

        // Test with range_1.end != range_2.start
        let result = ExtendedExtranonce::new(0..2, 2..4, 5..10, None);
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), ExtendedExtranonceError::InvalidRanges);
    }

    #[test]
    fn test_extranonce_from_downstream_extranonce() {
        let downstream_len = 10;

        let downstream_extranonce = Extranonce::new(downstream_len).unwrap();

        let range_0 = 0..4;
        let range_1 = 4..downstream_len;
        let range_2 = downstream_len..(downstream_len * 2 + 1);

        let extended_extraonce = ExtendedExtranonce::new(range_0, range_1, range_2, None).unwrap();

        let extranonce =
            extended_extraonce.extranonce_from_downstream_extranonce(downstream_extranonce);

        assert!(extranonce.is_err());
        assert_eq!(
            extranonce.unwrap_err(),
            ExtendedExtranonceError::InvalidDownstreamLength
        );

        // Test with a valid downstream extranonce
        let extra_content: Vec<u8> = vec![5; downstream_len];
        let downstream_extranonce =
            Extranonce::from_vec_with_len(extra_content.clone(), downstream_len);

        let range_0 = 0..4;
        let range_1 = 4..downstream_len;
        let range_2 = downstream_len..(downstream_len * 2);

        let extended_extraonce = ExtendedExtranonce::new(range_0, range_1, range_2, None).unwrap();

        let extranonce =
            extended_extraonce.extranonce_from_downstream_extranonce(downstream_extranonce);

        assert!(extranonce.is_ok());

        //validate that the extranonce is the concatenation of the upstream part and the downstream
        // part
        assert_eq!(
            extra_content,
            extranonce.unwrap().extranonce.to_vec()[downstream_len..downstream_len * 2]
        );
    }

    // Test from_vec_with_len
    #[test]
    fn test_extranonce_from_vec_with_len() {
        let extranonce = Extranonce::new(10).unwrap();
        let extranonce2 = Extranonce::from_vec_with_len(extranonce.extranonce, 22);
        assert_eq!(extranonce2.extranonce.len(), 22);
    }

    #[test]
    fn test_extranonce_without_upstream_part() {
        let downstream_len = 10;

        let downstream_extranonce = Extranonce::new(downstream_len).unwrap();

        let range_0 = 0..4;
        let range_1 = 4..downstream_len;
        let range_2 = downstream_len..(downstream_len * 2 + 1);

        let extended_extraonce = ExtendedExtranonce::new(range_0, range_1, range_2, None).unwrap();

        assert_eq!(
            extended_extraonce.without_upstream_part(Some(downstream_extranonce.clone())),
            Err(ExtendedExtranonceError::InvalidDownstreamLength)
        );

        let range_0 = 0..4;
        let range_1 = 4..downstream_len;
        let range_2 = downstream_len..(downstream_len * 2);
        let upstream_extranonce = Extranonce::from_vec_with_len(vec![5; 14], downstream_len);

        let extended_extraonce = ExtendedExtranonce::from_upstream_extranonce(
            upstream_extranonce.clone(),
            range_0,
            range_1.clone(),
            range_2,
        )
        .unwrap();

        let extranonce = extended_extraonce
            .without_upstream_part(Some(downstream_extranonce.clone()))
            .unwrap();
        assert_eq!(
            extranonce.extranonce[0..6],
            upstream_extranonce.extranonce[0..6]
        );
        assert_eq!(extranonce.extranonce[7..], vec![0; 9]);
    }

    // This test checks the behaviour of the function increment_bytes_be for a the MAX value
    // converted in be array of u8
    #[test]
    fn test_incrment_bytes_be_max() {
        let input = u8::MAX;
        let mut input = input.to_be_bytes();
        let result = increment_bytes_be(&mut input[..]);
        assert!(result == Err(()));
        assert!(u8::from_be_bytes(input) == u8::MAX);
    }

    // thest the function incrment_bytes_be for values different from MAX
    #[quickcheck_macros::quickcheck]
    fn test_increment_by_one(input: u8) -> bool {
        let expected1 = match input {
            u8::MAX => input,
            _ => input + 1,
        };
        let mut input = input.to_be_bytes();
        let _ = increment_bytes_be(&mut input[..]);
        let incremented_by_1 = u8::from_be_bytes(input);
        incremented_by_1 == expected1
    }
    use core::convert::TryFrom;

    // check that the composition of the functions Extranonce to U256 and U256 to Extranonce is the
    // identity function
    #[quickcheck_macros::quickcheck]
    fn test_extranonce_from_u256(mut input: Vec<u8>) -> bool {
        input.resize(MAX_EXTRANONCE_LEN, 0);

        let extranonce_start = Extranonce::try_from(input.clone()).unwrap();
        let u256 = U256::<'static>::from(extranonce_start.clone());
        let extranonce_final = Extranonce::from(u256);
        extranonce_start == extranonce_final
    }

    // do the same of the above but with B032 type
    #[quickcheck_macros::quickcheck]
    fn test_extranonce_from_b032(mut input: Vec<u8>) -> bool {
        input.resize(MAX_EXTRANONCE_LEN, 0);
        let extranonce_start = Extranonce::try_from(input.clone()).unwrap();
        let b032 = B032::<'static>::from(extranonce_start.clone());
        let extranonce_final = Extranonce::from(b032);
        extranonce_start == extranonce_final
    }

    // this test checks the functions from_upstream_extranonce and from_extranonce.
    #[quickcheck_macros::quickcheck]
    fn test_extranonce_from_extended_extranonce(input: (u8, u8, Vec<u8>, usize)) -> bool {
        let inner = from_arbitrary_vec_to_array(input.2.clone());
        let extranonce_len = input.3 % MAX_EXTRANONCE_LEN + 1;
        let r0 = input.0 as usize;
        let r1 = input.1 as usize;
        let r0 = r0 % (extranonce_len + 1);
        let r1 = r1 % (extranonce_len + 1);
        let mut ranges = Vec::from([r0, r1]);
        ranges.sort();
        let range_0 = 0..ranges[0];
        let range_1 = ranges[0]..ranges[1];
        let range_2 = ranges[1]..extranonce_len;

        let mut extended_extranonce_start = ExtendedExtranonce::new_with_inner_only_test(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            inner.to_vec(),
        )
        .unwrap();

        assert_eq!(extended_extranonce_start.get_len(), extranonce_len);
        assert_eq!(
            extended_extranonce_start.get_range2_len(),
            extranonce_len - ranges[1]
        );

        let extranonce_result = extended_extranonce_start.next_prefix_extended(0);

        // todo: refactor this test to avoid skipping the test if next_extended fails
        if extranonce_result.is_err() {
            return true; // Skip test if next_extended fails
        }

        let extranonce = extranonce_result.unwrap();

        let extended_extranonce_final = ExtendedExtranonce::from_upstream_extranonce(
            extranonce,
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
        );

        match extended_extranonce_final {
            Ok(extended_extranonce_final) => {
                for b in extended_extranonce_final.inner[range_2.start..range_2.end].iter() {
                    if b != &0 {
                        return false;
                    }
                }
                extended_extranonce_final.inner[range_0.clone().start..range_1.end]
                    == extended_extranonce_start.inner[range_0.start..range_1.end]
            }
            Err(_) => {
                // If from_upstream_extranonce fails, it should be because the inner bytes in
                // range_1..range_2 are not zero
                for b in inner[range_1.start..range_2.end].iter() {
                    if b != &0 {
                        return true;
                    }
                }
                false
            }
        }
    }

    // test next_standard_method
    #[quickcheck_macros::quickcheck]
    fn test_next_standard_extranonce(input: (u8, u8, Vec<u8>, usize)) -> bool {
        let inner = from_arbitrary_vec_to_array(input.2.clone());
        let extranonce_len = input.3 % MAX_EXTRANONCE_LEN + 1;
        let r0 = input.0 as usize;
        let r1 = input.1 as usize;
        let r0 = r0 % (extranonce_len + 1);
        let r1 = r1 % (extranonce_len + 1);
        let mut ranges = Vec::from([r0, r1]);
        ranges.sort();
        let range_0 = 0..ranges[0];
        let range_1 = ranges[0]..ranges[1];
        let range_2 = ranges[1]..extranonce_len;

        let extended_extranonce_start = ExtendedExtranonce::new_with_inner_only_test(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            inner.to_vec(),
        )
        .unwrap();

        let mut extranonce_copy: Extranonce =
            Extranonce::from(&mut extended_extranonce_start.clone());
        let extranonce_expected_b032: Option<B032> = extranonce_copy.next();

        match extended_extranonce_start.clone().next_prefix_standard() {
            Ok(extranonce_next) => match extranonce_expected_b032 {
                Some(b032) =>
                // the range_2 of extranonce_next must be equal to the range_2 of the
                // conversion of extranonce_copy.next() converted in extranonce
                {
                    extranonce_next.extranonce[range_2.start..range_2.end] == Extranonce::from(b032.clone()).extranonce[range_2.start..range_2.end]
                    // the range_1 of the conversion of extranonce_copy.next() converted in
                    // extranonce must remain unchanged
                    && Extranonce::from(b032.clone()).extranonce[range_1.start..range_1.end]== extended_extranonce_start.inner[range_1.start..range_1.end]
                }
                None => false,
            },
            // if .next_standard() method falls in None case, this means that the range_2 is at
            // maximum value, so every entry must be 255 as u8
            Err(ExtendedExtranonceError::MaxValueReached) => {
                for b in inner[range_2.start..range_2.end].iter() {
                    if b != &255_u8 {
                        return false;
                    }
                }
                true
            }
            Err(_) => false, // Other errors are not expected in this test
        }
    }

    #[quickcheck_macros::quickcheck]
    fn test_next_stndard2(input: (u8, u8, Vec<u8>, usize)) -> bool {
        let inner = from_arbitrary_vec_to_array(input.2.clone());
        let extranonce_len = input.3 % MAX_EXTRANONCE_LEN + 1;
        let r0 = input.0 as usize;
        let r1 = input.1 as usize;
        let r0 = r0 % (extranonce_len + 1);
        let r1 = r1 % (extranonce_len + 1);
        let mut ranges = Vec::from([r0, r1]);
        ranges.sort();
        let range_0 = 0..ranges[0];
        let range_1 = ranges[0]..ranges[1];
        let range_2 = ranges[1]..extranonce_len;

        let mut extended_extranonce_start = ExtendedExtranonce::new_with_inner_only_test(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            inner.to_vec(),
        )
        .unwrap();

        match extended_extranonce_start.next_prefix_standard() {
            Ok(v) => {
                extended_extranonce_start.inner[range_2.clone()] == v.extranonce[range_2]
                    && extended_extranonce_start.inner[range_0.clone()]
                        == v.extranonce[range_0.clone()]
            }
            Err(_) => true, // Any error is acceptable for this test
        }
    }

    #[quickcheck_macros::quickcheck]
    fn test_next_extended_extranonce(input: (u8, u8, Vec<u8>, usize, usize)) -> bool {
        let inner = from_arbitrary_vec_to_array(input.2.clone());
        let extranonce_len = input.3 % MAX_EXTRANONCE_LEN + 1;
        let r0 = input.0 as usize;
        let r1 = input.1 as usize;
        let r0 = r0 % (extranonce_len + 1);
        let r1 = r1 % (extranonce_len + 1);
        let required_len = input.4;
        let mut ranges = Vec::from([r0, r1]);
        ranges.sort();
        let range_0 = 0..ranges[0];
        let range_1 = ranges[0]..ranges[1];
        let range_2 = ranges[1]..extranonce_len;

        let mut extended_extranonce = ExtendedExtranonce::new_with_inner_only_test(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            inner.to_vec(),
        )
        .unwrap();

        match extended_extranonce.next_prefix_extended(required_len) {
            Ok(extranonce) => extended_extranonce.inner[..range_1.end] == extranonce.extranonce[..],
            Err(ExtendedExtranonceError::InvalidDownstreamLength) => {
                required_len > range_2.end - range_2.start
            }
            Err(ExtendedExtranonceError::MaxValueReached) => {
                let mut range_1_start = inner[range_1.clone()].to_vec();
                increment_bytes_be(&mut range_1_start).is_err()
            }
            Err(_) => false, // Other errors are not expected in this test
        }
    }

    #[quickcheck_macros::quickcheck]
    fn test_target_from_u256(input: (u128, u128)) -> bool {
        let target_expected = Target {
            head: input.0,
            tail: input.1,
        };

        let bytes = [&input.0.to_ne_bytes()[..], &input.1.to_ne_bytes()[..]].concat();
        let u256: U256 = bytes.try_into().unwrap();
        let target_final: Target = u256.clone().into();

        let u256_final: U256 = target_final.clone().into();

        target_expected == target_final && u256_final == u256
    }
    #[quickcheck_macros::quickcheck]
    fn test_target_to_u256(input: (u128, u128)) -> bool {
        let target_start = Target {
            head: input.0,
            tail: input.1,
        };
        let u256 = U256::<'static>::from(target_start.clone());
        let target_final = Target::from(u256);
        target_final == target_final
    }

    #[test]
    fn test_ord_with_equal_head_tail() {
        let target_1 = Target { head: 1, tail: 1 };
        let target_2 = Target { head: 1, tail: 2 };
        assert!(target_1 < target_2);

        //also test with equal tails
        let target_3 = Target { head: 2, tail: 2 };
        assert!(target_2 < target_3);
    }

    #[quickcheck_macros::quickcheck]
    fn test_ord_for_target_positive_increment(input: (u128, u128, u128, u128)) -> bool {
        let max = u128::MAX;
        // we want input.0 and input.1 >= 0 and < u128::MAX
        let input = (input.0 % max, input.1 % max, input.2, input.3);
        let target_start = Target {
            head: input.0,
            tail: input.1,
        };
        let positive_increment = (
            input.2 % (max - target_start.head) + 1,
            input.3 % (max - target_start.tail) + 1,
        );
        let target_final = Target {
            head: target_start.head + positive_increment.0,
            tail: target_start.tail + positive_increment.1,
        };
        target_final > target_start
    }

    #[quickcheck_macros::quickcheck]
    fn test_ord_for_target_negative_increment(input: (u128, u128, u128, u128)) -> bool {
        let max = u128::MAX;
        let input = (input.0 % max + 1, input.1 % max + 1, input.2, input.3);
        let target_start = Target {
            head: input.0,
            tail: input.1,
        };
        let negative_increment = (
            input.2 % target_start.head + 1,
            input.3 % target_start.tail + 1,
        );
        let target_final = Target {
            head: target_start.head - negative_increment.0,
            tail: target_start.tail - negative_increment.1,
        };
        target_final < target_start
    }

    #[quickcheck_macros::quickcheck]
    fn test_ord_for_target_zero_increment(input: (u128, u128)) -> bool {
        let target_start = Target {
            head: input.0,
            tail: input.1,
        };
        let target_final = target_start.clone();
        target_start == target_final
    }

    #[quickcheck_macros::quickcheck]
    fn test_vec_from_extranonce(input: Vec<u8>) -> bool {
        let input_start = from_arbitrary_vec_to_array(input).to_vec();
        let extranonce_start = Extranonce::try_from(input_start.clone()).unwrap();
        let vec_final = Vec::from(extranonce_start.clone());
        input_start == vec_final
    }

    use core::convert::TryInto;
    pub fn from_arbitrary_vec_to_array(vec: Vec<u8>) -> [u8; 32] {
        if vec.len() >= 32 {
            vec[0..32].try_into().unwrap()
        } else {
            let mut result = Vec::new();
            for _ in 0..(32 - vec.len()) {
                result.push(0);
            }
            for element in vec {
                result.push(element)
            }
            result[..].try_into().unwrap()
        }
    }

    #[test]
    fn test_extended_extranonce_get_prefix_len() {
        let range_0 = 0..2;
        let range_1 = 2..4;
        let range_2 = 4..9;
        let extended = ExtendedExtranonce::new(range_0, range_1, range_2, None).unwrap();
        let prefix_len = extended.get_prefix_len();
        assert!(prefix_len == 4);
    }

    #[test]
    fn test_extended_extranonce_with_additional_coinbase_script_data() {
        let range_0 = 0..0;
        let range_1 = 0..4;
        let range_2 = 4..8;
        let additional_data = vec![0x42, 0x43, 0x44]; // Some fixed data

        // Create an ExtendedExtranonce with additional_coinbase_script_data
        let extended = ExtendedExtranonce::new(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            Some(additional_data.clone()),
        )
        .unwrap();

        // Verify the additional data was stored
        assert_eq!(
            extended.additional_coinbase_script_data,
            Some(additional_data.clone())
        );

        // Verify the inner data contains the additional data
        assert_eq!(
            extended.inner[range_1.start..range_1.start + additional_data.len()],
            additional_data[..]
        );
    }

    #[test]
    fn test_extended_extranonce_invalid_additional_coinbase_script_data_length() {
        let range_0 = 0..0;
        let range_1 = 0..2; // Range length is 2
        let range_2 = 2..4;
        let additional_data = vec![0x42, 0x43, 0x44]; // Length 3 > range_1 length

        // Create an ExtendedExtranonce with additional_coinbase_script_data that's too long
        let result = ExtendedExtranonce::new(range_0, range_1, range_2, Some(additional_data));

        // Verify the correct error is returned
        assert!(result.is_err());
        assert_eq!(
            result.unwrap_err(),
            ExtendedExtranonceError::InvalidAdditionalCoinbaseScriptDataLength
        );
    }

    #[test]
    fn test_next_extended_with_additional_coinbase_script_data() {
        let range_0 = 0..0;
        let range_1 = 0..4;
        let range_2 = 4..8;
        let additional_data = vec![0x42, 0x43]; // Fixed data of length 2

        // Create an ExtendedExtranonce with additional_coinbase_script_data
        let mut extended = ExtendedExtranonce::new(
            range_0,
            range_1.clone(),
            range_2,
            Some(additional_data.clone()),
        )
        .unwrap();

        // Call next_extended
        let result = extended.next_prefix_extended(3).unwrap();

        // Verify the result contains the additional data
        assert_eq!(
            result.extranonce[0..additional_data.len()],
            additional_data[..]
        );

        // Call next_extended again
        let result2 = extended.next_prefix_extended(3).unwrap();

        // Verify the fixed part remains unchanged
        assert_eq!(
            result2.extranonce[0..additional_data.len()],
            additional_data[..]
        );

        // Verify the incremented part has changed
        assert_ne!(result.extranonce, result2.extranonce);
    }

    #[test]
    fn test_multiple_next_extended_with_additional_coinbase_script_data() {
        let range_0 = 0..0;
        let range_1 = 0..4;
        let range_2 = 4..8;
        let additional_data = vec![0x42, 0x43]; // Fixed data of length 2

        // Create an ExtendedExtranonce with additional_coinbase_script_data
        let mut extended = ExtendedExtranonce::new(
            range_0,
            range_1.clone(),
            range_2,
            Some(additional_data.clone()),
        )
        .unwrap();

        // Generate multiple extranonces and verify they all have the same fixed part
        let mut results = Vec::new();
        for _ in 0..5 {
            let result = extended.next_prefix_extended(3).unwrap();
            results.push(result);
        }

        // Verify all results have the same fixed part
        for result in &results {
            assert_eq!(
                result.extranonce[0..additional_data.len()],
                additional_data[..]
            );
        }

        // Verify all results have different incremented parts
        for i in 0..results.len() {
            for j in i + 1..results.len() {
                assert_ne!(results[i].extranonce, results[j].extranonce);
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/new_mining_job.rs">
use alloc::vec::Vec;
use binary_sv2::{binary_codec_sv2, Deserialize, Seq0255, Serialize, Sv2Option, B064K, U256};
use core::{convert::TryInto, fmt};

/// Message used by an upstream to provide an updated mining job to downstream.
///
/// This is used for Standard Channels only.
///
/// Note that Standard Jobs distrbuted through this message are restricted to a fixed Merkle Root,
/// and the only rollable bits are `version`, `nonce`, and `nTime` fields of the block header.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct NewMiningJob<'decoder> {
    /// Channel identifier for the channel that this job is valid for.
    ///
    /// This must be a Standard Channel.
    pub channel_id: u32,
    /// Upstreams identification of the mining job.
    ///
    /// This identifier must be provided to the upstream when shares are submitted.
    pub job_id: u32,
    /// Smallest `nTime` value available for hashing for the new mining job.
    ///
    /// An empty value indicates this is a future job and will be ready to mine on once a
    /// [`SetNewPrevHash`] message is received with a matching `job_id`.
    /// [`SetNewPrevHash`] message will also provide `prev_hash` and `min_ntime`.
    ///
    /// Otherwise, if [`NewMiningJob::min_ntime`] value is set, the downstream must start mining on
    /// it immediately. In this case, the new mining job uses the `prev_hash` from the last
    /// received [`SetNewPrevHash`] message.
    ///
    /// [`SetNewPrevHash`]: crate::SetNewPrevHash
    pub min_ntime: Sv2Option<'decoder, u32>,
    /// Version field that reflects the current network consensus.
    ///
    /// As specified in [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki),
    /// the general purpose bits can be freely manipulated by the downstream node.
    ///
    /// The downstream node must not rely on the upstream node to set the
    /// [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki) bits to any
    /// particular value.
    pub version: u32,
    /// Merkle root field as used in the bitcoin block header.
    ///
    /// Note that this field is fixed and cannot be modified by the downstream node.
    pub merkle_root: U256<'decoder>,
}

impl fmt::Display for NewMiningJob<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "NewMiningJob(channel_id: {}, job_id: {}, min_ntime: {}, version: {}, merkle_root: {})",
            self.channel_id, self.job_id, self.min_ntime, self.version, self.merkle_root
        )
    }
}

impl NewMiningJob<'_> {
    pub fn is_future(&self) -> bool {
        self.min_ntime.clone().into_inner().is_none()
    }
    pub fn set_future(&mut self) {
        self.min_ntime = Sv2Option::new(None);
    }
    pub fn set_no_future(&mut self, min_ntime: u32) {
        self.min_ntime = Sv2Option::new(Some(min_ntime));
    }
}

/// Message used by an upstream to provide an updated mining job to the downstream through
/// Extended or Group Channel only.
///
/// An Extended Job allows rolling Merkle Roots, giving extensive control over the search space so
/// that they can implement various advanced use cases such as: translation between Stratum V1 and
/// V2 protocols, difficulty aggregation and search space splitting.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct NewExtendedMiningJob<'decoder> {
    /// Identifier of the Extended Mining Channel that this job is valid for.
    ///
    /// For a Group Channel, the message is broadcasted to all standard channels belonging to the
    /// group.
    pub channel_id: u32,
    /// Upstreams identification of the mining job.
    ///
    /// This identifier must be provided to the upstream when shares are submitted later in the
    /// mining process.
    pub job_id: u32,
    /// Smallest `nTime` value available for hashing for the new mining job.
    ///
    /// An empty value indicates this is a future job and will be ready to mine on once a
    /// [`SetNewPrevHash`] message is received with a matching `job_id`.
    /// [`SetNewPrevHash`] message will also provide `prev_hash` and `min_ntime`.
    ///
    /// Otherwise, if [`NewMiningJob::min_ntime`] value is set, the downstream must start mining on
    /// it immediately. In this case, the new mining job uses the `prev_hash` from the last
    /// received [`SetNewPrevHash`] message.
    ///
    /// [`SetNewPrevHash`]: crate::SetNewPrevHash
    pub min_ntime: Sv2Option<'decoder, u32>,
    /// Version field that reflects the current network consensus.
    ///
    /// As specified in [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki),
    /// the general purpose bits can be freely manipulated by the downstream node.
    ///
    /// The downstream node must not rely on the upstream node to set the
    /// [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki) bits to any
    /// particular value.
    pub version: u32,
    /// If set to `true`, the general purpose bits of [`NewExtendedMiningJob::version`] (as
    /// specified in BIP320) can be freely manipulated by the downstream node.
    ///
    /// If set to `false`, the downstream node must use [`NewExtendedMiningJob::version`] as it is
    /// defined by this message.
    pub version_rolling_allowed: bool,
    /// Merkle path hashes ordered from deepest.
    pub merkle_path: Seq0255<'decoder, U256<'decoder>>,
    /// Prefix part of the coinbase transaction.
    pub coinbase_tx_prefix: B064K<'decoder>,
    /// Suffix part of the coinbase transaction.
    pub coinbase_tx_suffix: B064K<'decoder>,
}

impl fmt::Display for NewExtendedMiningJob<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "NewExtendedMiningJob(channel_id: {}, job_id: {}, min_ntime: {}, version: {}, version_rolling_allowed: {}, merkle_path: {}, coinbase_tx_prefix: {}, coinbase_tx_suffix: {})",
            self.channel_id,
            self.job_id,
            self.min_ntime,
            self.version,
            self.version_rolling_allowed,
            self.merkle_path,
            self.coinbase_tx_prefix,
            self.coinbase_tx_suffix
        )
    }
}

impl NewExtendedMiningJob<'_> {
    pub fn is_future(&self) -> bool {
        self.min_ntime.clone().into_inner().is_none()
    }
    pub fn set_future(&mut self) {
        self.min_ntime = Sv2Option::new(None);
    }
    pub fn set_no_future(&mut self, min_ntime: u32) {
        self.min_ntime = Sv2Option::new(Some(min_ntime));
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::tests::from_arbitrary_vec_to_array;
    use core::convert::TryFrom;

    #[quickcheck_macros::quickcheck]
    #[allow(clippy::too_many_arguments)]
    fn test_new_extended_mining_job(
        channel_id: u32,
        job_id: u32,
        min_ntime: Option<u32>,
        version: u32,
        version_rolling_allowed: bool,
        merkle_path: Vec<u8>,
        coinbase_tx_prefix: Vec<u8>,
        coinbase_tx_suffix: Vec<u8>,
    ) -> bool {
        let merkle_path = helpers::scan_to_u256_sequence(&merkle_path);
        let coinbase_tx_prefix = helpers::bytes_to_b064k(&coinbase_tx_prefix);
        let coinbase_tx_suffix = helpers::bytes_to_b064k(&coinbase_tx_suffix);
        let nemj = NewExtendedMiningJob {
            channel_id,
            job_id,
            min_ntime: Sv2Option::new(min_ntime),
            version,
            version_rolling_allowed,
            merkle_path: merkle_path.clone(),
            coinbase_tx_prefix: coinbase_tx_prefix.clone(),
            coinbase_tx_suffix: coinbase_tx_suffix.clone(),
        };
        let static_nmj = nemj.as_static();
        static_nmj.channel_id == nemj.channel_id
            && static_nmj.job_id == nemj.job_id
            && static_nmj.min_ntime == nemj.min_ntime
            && static_nmj.version == nemj.version
            && static_nmj.version_rolling_allowed == nemj.version_rolling_allowed
            && static_nmj.merkle_path == merkle_path
            && static_nmj.coinbase_tx_prefix == coinbase_tx_prefix
            && static_nmj.coinbase_tx_suffix == coinbase_tx_suffix
    }

    #[quickcheck_macros::quickcheck]
    fn test_new_mining_job(
        channel_id: u32,
        job_id: u32,
        min_ntime: Option<u32>,
        version: u32,
        merkle_root: Vec<u8>,
    ) -> bool {
        let merkle_root = from_arbitrary_vec_to_array(merkle_root);
        let nmj = NewMiningJob {
            channel_id,
            job_id,
            min_ntime: Sv2Option::new(min_ntime),
            version,
            merkle_root: U256::try_from(merkle_root.to_vec())
                .expect("NewMiningJob: failed to convert merkle_root to B032"),
        };
        let static_nmj = nmj.clone().as_static();
        static_nmj.channel_id == nmj.channel_id
            && static_nmj.job_id == nmj.job_id
            && static_nmj.min_ntime == nmj.min_ntime
            && static_nmj.version == nmj.version
            && static_nmj.merkle_root == nmj.merkle_root
    }

    pub mod helpers {
        use super::*;
        use alloc::borrow::ToOwned;

        pub fn scan_to_u256_sequence(bytes: &[u8]) -> Seq0255<U256> {
            let inner: Vec<U256> = bytes
                .chunks(32)
                .map(|chunk| {
                    let data = from_arbitrary_vec_to_array(chunk.to_vec());
                    U256::from(data)
                })
                .collect();
            Seq0255::new(inner).expect("Could not convert bytes to SEQ0255<U256")
        }

        pub fn bytes_to_b064k(bytes: &[u8]) -> B064K {
            B064K::try_from(bytes.to_owned()).expect("Failed to convert to B064K")
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/open_channel.rs">
use alloc::{string::ToString, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255, U32AsRef, B032, U256};
use core::{convert::TryInto, fmt};
/// Message used by a downstream to request opening a Standard Channel.
///
/// Upon receiving `SetupConnectionSuccess` message, the downstream should open channel(s) on the
/// connection within a reasonable period, otherwise the upstream should close the connection for
/// inactivity.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct OpenStandardMiningChannel<'decoder> {
    /// Specified by downstream role.
    ///
    /// Used for matching responses from upstream.
    ///
    /// The value must be connection-wide unique and is not interpreted by the upstream.
    pub request_id: U32AsRef<'decoder>,
    /// Unconstrained sequence of bytes.
    ///
    /// Whatever is needed by upstream role to identify/authenticate the downstream, e.g.
    /// test.worker1.
    ///
    /// Additional restrictions can be imposed by the upstream role (e.g. a pool). It is highly
    /// recommended to use UTF-8 encoding.
    pub user_identity: Str0255<'decoder>,
    /// Expected hash rate of the device (or cumulative hashrate on the channel if multiple devices
    /// are connected downstream) in h/s.
    ///
    /// Depending on upstreams target setting policy, this value can be used for setting a
    /// reasonable target for the channel.
    ///
    /// Proxy must send 0.0f when there are no mining devices connected yet.
    pub nominal_hash_rate: f32,
    /// Maximum target which can be accepted by the connected device(s).
    ///
    /// Upstream must accept the target or respond by sending [`OpenMiningChannelError`] message.
    pub max_target: U256<'decoder>,
}

impl fmt::Display for OpenStandardMiningChannel<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "OpenStandardMiningChannel(request_id: {}, user_identity: {}, nominal_hash_rate: {}, max_target: {})",
            self.request_id,
            self.user_identity.as_utf8_or_hex(),
            self.nominal_hash_rate,
            self.max_target
        )
    }
}

impl OpenStandardMiningChannel<'_> {
    pub fn get_request_id_as_u32(&self) -> u32 {
        (&self.request_id).into()
    }

    pub fn update_id(&mut self, new_id: u32) {
        let bytes_new = new_id.to_le_bytes();
        let bytes_old = self.request_id.inner_as_mut();
        bytes_old[0] = bytes_new[0];
        bytes_old[1] = bytes_new[1];
        bytes_old[2] = bytes_new[2];
        bytes_old[3] = bytes_new[3];
    }
}

/// Message used by upstream to accept [`OpenStandardMiningChannel`] request from downstream.
#[allow(clippy::derive_partial_eq_without_eq)]
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct OpenStandardMiningChannelSuccess<'decoder> {
    /// Used for matching requests/responses.
    ///
    /// Specified by downstream role and should be extracted from the corresponding
    /// [`OpenStandardMiningChannel`] message.
    pub request_id: U32AsRef<'decoder>,
    /// Newly assigned identifier of the channel, stable for the whole lifetime of the connection.
    ///
    /// This will also be used for broadcasting new jobs by [`crate::NewMiningJob`].
    pub channel_id: u32,
    /// Initial target for the mining channel.
    pub target: U256<'decoder>,
    /// Bytes used as implicit first part of extranonce for the scenario when the job is served by
    /// the downstream role for a set of standard channels that belong to the same group.
    pub extranonce_prefix: B032<'decoder>,
    /// Group channel into which the new channel belongs. See SetGroupChannel for details.
    pub group_channel_id: u32,
}

impl fmt::Display for OpenStandardMiningChannelSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "OpenStandardMiningChannelSuccess(request_id: {}, channel_id: {}, target: {}, extranonce_prefix: {}, group_channel_id: {})",
            self.request_id,
            self.channel_id,
            self.target,
            self.extranonce_prefix,
            self.group_channel_id
        )
    }
}

impl OpenStandardMiningChannelSuccess<'_> {
    pub fn get_request_id_as_u32(&self) -> u32 {
        (&self.request_id).into()
    }

    pub fn update_id(&mut self, new_id: u32) {
        let bytes_new = new_id.to_le_bytes();
        let bytes_old = self.request_id.inner_as_mut();
        bytes_old[0] = bytes_new[0];
        bytes_old[1] = bytes_new[1];
        bytes_old[2] = bytes_new[2];
        bytes_old[3] = bytes_new[3];
    }
}

/// Message used by a downstream to request opening an Extended Channel with an upstream role.
///
/// Similar to [`OpenStandardMiningChannel`] but requests to open an Extended Channel instead of
/// standard channel.
///
/// The main difference is the extranonce size is not fixed for a Extended Channel and can be set
/// by the upstream role based on the [`OpenExtendedMiningChannel::min_extranonce_size`] requested
/// by the downstream.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq)]
pub struct OpenExtendedMiningChannel<'decoder> {
    /// Specified by downstream role.
    ///
    /// Used for matching responses from upstream.
    ///
    /// The value must be connection-wide unique and is not interpreted by the upstream.
    pub request_id: u32,
    /// Unconstrained sequence of bytes.
    ///
    /// Whatever is needed by upstream role to identify/authenticate the downstream, e.g.
    /// name.worker1.
    ///
    /// Additional restrictions can be imposed by the upstream role (e.g. a pool). It is highly
    /// recommended to use UTF-8 encoding.
    pub user_identity: Str0255<'decoder>,
    /// Expected hash rate of the device (or cumulative hashrate on the channel if multiple devices
    /// are connected downstream) in h/s.
    ///
    /// Depending on upstreams target setting policy, this value can be used for setting a
    /// reasonable target for the channel.
    ///
    /// Proxy must send 0.0f when there are no mining devices connected yet.
    pub nominal_hash_rate: f32,
    /// Maximum target which can be accepted by the connected device or devices.
    ///
    /// Upstream must accept the target or respond by sending [`OpenMiningChannelError`] message.
    pub max_target: U256<'decoder>,
    /// Minimum size of extranonce needed by the downstream device/role.
    pub min_extranonce_size: u16,
}

impl fmt::Display for OpenExtendedMiningChannel<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "OpenExtendedMiningChannel(request_id: {}, user_identity: {}, nominal_hash_rate: {}, max_target: {}, min_extranonce_size: {})",
            self.request_id,
            self.user_identity.as_utf8_or_hex(),
            self.nominal_hash_rate,
            self.max_target,
            self.min_extranonce_size
        )
    }
}

impl OpenExtendedMiningChannel<'_> {
    pub fn get_request_id_as_u32(&self) -> u32 {
        self.request_id
    }
}

/// Message used by upstream to accept [`OpenExtendedMiningChannel` request from downstream.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct OpenExtendedMiningChannelSuccess<'decoder> {
    /// Used for matching requests/responses.
    ///
    /// Specified by downstream role and should be extracted from the corresponding
    /// [`OpenExtendedMiningChannel`] message.
    pub request_id: u32,
    /// Newly assigned identifier of the channel, stable for the whole lifetime of the connection.
    ///
    /// This will also be used for broadcasting new jobs by [`crate::NewExtendedMiningJob`].
    pub channel_id: u32,
    /// Initial target for the mining channel.
    pub target: U256<'decoder>,
    /// Extranonce size (in bytes) set for the channel.
    pub extranonce_size: u16,
    /// Bytes used as implicit first part of extranonce
    pub extranonce_prefix: B032<'decoder>,
}

impl fmt::Display for OpenExtendedMiningChannelSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "OpenExtendedMiningChannelSuccess(request_id: {}, channel_id: {}, target: {}, extranonce_size: {}, extranonce_prefix: {})",
            self.request_id,
            self.channel_id,
            self.target,
            self.extranonce_size,
            self.extranonce_prefix
        )
    }
}

/// Message used by upstream to reject [`OpenExtendedMiningChannel`] or
/// [`OpenStandardMiningchannel`] request from downstream.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct OpenMiningChannelError<'decoder> {
    /// Used for matching requests/responses.
    ///
    /// Specified by downstream role and should be extracted from the corresponding
    /// [`OpenExtendedMiningChannel`] or [`OpenStandardMiningchannel`] message.
    pub request_id: u32,
    /// Human-readable error code(s).
    ///
    /// Possible error codes:
    ///
    /// - unknown-user
    /// - max-target-out-of-range
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for OpenMiningChannelError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "OpenMiningChannelError(request_id: {}, error_code: {})",
            self.request_id,
            self.error_code.as_utf8_or_hex()
        )
    }
}

impl OpenMiningChannelError<'_> {
    pub fn new_max_target_out_of_range(request_id: u32) -> Self {
        Self {
            request_id,
            error_code: "max-target-out-of-range".to_string().try_into().unwrap(),
        }
    }
    pub fn unsupported_extranonce_size(request_id: u32) -> Self {
        Self {
            request_id,
            error_code: "unsupported-min-extranonce-size"
                .to_string()
                .try_into()
                .unwrap(),
        }
    }
    pub fn new_unknown_user(request_id: u32) -> Self {
        Self {
            request_id,
            error_code: "unknown-user".to_string().try_into().unwrap(),
        }
    }
}

#[cfg(test)]
mod tests {

    use super::*;
    use crate::tests::from_arbitrary_vec_to_array;
    use alloc::{string::String, vec::Vec};
    use core::convert::TryFrom;

    // *** OPEN STANDARD MINING CHANNEL ***
    #[quickcheck_macros::quickcheck]
    fn test_open_standard_mining_channel_fns(
        request_id: u32,
        user_identity: String,
        nominal_hash_rate: f32,
        max_target: Vec<u8>,
        new_request_id: u32,
    ) -> bool {
        let max_target: [u8; 32] = from_arbitrary_vec_to_array(max_target);
        let mut osmc = OpenStandardMiningChannel {
            request_id: U32AsRef::from(request_id),
            user_identity: Str0255::try_from(user_identity.clone())
                .expect("could not convert string to Str0255"),
            nominal_hash_rate,
            max_target: U256::from(max_target),
        };
        let test_request_id_1 = osmc.get_request_id_as_u32();
        osmc.update_id(new_request_id);
        let test_request_id_2 = osmc.get_request_id_as_u32();
        request_id == test_request_id_1
            && new_request_id == test_request_id_2
            && helpers::compare_static_osmc(osmc)
    }

    #[quickcheck_macros::quickcheck]
    fn test_open_standard_mining_channel_success(
        request_id: u32,
        channel_id: u32,
        target: Vec<u8>,
        extranonce_prefix: Vec<u8>,
        group_channel_id: u32,
        new_request_id: u32,
    ) -> bool {
        let target = from_arbitrary_vec_to_array(target);
        let extranonce_prefix = from_arbitrary_vec_to_array(extranonce_prefix);
        let mut osmcs = OpenStandardMiningChannelSuccess {
            request_id: U32AsRef::from(request_id),
            channel_id,
            target: U256::from(target),
            extranonce_prefix: B032::try_from(extranonce_prefix.to_vec()).expect(
                "OpenStandardMiningChannelSuccess: failed to convert extranonce_prefix to B032",
            ),
            group_channel_id,
        };
        let test_request_id_1 = osmcs.get_request_id_as_u32();
        osmcs.update_id(new_request_id);
        let test_request_id_2 = osmcs.get_request_id_as_u32();
        request_id == test_request_id_1 && new_request_id == test_request_id_2
    }
    // *** OPEN EXTENDED MINING CHANNEL SUCCESS ***
    #[quickcheck_macros::quickcheck]
    fn test_extended_standard_mining_channel_fns(
        request_id: u32,
        user_identity: String,
        nominal_hash_rate: f32,
        max_target: Vec<u8>,
        min_extranonce_size: u16,
    ) -> bool {
        let max_target: [u8; 32] = from_arbitrary_vec_to_array(max_target);
        let oemc = OpenExtendedMiningChannel {
            request_id,
            user_identity: Str0255::try_from(user_identity.clone())
                .expect("could not convert string to Str0255"),
            nominal_hash_rate,
            max_target: U256::from(max_target),
            min_extranonce_size,
        };
        let test_request_id_1 = oemc.get_request_id_as_u32();
        request_id == test_request_id_1
    }

    // *** HELPERS ***
    mod helpers {
        use super::*;
        pub fn compare_static_osmc(osmc: OpenStandardMiningChannel) -> bool {
            let static_osmc = OpenStandardMiningChannel::into_static(osmc.clone());
            static_osmc.request_id == osmc.request_id
                && static_osmc.user_identity == osmc.user_identity
                && static_osmc.nominal_hash_rate.to_ne_bytes()
                    == osmc.nominal_hash_rate.to_ne_bytes()
                && static_osmc.max_target == osmc.max_target
        }
    }

    // "placeholder to allow in file unit tests for quickcheck";
    #[test]
    fn test() {}
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_custom_mining_job.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Seq0255, Serialize, Str0255, B0255, B064K, U256};
use core::convert::TryInto;

/// Message used by downstream role to set a custom job to an upstream (Pool).
///
/// The [`SetCustomMiningJob::token`] should provide the information for the upstream to authorize
/// the custom job that has been or will be negotiated between the Job Declarator Client and Job
/// Declarator Server.
///
/// Can be sent only on extended channel.
///
/// Previously exchanged `SetupConnection::flags` must contain `REQUIRES_WORK_SELECTION` flag i.e.,
/// work selection feature was successfully negotiated.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct SetCustomMiningJob<'decoder> {
    /// Extended mining channel identifier.
    pub channel_id: u32,
    /// Specified by downstream role.
    ///
    /// Used for matching responses from upstream.
    ///
    /// The value must be connection-wide unique and is not interpreted by the upstream.
    pub request_id: u32,
    /// Provide the information for the upstream to authorize the custom job that has been or will
    /// be negotiated between the Job Declarator Client and Job Declarator Server.
    pub token: B0255<'decoder>,
    /// Version field that reflects the current network consensus.
    ///
    /// The general purpose bits (as specified in BIP320) can be freely manipulated by the
    /// downstream role. The downstream role must not rely on the upstream role to set the BIP320
    /// bits to any particular value.
    pub version: u32,
    /// Previous blocks hash.
    pub prev_hash: U256<'decoder>,
    /// Smallest `nTime` value available for hashing.
    pub min_ntime: u32,
    /// Block header field.
    pub nbits: u32,
    /// The coinbase transaction `nVersion` field.
    pub coinbase_tx_version: u32,
    /// Up to 8 bytes (not including the length byte) which are to be placed at the beginning of
    /// the coinbase field in the coinbase transaction.
    pub coinbase_prefix: B0255<'decoder>,
    /// The coinbase transaction inputs nSequence field.
    pub coinbase_tx_input_n_sequence: u32,
    /// All the outputs that will be included in the coinbase txs
    pub coinbase_tx_outputs: B064K<'decoder>,
    /// The `locktime` field in the coinbase transaction.
    pub coinbase_tx_locktime: u32,
    /// Merkle path hashes ordered from deepest.
    pub merkle_path: Seq0255<'decoder, U256<'decoder>>,
}

impl fmt::Display for SetCustomMiningJob<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "SetCustomMiningJob(channel_id={}, request_id={}, token={}, version={}, prev_hash={}, min_ntime={}, nbits={}, coinbase_tx_version={}, coinbase_prefix={}, coinbase_tx_input_n_sequence={}, coinbase_tx_outputs={}, coinbase_tx_locktime={}, merkle_path={})",
            self.channel_id,
            self.request_id,
            self.token,
            self.version,
            self.prev_hash,
            self.min_ntime,
            self.nbits,
            self.coinbase_tx_version,
            self.coinbase_prefix,
            self.coinbase_tx_input_n_sequence,
            self.coinbase_tx_outputs,
            self.coinbase_tx_locktime,
            self.merkle_path
        )
    }
}

/// Message used by upstream to accept [`SetCustomMiningJob`] request.
///
/// Upon receiving this message, downstream can start submitting shares for this job immediately (by
/// using the [`SetCustomMiningJobSuccess::job_id`] provided within this response).
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetCustomMiningJobSuccess {
    /// Extended mining channel identifier.
    pub channel_id: u32,
    /// Request identifier set by the downstream role.
    pub request_id: u32,
    /// Upstreams identification of the mining job.
    pub job_id: u32,
}

impl fmt::Display for SetCustomMiningJobSuccess {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetCustomMiningJobSuccess(channel_id={}, request_id={}, job_id={})",
            self.channel_id, self.request_id, self.job_id
        )
    }
}

/// Message used by upstream to reject [`SetCustomMiningJob`] request.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetCustomMiningJobError<'decoder> {
    /// Extended mining channel identifier.
    pub channel_id: u32,
    /// Request identifier set by the downstream role.
    pub request_id: u32,
    /// Rejection reason.
    ///
    /// Possible errors:
    /// - invalid-channel-id
    /// - invalid-mining-job-token
    /// - invalid-job-param-value-{field_name}
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for SetCustomMiningJobError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetCustomMiningJobError(channel_id={}, request_id={}, error_code={})",
            self.channel_id,
            self.request_id,
            self.error_code.as_utf8_or_hex()
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_extranonce_prefix.rs">
use alloc::{fmt, vec::Vec};

use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, B032};

use core::convert::TryInto;

/// Message used by upstream to change downstream nodes extranonce prefix.
///
/// [`SetExtranoncePrefix::extranonce_prefix`], a constant, is part of the full extranonce and is
/// set by the upstream.
///
/// Note that this message is applicable only for opened Standard or Extended Channels, not Group
/// Channels.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetExtranoncePrefix<'decoder> {
    /// Extended or Standard Channel identifier.
    pub channel_id: u32,
    /// New extranonce prefix.
    pub extranonce_prefix: B032<'decoder>,
}

impl fmt::Display for SetExtranoncePrefix<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetExtranoncePrefix(channel_id={}, extranonce_prefix={})",
            self.channel_id, self.extranonce_prefix
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_group_channel.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Seq064K, Serialize};
use core::convert::TryInto;

/// Message used by upstream to associate a set of Standard Channel(s) to a Group Channel.
///
/// A channel becomes a group channel when it is used by this message as
/// [`SetGroupChannel::group_channel_id`].
///
/// Every standard channel is a member of a group of standard channels, addressed by the upstream
/// servers provided identifier. The group channel is used mainly for efficient job distribution
/// to multiple standard channels at once.
///
/// The upstream must ensure that a group channel has a unique channel ID within one connection.
///
/// Channel reinterpretation is not allowed.
///
/// This message can be sent only to connections that didnt set `REQUIRES_STANDARD_JOBS` flag in
/// `SetupConnection` message.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetGroupChannel<'decoder> {
    /// Identifier of the group where the standard channel belongs.
    pub group_channel_id: u32,
    /// A sequence of opened standard channel IDs, for which the group channel is being redefined.
    pub channel_ids: Seq064K<'decoder, u32>,
}

impl fmt::Display for SetGroupChannel<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetGroupChannel(group_channel_id={}, channel_ids={})",
            self.group_channel_id, self.channel_ids
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_new_prev_hash.rs">
use alloc::vec::Vec;
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, U256};
use core::{convert::TryInto, fmt};

/// Message used by upstream to share or distribute the latest block hash.
///
/// This message may be shared by all downstream nodes (sent only once to each channel group).
///
/// Downstream must immediately start to mine on the provided [`SetNewPrevHash::prevhash`].
///
/// When a downstream receives this message, only the job referenced by [`SetNewPrevHash::job_id`]
/// is valid. Remaining jobs have to be dropped.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetNewPrevHash<'decoder> {
    /// Group channel or channel that this prevhash is valid for.
    pub channel_id: u32,
    /// Job identfier that is to be used for mining.
    ///
    /// A pool may have provided multiple jobs for the next block height (e.g. an empty block or a
    /// block with transactions that are complementary to the set of transactions present in the
    /// current block template).
    pub job_id: u32,
    /// Latest block hash observed by the Template Provider.
    pub prev_hash: U256<'decoder>,
    /// Smallest `nTime` value available for hashing.
    pub min_ntime: u32,
    /// Block header field.
    pub nbits: u32,
}

impl fmt::Display for SetNewPrevHash<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetNewPrevHash(channel_id={}, job_id={}, prev_hash={}, min_ntime={}, nbits={})",
            self.channel_id, self.job_id, self.prev_hash, self.min_ntime, self.nbits
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/set_target.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, U256};
use core::convert::TryInto;

/// Message used by upstream to control the downstream submission rate by adjusting the difficulty
/// target on a specified channel.
///
/// All submits leading to hashes higher than the specified target are expected to be rejected by
/// the upstream.
///
/// [`SetTarget::maximum_target`] is valid until the next [`SetTarget`] message is sent and is
/// applicable for all jobs received on the channel in the future or already received with flag
/// `future_job=true`.
///
/// When this message is sent to a group channel, the maximum target is applicable to all channels
/// in the group.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SetTarget<'decoder> {
    /// Channel identifier.
    pub channel_id: u32,
    /// Maximum value of produced hash that will be accepted by a upstream to accept shares.
    pub maximum_target: U256<'decoder>,
}

impl fmt::Display for SetTarget<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetTarget(channel_id={}, maximum_target={})",
            self.channel_id, self.maximum_target
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/submit_shares.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255, B032};
use core::convert::TryInto;

/// Message used by downstream to send result of its hashing work to an upstream.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SubmitSharesStandard {
    /// Channel identification.
    pub channel_id: u32,
    /// Unique sequential identifier of the submit within the channel.
    pub sequence_number: u32,
    /// Identifier of the job as provided by [`NewMiningJob`] or [`NewExtendedMiningJob`] message.
    ///
    /// [`NewMiningJob`]: crate::NewMiningJob
    /// [`NewExtendedMiningJob`]: crate::NewExtendedMiningJob
    pub job_id: u32,
    /// Nonce leading to the hash being submitted.
    pub nonce: u32,
    /// The `nTime` field in the block header. This must be greater than or equal to the
    /// `header_timestamp` field in the latest [`SetNewPrevHash`] message and lower than or equal
    /// to that value plus the number of seconds since the receipt of that message.
    ///
    /// [`SetNewPrevHash`]: crate::SetNewPrevHash
    pub ntime: u32,
    /// Full `nVersion` field.
    pub version: u32,
}

impl fmt::Display for SubmitSharesStandard {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SubmitSharesStandard(channel_id={}, sequence_number={}, job_id={}, nonce={}, ntime={}, version={})",
            self.channel_id, self.sequence_number, self.job_id, self.nonce, self.ntime, self.version
        )
    }
}

/// Message used by downstream to send result of its hashing work to an upstream.
///
/// The message is the same as [`SubmitShares`], but with an additional field,
/// [`SubmitSharesExtended::extranonce`].
///
/// Only relevant for Extended Channels.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SubmitSharesExtended<'decoder> {
    /// Channel identification.
    pub channel_id: u32,
    /// Unique sequential identifier of the submit within the channel.
    pub sequence_number: u32,
    /// Identifier of the job as provided by [`NewMiningJob`] or [`NewExtendedMiningJob`] message.
    ///
    /// [`NewMiningJob`]: crate::NewMiningJob
    /// [`NewExtendedMiningJob`]: crate::NewExtendedMiningJob
    pub job_id: u32,
    /// Nonce leading to the hash being submitted.
    pub nonce: u32,
    /// The nTime field in the block header. This must be greater than or equal to the
    /// `header_timestamp` field in the latest [`SetNewPrevHash`] message and lower than or equal
    /// to that value plus the number of seconds since the receipt of that message.
    ///
    /// [`SetNewPrevHash`]: crate::SetNewPrevHash
    pub ntime: u32,
    /// Full nVersion field.
    pub version: u32,
    /// Extranonce bytes which need to be added to the coinbase tx to form a fully valid submission
    /// (`full coinbase = coinbase_tx_prefix + extranonce_prefix + extranonce +
    /// coinbase_tx_suffix`).
    ///
    /// The size of the provided extranonce must be equal to the negotiated extranonce size from
    /// channel opening flow.
    pub extranonce: B032<'decoder>,
}

impl fmt::Display for SubmitSharesExtended<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SubmitSharesExtended(channel_id={}, sequence_number={}, job_id={}, nonce={}, ntime={}, version={}, extranonce={})",
            self.channel_id, self.sequence_number, self.job_id, self.nonce, self.ntime, self.version, self.extranonce
        )
    }
}

/// Message used by upstream to accept [`SubmitSharesStandard`] or [`SubmitSharesExtended`].
///
/// Because it is a common case that shares submission is successful, this response can be provided
/// for multiple [`SubmitShare`] messages aggregated together.
///
/// The upstream doesnt have to double check that the sequence numbers sent by a downstream are
/// actually increasing. It can use the last one received when sending a response. It is the
/// downstreams responsibility to keep the sequence numbers correct/useful.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SubmitSharesSuccess {
    /// Channel identifier.
    pub channel_id: u32,
    /// Most recent sequence number with a correct result.
    pub last_sequence_number: u32,
    /// Count of new submits acknowledged within this batch.
    pub new_submits_accepted_count: u32,
    /// Sum of shares acknowledged within this batch.
    pub new_shares_sum: u64,
}

impl fmt::Display for SubmitSharesSuccess {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SubmitSharesSuccess(channel_id={}, last_sequence_number={}, new_submits_accepted_count={}, new_shares_sum={})",
            self.channel_id, self.last_sequence_number, self.new_submits_accepted_count, self.new_shares_sum
        )
    }
}

/// Message used by upstream to reject [`SubmitSharesStandard`] or [`SubmitSharesExtended`].
///
/// In case the upstream is not able to immediately validate the submission, the error is sent as
/// soon as the result is known. This delayed validation can occur when a miner gets faster
/// updates about a new `prevhash` than the upstream does.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct SubmitSharesError<'decoder> {
    /// Channel identification.
    pub channel_id: u32,
    /// Unique sequential identifier of the submit within the channel.
    pub sequence_number: u32,
    /// Rejection reason.
    ///
    /// Possible error codes:
    ///
    /// - invalid-channel-id
    /// - stale-share
    /// - difficulty-too-low
    /// - invalid-job-id
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for SubmitSharesError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SubmitSharesError(channel_id={}, sequence_number={}, error_code={})",
            self.channel_id, self.sequence_number, self.error_code
        )
    }
}

impl SubmitSharesError<'_> {
    pub fn invalid_channel_error_code() -> &'static str {
        "invalid-channel-id"
    }
    pub fn stale_share_error_code() -> &'static str {
        "stale-share"
    }
    pub fn difficulty_too_low_error_code() -> &'static str {
        "difficulty-too-low"
    }
    pub fn invalid_job_id_error_code() -> &'static str {
        "invalid-job-id"
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/mining/src/update_channel.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize, Str0255, U256};
use core::convert::TryInto;

/// Message used by downstream to notify an upstream about changes on a specified channel.
///
/// If a downstream performs device/connection aggregation (i.e. it is a proxy), it must send this
/// message when downstream channels change.
///
/// Only relevant for Extended Channels.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct UpdateChannel<'decoder> {
    /// Channel identification.
    pub channel_id: u32,
    /// Expected hash rate of the device (or cumulative hashrate on the channel if multiple devices
    /// are connected downstream) in h/s.
    ///
    /// Depending on upstreams target setting policy, this value can be used for setting a
    /// reasonable target for the channel.
    ///
    /// Proxy must send 0.0f when there are no mining devices connected yet.
    pub nominal_hash_rate: f32,
    /// As there can be some delay between [`UpdateChannel`] and corresponding [`SetTarget`]
    /// messages, based on new job readiness on the server, this field is understood as
    /// downstreams request.
    ///
    /// When maximum target is smaller than currently used maximum target for the channel,
    /// upstream node must reflect the downstreamss request (and send appropriate [`SetTarget`]
    /// message).
    ///
    /// Upstream can change maximum target by sending [`SetTarget`] message.
    ///
    /// [`SetTarget`]: crate::SetTarget
    pub maximum_target: U256<'decoder>,
}

impl fmt::Display for UpdateChannel<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            " UpdateChannel(channel_id={}, nominal_hash_rate={}, maximum_target={})",
            self.channel_id, self.nominal_hash_rate, self.maximum_target
        )
    }
}

/// Message used by upstream to notify downstream about an error in the [`UpdateChannel`] message.
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct UpdateChannelError<'decoder> {
    /// Channel identification.
    pub channel_id: u32,
    /// Reason for channel update error.
    ///
    /// Possible error codes:
    /// - max-target-out-of-range
    /// - invalid-channel-id
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for UpdateChannelError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "UpdateChannelError(channel_id={}, error_code={})",
            self.channel_id, self.error_code
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/Cargo.toml">
[package]
name = "template_distribution_sv2"
version = "3.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
readme = "README.md"
description = "Sv2 template distribution subprotocol"
documentation = "https://docs.rs/template_distribution_sv2"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
binary_sv2 = { path = "../../binary-sv2", version = "^3.0.0" }
quickcheck = { version = "1.0.3", optional=true }
quickcheck_macros = { version = "1", optional=true }

[features]
prop_test = ["quickcheck"]
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/README.md">
# template_distribution_sv2

[![crates.io](https://img.shields.io/crates/v/template_distribution_sv2.svg)](https://crates.io/crates/template_distribution_sv2)
[![docs.rs](https://docs.rs/template_distribution_sv2/badge.svg)](https://docs.rs/template_distribution_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg)](https://app.codecov.io/gh/stratum-mining/stratum/tree/main/protocols%2Fv2%2Ftemplate_distribution_sv2)

`template_distribution_sv2` is a Rust `#![no_std]` crate that implements a set of messages defined in the
Template Distribution Protocol of Stratum V2. The Template Distribution protocol can be used to
receive updates of the block templates to use in mining.

For further information about the messages, please refer to [Stratum V2 documentation - Job Distribution](https://stratumprotocol.org/specification/07-Template-Distribution-Protocol/).

## Build Options

This crate can be built with the following features:
- `prop_test`: Enables support for property testing.

## Usage

To include this crate in your project, run:

```bash
$ cargo add template_distribution_sv2
```
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/coinbase_output_constraints.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{binary_codec_sv2, Deserialize, Serialize};
use core::convert::TryInto;

/// Message used by a downstream to indicate the size of the additional bytes they will need in
/// coinbase transaction outputs.
///
/// As the pool is responsible for adding coinbase transaction outputs for payouts and other uses,
/// the Template Provider will need to consider this reserved space when selecting transactions for
/// inclusion in a block(to avoid an invalid, oversized block).  Thus, this message indicates that
/// additional space in the block/coinbase transaction must be reserved for, assuming they will use
/// the entirety of this space.
///
/// The Job Declarator **must** discover the maximum serialized size of the additional outputs which
/// will be added by the pools it intends to use this work. It then **must** communicate the sum of
/// such size to the Template Provider via this message.
///
/// The Template Provider **must not** provide [`NewTemplate`] messages which would represent
/// consensus-invalid blocks once this additional size  along with a maximally-sized (100 byte)
/// coinbase field  is added. Further, the Template Provider **must** consider the maximum
/// additional bytes required in the output count variable-length integer in the coinbase
/// transaction when complying with the size limits.
///
/// [`NewTemplate`]: crate::NewTemplate
#[derive(Serialize, Deserialize, Copy, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct CoinbaseOutputConstraints {
    /// Additional serialized bytes needed in coinbase transaction outputs.
    pub coinbase_output_max_additional_size: u32,
    /// Additional sigops needed in coinbase transaction outputs.
    pub coinbase_output_max_additional_sigops: u16,
}

impl fmt::Display for CoinbaseOutputConstraints {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "CoinbaseOutputConstraints(coinbase_output_max_additional_size: {}, coinbase_output_max_additional_sigops: {})",
            self.coinbase_output_max_additional_size,
            self.coinbase_output_max_additional_sigops
        )
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/lib.rs">
//! # Stratum V2 Template Distribution Protocol Messages Crate
//!
//!
//! `template_distribution_sv2` is a Rust crate that implements a set of messages defined in the
//! Template Distribution Protocol of Stratum V2. The Template Distribution protocol can be used
//! to receive updates of the block templates to use in mining.
//!
//! ## Build Options
//! This crate can be built with the following features:
//! - `std`: Enables support for standard library features.
//! - `prop_test`: Enables support for property testing.
//!
//! For further information about the messages, please refer to [Stratum V2 documentation - Job
//! Distribution](https://stratumprotocol.org/specification/07-Template-Distribution-Protocol/).

#![no_std]

extern crate alloc;

#[cfg(feature = "prop_test")]
use alloc::vec;
#[cfg(feature = "prop_test")]
use core::convert::TryInto;
#[cfg(feature = "prop_test")]
use quickcheck::{Arbitrary, Gen};

mod coinbase_output_constraints;
mod new_template;
mod request_transaction_data;
mod set_new_prev_hash;
mod submit_solution;

pub use coinbase_output_constraints::CoinbaseOutputConstraints;
pub use new_template::{CNewTemplate, NewTemplate};
pub use request_transaction_data::{
    CRequestTransactionDataError, CRequestTransactionDataSuccess, RequestTransactionData,
    RequestTransactionDataError, RequestTransactionDataSuccess,
};
pub use set_new_prev_hash::{CSetNewPrevHash, SetNewPrevHash};
pub use submit_solution::{CSubmitSolution, SubmitSolution};

// Template Distribution Protocol message types.
pub const MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS: u8 = 0x70;
pub const MESSAGE_TYPE_NEW_TEMPLATE: u8 = 0x71;
pub const MESSAGE_TYPE_SET_NEW_PREV_HASH: u8 = 0x72;
pub const MESSAGE_TYPE_REQUEST_TRANSACTION_DATA: u8 = 0x73;
pub const MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS: u8 = 0x74;
pub const MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR: u8 = 0x75;
pub const MESSAGE_TYPE_SUBMIT_SOLUTION: u8 = 0x76;

// For the Template Distribution protocol, the channel bit is always unset.
pub const CHANNEL_BIT_COINBASE_OUTPUT_CONSTRAINTS: bool = false;
pub const CHANNEL_BIT_NEW_TEMPLATE: bool = false;
pub const CHANNEL_BIT_SET_NEW_PREV_HASH: bool = false;
pub const CHANNEL_BIT_REQUEST_TRANSACTION_DATA: bool = false;
pub const CHANNEL_BIT_REQUEST_TRANSACTION_DATA_SUCCESS: bool = false;
pub const CHANNEL_BIT_REQUEST_TRANSACTION_DATA_ERROR: bool = false;
pub const CHANNEL_BIT_SUBMIT_SOLUTION: bool = false;

/// Exports the [`CoinbaseOutputConstraints`] struct to C.
#[no_mangle]
pub extern "C" fn _c_export_coinbase_out(_a: CoinbaseOutputConstraints) {}

/// Exports the [`RequestTransactionData`] struct to C.
#[no_mangle]
pub extern "C" fn _c_export_req_tx_data(_a: RequestTransactionData) {}

#[cfg(feature = "prop_test")]
impl NewTemplate<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let mut coinbase_prefix_gen = Gen::new(255);
        let mut coinbase_prefix: vec::Vec<u8> = vec::Vec::new();
        coinbase_prefix.resize_with(255, || u8::arbitrary(&mut coinbase_prefix_gen));
        let coinbase_prefix: binary_sv2::B0255 = coinbase_prefix.try_into().unwrap();

        let mut coinbase_tx_outputs_gen = Gen::new(64);
        let mut coinbase_tx_outputs: vec::Vec<u8> = vec::Vec::new();
        coinbase_tx_outputs.resize_with(64, || u8::arbitrary(&mut coinbase_tx_outputs_gen));
        let coinbase_tx_outputs: binary_sv2::B064K = coinbase_tx_outputs.try_into().unwrap();

        let mut merkle_path_inner_gen = Gen::new(32);
        let mut merkle_path_inner: vec::Vec<u8> = vec::Vec::new();
        merkle_path_inner.resize_with(32, || u8::arbitrary(&mut merkle_path_inner_gen));
        let merkle_path_inner: binary_sv2::U256 = merkle_path_inner.try_into().unwrap();

        let merkle_path: binary_sv2::Seq0255<binary_sv2::U256> = vec![merkle_path_inner].into();
        NewTemplate {
            template_id: u64::arbitrary(g),
            future_template: bool::arbitrary(g),
            version: u32::arbitrary(g),
            coinbase_tx_version: u32::arbitrary(g),
            coinbase_prefix,
            coinbase_tx_input_sequence: u32::arbitrary(g),
            coinbase_tx_value_remaining: u64::arbitrary(g),
            coinbase_tx_outputs_count: u32::arbitrary(g),
            coinbase_tx_outputs,
            coinbase_tx_locktime: u32::arbitrary(g),
            merkle_path,
        }
    }
}
#[cfg(feature = "prop_test")]
impl CoinbaseOutputConstraints {
    pub fn from_gen(g: &mut Gen) -> Self {
        CoinbaseOutputConstraints {
            coinbase_output_max_additional_size: u32::arbitrary(g).try_into().unwrap(),
            coinbase_output_max_additional_sigops: u16::arbitrary(g).try_into().unwrap(),
        }
    }
}

#[cfg(feature = "prop_test")]
impl RequestTransactionData {
    pub fn from_gen(g: &mut Gen) -> Self {
        RequestTransactionData {
            template_id: u64::arbitrary(g).try_into().unwrap(),
        }
    }
}

#[cfg(feature = "prop_test")]
impl RequestTransactionDataError<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let mut error_code_gen = Gen::new(255);
        let mut error_code: vec::Vec<u8> = vec::Vec::new();
        error_code.resize_with(255, || u8::arbitrary(&mut error_code_gen));
        let error_code: binary_sv2::Str0255 = error_code.try_into().unwrap();

        RequestTransactionDataError {
            template_id: u64::arbitrary(g).try_into().unwrap(),
            error_code,
        }
    }
}

#[cfg(feature = "prop_test")]
impl RequestTransactionDataSuccess<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let excess_data: binary_sv2::B064K = vec::Vec::<u8>::arbitrary(g).try_into().unwrap();
        let transaction_list_inner = binary_sv2::B016M::from_gen(g);
        let transaction_list: binary_sv2::Seq064K<binary_sv2::B016M> =
            vec![transaction_list_inner].into();

        RequestTransactionDataSuccess {
            template_id: u64::arbitrary(g).try_into().unwrap(),
            excess_data,
            transaction_list,
        }
    }
}

#[cfg(feature = "prop_test")]
impl SetNewPrevHash<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let prev_hash = binary_sv2::U256::from_gen(g);
        let target = binary_sv2::U256::from_gen(g);
        SetNewPrevHash {
            template_id: u64::arbitrary(g).try_into().unwrap(),
            prev_hash,
            header_timestamp: u32::arbitrary(g).try_into().unwrap(),
            n_bits: u32::arbitrary(g).try_into().unwrap(),
            target,
        }
    }
}

#[cfg(feature = "prop_test")]
impl SubmitSolution<'static> {
    pub fn from_gen(g: &mut Gen) -> Self {
        let coinbase_tx: binary_sv2::B064K = vec::Vec::<u8>::arbitrary(g).try_into().unwrap();
        SubmitSolution {
            template_id: u64::arbitrary(g).try_into().unwrap(),
            version: u32::arbitrary(g).try_into().unwrap(),
            header_timestamp: u32::arbitrary(g).try_into().unwrap(),
            header_nonce: u32::arbitrary(g).try_into().unwrap(),
            coinbase_tx,
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/new_template.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{
    binary_codec_sv2::{self, free_vec, free_vec_2, CVec, CVec2},
    Deserialize, Error, Seq0255, Serialize, B0255, B064K, U256,
};
use core::convert::TryInto;

/// Message used by an upstream(Template Provider) to provide a new template for downstream to mine
/// on.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct NewTemplate<'decoder> {
    /// Upstreams identification of the template.
    ///
    /// Should be strictly increasing.
    pub template_id: u64,
    /// If `True`, the template is intended for future [`crate::SetNewPrevHash`] message sent on
    /// the channel.
    ///
    /// If `False`, the job relates to the last sent [`crate::SetNewPrevHash`] message on the
    /// channel and the miner should start to work on the job immediately.
    pub future_template: bool,
    /// Valid header version field that reflects the current network consensus.
    ///
    /// The general purpose bits, as specified in
    /// [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki), can be freely
    /// manipulated by the downstream node.
    ///
    /// The downstream **must not** rely on the upstream to set the
    /// [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki) bits to any
    /// particular value.
    pub version: u32,
    /// The coinbase transaction `nVersion` field.
    pub coinbase_tx_version: u32,
    /// Up to 8 bytes (not including the length byte) which are to be placed at the beginning of
    /// the coinbase field in the coinbase transaction.
    pub coinbase_prefix: B0255<'decoder>,
    /// The coinbase transaction inputs `nSequence` field.
    pub coinbase_tx_input_sequence: u32,
    /// The value, in satoshis, available for spending in coinbase outputs added by the downstream.
    ///
    /// Includes both transaction fees and block subsidy.
    pub coinbase_tx_value_remaining: u64,
    /// The number of transaction outputs included in [`NewTemplate::coinbase_tx_outputs`].
    pub coinbase_tx_outputs_count: u32,
    /// Bitcoin transaction outputs to be included as the last outputs in the coinbase transaction.
    ///
    /// Note that those bytes will appear as is at the end of the coinbase transaction.
    pub coinbase_tx_outputs: B064K<'decoder>,
    /// The `locktime` field in the coinbase transaction.
    pub coinbase_tx_locktime: u32,
    /// Merkle path hashes ordered from deepest.
    pub merkle_path: Seq0255<'decoder, U256<'decoder>>,
}

impl fmt::Display for NewTemplate<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "NewTemplate(template_id: {}, future_template: {}, version: {}, coinbase_tx_version: {}, \
             coinbase_prefix: {}, coinbase_tx_input_sequence: {}, coinbase_tx_value_remaining: {}, \
             coinbase_tx_outputs_count: {}, coinbase_tx_outputs: {}, coinbase_tx_locktime: {}, \
             merkle_path: {})",
            self.template_id,
            self.future_template,
            self.version,
            self.coinbase_tx_version,
            self.coinbase_prefix,
            self.coinbase_tx_input_sequence,
            self.coinbase_tx_value_remaining,
            self.coinbase_tx_outputs_count,
            self.coinbase_tx_outputs,
            self.coinbase_tx_locktime,
            self.merkle_path
        )
    }
}

/// C representation of [`NewTemplate`].
#[repr(C)]
pub struct CNewTemplate {
    template_id: u64,
    future_template: bool,
    version: u32,
    coinbase_tx_version: u32,
    coinbase_prefix: CVec,
    coinbase_tx_input_sequence: u32,
    coinbase_tx_value_remaining: u64,
    coinbase_tx_outputs_count: u32,
    coinbase_tx_outputs: CVec,
    coinbase_tx_locktime: u32,
    merkle_path: CVec2,
}

/// Drops the [`CNewTemplate`] object.
#[no_mangle]
pub extern "C" fn free_new_template(s: CNewTemplate) {
    drop(s)
}

impl Drop for CNewTemplate {
    fn drop(&mut self) {
        free_vec(&mut self.coinbase_prefix);
        free_vec(&mut self.coinbase_tx_outputs);
        free_vec_2(&mut self.merkle_path);
    }
}

impl<'a> From<NewTemplate<'a>> for CNewTemplate {
    fn from(v: NewTemplate<'a>) -> Self {
        Self {
            template_id: v.template_id,
            future_template: v.future_template,
            version: v.version,
            coinbase_tx_version: v.coinbase_tx_version,
            coinbase_prefix: v.coinbase_prefix.into(),
            coinbase_tx_input_sequence: v.coinbase_tx_input_sequence,
            coinbase_tx_value_remaining: v.coinbase_tx_value_remaining,
            coinbase_tx_outputs_count: v.coinbase_tx_outputs_count,
            coinbase_tx_outputs: v.coinbase_tx_outputs.into(),
            coinbase_tx_locktime: v.coinbase_tx_locktime,
            merkle_path: v.merkle_path.into(),
        }
    }
}

impl<'a> CNewTemplate {
    /// Converts from C to Rust representation.
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<NewTemplate<'a>, Error> {
        let coinbase_prefix: B0255 = self.coinbase_prefix.as_mut_slice().try_into()?;

        let merkle_path_ = self.merkle_path.as_mut_slice();
        let mut merkle_path: Vec<U256> = Vec::new();
        for cvec in merkle_path_ {
            merkle_path.push(cvec.as_mut_slice().try_into()?);
        }
        let merkle_path = Seq0255::new(merkle_path)?;

        let coinbase_tx_outputs = self.coinbase_tx_outputs.as_mut_slice().try_into()?;

        Ok(NewTemplate {
            template_id: self.template_id,
            future_template: self.future_template,
            version: self.version,
            coinbase_tx_version: self.coinbase_tx_version,
            coinbase_prefix,
            coinbase_tx_input_sequence: self.coinbase_tx_input_sequence,
            coinbase_tx_value_remaining: self.coinbase_tx_value_remaining,
            coinbase_tx_outputs_count: self.coinbase_tx_outputs_count,
            coinbase_tx_outputs,
            coinbase_tx_locktime: self.coinbase_tx_locktime,
            merkle_path,
        })
    }
}

#[cfg(feature = "prop_test")]
use quickcheck::{Arbitrary, Gen};

#[cfg(feature = "prop_test")]
use alloc::vec;

#[cfg(feature = "prop_test")]
impl Arbitrary for NewTemplate<'static> {
    fn arbitrary(g: &mut Gen) -> NewTemplate<'static> {
        let coinbase_tx_version = (u32::arbitrary(g) % 2) + 1;
        let mut coinbase_prefix = vec::Vec::new();
        let coinbase_prefix_len = match coinbase_tx_version {
            1 => u8::arbitrary(g) as usize,
            2 => u8::arbitrary(g).checked_add(4).unwrap_or(4) as usize,
            _ => panic!(),
        };
        for _ in 0..coinbase_prefix_len {
            coinbase_prefix.push(u8::arbitrary(g))
        }
        let coinbase_prefix: binary_sv2::B0255 = coinbase_prefix.try_into().unwrap();

        // TODO uncomment when node provided outputs are supported
        //let mut coinbase_tx_outputs = vec::Vec::new();
        //let coinbase_tx_outputs_len = u16::arbitrary(g) as usize;
        //for _ in 0..coinbase_tx_outputs_len {
        //    coinbase_tx_outputs.push(u8::arbitrary(g))
        //};
        //coinbase_tx_outputs.resize(coinbase_tx_outputs.len() - coinbase_tx_outputs.len() % 36,0);
        //let coinbase_tx_outputs: binary_sv2::B064K = coinbase_tx_outputs.try_into().unwrap();

        let mut merkle_path = vec::Vec::new();
        let merkle_path_len = u8::arbitrary(g);
        for _ in 0..merkle_path_len {
            let mut path = Vec::new();
            for _ in 0..32 {
                path.push(u8::arbitrary(g));
            }
            let path: binary_sv2::U256 = path.try_into().unwrap();
            merkle_path.push(path);
        }
        let merkle_path: binary_sv2::Seq0255<binary_sv2::U256> = merkle_path.into();

        NewTemplate {
            template_id: u64::arbitrary(g) % u64::MAX,
            future_template: bool::arbitrary(g),
            version: u32::arbitrary(g),
            coinbase_tx_version,
            coinbase_prefix,
            coinbase_tx_input_sequence: u32::arbitrary(g),
            coinbase_tx_value_remaining: u64::arbitrary(g),
            // the belows should be used when node provided outputs are enabled
            //coinbase_tx_outputs_count: coinbase_tx_outputs.len().checked_div(36).unwrap_or(0) as
            // u32, coinbase_tx_outputs,
            coinbase_tx_outputs_count: 0,
            coinbase_tx_outputs: Vec::new().try_into().unwrap(),
            coinbase_tx_locktime: u32::arbitrary(g),
            merkle_path,
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/request_transaction_data.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{
    binary_codec_sv2::{self, free_vec, free_vec_2, CVec, CVec2},
    Deserialize, Error, Seq064K, Serialize, Str0255, B016M, B064K,
};
use core::convert::TryInto;

/// Message used by a downstream to request data about all transactions in a block template.
///
/// Data includes the full transaction data and any additional data required to block validation.
///
/// Note that the coinbase transaction is excluded from this data.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq, Copy)]
#[repr(C)]
pub struct RequestTransactionData {
    /// Identifier of the template that the downstream node is requesting transaction data for.
    ///
    /// This must be identical to previously exchanged [`crate::NewTemplate::template_id`].
    pub template_id: u64,
}

impl fmt::Display for RequestTransactionData {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "RequestTransactionData(template_id: {})",
            self.template_id
        )
    }
}

/// Message used by an upstream(Template Provider) to respond successfully to a
/// [`RequestTransactionData`] message.
///
/// A response to [`RequestTransactionData`] which contains the set of full transaction data and
/// excess data required for block validation. For practical purposes, the excess data is usually
/// the SegWit commitment, however the Job Declarator **must not** have any assumptions about it.
///
/// Note that the transaction data **must** be treated as opaque blobs and **must** include any
/// SegWit or other data which the downstream may require to verify the transaction. For practical
/// purposes, the transaction data is likely the witness-encoded transaction today. However, to
/// ensure backward compatibility, the transaction data **may** be encoded in a way that is
/// different from the consensus serialization of Bitcoin transactions.
///
/// The [`RequestTransactionDataSuccess`] sender **must** ensure that provided data is forward and
/// backward compatible. This way the receiver of the data can interpret it, even in the face of
/// new, consensus-optional data.  This allows significantly more flexibility on both the
/// [`RequestTransactionDataSuccess`] generating and interpreting sides during upgrades, at the
/// cost of breaking some potential optimizations which would require version negotiation to
/// provide support for previous versions.
///
/// Having some method of negotiating the specific format of transactions between the Template
/// Provider and the downstream would be helpful but overly burdensome, thus the above requirements
/// are made explicit.
///
/// As a result, and as a non-normative suggested implementation for Bitcoin Core, this implies
/// that additional consensus-optional data appended at the end of transaction data will simply be
/// ignored by versions which do not understand it.
///
/// To work around the limitation of not being able to negotiate e.g. a transaction compression
/// scheme, the format of the opaque data in [`RequestTransactionDataSuccess`] messages **may** be
/// changed in a non-compatible way at the time of fork activation, given sufficient time from
/// code-release to activation and there being in protocol(Template Declaration) signaling of
/// support for the new fork (e.g. for soft-forks activated using [BIP 9]).
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct RequestTransactionDataSuccess<'decoder> {
    /// The template_id corresponding to a NewTemplate/RequestTransactionData message.
    pub template_id: u64,
    /// Extra data which the Pool may require to validate the work.
    pub excess_data: B064K<'decoder>,
    /// The transaction data, serialized as a series of B0_16M byte arrays.
    pub transaction_list: Seq064K<'decoder, B016M<'decoder>>,
}

impl fmt::Display for RequestTransactionDataSuccess<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "RequestTransactionDataSuccess(template_id: {}, excess_data: {}, transaction_list: {})",
            self.template_id, self.excess_data, self.transaction_list
        )
    }
}

/// C representation of [`RequestTransactionDataSuccess`].
#[repr(C)]
pub struct CRequestTransactionDataSuccess {
    template_id: u64,
    excess_data: CVec,
    transaction_list: CVec2,
}

impl<'a> CRequestTransactionDataSuccess {
    /// Converts C struct to Rust struct.
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<RequestTransactionDataSuccess<'a>, Error> {
        let excess_data: B064K = self.excess_data.as_mut_slice().try_into()?;
        let transaction_list_ = self.transaction_list.as_mut_slice();
        let mut transaction_list: Vec<B016M> = Vec::new();
        for cvec in transaction_list_ {
            transaction_list.push(cvec.as_mut_slice().try_into()?);
        }
        let transaction_list = Seq064K::new(transaction_list)?;
        Ok(RequestTransactionDataSuccess {
            template_id: self.template_id,
            excess_data,
            transaction_list,
        })
    }
}

/// Drops the CRequestTransactionDataSuccess object.
#[no_mangle]
pub extern "C" fn free_request_tx_data_success(s: CRequestTransactionDataSuccess) {
    drop(s)
}

impl Drop for CRequestTransactionDataSuccess {
    fn drop(&mut self) {
        free_vec(&mut self.excess_data);
        free_vec_2(&mut self.transaction_list);
    }
}

impl<'a> From<RequestTransactionDataSuccess<'a>> for CRequestTransactionDataSuccess {
    fn from(v: RequestTransactionDataSuccess<'a>) -> Self {
        Self {
            template_id: v.template_id,
            excess_data: v.excess_data.into(),
            transaction_list: v.transaction_list.into(),
        }
    }
}

/// Message used by an upstream(Template Provider) to respond with an error to a
/// [`RequestTransactionData`] message.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct RequestTransactionDataError<'decoder> {
    /// Identifier of the template that the downstream node is requesting transaction data for.
    pub template_id: u64,
    /// Reason why no transaction data has been provided.
    ///
    /// Possible error codes:
    /// - template-id-not-found
    pub error_code: Str0255<'decoder>,
}

impl fmt::Display for RequestTransactionDataError<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "RequestTransactionDataError(template_id: {}, error_code: {})",
            self.template_id,
            self.error_code.as_utf8_or_hex()
        )
    }
}

/// C representation of [`RequestTransactionDataError`].
#[repr(C)]
pub struct CRequestTransactionDataError {
    template_id: u64,
    error_code: CVec,
}

impl<'a> CRequestTransactionDataError {
    /// Converts C struct to Rust struct.
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<RequestTransactionDataError<'a>, Error> {
        let error_code: Str0255 = self.error_code.as_mut_slice().try_into()?;
        Ok(RequestTransactionDataError {
            template_id: self.template_id,
            error_code,
        })
    }
}

/// Drops the CRequestTransactionDataError object.
#[no_mangle]
pub extern "C" fn free_request_tx_data_error(s: CRequestTransactionDataError) {
    drop(s)
}

impl Drop for CRequestTransactionDataError {
    fn drop(&mut self) {
        free_vec(&mut self.error_code);
    }
}

impl<'a> From<RequestTransactionDataError<'a>> for CRequestTransactionDataError {
    fn from(v: RequestTransactionDataError<'a>) -> Self {
        Self {
            template_id: v.template_id,
            error_code: v.error_code.into(),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/set_new_prev_hash.rs">
use alloc::vec::Vec;
use binary_sv2::{
    binary_codec_sv2::{self, free_vec, CVec},
    Deserialize, Error, Serialize, U256,
};
use core::{convert::TryInto, fmt};

/// Message used by an upstream(Template Provider) to indicate the latest block header hash
/// to mine on.
///
/// Upon validating a new best block, the upstream **must** immediately send this message.
///
/// If a [`crate::NewTemplate`] message has previously been sent with the
/// [`crate::NewTemplate::future_template`] flag set, the [`SetNewPrevHash::template_id`] field
/// **should** be set to the [`crate::NewTemplate::template_id`].
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct SetNewPrevHash<'decoder> {
    /// Identifier of the template to mine on.
    ///
    /// This must be identical to previously sent [`crate::NewTemplate`] message.
    pub template_id: u64,
    /// Previous blocks hash, as it must appear in the next blocks header.
    pub prev_hash: U256<'decoder>,
    /// `nTime` field in the block header at which the client should start (usually current time).
    ///
    /// This is **not** the minimum valid `nTime` value.
    pub header_timestamp: u32,
    /// Block header field.
    pub n_bits: u32,
    /// The maximum double-SHA256 hash value which would represent a valid block. Note that this
    /// may be lower than the target implied by nBits in several cases, including weak-block based
    /// block propagation.
    pub target: U256<'decoder>,
}

impl fmt::Display for SetNewPrevHash<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SetNewPrevHash {{ template_id: {}, prev_hash: {}, header_timestamp: {}, n_bits: {}, target: {} }}",
            self.template_id,
            self.prev_hash,
            self.header_timestamp,
            self.n_bits,
            self.target
        )
    }
}

/// C representation of [`SetNewPrevHash`].
#[repr(C)]
pub struct CSetNewPrevHash {
    template_id: u64,
    prev_hash: CVec,
    header_timestamp: u32,
    n_bits: u32,
    target: CVec,
}

impl<'a> CSetNewPrevHash {
    /// Converts CSetNewPrevHash(C representation) to SetNewPrevHash(Rust representation).
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<SetNewPrevHash<'a>, Error> {
        let prev_hash: U256 = self.prev_hash.as_mut_slice().try_into()?;
        let target: U256 = self.target.as_mut_slice().try_into()?;

        Ok(SetNewPrevHash {
            template_id: self.template_id,
            prev_hash,
            header_timestamp: self.header_timestamp,
            n_bits: self.n_bits,
            target,
        })
    }
}

/// Drops the CSetNewPrevHash object.
#[no_mangle]
pub extern "C" fn free_set_new_prev_hash(s: CSetNewPrevHash) {
    drop(s)
}

impl Drop for CSetNewPrevHash {
    fn drop(&mut self) {
        free_vec(&mut self.target);
    }
}

impl<'a> From<SetNewPrevHash<'a>> for CSetNewPrevHash {
    fn from(v: SetNewPrevHash<'a>) -> Self {
        Self {
            template_id: v.template_id,
            prev_hash: v.prev_hash.into(),
            header_timestamp: v.header_timestamp,
            n_bits: v.n_bits,
            target: v.target.into(),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/subprotocols/template-distribution/src/submit_solution.rs">
use alloc::{fmt, vec::Vec};
use binary_sv2::{
    binary_codec_sv2::{self, free_vec, CVec},
    Deserialize, Error, Serialize, B064K,
};
use core::convert::TryInto;

/// Message used by a downstream to submit a successful solution to a previously provided template.
///
/// The downstream is expected to send this message in addition to the `SubmitSolution` message
/// from the Mining Protocol in order to propagate the solution to the Bitcoin network as soon as
/// possible.
///
/// Upon receiving this message, upstream(Template Provider) **must** immediately construct the
/// corresponding full block and attempt to propagate it to the Bitcoin network.
#[derive(Serialize, Deserialize, Debug, Clone, PartialEq, Eq)]
pub struct SubmitSolution<'decoder> {
    /// Identifies the template to which this solution corresponds.
    ///
    /// This is acquired from the [`crate::NewTemplate`] message.
    pub template_id: u64,
    /// Version field in the block header.
    ///
    /// Bits not defined by
    /// [BIP320](https://github.com/bitcoin/bips/blob/master/bip-0320.mediawiki) as additional
    /// nonce **must** be the same as they appear in the `NewMiningJob` or `NewExtendedMiningJob`
    /// message, other bits may be set to any value.
    pub version: u32,
    /// nTime field in the block header.
    ///
    /// This **must** be greater than or equal to previously received
    /// [`crate::SetNewPrevHash::header_timestamp`] and lower than or equal to that value plus the
    /// number of seconds since receiving [`crate::SetNewPrevHash`] that message.
    pub header_timestamp: u32,
    /// Nonce field in the header.
    pub header_nonce: u32,
    /// Full serialized coinbase transaction, meeting all the requirements of the `NewMiningJob` or
    /// `NewExtendedMiningJob` message.
    pub coinbase_tx: B064K<'decoder>,
}

impl fmt::Display for SubmitSolution<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(
            f,
            "SubmitSolution {{ template_id: {}, version: {}, header_timestamp: {}, header_nonce: {}, coinbase_tx: {} }}",
            self.template_id,
            self.version,
            self.header_timestamp,
            self.header_nonce,
            self.coinbase_tx
        )
    }
}

/// C representation of [`SubmitSolution`].
#[repr(C)]
pub struct CSubmitSolution {
    template_id: u64,
    version: u32,
    header_timestamp: u32,
    header_nonce: u32,
    coinbase_tx: CVec,
}

impl<'a> CSubmitSolution {
    /// Converts CSubmitSolution(C representation) to SubmitSolution(Rust representation).
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<SubmitSolution<'a>, Error> {
        let coinbase_tx: B064K = self.coinbase_tx.as_mut_slice().try_into()?;

        Ok(SubmitSolution {
            template_id: self.template_id,
            version: self.version,
            header_timestamp: self.header_timestamp,
            header_nonce: self.header_nonce,
            coinbase_tx,
        })
    }
}

/// Drops the CSubmitSolution object.
#[no_mangle]
pub extern "C" fn free_submit_solution(s: CSubmitSolution) {
    drop(s)
}

impl Drop for CSubmitSolution {
    fn drop(&mut self) {
        free_vec(&mut self.coinbase_tx);
    }
}

impl<'a> From<SubmitSolution<'a>> for CSubmitSolution {
    fn from(v: SubmitSolution<'a>) -> Self {
        Self {
            template_id: v.template_id,
            version: v.version,
            header_timestamp: v.header_timestamp,
            header_nonce: v.header_nonce,
            coinbase_tx: v.coinbase_tx.into(),
        }
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/sv2-ffi/Cargo.toml">
[package]
name = "sv2_ffi"
version = "2.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "SV2 FFI"
documentation = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
repository = "https://github.com/stratum-mining/stratum"
homepage = "https://stratumprotocol.org"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[lib]
crate-type = ["staticlib"]

[dependencies]
codec_sv2 = { path = "../codec-sv2", version = "^2.0.0" }
binary_sv2 = { path = "../binary-sv2", version = "^3.0.0" }
common_messages_sv2 = { path = "../subprotocols/common-messages", version = "^5.0.0" }
template_distribution_sv2 = { path = "../subprotocols/template-distribution", version = "^3.0.0" }

[dev-dependencies]
quickcheck = "1.0.3"
quickcheck_macros = "1"

[features]
prop_test = ["binary_sv2/prop_test", "common_messages_sv2/prop_test", "template_distribution_sv2/prop_test"]
</file>

<file path="stratum-1.4.0/protocols/v2/sv2-ffi/src/lib.rs">
use std::{
    fmt,
    fmt::{Display, Formatter},
};

use codec_sv2::{Encoder, StandardDecoder, StandardSv2Frame};
use common_messages_sv2::*;
use template_distribution_sv2::*;

use binary_sv2::{
    binary_codec_sv2::CVec,
    decodable::{DecodableField, FieldMarker},
    encodable::EncodableField,
    from_bytes, Deserialize, Error,
};

use core::convert::{TryFrom, TryInto};

#[derive(Clone, Debug)]
pub enum Sv2Message<'a> {
    CoinbaseOutputConstraints(CoinbaseOutputConstraints),
    NewTemplate(NewTemplate<'a>),
    RequestTransactionData(RequestTransactionData),
    RequestTransactionDataError(RequestTransactionDataError<'a>),
    RequestTransactionDataSuccess(RequestTransactionDataSuccess<'a>),
    SetNewPrevHash(template_distribution_sv2::SetNewPrevHash<'a>),
    SubmitSolution(SubmitSolution<'a>),
    ChannelEndpointChanged(ChannelEndpointChanged),
    SetupConnection(SetupConnection<'a>),
    SetupConnectionError(SetupConnectionError<'a>),
    SetupConnectionSuccess(SetupConnectionSuccess),
}

impl Sv2Message<'_> {
    pub fn message_type(&self) -> u8 {
        match self {
            Sv2Message::CoinbaseOutputConstraints(_) => MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS,
            Sv2Message::NewTemplate(_) => MESSAGE_TYPE_NEW_TEMPLATE,
            Sv2Message::RequestTransactionData(_) => MESSAGE_TYPE_REQUEST_TRANSACTION_DATA,
            Sv2Message::RequestTransactionDataError(_) => {
                MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR
            }
            Sv2Message::RequestTransactionDataSuccess(_) => {
                MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS
            }
            Sv2Message::SetNewPrevHash(_) => MESSAGE_TYPE_SET_NEW_PREV_HASH,
            Sv2Message::SubmitSolution(_) => MESSAGE_TYPE_SUBMIT_SOLUTION,
            Sv2Message::ChannelEndpointChanged(_) => MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
            Sv2Message::SetupConnection(_) => MESSAGE_TYPE_SETUP_CONNECTION,
            Sv2Message::SetupConnectionError(_) => MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
            Sv2Message::SetupConnectionSuccess(_) => MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        }
    }

    pub fn channel_bit(&self) -> bool {
        match self {
            Sv2Message::CoinbaseOutputConstraints(_) => CHANNEL_BIT_COINBASE_OUTPUT_CONSTRAINTS,
            Sv2Message::NewTemplate(_) => CHANNEL_BIT_NEW_TEMPLATE,
            Sv2Message::RequestTransactionData(_) => CHANNEL_BIT_REQUEST_TRANSACTION_DATA,
            Sv2Message::RequestTransactionDataError(_) => {
                CHANNEL_BIT_REQUEST_TRANSACTION_DATA_ERROR
            }
            Sv2Message::RequestTransactionDataSuccess(_) => {
                CHANNEL_BIT_REQUEST_TRANSACTION_DATA_SUCCESS
            }
            Sv2Message::SetNewPrevHash(_) => CHANNEL_BIT_SET_NEW_PREV_HASH,
            Sv2Message::SubmitSolution(_) => CHANNEL_BIT_SUBMIT_SOLUTION,
            Sv2Message::ChannelEndpointChanged(_) => CHANNEL_BIT_CHANNEL_ENDPOINT_CHANGED,
            Sv2Message::SetupConnection(_) => CHANNEL_BIT_SETUP_CONNECTION,
            Sv2Message::SetupConnectionError(_) => CHANNEL_BIT_SETUP_CONNECTION_ERROR,
            Sv2Message::SetupConnectionSuccess(_) => CHANNEL_BIT_SETUP_CONNECTION_SUCCESS,
        }
    }
}

impl Display for Sv2Message<'_> {
    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {
        write!(f, "{:?}", *self)
    }
}

#[repr(C)]
pub enum CSv2Message {
    CoinbaseOutputConstraints(CoinbaseOutputConstraints),
    NewTemplate(CNewTemplate),
    RequestTransactionData(RequestTransactionData),
    RequestTransactionDataError(CRequestTransactionDataError),
    RequestTransactionDataSuccess(CRequestTransactionDataSuccess),
    SetNewPrevHash(CSetNewPrevHash),
    SubmitSolution(CSubmitSolution),
    ChannelEndpointChanged(ChannelEndpointChanged),
    SetupConnection(CSetupConnection),
    SetupConnectionError(CSetupConnectionError),
    SetupConnectionSuccess(SetupConnectionSuccess),
}

#[no_mangle]
pub extern "C" fn drop_sv2_message(s: CSv2Message) {
    match s {
        CSv2Message::CoinbaseOutputConstraints(_) => (),
        CSv2Message::NewTemplate(a) => drop(a),
        CSv2Message::RequestTransactionData(_) => (),
        CSv2Message::RequestTransactionDataError(a) => drop(a),
        CSv2Message::RequestTransactionDataSuccess(a) => drop(a),
        CSv2Message::SetNewPrevHash(a) => drop(a),
        CSv2Message::SubmitSolution(a) => drop(a),
        CSv2Message::ChannelEndpointChanged(_) => (),
        CSv2Message::SetupConnection(a) => drop(a),
        CSv2Message::SetupConnectionError(a) => drop(a),
        CSv2Message::SetupConnectionSuccess(_) => (),
    }
}

/// This function does nothing unless there is some heap allocated data owned by the C side that
/// needs to be dropped (specifically a `CVec`). In this case, `free_vec` is used in order to drop
/// that memory.
#[no_mangle]
pub extern "C" fn drop_sv2_error(s: Sv2Error) {
    match s {
        Sv2Error::BinaryError(a) => drop(a),
        Sv2Error::CodecError(_) => (),
        Sv2Error::EncoderBusy => (),
        Sv2Error::InvalidSv2Frame => (),
        Sv2Error::MissingBytes => (),
        Sv2Error::PayloadTooBig(_) => (),
        Sv2Error::Unknown => (),
    }
}

impl<'a> From<Sv2Message<'a>> for CSv2Message {
    fn from(v: Sv2Message<'a>) -> Self {
        match v {
            Sv2Message::CoinbaseOutputConstraints(a) => Self::CoinbaseOutputConstraints(a),
            Sv2Message::NewTemplate(a) => Self::NewTemplate(a.into()),
            Sv2Message::RequestTransactionData(a) => Self::RequestTransactionData(a),
            Sv2Message::RequestTransactionDataError(a) => {
                Self::RequestTransactionDataError(a.into())
            }
            Sv2Message::RequestTransactionDataSuccess(a) => {
                Self::RequestTransactionDataSuccess(a.into())
            }
            Sv2Message::SetNewPrevHash(a) => Self::SetNewPrevHash(a.into()),
            Sv2Message::SubmitSolution(a) => Self::SubmitSolution(a.into()),
            Sv2Message::ChannelEndpointChanged(a) => Self::ChannelEndpointChanged(a),
            Sv2Message::SetupConnection(a) => Self::SetupConnection(a.into()),
            Sv2Message::SetupConnectionError(a) => Self::SetupConnectionError(a.into()),
            Sv2Message::SetupConnectionSuccess(a) => Self::SetupConnectionSuccess(a),
        }
    }
}

impl<'a> CSv2Message {
    #[allow(clippy::wrong_self_convention)]
    pub fn to_rust_rep_mut(&'a mut self) -> Result<Sv2Message<'a>, Error> {
        match self {
            CSv2Message::NewTemplate(v) => Ok(Sv2Message::NewTemplate(v.to_rust_rep_mut()?)),
            CSv2Message::SetNewPrevHash(v) => Ok(Sv2Message::SetNewPrevHash(v.to_rust_rep_mut()?)),
            CSv2Message::SubmitSolution(v) => Ok(Sv2Message::SubmitSolution(v.to_rust_rep_mut()?)),
            CSv2Message::SetupConnectionError(v) => {
                Ok(Sv2Message::SetupConnectionError(v.to_rust_rep_mut()?))
            }
            CSv2Message::SetupConnectionSuccess(v) => Ok(Sv2Message::SetupConnectionSuccess(*v)),
            CSv2Message::CoinbaseOutputConstraints(v) => {
                Ok(Sv2Message::CoinbaseOutputConstraints(*v))
            }
            CSv2Message::RequestTransactionData(v) => Ok(Sv2Message::RequestTransactionData(*v)),
            CSv2Message::RequestTransactionDataError(v) => Ok(
                Sv2Message::RequestTransactionDataError(v.to_rust_rep_mut()?),
            ),
            CSv2Message::RequestTransactionDataSuccess(v) => Ok(
                Sv2Message::RequestTransactionDataSuccess(v.to_rust_rep_mut()?),
            ),
            CSv2Message::ChannelEndpointChanged(v) => Ok(Sv2Message::ChannelEndpointChanged(*v)),
            CSv2Message::SetupConnection(v) => {
                Ok(Sv2Message::SetupConnection(v.to_rust_rep_mut()?))
            }
        }
    }
}

impl<'decoder> From<Sv2Message<'decoder>> for EncodableField<'decoder> {
    fn from(m: Sv2Message<'decoder>) -> Self {
        match m {
            Sv2Message::CoinbaseOutputConstraints(a) => a.into(),
            Sv2Message::NewTemplate(a) => a.into(),
            Sv2Message::RequestTransactionData(a) => a.into(),
            Sv2Message::RequestTransactionDataError(a) => a.into(),
            Sv2Message::RequestTransactionDataSuccess(a) => a.into(),
            Sv2Message::SetNewPrevHash(a) => a.into(),
            Sv2Message::SubmitSolution(a) => a.into(),
            Sv2Message::ChannelEndpointChanged(a) => a.into(),
            Sv2Message::SetupConnection(a) => a.into(),
            Sv2Message::SetupConnectionError(a) => a.into(),
            Sv2Message::SetupConnectionSuccess(a) => a.into(),
        }
    }
}

impl binary_sv2::GetSize for Sv2Message<'_> {
    fn get_size(&self) -> usize {
        match self {
            Sv2Message::CoinbaseOutputConstraints(a) => a.get_size(),
            Sv2Message::NewTemplate(a) => a.get_size(),
            Sv2Message::RequestTransactionData(a) => a.get_size(),
            Sv2Message::RequestTransactionDataError(a) => a.get_size(),
            Sv2Message::RequestTransactionDataSuccess(a) => a.get_size(),
            Sv2Message::SetNewPrevHash(a) => a.get_size(),
            Sv2Message::SubmitSolution(a) => a.get_size(),
            Sv2Message::ChannelEndpointChanged(a) => a.get_size(),
            Sv2Message::SetupConnection(a) => a.get_size(),
            Sv2Message::SetupConnectionError(a) => a.get_size(),
            Sv2Message::SetupConnectionSuccess(a) => a.get_size(),
        }
    }
}

impl<'decoder> Deserialize<'decoder> for Sv2Message<'decoder> {
    fn get_structure(_v: &[u8]) -> std::result::Result<Vec<FieldMarker>, binary_sv2::Error> {
        unimplemented!()
    }
    fn from_decoded_fields(
        _v: Vec<DecodableField<'decoder>>,
    ) -> std::result::Result<Self, binary_sv2::Error> {
        unimplemented!()
    }
}

impl<'a> TryFrom<(u8, &'a mut [u8])> for Sv2Message<'a> {
    type Error = Error;

    fn try_from(v: (u8, &'a mut [u8])) -> Result<Self, Self::Error> {
        let msg_type = v.0;
        match msg_type {
            MESSAGE_TYPE_SETUP_CONNECTION => {
                let message: SetupConnection<'a> = from_bytes(v.1)?;
                Ok(Sv2Message::SetupConnection(message))
            }
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS => {
                let message: SetupConnectionSuccess = from_bytes(v.1)?;
                Ok(Sv2Message::SetupConnectionSuccess(message))
            }
            MESSAGE_TYPE_SETUP_CONNECTION_ERROR => {
                let message: SetupConnectionError<'a> = from_bytes(v.1)?;
                Ok(Sv2Message::SetupConnectionError(message))
            }
            MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED => {
                let message: ChannelEndpointChanged = from_bytes(v.1)?;
                Ok(Sv2Message::ChannelEndpointChanged(message))
            }
            MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS => {
                let message: CoinbaseOutputConstraints = from_bytes(v.1)?;
                Ok(Sv2Message::CoinbaseOutputConstraints(message))
            }
            MESSAGE_TYPE_NEW_TEMPLATE => {
                let message: NewTemplate<'a> = from_bytes(v.1)?;
                Ok(Sv2Message::NewTemplate(message))
            }
            MESSAGE_TYPE_SET_NEW_PREV_HASH => {
                let message: template_distribution_sv2::SetNewPrevHash<'a> = from_bytes(v.1)?;
                Ok(Sv2Message::SetNewPrevHash(message))
            }
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA => {
                let message: RequestTransactionData = from_bytes(v.1)?;
                Ok(Sv2Message::RequestTransactionData(message))
            }
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS => {
                let message: RequestTransactionDataSuccess = from_bytes(v.1)?;
                Ok(Sv2Message::RequestTransactionDataSuccess(message))
            }
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR => {
                let message: RequestTransactionDataError = from_bytes(v.1)?;
                Ok(Sv2Message::RequestTransactionDataError(message))
            }
            MESSAGE_TYPE_SUBMIT_SOLUTION => {
                let message: SubmitSolution = from_bytes(v.1)?;
                Ok(Sv2Message::SubmitSolution(message))
            }
            _ => Err(Error::UnknownMessageType(msg_type)),
        }
    }
}

#[repr(C)]
pub enum CResult<T, E> {
    Ok(T),
    Err(E),
}

#[repr(C)]
pub enum Sv2Error {
    BinaryError(binary_sv2::CError),
    CodecError(codec_sv2::CError),
    EncoderBusy,
    InvalidSv2Frame,
    MissingBytes,
    PayloadTooBig(CVec),
    Unknown,
}

impl fmt::Display for Sv2Error {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        use Sv2Error::*;
        match self {
            BinaryError(ref e) => write!(f, "{e:?}"),
            CodecError(ref e) => write!(f, "{e:?}"),
            PayloadTooBig(ref e) => write!(f, "Payload is too big: {e:?}"),
            InvalidSv2Frame => write!(f, "Invalid Sv2 frame"),
            MissingBytes => write!(f, "Missing expected bytes"),
            EncoderBusy => write!(f, "Encoder is busy"),
            Unknown => write!(f, "Unknown error occurred"),
        }
    }
}

impl From<binary_sv2::CError> for Sv2Error {
    fn from(e: binary_sv2::CError) -> Sv2Error {
        Sv2Error::BinaryError(e)
    }
}

impl From<binary_sv2::Error> for Sv2Error {
    fn from(e: binary_sv2::Error) -> Sv2Error {
        Sv2Error::BinaryError(e.into())
    }
}

impl From<codec_sv2::CError> for Sv2Error {
    fn from(e: codec_sv2::CError) -> Sv2Error {
        Sv2Error::CodecError(e)
    }
}

#[no_mangle]
pub extern "C" fn is_ok(cresult: &CResult<CSv2Message, Sv2Error>) -> bool {
    match cresult {
        CResult::Ok(_) => true,
        CResult::Err(_) => false,
    }
}

impl<T, E> From<Result<T, E>> for CResult<T, E> {
    fn from(v: Result<T, E>) -> Self {
        match v {
            Ok(v) => Self::Ok(v),
            Err(e) => Self::Err(e),
        }
    }
}

#[derive(Debug)]
pub struct EncoderWrapper {
    encoder: Encoder<Sv2Message<'static>>,
    free: bool,
}

#[no_mangle]
pub extern "C" fn new_encoder() -> *mut EncoderWrapper {
    let encoder: Encoder<Sv2Message<'static>> = Encoder::new();
    let s = Box::new(EncoderWrapper {
        encoder,
        free: true,
    });
    Box::into_raw(s)
}

#[no_mangle]
#[allow(clippy::not_unsafe_ptr_arg_deref)]
pub extern "C" fn flush_encoder(encoder: *mut EncoderWrapper) {
    let mut encoder = unsafe { Box::from_raw(encoder) };
    encoder.free = true;
    let _ = Box::into_raw(encoder);
}

fn encode_(
    message: &'static mut CSv2Message,
    encoder: &mut EncoderWrapper,
) -> Result<CVec, Sv2Error> {
    let message: Sv2Message = message.to_rust_rep_mut()?;
    let m_type = message.message_type();
    let c_bit = message.channel_bit();
    let frame =
        StandardSv2Frame::<Sv2Message<'static>>::from_message(message.clone(), m_type, 0, c_bit)
            .ok_or(Sv2Error::PayloadTooBig(
                format!("{message}").as_bytes().into(),
            ))?;
    encoder
        .encoder
        .encode(frame)
        .map_err(|e| Sv2Error::CodecError(e.into()))
        .map(|x| x.into())
}

#[no_mangle]
#[allow(clippy::not_unsafe_ptr_arg_deref)]
pub extern "C" fn free_decoder(decoder: *mut DecoderWrapper) {
    let decoder = unsafe { Box::from_raw(decoder) };
    drop(decoder);
}

/// # Safety
#[no_mangle]
pub unsafe extern "C" fn encode(
    message: &'static mut CSv2Message,
    encoder: *mut EncoderWrapper,
) -> CResult<CVec, Sv2Error> {
    let mut encoder = Box::from_raw(encoder);
    if encoder.free {
        let result = encode_(message, &mut encoder);
        encoder.free = false;
        let _ = Box::into_raw(encoder);
        result.into()
    } else {
        CResult::Err(Sv2Error::EncoderBusy)
    }
}

#[derive(Debug)]
pub struct DecoderWrapper(StandardDecoder<Sv2Message<'static>>);

#[no_mangle]
pub extern "C" fn new_decoder() -> *mut DecoderWrapper {
    let s = Box::new(DecoderWrapper(StandardDecoder::new()));
    Box::into_raw(s)
}

#[no_mangle]
#[allow(clippy::not_unsafe_ptr_arg_deref)]
pub extern "C" fn get_writable(decoder: *mut DecoderWrapper) -> CVec {
    let mut decoder = unsafe { Box::from_raw(decoder) };
    let writable = decoder.0.writable();
    let res = CVec::as_shared_buffer(writable);
    let _ = Box::into_raw(decoder);
    res
}

#[no_mangle]
#[allow(clippy::not_unsafe_ptr_arg_deref)]
pub extern "C" fn next_frame(decoder: *mut DecoderWrapper) -> CResult<CSv2Message, Sv2Error> {
    let mut decoder = unsafe { Box::from_raw(decoder) };

    match decoder.0.next_frame() {
        Ok(mut f) => {
            let msg_type = match f.get_header() {
                Some(header) => header.msg_type(),
                None => return CResult::Err(Sv2Error::InvalidSv2Frame),
            };
            let payload = f.payload();
            let len = payload.len();
            let ptr = payload.as_mut_ptr();
            let payload = unsafe { std::slice::from_raw_parts_mut(ptr, len) };
            let _ = Box::into_raw(decoder);
            (msg_type, payload)
                .try_into()
                .map(|x: Sv2Message| x.into())
                .map_err(|_| Sv2Error::Unknown)
                .into()
        }
        Err(_) => {
            let _ = Box::into_raw(decoder);
            CResult::Err(Sv2Error::MissingBytes)
        }
    }
}
#[cfg(test)]
#[cfg(feature = "prop_test")]
mod tests {
    use super::*;
    use core::convert::TryInto;
    use quickcheck::{Arbitrary, Gen};
    use quickcheck_macros;
    use template_distribution_sv2::CoinbaseOutputConstraints;

    #[derive(Clone, Debug)]
    pub struct RandomCoinbaseOutputConstraints(pub CoinbaseOutputConstraints);

    impl Arbitrary for RandomCoinbaseOutputConstraints {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomCoinbaseOutputConstraints(CoinbaseOutputConstraints::from_gen(g))
        }
    }

    fn get_setup_connection() -> SetupConnection<'static> {
        get_setup_connection_w_params(
            common_messages_sv2::Protocol::TemplateDistributionProtocol,
            2,
            2,
            0,
            "0.0.0.0".to_string(),
            8081,
            "Bitmain".to_string(),
            "901".to_string(),
            "abcX".to_string(),
            "89567".to_string(),
        )
    }

    fn get_setup_connection_w_params(
        protocol: common_messages_sv2::Protocol,
        min_version: u16,
        max_version: u16,
        flags: u32,
        endpoint_host: String,
        endpoint_port: u16,
        vendor: String,
        hardware_version: String,
        firmware: String,
        device_id: String,
    ) -> SetupConnection<'static> {
        SetupConnection {
            protocol,
            min_version,
            max_version,
            flags,
            endpoint_host: endpoint_host.into_bytes().try_into().unwrap(),
            endpoint_port,
            vendor: vendor.into_bytes().try_into().unwrap(),
            hardware_version: hardware_version.into_bytes().try_into().unwrap(),
            firmware: firmware.into_bytes().try_into().unwrap(),
            device_id: device_id.into_bytes().try_into().unwrap(),
        }
    }

    #[test]
    fn test_message_type_cb_output_data_size() {
        let expect = MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS;
        let cb_output_data_size = CoinbaseOutputConstraints {
            coinbase_output_max_additional_size: 0,
            coinbase_output_max_additional_sigops: 0,
        };
        let sv2_message = Sv2Message::CoinbaseOutputConstraints(cb_output_data_size);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_new_template() {
        let expect = MESSAGE_TYPE_NEW_TEMPLATE;
        let new_template = NewTemplate {
            template_id: 0,
            future_template: false,
            version: 0x01000000,
            coinbase_tx_version: 0x01000000,
            coinbase_prefix: "0".to_string().into_bytes().try_into().unwrap(),
            coinbase_tx_input_sequence: 0xffffffff,
            coinbase_tx_value_remaining: 0x00f2052a,
            coinbase_tx_outputs_count: 1,
            coinbase_tx_outputs: vec![].try_into().unwrap(),
            coinbase_tx_locktime: 0x00000000,
            merkle_path: binary_sv2::Seq0255::new(Vec::<binary_sv2::U256>::new()).unwrap(),
        };
        let sv2_message = Sv2Message::NewTemplate(new_template);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_request_transaction_data() {
        let expect = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA;
        let request_tx_data = RequestTransactionData { template_id: 0 };
        let sv2_message = Sv2Message::RequestTransactionData(request_tx_data);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_request_transaction_data_error() {
        let expect = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR;
        let request_tx_data_err = RequestTransactionDataError {
            template_id: 0,
            error_code: "an error code".to_string().into_bytes().try_into().unwrap(),
        };
        let sv2_message = Sv2Message::RequestTransactionDataError(request_tx_data_err);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_request_transaction_data_success() {
        let expect = MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS;

        let request_tx_data_success = RequestTransactionDataSuccess {
            template_id: 0,
            excess_data: "some_excess_data"
                .to_string()
                .into_bytes()
                .try_into()
                .unwrap(),
            transaction_list: binary_sv2::Seq064K::new(Vec::new()).unwrap(),
        };
        let sv2_message = Sv2Message::RequestTransactionDataSuccess(request_tx_data_success);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_set_new_prev_hash() {
        let expect = MESSAGE_TYPE_SET_NEW_PREV_HASH;

        let mut u256 = [0_u8; 32];
        let u256_prev_hash: binary_sv2::U256 = (&mut u256[..]).try_into().unwrap();

        let mut u256 = [0_u8; 32];
        let u256_target: binary_sv2::U256 = (&mut u256[..]).try_into().unwrap();

        let set_new_prev_hash = SetNewPrevHash {
            template_id: 0,
            prev_hash: u256_prev_hash,
            header_timestamp: 0x29ab5f49,
            n_bits: 0xffff001d,
            target: u256_target,
        };
        let sv2_message = Sv2Message::SetNewPrevHash(set_new_prev_hash);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_submit_solution() {
        let expect = MESSAGE_TYPE_SUBMIT_SOLUTION;

        let submit_solution = SubmitSolution {
            template_id: 0,
            version: 0x01000000,
            header_timestamp: 0x29ab5f49,
            header_nonce: 0x1dac2b7c,
            coinbase_tx: "01000000010000000000000000000000000000000000000000000000000000000000000000ffffffff4d04ffff001d0104455468652054696d65732030332f4a616e2f32303039204368616e63656c6c6f72206f6e206272696e6b206f66207365636f6e64206261696c6f757420666f722062616e6b73ffffffff0100f2052a01000000434104678afdb0fe5548271967f1a67130b7105cd6a828e03909a67962e0ea1f61deb649f6bc3f4cef38c4f35504e51ec112de5c384df7ba0b8d578a4c702b6bf11d5fac00000000"
                .to_string()
                .into_bytes()
                .try_into()
                .unwrap(),
        };

        let sv2_message = Sv2Message::SubmitSolution(submit_solution);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_channel_endpoint_changed() {
        let expect = MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED;

        let channel_endpoint_changed = ChannelEndpointChanged { channel_id: 0 };

        let sv2_message = Sv2Message::ChannelEndpointChanged(channel_endpoint_changed);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_setup_connection() {
        let expect = MESSAGE_TYPE_SETUP_CONNECTION;

        let setup_connection = get_setup_connection();

        let sv2_message = Sv2Message::SetupConnection(setup_connection);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_setup_connection_error() {
        let expect = MESSAGE_TYPE_SETUP_CONNECTION_ERROR;

        let setup_connection_err = SetupConnectionError {
            flags: 0,
            error_code: "an error code".to_string().into_bytes().try_into().unwrap(),
        };

        let sv2_message = Sv2Message::SetupConnectionError(setup_connection_err);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    fn test_message_type_setup_connection_success() {
        let expect = MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS;

        let setup_connection_success = SetupConnectionSuccess {
            used_version: 1,
            flags: 0,
        };

        let sv2_message = Sv2Message::SetupConnectionSuccess(setup_connection_success);
        let actual = sv2_message.message_type();

        assert_eq!(expect, actual);
    }

    #[test]
    #[ignore]
    fn test_next_frame() {
        let decoder = StandardDecoder::<Sv2Message<'static>>::new();
        println!("DECODER: {:?}", &decoder);
        println!("DECODER 2: {:?}", &decoder);
        let mut decoder_wrapper = DecoderWrapper(decoder);
        let _res = next_frame(&mut decoder_wrapper);
    }

    // RR

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_coinbase_output_data_size(message: RandomCoinbaseOutputConstraints) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<CoinbaseOutputConstraints>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::CoinbaseOutputConstraints(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomNewTemplate(pub NewTemplate<'static>);
    impl Arbitrary for RandomNewTemplate {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomNewTemplate(NewTemplate::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_new_template_id(message: RandomNewTemplate) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<NewTemplate>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        // Create frame
        let frame =
            StandardSv2Frame::from_message(message.0, MESSAGE_TYPE_NEW_TEMPLATE, 0, false).unwrap();
        // Encode frame
        let encoded_frame = encoder.encode(frame).unwrap();

        // Decode encoded frame
        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        // Puts decoder in the next state (next 6 bytes). If frame is incomplete, returns an error
        // prompting to add more bytes to decode the frame
        // Required between two writes because of how this is intended to use the decoder in a loop
        // read from a stream.
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        // Decoded frame, complete frame is filled
        let mut decoded = decoder.next_frame().unwrap();

        // Extract payload of the frame which is the NewTemplate message
        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::NewTemplate(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomRequestTransactionData(pub RequestTransactionData);
    impl Arbitrary for RandomRequestTransactionData {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomRequestTransactionData(RequestTransactionData::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_request_transaction_data(message: RandomRequestTransactionData) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<RequestTransactionData>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::RequestTransactionData(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomRequestTransactionDataError(pub RequestTransactionDataError<'static>);

    impl Arbitrary for RandomRequestTransactionDataError {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomRequestTransactionDataError(RequestTransactionDataError::from_gen(g))
        }
    }
    #[quickcheck_macros::quickcheck]
    fn encode_with_c_request_transaction_data_error(
        message: RandomRequestTransactionDataError,
    ) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<RequestTransactionDataError>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::RequestTransactionDataError(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomRequestTransactionDataSuccess(pub RequestTransactionDataSuccess<'static>);

    impl Arbitrary for RandomRequestTransactionDataSuccess {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomRequestTransactionDataSuccess(RequestTransactionDataSuccess::from_gen(g))
        }
    }
    #[quickcheck_macros::quickcheck]
    fn encode_with_c_request_transaction_data_success(
        message: RandomRequestTransactionDataSuccess,
    ) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<RequestTransactionDataSuccess>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::RequestTransactionDataSuccess(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomSetNewPrevHash(pub SetNewPrevHash<'static>);

    impl Arbitrary for RandomSetNewPrevHash {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomSetNewPrevHash(SetNewPrevHash::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_set_new_prev_hash(message: RandomSetNewPrevHash) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<SetNewPrevHash>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame =
            StandardSv2Frame::from_message(message.0, MESSAGE_TYPE_SET_NEW_PREV_HASH, 0, false)
                .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::SetNewPrevHash(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomSubmitSolution(pub SubmitSolution<'static>);

    impl Arbitrary for RandomSubmitSolution {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomSubmitSolution(SubmitSolution::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_submit_solution(message: RandomSubmitSolution) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<SubmitSolution>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame =
            StandardSv2Frame::from_message(message.0, MESSAGE_TYPE_SUBMIT_SOLUTION, 0, false)
                .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::SubmitSolution(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomSetupConnection(pub SetupConnection<'static>);

    impl Arbitrary for RandomSetupConnection {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomSetupConnection(SetupConnection::from_gen(g))
        }
    }

    #[derive(Clone, Debug)]
    pub struct RandomChannelEndpointChanged(pub ChannelEndpointChanged);

    impl Arbitrary for RandomChannelEndpointChanged {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomChannelEndpointChanged(ChannelEndpointChanged::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_channel_endpoint_changed(message: RandomChannelEndpointChanged) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<ChannelEndpointChanged>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::ChannelEndpointChanged(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_setup_connection(message: RandomSetupConnection) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<SetupConnection>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame =
            StandardSv2Frame::from_message(message.0, MESSAGE_TYPE_SETUP_CONNECTION, 0, false)
                .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::SetupConnection(m) => m,
            _ => panic!(),
        };

        decoded_message == expected
    }

    #[derive(Clone, Debug)]
    pub struct RandomSetupConnectionError(pub SetupConnectionError<'static>);

    impl Arbitrary for RandomSetupConnectionError {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomSetupConnectionError(SetupConnectionError::from_gen(g))
        }
    }

    #[quickcheck_macros::quickcheck]
    fn encode_with_c_setup_connection_error(message: RandomSetupConnectionError) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<SetupConnectionError>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_SETUP_CONNECTION_ERROR,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::SetupConnectionError(m) => m,
            _ => panic!(),
        };

        decoded_message.flags == expected.flags
    }

    #[derive(Clone, Debug)]
    pub struct RandomSetupConnectionSuccess(pub SetupConnectionSuccess);

    #[cfg(feature = "prop_test")]
    impl Arbitrary for RandomSetupConnectionSuccess {
        fn arbitrary(g: &mut Gen) -> Self {
            RandomSetupConnectionSuccess(SetupConnectionSuccess::from_gen(g))
        }
    }
    #[quickcheck_macros::quickcheck]
    fn encode_with_c_setup_connection_success(message: RandomSetupConnectionSuccess) -> bool {
        let expected = message.clone().0;

        let mut encoder = Encoder::<SetupConnectionSuccess>::new();
        let mut decoder = StandardDecoder::<Sv2Message<'static>>::new();

        let frame = StandardSv2Frame::from_message(
            message.0,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
            0,
            false,
        )
        .unwrap();
        let encoded_frame = encoder.encode(frame).unwrap();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i]
        }
        let _ = decoder.next_frame();

        let buffer = decoder.writable();
        for i in 0..buffer.len() {
            buffer[i] = encoded_frame[i + 6]
        }

        let mut decoded = decoder.next_frame().unwrap();

        let msg_type = decoded.get_header().unwrap().msg_type();
        let payload = decoded.payload();
        let decoded_message: Sv2Message = (msg_type, payload).try_into().unwrap();
        let decoded_message = match decoded_message {
            Sv2Message::SetupConnectionSuccess(m) => m,
            _ => panic!(),
        };

        decoded_message.flags == expected.flags
    }
}
</file>

<file path="stratum-1.4.0/protocols/v2/sv2-ffi/sv2.h">
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

/// Identifier for the extension_type field in the SV2 frame, indicating no
/// extensions.
static const uint16_t EXTENSION_TYPE_NO_EXTENSION = 0;

/// Size of the SV2 frame header in bytes.
static const uintptr_t SV2_FRAME_HEADER_SIZE = 6;

/// Maximum size of an SV2 frame chunk in bytes.
static const uintptr_t SV2_FRAME_CHUNK_SIZE = 65535;

/// Size of the MAC for supported AEAD encryption algorithm (ChaChaPoly).
static const uintptr_t AEAD_MAC_LEN = 16;

/// Size of the encrypted SV2 frame header, including the MAC.
static const uintptr_t ENCRYPTED_SV2_FRAME_HEADER_SIZE = (SV2_FRAME_HEADER_SIZE + AEAD_MAC_LEN);

/// Size of the Noise protocol frame header in bytes.
static const uintptr_t NOISE_FRAME_HEADER_SIZE = 2;

static const uintptr_t NOISE_FRAME_HEADER_LEN_OFFSET = 0;

/// Size in bytes of the encoded elliptic curve point using ElligatorSwift
/// encoding. This encoding produces a 64-byte representation of the
/// X-coordinate of a secp256k1 curve point.
static const uintptr_t ELLSWIFT_ENCODING_SIZE = 64;

static const uintptr_t MAC = 16;

/// Size in bytes of the encrypted ElligatorSwift encoded data, which includes
/// the original ElligatorSwift encoded data and a MAC for integrity
/// verification.
static const uintptr_t ENCRYPTED_ELLSWIFT_ENCODING_SIZE = (ELLSWIFT_ENCODING_SIZE + MAC);

/// Size in bytes of the SIGNATURE_NOISE_MESSAGE, which contains information and
/// a signature for the handshake initiator, formatted according to the Noise
/// Protocol specifications.
static const uintptr_t SIGNATURE_NOISE_MESSAGE_SIZE = 74;

/// Size in bytes of the encrypted signature noise message, which includes the
/// SIGNATURE_NOISE_MESSAGE and a MAC for integrity verification.
static const uintptr_t ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE = (SIGNATURE_NOISE_MESSAGE_SIZE + MAC);

/// Size in bytes of the handshake message expected by the initiator,
/// encompassing:
/// - ElligatorSwift encoded public key
/// - Encrypted ElligatorSwift encoding
/// - Encrypted SIGNATURE_NOISE_MESSAGE
static const uintptr_t INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE = ((ELLSWIFT_ENCODING_SIZE + ENCRYPTED_ELLSWIFT_ENCODING_SIZE) + ENCRYPTED_SIGNATURE_NOISE_MESSAGE_SIZE);

static const uint8_t SV2_MINING_PROTOCOL_DISCRIMINANT = 0;

static const uint8_t SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT = 1;

static const uint8_t SV2_TEMPLATE_DISTR_PROTOCOL_DISCRIMINANT = 2;

static const uint8_t MESSAGE_TYPE_SETUP_CONNECTION = 0;

static const uint8_t MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS = 1;

static const uint8_t MESSAGE_TYPE_SETUP_CONNECTION_ERROR = 2;

static const uint8_t MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED = 3;

static const uint8_t MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL = 16;

static const uint8_t MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS = 17;

static const uint8_t MESSAGE_TYPE_OPEN_MINING_CHANNEL_ERROR = 18;

static const uint8_t MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL = 19;

static const uint8_t MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCES = 20;

static const uint8_t MESSAGE_TYPE_NEW_MINING_JOB = 21;

static const uint8_t MESSAGE_TYPE_UPDATE_CHANNEL = 22;

static const uint8_t MESSAGE_TYPE_UPDATE_CHANNEL_ERROR = 23;

static const uint8_t MESSAGE_TYPE_CLOSE_CHANNEL = 24;

static const uint8_t MESSAGE_TYPE_SET_EXTRANONCE_PREFIX = 25;

static const uint8_t MESSAGE_TYPE_SUBMIT_SHARES_STANDARD = 26;

static const uint8_t MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED = 27;

static const uint8_t MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS = 28;

static const uint8_t MESSAGE_TYPE_SUBMIT_SHARES_ERROR = 29;

static const uint8_t MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB = 31;

static const uint8_t MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH = 32;

static const uint8_t MESSAGE_TYPE_SET_TARGET = 33;

static const uint8_t MESSAGE_TYPE_SET_CUSTOM_MINING_JOB = 34;

static const uint8_t MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_SUCCESS = 35;

static const uint8_t MESSAGE_TYPE_SET_CUSTOM_MINING_JOB_ERROR = 36;

static const uint8_t MESSAGE_TYPE_RECONNECT = 37;

static const uint8_t MESSAGE_TYPE_SET_GROUP_CHANNEL = 38;

static const uint8_t MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN = 80;

static const uint8_t MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS = 81;

static const uint8_t MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS = 85;

static const uint8_t MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS = 86;

static const uint8_t MESSAGE_TYPE_DECLARE_MINING_JOB = 87;

static const uint8_t MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS = 88;

static const uint8_t MESSAGE_TYPE_DECLARE_MINING_JOB_ERROR = 89;

static const uint8_t MESSAGE_TYPE_PUSH_SOLUTION = 96;

static const uint8_t MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS = 112;

static const uint8_t MESSAGE_TYPE_NEW_TEMPLATE = 113;

static const uint8_t MESSAGE_TYPE_SET_NEW_PREV_HASH = 114;

static const uint8_t MESSAGE_TYPE_REQUEST_TRANSACTION_DATA = 115;

static const uint8_t MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_SUCCESS = 116;

static const uint8_t MESSAGE_TYPE_REQUEST_TRANSACTION_DATA_ERROR = 117;

static const uint8_t MESSAGE_TYPE_SUBMIT_SOLUTION = 118;

static const bool CHANNEL_BIT_SETUP_CONNECTION = false;

static const bool CHANNEL_BIT_SETUP_CONNECTION_SUCCESS = false;

static const bool CHANNEL_BIT_SETUP_CONNECTION_ERROR = false;

static const bool CHANNEL_BIT_CHANNEL_ENDPOINT_CHANGED = true;

static const bool CHANNEL_BIT_COINBASE_OUTPUT_CONSTRAINTS = false;

static const bool CHANNEL_BIT_NEW_TEMPLATE = false;

static const bool CHANNEL_BIT_SET_NEW_PREV_HASH = false;

static const bool CHANNEL_BIT_REQUEST_TRANSACTION_DATA = false;

static const bool CHANNEL_BIT_REQUEST_TRANSACTION_DATA_SUCCESS = false;

static const bool CHANNEL_BIT_REQUEST_TRANSACTION_DATA_ERROR = false;

static const bool CHANNEL_BIT_SUBMIT_SOLUTION = false;

static const bool CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN = false;

static const bool CHANNEL_BIT_ALLOCATE_MINING_JOB_TOKEN_SUCCESS = false;

static const bool CHANNEL_BIT_DECLARE_MINING_JOB = false;

static const bool CHANNEL_BIT_DECLARE_MINING_JOB_SUCCESS = false;

static const bool CHANNEL_BIT_DECLARE_MINING_JOB_ERROR = false;

static const bool CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS = false;

static const bool CHANNEL_BIT_PROVIDE_MISSING_TRANSACTIONS_SUCCESS = false;

static const bool CHANNEL_BIT_SUBMIT_SOLUTION_JD = true;

static const bool CHANNEL_BIT_CLOSE_CHANNEL = true;

static const bool CHANNEL_BIT_NEW_EXTENDED_MINING_JOB = true;

static const bool CHANNEL_BIT_NEW_MINING_JOB = true;

static const bool CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL = false;

static const bool CHANNEL_BIT_OPEN_EXTENDED_MINING_CHANNEL_SUCCES = false;

static const bool CHANNEL_BIT_OPEN_MINING_CHANNEL_ERROR = false;

static const bool CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL = false;

static const bool CHANNEL_BIT_OPEN_STANDARD_MINING_CHANNEL_SUCCESS = false;

static const bool CHANNEL_BIT_RECONNECT = false;

static const bool CHANNEL_BIT_SET_CUSTOM_MINING_JOB = false;

static const bool CHANNEL_BIT_SET_CUSTOM_MINING_JOB_ERROR = false;

static const bool CHANNEL_BIT_SET_CUSTOM_MINING_JOB_SUCCESS = false;

static const bool CHANNEL_BIT_SET_EXTRANONCE_PREFIX = true;

static const bool CHANNEL_BIT_SET_GROUP_CHANNEL = false;

static const bool CHANNEL_BIT_MINING_SET_NEW_PREV_HASH = true;

static const bool CHANNEL_BIT_SET_TARGET = true;

static const bool CHANNEL_BIT_SUBMIT_SHARES_ERROR = true;

static const bool CHANNEL_BIT_SUBMIT_SHARES_EXTENDED = true;

static const bool CHANNEL_BIT_SUBMIT_SHARES_STANDARD = true;

static const bool CHANNEL_BIT_SUBMIT_SHARES_SUCCESS = true;

static const bool CHANNEL_BIT_UPDATE_CHANNEL = true;

static const bool CHANNEL_BIT_UPDATE_CHANNEL_ERROR = true;
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

/// A struct to facilitate transferring a `Vec<u8>` across FFI boundaries.
struct CVec {
  uint8_t *data;
  uintptr_t len;
  uintptr_t capacity;
};

/// A struct to manage a collection of `CVec` objects across FFI boundaries.
struct CVec2 {
  CVec *data;
  uintptr_t len;
  uintptr_t capacity;
};

/// Represents a 24-bit unsigned integer (`U24`), supporting SV2 serialization and deserialization.
/// Only first 3 bytes of a u32 is considered to get the SV2 value, and rest are ignored (in little
/// endian).
struct U24 {
  uint32_t _0;
};

extern "C" {

/// Creates a `CVec` from a buffer that was allocated in C.
///
/// # Safety
/// The caller must ensure that the buffer is valid and that
/// the data length does not exceed the allocated size.
CVec cvec_from_buffer(const uint8_t *data, uintptr_t len);

/// Initializes an empty `CVec2`.
///
/// # Safety
/// The caller is responsible for freeing the `CVec2` when it is no longer needed.
CVec2 init_cvec2();

/// Adds a `CVec` to a `CVec2`.
///
/// # Safety
/// The caller must ensure no duplicate `CVec`s are added, as duplicates may
/// lead to double-free errors when the message is dropped.
void cvec2_push(CVec2 *cvec2, CVec cvec);

/// Exported FFI functions for interoperability with C code for u24
void _c_export_u24(U24 _a);

/// Exported FFI functions for interoperability with C code for CVec
void _c_export_cvec(CVec _a);

/// Exported FFI functions for interoperability with C code for CVec2
void _c_export_cvec2(CVec2 _a);

} // extern "C"
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

/// This enum has a list of the different Stratum V2 subprotocols.
enum class Protocol : uint8_t {
  /// Mining protocol.
  MiningProtocol = SV2_MINING_PROTOCOL_DISCRIMINANT,
  /// Job declaration protocol.
  JobDeclarationProtocol = SV2_JOB_DECLARATION_PROTOCOL_DISCRIMINANT,
  /// Template distribution protocol.
  TemplateDistributionProtocol = SV2_TEMPLATE_DISTR_PROTOCOL_DISCRIMINANT,
};

/// Message used by an upstream role for announcing a mining channel endpoint change.
///
/// This message should be sent when a mining channels upstream or downstream endpoint changes and
/// that channel had previously exchanged message(s) with `channel_msg` bitset of unknown
/// `extension_type`.
///
/// When a downstream receives such a message, any extension state (including version and extension
/// support) must be reset and renegotiated.
struct ChannelEndpointChanged {
  /// Unique identifier of the channel that has changed its endpoint.
  uint32_t channel_id;
};

/// Message used by an upstream role to accept a connection setup request from a downstream role.
///
/// This message is sent in response to a [`SetupConnection`] message.
struct SetupConnectionSuccess {
  /// Selected version based on the [`SetupConnection::min_version`] and
  /// [`SetupConnection::max_version`] sent by the downstream role.
  ///
  /// This version will be used on the connection for the rest of its life.
  uint16_t used_version;
  /// Flags indicating optional protocol features supported by the upstream.
  ///
  /// The downstream is required to verify this set of flags and act accordingly.
  ///
  /// Each [`SetupConnection::protocol`] field has its own values/flags.
  uint32_t flags;
};

/// C representation of [`SetupConnection`]
struct CSetupConnection {
  /// Protocol to be used for the connection.
  Protocol protocol;
  /// The minimum protocol version supported.
  ///
  /// Currently must be set to 2.
  uint16_t min_version;
  /// The maximum protocol version supported.
  ///
  /// Currently must be set to 2.
  uint16_t max_version;
  /// Flags indicating optional protocol features supported by the downstream.
  ///
  /// Each [`SetupConnection::protocol`] value has it's own flags.
  uint32_t flags;
  /// ASCII representation of the connection hostname or IP address.
  CVec endpoint_host;
  /// Connection port value.
  uint16_t endpoint_port;
  /// Device vendor name.
  CVec vendor;
  /// Device hardware version.
  CVec hardware_version;
  /// Device firmware version.
  CVec firmware;
  /// Device identifier.
  CVec device_id;
};

/// C representation of [`SetupConnectionError`]
struct CSetupConnectionError {
  uint32_t flags;
  CVec error_code;
};

extern "C" {

/// A C-compatible function that exports the [`ChannelEndpointChanged`] struct.
void _c_export_channel_endpoint_changed(ChannelEndpointChanged _a);

/// A C-compatible function that exports the `SetupConnection` struct.
void _c_export_setup_conn_succ(SetupConnectionSuccess _a);

void free_setup_connection(CSetupConnection s);

void free_setup_connection_error(CSetupConnectionError s);

} // extern "C"
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

/// Message used by a downstream to indicate the size of the additional bytes they will need in
/// coinbase transaction outputs.
///
/// As the pool is responsible for adding coinbase transaction outputs for payouts and other uses,
/// the Template Provider will need to consider this reserved space when selecting transactions for
/// inclusion in a block(to avoid an invalid, oversized block).  Thus, this message indicates that
/// additional space in the block/coinbase transaction must be reserved for, assuming they will use
/// the entirety of this space.
///
/// The Job Declarator **must** discover the maximum serialized size of the additional outputs which
/// will be added by the pools it intends to use this work. It then **must** communicate the sum of
/// such size to the Template Provider via this message.
///
/// The Template Provider **must not** provide [`NewTemplate`] messages which would represent
/// consensus-invalid blocks once this additional size  along with a maximally-sized (100 byte)
/// coinbase field  is added. Further, the Template Provider **must** consider the maximum
/// additional bytes required in the output count variable-length integer in the coinbase
/// transaction when complying with the size limits.
///
/// [`NewTemplate`]: crate::NewTemplate
struct CoinbaseOutputConstraints {
  /// Additional serialized bytes needed in coinbase transaction outputs.
  uint32_t coinbase_output_max_additional_size;
  /// Additional sigops needed in coinbase transaction outputs.
  uint16_t coinbase_output_max_additional_sigops;
};

/// Message used by a downstream to request data about all transactions in a block template.
///
/// Data includes the full transaction data and any additional data required to block validation.
///
/// Note that the coinbase transaction is excluded from this data.
struct RequestTransactionData {
  /// Identifier of the template that the downstream node is requesting transaction data for.
  ///
  /// This must be identical to previously exchanged [`crate::NewTemplate::template_id`].
  uint64_t template_id;
};

/// C representation of [`NewTemplate`].
struct CNewTemplate {
  uint64_t template_id;
  bool future_template;
  uint32_t version;
  uint32_t coinbase_tx_version;
  CVec coinbase_prefix;
  uint32_t coinbase_tx_input_sequence;
  uint64_t coinbase_tx_value_remaining;
  uint32_t coinbase_tx_outputs_count;
  CVec coinbase_tx_outputs;
  uint32_t coinbase_tx_locktime;
  CVec2 merkle_path;
};

/// C representation of [`RequestTransactionDataSuccess`].
struct CRequestTransactionDataSuccess {
  uint64_t template_id;
  CVec excess_data;
  CVec2 transaction_list;
};

/// C representation of [`RequestTransactionDataError`].
struct CRequestTransactionDataError {
  uint64_t template_id;
  CVec error_code;
};

/// C representation of [`SetNewPrevHash`].
struct CSetNewPrevHash {
  uint64_t template_id;
  CVec prev_hash;
  uint32_t header_timestamp;
  uint32_t n_bits;
  CVec target;
};

/// C representation of [`SubmitSolution`].
struct CSubmitSolution {
  uint64_t template_id;
  uint32_t version;
  uint32_t header_timestamp;
  uint32_t header_nonce;
  CVec coinbase_tx;
};

extern "C" {

/// Exports the [`CoinbaseOutputConstraints`] struct to C.
void _c_export_coinbase_out(CoinbaseOutputConstraints _a);

/// Exports the [`RequestTransactionData`] struct to C.
void _c_export_req_tx_data(RequestTransactionData _a);

/// Drops the [`CNewTemplate`] object.
void free_new_template(CNewTemplate s);

/// Drops the CRequestTransactionDataSuccess object.
void free_request_tx_data_success(CRequestTransactionDataSuccess s);

/// Drops the CRequestTransactionDataError object.
void free_request_tx_data_error(CRequestTransactionDataError s);

/// Drops the CSetNewPrevHash object.
void free_set_new_prev_hash(CSetNewPrevHash s);

/// Drops the CSubmitSolution object.
void free_submit_solution(CSubmitSolution s);

} // extern "C"
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

/// C-compatible enumeration of possible errors in the `codec_sv2` module.
///
/// This enum mirrors the [`Error`] enum but is designed to be used in C code through FFI. It
/// represents the same set of errors as [`Error`], making them accessible to C programs.
struct CError {
  enum class Tag {
    /// AEAD (`snow`) error in the Noise protocol.
    AeadError,
    /// Binary Sv2 data format error.
    BinarySv2Error,
    /// Framing Sv2 error.
    FramingError,
    /// Framing Sv2 error.
    FramingSv2Error,
    /// Invalid step for initiator in the Noise protocol.
    InvalidStepForInitiator,
    /// Invalid step for responder in the Noise protocol.
    InvalidStepForResponder,
    /// Missing bytes in the Noise protocol.
    MissingBytes,
    /// Sv2 Noise protocol error.
    NoiseSv2Error,
    /// Noise protocol is not in the expected handshake state.
    NotInHandShakeState,
    /// Unexpected state in the Noise protocol.
    UnexpectedNoiseState,
  };

  struct MissingBytes_Body {
    uintptr_t _0;
  };

  Tag tag;
  union {
    MissingBytes_Body missing_bytes;
  };
};

extern "C" {

/// Force `cbindgen` to create a header for [`CError`].
///
/// It ensures that [`CError`] is included in the generated C header file. This function is not
/// meant to be called and will panic if called. Its only purpose is to make [`CError`] visible to
/// `cbindgen`.
CError export_cerror();

} // extern "C"
#include <cstdarg>
#include <cstdint>
#include <cstdlib>
#include <ostream>
#include <new>

struct DecoderWrapper;

struct EncoderWrapper;

struct CSv2Message {
  enum class Tag {
    CoinbaseOutputConstraints,
    NewTemplate,
    RequestTransactionData,
    RequestTransactionDataError,
    RequestTransactionDataSuccess,
    SetNewPrevHash,
    SubmitSolution,
    ChannelEndpointChanged,
    SetupConnection,
    SetupConnectionError,
    SetupConnectionSuccess,
  };

  struct CoinbaseOutputConstraints_Body {
    CoinbaseOutputConstraints _0;
  };

  struct NewTemplate_Body {
    CNewTemplate _0;
  };

  struct RequestTransactionData_Body {
    RequestTransactionData _0;
  };

  struct RequestTransactionDataError_Body {
    CRequestTransactionDataError _0;
  };

  struct RequestTransactionDataSuccess_Body {
    CRequestTransactionDataSuccess _0;
  };

  struct SetNewPrevHash_Body {
    CSetNewPrevHash _0;
  };

  struct SubmitSolution_Body {
    CSubmitSolution _0;
  };

  struct ChannelEndpointChanged_Body {
    ChannelEndpointChanged _0;
  };

  struct SetupConnection_Body {
    CSetupConnection _0;
  };

  struct SetupConnectionError_Body {
    CSetupConnectionError _0;
  };

  struct SetupConnectionSuccess_Body {
    SetupConnectionSuccess _0;
  };

  Tag tag;
  union {
    CoinbaseOutputConstraints_Body coinbase_output_constraints;
    NewTemplate_Body new_template;
    RequestTransactionData_Body request_transaction_data;
    RequestTransactionDataError_Body request_transaction_data_error;
    RequestTransactionDataSuccess_Body request_transaction_data_success;
    SetNewPrevHash_Body set_new_prev_hash;
    SubmitSolution_Body submit_solution;
    ChannelEndpointChanged_Body channel_endpoint_changed;
    SetupConnection_Body setup_connection;
    SetupConnectionError_Body setup_connection_error;
    SetupConnectionSuccess_Body setup_connection_success;
  };
};

struct Sv2Error {
  enum class Tag {
    BinaryError,
    CodecError,
    EncoderBusy,
    InvalidSv2Frame,
    MissingBytes,
    PayloadTooBig,
    Unknown,
  };

  struct BinaryError_Body {
    CError _0;
  };

  struct CodecError_Body {
    CError _0;
  };

  struct PayloadTooBig_Body {
    CVec _0;
  };

  Tag tag;
  union {
    BinaryError_Body binary_error;
    CodecError_Body codec_error;
    PayloadTooBig_Body payload_too_big;
  };
};

template<typename T, typename E>
struct CResult {
  enum class Tag {
    Ok,
    Err,
  };

  struct Ok_Body {
    T _0;
  };

  struct Err_Body {
    E _0;
  };

  Tag tag;
  union {
    Ok_Body ok;
    Err_Body err;
  };
};

extern "C" {

void drop_sv2_message(CSv2Message s);

/// This function does nothing unless there is some heap allocated data owned by the C side that
/// needs to be dropped (specifically a `CVec`). In this case, `free_vec` is used in order to drop
/// that memory.
void drop_sv2_error(Sv2Error s);

bool is_ok(const CResult<CSv2Message, Sv2Error> *cresult);

EncoderWrapper *new_encoder();

void flush_encoder(EncoderWrapper *encoder);

void free_decoder(DecoderWrapper *decoder);

/// # Safety
CResult<CVec, Sv2Error> encode(CSv2Message *message, EncoderWrapper *encoder);

DecoderWrapper *new_decoder();

CVec get_writable(DecoderWrapper *decoder);

CResult<CSv2Message, Sv2Error> next_frame(DecoderWrapper *decoder);

} // extern "C"
</file>

<file path="stratum-1.4.0/README.md">
<h1 align="center">
  <br>
  <a href="https://stratumprotocol.org"><img src="https://github.com/stratum-mining/stratumprotocol.org/blob/660ecc6ccd2eca82d0895cef939f4670adc6d1f4/src/.vuepress/public/assets/stratum-logo%402x.png" alt="SRI" width="200"></a>
  <br>
Stratum V2 Reference Implementation (SRI)
  <br>
</h1>

<h4 align="center">SRI is a reference implementation of the Stratum V2 protocol written in Rust .</h4>

<p align="center">
  <a href="https://codecov.io/gh/stratum-mining/stratum">
    <img src="https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg" alt="codecov">
  </a>
  <a href="https://twitter.com/intent/follow?screen_name=stratumv2">
    <img src="https://img.shields.io/twitter/follow/stratumv2?style=social" alt="X (formerly Twitter) Follow">
  </a>
</p>

##  Table of Contents

<p align="center">
  <a href="#-introduction">Introduction</a> 
  <a href="#%EF%B8%8F-getting-started">Getting Started</a> 
  <a href="#-use-cases">Use Cases</a> 
  <a href="#-roadmap">Roadmap</a> 
  <a href="#-contribute">Contribute</a> 
  <a href="#-support">Support</a> 
  <a href="#-donate">Donate</a> 
  <a href="#-supporters">Supporters</a> 
  <a href="#-license">License</a> 
  <a href="#-msrv">MSRV</a>
</p>

##  Introduction

Welcome to the official GitHub repository for the **SRI - Stratum V2 Reference Implementation**. 

[Stratum V2](https://stratumprotocol.org) is a next-generation bitcoin mining protocol designed to enhance the efficiency, security, flexibility and decentralization. 
SRI is fully open-source, community-developed, independent of any single entity, aiming to be fully compatible with [Stratum V2 Specification](https://github.com/stratum-mining/sv2-spec).

##  Getting Started

To get started with the Stratum V2 Reference Implementation (SRI), please follow the detailed setup instructions available on the official website:

[Getting Started with Stratum V2](https://stratumprotocol.org/getting-started/)

This guide provides all the necessary information on prerequisites, installation, and configuration to help you begin using, testing or contributing to SRI.

##  Use Cases

The library is modular to address different use-cases and desired functionality. Examples include:

###  Miners

- SV1 Miners can use the translator proxy (`roles/translator`) to connect with a SV2-compatible pool.
- SV1 mining farms mining to a SV2-compatible pool gain some of the security and efficiency improvements SV2 offers over Stratum V1 (SV1). The SV1<->SV2 translator proxy does not support  _all_ the features of SV2, but works as a temporary measure before upgrading completely to SV2-compatible firmware. (The SV1<->SV2 translation proxy implementation is a work in progress.)

###  Pools

- Pools supporting SV2 can deploy the open source binary crate (`roles/pool`) to offer their clients (miners participating in said pool) an SV2-compatible pool.
- The Rust helper library provides a suite of tools for mining pools to build custom SV2 compatible pool implementations.
- The C library provides a set of FFI bindings to the Rust helper library for miners to integrate SV2 into their existing firmware stack.

##  Roadmap 

Our roadmap is publicly available, outlining current and future plans. Decisions on the roadmap are made through a consensus-driven approach, through participation on dev meetings, Discord or GitHub.

[View the SRI Roadmap](https://github.com/orgs/stratum-mining/projects/5)

###  Project Maturity

Low-level crates (`protocols` directory) are considered **beta** software. Rust API Docs is a [work-in-progress](https://github.com/stratum-mining/stratum/issues/845), and the community should still expect small breaking API changes and patches.

Application-level crates (`roles` directory) are considered **alpha** software, and bugs are expected. They should be used as a guide on how to consume the low-level crates as dependencies.

###  Goals

The goals of this project are to provide:

1. A robust set of Stratum V2 (SV2) primitives as Rust library crates which anyone can use
   to expand the protocol or implement a role. For example:
   - Pools supporting SV2
   - Mining-device/hashrate producers integrating SV2 into their firmware
   - Bitcoin nodes implementing Template Provider to build the `blocktemplate`
2. The above Rust primitives as a C library available for use in other languages via FFI.
3. A set of helpers built on top of the above primitives and the external Bitcoin-related Rust crates for anyone to implement the SV2 roles.
4. An open-source implementation of a SV2 proxy for miners.
5. An open-source implementation of a SV2 pool for mining pool operators.

##  Contribute 

If you are a developer looking to help, but you're not sure where to begin, check the [good first issue label](https://github.com/stratum-mining/stratum/labels/good%20first%20issue), which contains small pieces of work that have been specifically flagged as being friendly to new contributors.

Contributors looking to do something a bit more challenging, before opening a pull request, please join [our community chat](https://discord.gg/fsEW23wFYs) or [start a GitHub issue](https://github.com/stratum-mining/stratum/issues) to get early feedback, discuss the best ways to tackle the problem, and ensure there is no work duplication and consensus.

##  Support

Join our Discord community to get help, share your ideas, or discuss anything related to Stratum V2 and its reference implementation. 

Whether you're looking for technical support, want to contribute, or are just interested in learning more about the project, our community is the place to be.

[Join the Stratum V2 Discord Community](https://discord.gg/fsEW23wFYs)

##  Donate

###  Individual Donations 
If you wish to support the development and maintenance of the Stratum V2 Reference Implementation, individual donations are greatly appreciated. You can donate through OpenSats, a 501(c)(3) public charity dedicated to supporting open-source Bitcoin projects.

[Donate through OpenSats](https://opensats.org/projects/stratumv2)

###  Corporate Donations
For corporate entities interested in providing more substantial support, such as grants to SRI contributors, please get in touch with us directly. Your support can make a significant difference in accelerating development, research, and innovation.

Email us at: stratumv2@gmail.com

##  Supporters

SRI contributors are independently, financially supported by following entities: 

<p float="left">
  <a href="https://hrf.org"><img src="https://raw.githubusercontent.com/stratum-mining/stratumprotocol.org/refs/heads/main/public/assets/hrf-logo-boxed.svg" width="250" /></a>
  <a href="https://spiral.xyz"><img src="https://raw.githubusercontent.com/stratum-mining/stratumprotocol.org/refs/heads/main/public/assets/Spiral-logo-boxed.svg" width="250" /></a>
  <a href="https://opensats.org/"><img src="https://raw.githubusercontent.com/stratum-mining/stratumprotocol.org/refs/heads/main/public/assets/opensats-logo-boxed.svg" width="250" /></a>
  <a href="https://vinteum.org/"><img src="https://raw.githubusercontent.com/stratum-mining/stratumprotocol.org/refs/heads/main/public/assets/vinteum-logo-boxed.png" width="250" /></a>
</p>

##  License
This software is licensed under Apache 2.0 or MIT, at your option.

##  MSRV
Minimum Supported Rust Version: 1.75.0

---

> Website [stratumprotocol.org](https://www.stratumprotocol.org) &nbsp;&middot;&nbsp;
> Discord [SV2 Discord](https://discord.gg/fsEW23wFYs) &nbsp;&middot;&nbsp;
> Twitter [@Stratumv2](https://twitter.com/StratumV2)
</file>

<file path="stratum-1.4.0/RELEASE.md">
## Changelog

The changelog is a list of notable changes for each version of a project. 
It is a way to keep track of the project's progress and to communicate 
the changes to the users and the community.

The changelog is automatically generated on each release page, and extra contextual
information is added as needed, such as:
 - General release information.
 - Breaking changes.
 - Notable changes.

## Release Process

Try to constrain each development cycle to a fixed time period, after which a
release is made. After all release tasks are completed, initiate the release
process by creating a new release branch, named `x.y.z.` referring to the
version number, from the `main` branch.  The release branch is used as a
breaking point and future reference for the release and should include the
changelog entries for the release and any other release-specific tasks. Any bug
fixes or changes for the release should be done on the release branch. When the
release branch is ready, create a new tag and initiate any publishing tasks.

Usually the release process is as follows:

1. Create a new release branch from the `main` branch.
2. Create a new tag for the release branch.
3. Publish the release.

## Versioning

Crates under `protocols` and `utils` workspaces follow SemVer 2.0.0. The version number is stored in
the `Cargo.toml` file of each crate. If a breaking change is introduced to one
of the crates, the version number must be updated accordingly, otherwise a
SemVer CI check would fail. Note that this does not apply to the `roles` and
other crates in the repository.

The global repository releases follow `X.Y.Z`, which is changed under some subjective criteria:
- Changes in `roles` are not taken into account. `roles` crates are still in Proof of Concept phase and not production ready. 
- If a release includes only bug fixes in `protocols` and/or `utils`, then `Z` is bumped.
- If a release includes breaking and/or non-breaking changes to `protocols` and/or `utils`, then `Y` is bumped.
- If a release marks a milestone i.e., `protocols` is reaching a new maturity level, then `X` is bumped.

## Tags and Branches

- Changes to `main` branch should be added through a merge commit.
- The `main` branch is the default branch and it is always active.
- The `main` branch is protected and requires a pull request to merge changes
  with at least 2 approvals.
- Each release is tagged with the version number of the release and a release
  branch is kept for future reference or fixes.
</file>

<file path="stratum-1.4.0/roles/Cargo.toml">
[workspace]
resolver="2"

members = [
    "pool",
    "test-utils/mining-device",
    "test-utils/mining-device-sv1",
    "translator",
    "jd-client",
    "jd-server",
    "roles-utils/network-helpers"
]

[profile.dev]
# Required by super_safe_lock
opt-level = 1

[profile.test]
# Required by super_safe_lock
opt-level = 1
</file>

<file path="stratum-1.4.0/roles/jd-client/Cargo.toml">
[package]
name = "jd_client"
version = "0.1.4"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "Job Declarator Client (JDC) role"
documentation = "https://docs.rs/jd_client"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[lib]
name = "jd_client"
path = "src/lib/mod.rs"

[dependencies]
secp256k1 = { version = "0.28.2", default-features = false, features = ["alloc", "rand", "rand-std"] }
async-channel = "1.5.1"
async-recursion = "0.3.2"
buffer_sv2 = { path = "../../utils/buffer" }
stratum-common = { path = "../../common", features = ["with_network_helpers"] }
serde = { version = "1.0.89", default-features = false, features = ["derive", "alloc"] }
futures = "0.3.25"
tokio = { version = "1.44.1", features = ["full"] }
ext-config = { version = "0.14.0", features = ["toml"], package = "config" }
tracing = { version = "0.1" }
error_handling = { path = "../../utils/error-handling" }
nohash-hasher = "0.2.0"
key-utils = { path = "../../utils/key-utils" }
primitive-types = "0.13.1"
config-helpers = { path = "../roles-utils/config-helpers" }
clap = { version = "4.5.39", features = ["derive"] }
</file>

<file path="stratum-1.4.0/roles/jd-client/config-examples/jdc-config-hosted-example.toml">
# SRI JDC config
listening_address = "127.0.0.1:34265"

# Version support
max_supported_version = 2
min_supported_version = 2

# Withhold
withhold = false

# Auth keys for open encrypted connection downstream
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600

# Template Provider config
# Local TP (this is pointing to localhost so you must run a TP locally for this configuration to work)
# tp_address = "127.0.0.1:8442"
# Hosted testnet TP 
tp_address = "75.119.150.111:8442"
tp_authority_public_key = "9bwHCYnjhbHm4AS3pWg9MtAH83mzWohoJJJDELYBqZhDNqszDLc"

# string to be added into `extranonce_prefix`
# note: these bytes are fixed and they effectively reduce the search space available for the extranonce
# the bigger this field, the smaller the search space available for downstream
jdc_signature = "JDC"

# Solo Mining config
# Coinbase output used to build the coinbase tx in case of Solo Mining (as last-resort solution of the pools fallback system)
#
# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./jd-client.log"

[timeout]
unit = "secs"
value = 1

# List of upstreams (JDS) used as backup endpoints
# In case of shares refused by the JDS, the fallback system will propose the same job to the next upstream in this list
[[upstreams]]
authority_pubkey = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
pool_address = "75.119.150.111:34254"
jd_address = "75.119.150.111:34264"
 
# [[upstreams]]
# authority_pubkey = "2di19GHYQnAZJmEpoUeP7C3Eg9TCcksHr23rZCC83dvUiZgiDL"
# pool_address = "127.0.0.1:34254"
# jd_address = "127.0.0.1:34264"
</file>

<file path="stratum-1.4.0/roles/jd-client/config-examples/jdc-config-local-example.toml">
# SRI JDC config
listening_address = "127.0.0.1:34265"

# Version support
max_supported_version = 2
min_supported_version = 2

# Withhold
withhold = false

# Auth keys for open encrypted connection downstream
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600

# Template Provider config
# Local TP (this is pointing to localhost so you must run a TP locally for this configuration to work)
tp_address = "127.0.0.1:8442"
# Hosted testnet TP 
# tp_address = "75.119.150.111:8442"

# string to be added into `extranonce_prefix`
# note: these bytes are fixed and they effectively reduce the search space available for the extranonce
# the bigger this field, the smaller the search space available for downstream
jdc_signature = "JDC"

# Solo Mining config
# Coinbase output used to build the coinbase tx in case of Solo Mining (as last-resort solution of the pools fallback system)
#
# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./jd-client.log"

[timeout]
unit = "secs"
value = 1

# List of upstreams (JDS) used as backup endpoints
# In case of shares refused by the JDS, the fallback system will propose the same job to the next upstream in this list
[[upstreams]]
authority_pubkey = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
pool_address = "127.0.0.1:34254"
jd_address = "127.0.0.1:34264"
 
# [[upstreams]]
# authority_pubkey = "2di19GHYQnAZJmEpoUeP7C3Eg9TCcksHr23rZCC83dvUiZgiDL"
# pool_address = "127.0.0.1:34254"
# jd_address = "127.0.0.1:34264"
</file>

<file path="stratum-1.4.0/roles/jd-client/README.md">
# JD Client

* connect to the jd-server
* connect to the template-provider
The JD Client receives custom block templates from a Template Provider and declares use of the template with the pool using the Job Declaration Protocol. Further distributes the jobs to Mining Proxy (or Proxies) using the Job Distribution Protocol. ```
* transparently relay the `OpenExtendedChannel` to upstream 

## Setup

### Configuration File

The configuration file contains the following information:

1. The downstream socket information, which includes the listening IP address (`downstream_address`) and port (`downstream_port`).
2. The maximum and minimum SRI versions (`max_supported_version` and `min_supported_version`) with size as (`min_extranonce2_size`)
3. The authentication keys for the downstream connection (`authority_public_key`, `authority_secret_key`)
4. A `retry` parameter which tells JDC the number of times to reinitialize itself after a failure.
6. The Template Provider address (`tp_address`).
7. Optionally, you may want to verify that your TP connection is authentic. You may get `tp_authority_public_key` from the logs of your TP, for example:

# 2024-02-13T14:59:24Z Template Provider authority key: EguTM8URcZDQVeEBsM4B5vg9weqEUnufA8pm85fG4bZd

### Run

Run the Job Declarator Client (JDC):
There are two files when you cd into roles/jd-client/config-examples/

1. `jdc-config-hosted-example.toml` connects to the community-hosted roles.
2. `jdc-config-local-example.toml` connects to self-hosted Job Declarator Client (JDC) and Translator Proxy

``` bash
cd roles/jd-client/config-examples/
cargo run -- -c jdc-config-hosted-example.toml
```
</file>

<file path="stratum-1.4.0/roles/jd-client/src/args.rs">
//! ## CLI Arguments Parsing Module
//!
//! This module is responsible for parsing the command-line arguments provided
//! to the application.

use clap::Parser;
use ext_config::{Config, File, FileFormat};
use jd_client::{
    config::JobDeclaratorClientConfig,
    error::{Error, ProxyResult},
};

use std::path::PathBuf;
use tracing::error;
#[derive(Debug, Parser)]
#[command(author, version, about = "JD Client", long_about = None)]
pub struct Args {
    #[arg(
        short = 'c',
        long = "config",
        help = "Path to the TOML configuration file",
        default_value = "jdc-config.toml"
    )]
    pub config_path: PathBuf,
    #[arg(
        short = 'f',
        long = "log-file",
        help = "Path to the log file. If not set, logs will only be written to stdout."
    )]
    pub log_file: Option<PathBuf>,
}

/// Process CLI args and load configuration.
#[allow(clippy::result_large_err)]
pub fn process_cli_args<'a>() -> ProxyResult<'a, JobDeclaratorClientConfig> {
    // Parse CLI arguments
    let args = Args::parse();

    // Build configuration from the provided file path
    let config_path = args.config_path.to_str().ok_or_else(|| {
        error!("Invalid configuration path.");
        Error::BadCliArgs
    })?;

    let settings = Config::builder()
        .add_source(File::new(config_path, FileFormat::Toml))
        .build()?;

    // Deserialize settings into JobDeclaratorClientConfig
    let mut config = settings.try_deserialize::<JobDeclaratorClientConfig>()?;

    config.set_log_file(args.log_file);

    Ok(config)
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/config.rs">
//! ## JDC Configuration Module
//!
//! The main configuration struct is [`JobDeclaratorClientConfig`], which is typically
//! loaded from a configuration file (e.g., TOML). Helper structs like [`PoolConfig`],
//! [`TPConfig`], [`ProtocolConfig`], and [`Upstream`] are used during the construction
//! of the main configuration.

#![allow(dead_code)]
use config_helpers::CoinbaseOutput;
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use serde::Deserialize;
use std::{
    net::SocketAddr,
    path::{Path, PathBuf},
    time::Duration,
};
use stratum_common::roles_logic_sv2::bitcoin::{Amount, TxOut};

/// Represents the configuration of a Job Declarator Client (JDC).
///
/// This struct holds all the necessary configuration parameters for a JDC instance.
/// JDC can operate in two modes:
///
/// 1. Downstream: Connects to a mining pool (specifically Pool and JDS) and a Template Provider
///    (TP) to receive job templates. The pool and jds connection details are specified in the
///    `upstreams` field, and the TP connection details are in `tp_address`.
/// 2. Upstream: Listens for incoming connections from other downstreams on the address specified in
///    `listening_address`.

#[derive(Debug, Deserialize, Clone)]
pub struct JobDeclaratorClientConfig {
    // The address on which the JDC will listen for incoming connections when acting as an
    // upstream.
    listening_address: SocketAddr,
    // The maximum supported SV2 protocol version.
    max_supported_version: u16,
    // The minimum supported SV2 protocol version.
    min_supported_version: u16,
    // Needs more discussion..
    withhold: bool,
    // The public key used by this JDC for noise encryption.
    authority_public_key: Secp256k1PublicKey,
    /// The secret key used by this JDC for noise encryption.
    authority_secret_key: Secp256k1SecretKey,
    /// The validity period (in seconds) for the certificate used in noise.
    cert_validity_sec: u64,
    /// The address of the TP that this JDC will connect to.
    tp_address: String,
    /// The expected public key of the TP's authority for authentication (optional).
    tp_authority_public_key: Option<Secp256k1PublicKey>,
    /// A list of upstream Job Declarator Servers (JDS) that this JDC can connect to.
    /// JDC can fallover between these upstreams.
    upstreams: Vec<Upstream>,
    /// The timeout duration for network operations.
    #[serde(deserialize_with = "config_helpers::duration_from_toml")]
    timeout: Duration,
    /// A list of coinbase outputs to be included in the block templates.
    /// This is only used during solo-mining.
    #[serde(alias = "coinbase_output")] // only one is allowed, so don't make the user type the plural
    #[serde(deserialize_with = "config_helpers::deserialize_vec_exactly_1")]
    coinbase_outputs: Vec<CoinbaseOutput>,
    /// A signature string identifying this JDC instance.
    jdc_signature: String,
    /// The path to the log file where JDC will write logs.
    log_file: Option<PathBuf>,
}

impl JobDeclaratorClientConfig {
    /// Creates a new instance of [`JobDeclaratorClientConfig`].
    ///
    /// # Panics
    ///
    /// Panics if `protocol_config.coinbase_outputs` is empty.
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        listening_address: SocketAddr,
        protocol_config: ProtocolConfig,
        withhold: bool,
        pool_config: PoolConfig,
        tp_config: TPConfig,
        upstreams: Vec<Upstream>,
        timeout: Duration,
        jdc_signature: String,
    ) -> Self {
        assert!(
            !protocol_config.coinbase_outputs.is_empty(),
            "set of coinbase outputs must be nonempty"
        );
        Self {
            listening_address,
            max_supported_version: protocol_config.max_supported_version,
            min_supported_version: protocol_config.min_supported_version,
            withhold,
            authority_public_key: pool_config.authority_public_key,
            authority_secret_key: pool_config.authority_secret_key,
            cert_validity_sec: tp_config.cert_validity_sec,
            tp_address: tp_config.tp_address,
            tp_authority_public_key: tp_config.tp_authority_public_key,
            upstreams,
            timeout,
            coinbase_outputs: protocol_config.coinbase_outputs,
            jdc_signature,
            log_file: None,
        }
    }

    /// Returns the listening address of the Job Declartor Client.
    pub fn listening_address(&self) -> &SocketAddr {
        &self.listening_address
    }

    /// Returns the list of upstreams.
    ///
    /// JDC will try to fallback to the next upstream in case of failure of the current one.
    pub fn upstreams(&self) -> &Vec<Upstream> {
        &self.upstreams
    }

    /// Returns the timeout duration.
    pub fn timeout(&self) -> Duration {
        self.timeout
    }

    /// Returns the withhold flag.
    pub fn withhold(&self) -> bool {
        self.withhold
    }

    /// Returns the authority public key.
    pub fn authority_public_key(&self) -> &Secp256k1PublicKey {
        &self.authority_public_key
    }

    /// Returns the authority secret key.
    pub fn authority_secret_key(&self) -> &Secp256k1SecretKey {
        &self.authority_secret_key
    }

    /// Returns the certificate validity in seconds.
    pub fn cert_validity_sec(&self) -> u64 {
        self.cert_validity_sec
    }

    /// Returns Template Provider address.
    pub fn tp_address(&self) -> &str {
        &self.tp_address
    }

    /// Returns Template Provider authority public key.
    pub fn tp_authority_public_key(&self) -> Option<&Secp256k1PublicKey> {
        self.tp_authority_public_key.as_ref()
    }

    /// Returns the minimum supported version.
    pub fn min_supported_version(&self) -> u16 {
        self.min_supported_version
    }

    /// Returns the maximum supported version.
    pub fn max_supported_version(&self) -> u16 {
        self.max_supported_version
    }

    /// Returns the JDC signature.
    pub fn jdc_signature(&self) -> &str {
        &self.jdc_signature
    }

    pub fn get_txout(&self) -> Vec<TxOut> {
        self.coinbase_outputs
            .iter()
            .map(|out| TxOut {
                value: Amount::from_sat(0),
                script_pubkey: out.script_pubkey().to_owned(),
            })
            .collect()
    }

    pub fn log_file(&self) -> Option<&Path> {
        self.log_file.as_deref()
    }
    pub fn set_log_file(&mut self, log_file: Option<PathBuf>) {
        if let Some(log_file) = log_file {
            self.log_file = Some(log_file);
        }
    }
}

/// Represents pool specific encryption keys.
pub struct PoolConfig {
    authority_public_key: Secp256k1PublicKey,
    authority_secret_key: Secp256k1SecretKey,
}

impl PoolConfig {
    /// Creates a new instance of [`PoolConfig`].
    pub fn new(
        authority_public_key: Secp256k1PublicKey,
        authority_secret_key: Secp256k1SecretKey,
    ) -> Self {
        Self {
            authority_public_key,
            authority_secret_key,
        }
    }
}

/// Represent template provider config for JDC to connect.
pub struct TPConfig {
    // The validity period (in seconds) expected for the Template Provider's certificate.
    cert_validity_sec: u64,
    // The network address of the Template Provider.
    tp_address: String,
    // The expected public key of the Template Provider's authority (optional).
    tp_authority_public_key: Option<Secp256k1PublicKey>,
}

impl TPConfig {
    // Creates a new instance of [`TPConfig`].
    pub fn new(
        cert_validity_sec: u64,
        tp_address: String,
        tp_authority_public_key: Option<Secp256k1PublicKey>,
    ) -> Self {
        Self {
            cert_validity_sec,
            tp_address,
            tp_authority_public_key,
        }
    }
}

/// Represent protocol versioning the JDC supports.
pub struct ProtocolConfig {
    // The maximum supported SV2 protocol version.
    max_supported_version: u16,
    // The minimum supported SV2 protocol version.
    min_supported_version: u16,
    // A list of coinbase outputs to be included in block templates.
    coinbase_outputs: Vec<CoinbaseOutput>,
}

impl ProtocolConfig {
    // Creates a new instance of [`ProtocolConfig`].
    pub fn new(
        max_supported_version: u16,
        min_supported_version: u16,
        coinbase_outputs: Vec<CoinbaseOutput>,
    ) -> Self {
        Self {
            max_supported_version,
            min_supported_version,
            coinbase_outputs,
        }
    }
}

/// Represents necessary fields required to connect to JDS
#[derive(Debug, Deserialize, Clone)]
pub struct Upstream {
    // The public key of the upstream pool's authority for authentication.
    pub authority_pubkey: Secp256k1PublicKey,
    // The address of the upstream pool's main server.
    pub pool_address: String,
    // The network address of the JDS.
    pub jd_address: String,
}

impl Upstream {
    /// Creates a new instance of [`Upstream`].
    pub fn new(
        authority_pubkey: Secp256k1PublicKey,
        pool_address: String,
        jd_address: String,
    ) -> Self {
        Self {
            authority_pubkey,
            pool_address,
            jd_address,
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/downstream.rs">
//! ## Downstream Module
//!
//! It contains the logic and structures for the Job Declarator Client (JDC) to spawn a server and
//! provide ports for downstream proxies or mining nodes to connect to.
//!
//! It handles the lifecycle of downstream connections, including establishing secure
//! communication, processing incoming SV2 messages from the downstream, interpreting
//! these messages based on the SV2 Mining Protocol, and forwarding relevant messages
//! to the appropriate upstream components (either the main Pool or the Job Declarator Server).
//!
//! All the traits required for downstream connection handling, message parsing,
//! and message interpretation are implemented within this module for the `DownstreamMiningNode`
//! struct.
//!
//! Implemented Traits:
//! - [`IsDownstream`]  Provides access to common downstream data.
//! - [`ParseMiningMessagesFromDownstream<UpstreamMiningNode>`]  Handles all messages specific to
//!   the SV2 Mining Protocol received from the downstream.
//! - [`ParseCommonMessagesFromDownstream`]  Handles common SV2 messages like `SetupConnection`
//!   received during the initial handshake.
//! - [`IsMiningDownstream`]  (possibly redundant.)

use super::{config::JobDeclaratorClientConfig, template_receiver::TemplateRx, PoolChangerTrigger};

use super::{
    job_declarator::JobDeclarator,
    status::{self, State},
    upstream_sv2::Upstream as UpstreamMiningNode,
};
use async_channel::{bounded, Receiver, SendError, Sender};
use stratum_common::roles_logic_sv2::{
    self,
    bitcoin::{consensus::deserialize, Amount, TxOut},
    channel_logic::channel_factory::{OnNewShare, PoolChannelFactory, Share},
    codec_sv2,
    common_messages_sv2::{SetupConnection, SetupConnectionSuccess},
    errors::Error,
    handlers::{
        common::{ParseCommonMessagesFromDownstream, SendTo as SendToCommon},
        mining::{ParseMiningMessagesFromDownstream, SendTo, SupportedChannelTypes},
    },
    job_creator::JobsCreators,
    mining_sv2::*,
    parsers::{AnyMessage, Mining, MiningDeviceMessages},
    template_distribution_sv2::{NewTemplate, SubmitSolution},
    utils::Mutex,
};
use tokio::sync::Notify;
use tracing::{debug, error, info, warn};

use codec_sv2::{HandshakeRole, Responder, StandardEitherFrame, StandardSv2Frame};
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};

pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

/// Represents a connection to a single downstream mining node or proxy.
///
/// **NOTE:** The current implementation of the JDC's downstream handling is
/// limited to a one-to-one connection with a single downstream entity. This is
/// noted in the code as a conceptual mistake that needs refactoring to support
/// multiple downstream connections.
///
/// A downstream can be either a direct mining device or another proxy. It is
/// assumed that a single downstream node connects to only one upstream pool
/// at a time via this JDC.
#[derive(Debug)]
pub struct DownstreamMiningNode {
    // Receiver channel for incoming messages from the downstream node.
    receiver: Receiver<EitherFrame>,
    // Sender channel for sending messages to the downstream node.
    sender: Sender<EitherFrame>,
    /// The current status of the downstream connection, tracking its lifecycle stage.
    pub status: DownstreamMiningNodeStatus,
    // This field might be used in future for job tracking. or not sure.
    #[allow(dead_code)]
    pub prev_job_id: Option<u32>,
    // Sender channel for forwarding validated miner solutions to the template receiver
    // or other components that handle block solution submission.
    solution_sender: Sender<SubmitSolution<'static>>,
    // Needs more discussion..
    withhold: bool,
    task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    // Sender for communicating status updates (e.g., disconnection) back to the main Status loop.
    tx_status: status::Sender,
    // The miner's configured coinbase output(s). Used in solo mining mode.
    miner_coinbase_output: Vec<TxOut>,
    // The template ID of the last job sent to this downstream. Used to correlate
    // submitted shares with the correct job ID when sending upstream.
    last_template_id: u64,
    /// `JobDeclarator` instance. Present when connected to a pool and using the Job Declaration
    /// Protocol. Absent in solo mining mode.
    pub jd: Option<Arc<Mutex<JobDeclarator>>>,
    // The JDC's signature string, used in solo mining channel setup.
    jdc_signature: String,
}

/// Represents the different lifecycle stages of a connection with a downstream mining node or
/// proxy.
///
/// This enum tracks the state transitions for both regular downstream connections
/// (paired with an upstream pool) and solo mining downstream connections.
#[allow(clippy::large_enum_variant)]
#[derive(Debug)]
pub enum DownstreamMiningNodeStatus {
    /// The downstream node is in the initial handshake/initialization phase.
    /// Holds an optional reference to the upstream if connecting to a pool.
    Initializing(Option<Arc<Mutex<UpstreamMiningNode>>>),
    /// The downstream node has completed the initial setup and is paired with an upstream pool.
    /// Holds common downstream data and a reference to the upstream.
    Paired(Arc<Mutex<UpstreamMiningNode>>),
    /// A mining channel (specifically an Extended channel in this implementation)
    /// has been successfully opened for the downstream node.
    /// Holds the `PoolChannelFactory` for this channel and a reference to the upstream.
    ChannelOpened((PoolChannelFactory, Arc<Mutex<UpstreamMiningNode>>)),
    /// The downstream node has completed initialization and is operating in solo mining mode.
    /// Holds common downstream data.
    SoloMinerPaired,
    /// The solo miner has opened a mining channel.
    /// Holds the `PoolChannelFactory` for this channel and common downstream data.
    SoloMinerChannelOpend(PoolChannelFactory),
}

impl DownstreamMiningNodeStatus {
    // Checks if the downstream connection is in a paired state (either with an upstream pool or in
    // solo mining).
    fn is_paired(&self) -> bool {
        match self {
            DownstreamMiningNodeStatus::Initializing(_) => false,
            DownstreamMiningNodeStatus::Paired(_) => true,
            DownstreamMiningNodeStatus::ChannelOpened(_) => true,
            DownstreamMiningNodeStatus::SoloMinerPaired => true,
            DownstreamMiningNodeStatus::SoloMinerChannelOpend(_) => true,
        }
    }

    // Transitions the status from `Initializing` to either `Paired` (if an upstream exists)
    // or `SoloMinerPaired` (if in solo mining mode).
    fn pair(&mut self) {
        match self {
            DownstreamMiningNodeStatus::Initializing(Some(up)) => {
                let self_ = Self::Paired(up.clone());
                let _ = std::mem::replace(self, self_);
            }
            DownstreamMiningNodeStatus::Initializing(None) => {
                let self_ = Self::SoloMinerPaired;
                let _ = std::mem::replace(self, self_);
            }
            _ => panic!("Try to pair an already paired downstream"),
        }
    }

    // Sets the `PoolChannelFactory` for the downstream connection and transitions the status
    // to either `ChannelOpened` (if paired with upstream) or `SoloMinerChannelOpend`
    // (if in solo mining mode).
    fn set_channel(&mut self, channel: PoolChannelFactory) -> bool {
        match self {
            DownstreamMiningNodeStatus::Initializing(_) => false,
            DownstreamMiningNodeStatus::Paired(up) => {
                let self_ = Self::ChannelOpened((channel, up.clone()));
                let _ = std::mem::replace(self, self_);
                true
            }
            DownstreamMiningNodeStatus::ChannelOpened(_) => false,
            DownstreamMiningNodeStatus::SoloMinerPaired => {
                let self_ = Self::SoloMinerChannelOpend(channel);
                let _ = std::mem::replace(self, self_);
                true
            }
            DownstreamMiningNodeStatus::SoloMinerChannelOpend(_) => false,
        }
    }

    /// Returns a mutable reference to the `PoolChannelFactory` if the downstream is in a
    /// channel-opened state (either pooled or solo mining).
    pub fn get_channel(&mut self) -> &mut PoolChannelFactory {
        match self {
            DownstreamMiningNodeStatus::Initializing(_) => panic!(),
            DownstreamMiningNodeStatus::Paired(_) => panic!(),
            DownstreamMiningNodeStatus::ChannelOpened((channel, _)) => channel,
            DownstreamMiningNodeStatus::SoloMinerPaired => panic!(),
            DownstreamMiningNodeStatus::SoloMinerChannelOpend(channel) => channel,
        }
    }

    // Checks if the downstream connection has an opened mining channel.
    fn have_channel(&self) -> bool {
        match self {
            DownstreamMiningNodeStatus::Initializing(_) => false,
            DownstreamMiningNodeStatus::Paired(_) => false,
            DownstreamMiningNodeStatus::ChannelOpened(_) => true,
            DownstreamMiningNodeStatus::SoloMinerPaired => false,
            DownstreamMiningNodeStatus::SoloMinerChannelOpend(_) => true,
        }
    }

    // Returns an optional Arc-wrapped Mutex reference to the upstream mining node
    // if the downstream is connected to one.
    fn get_upstream(&mut self) -> Option<Arc<Mutex<UpstreamMiningNode>>> {
        match self {
            DownstreamMiningNodeStatus::Initializing(Some(up)) => Some(up.clone()),
            DownstreamMiningNodeStatus::Paired(up) => Some(up.clone()),
            DownstreamMiningNodeStatus::ChannelOpened((_, up)) => Some(up.clone()),
            DownstreamMiningNodeStatus::Initializing(None) => None,
            DownstreamMiningNodeStatus::SoloMinerPaired => None,
            DownstreamMiningNodeStatus::SoloMinerChannelOpend(_) => None,
        }
    }

    // Checks if the downstream connection is operating in solo mining mode.
    fn is_solo_miner(&mut self) -> bool {
        matches!(
            self,
            DownstreamMiningNodeStatus::Initializing(None)
                | DownstreamMiningNodeStatus::SoloMinerPaired
                | DownstreamMiningNodeStatus::SoloMinerChannelOpend(_)
        )
    }
}

use core::convert::TryInto;
use std::{net::IpAddr, str::FromStr, sync::Arc};

impl DownstreamMiningNode {
    /// Constructs a new `DownstreamMiningNode` instance.
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        receiver: Receiver<EitherFrame>,
        sender: Sender<EitherFrame>,
        upstream: Option<Arc<Mutex<UpstreamMiningNode>>>,
        solution_sender: Sender<SubmitSolution<'static>>,
        withhold: bool,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
        tx_status: status::Sender,
        miner_coinbase_output: Vec<TxOut>,
        jd: Option<Arc<Mutex<JobDeclarator>>>,
        jdc_signature: String,
    ) -> Self {
        Self {
            receiver,
            sender,
            status: DownstreamMiningNodeStatus::Initializing(upstream),
            prev_job_id: None,
            solution_sender,
            withhold,
            task_collector,
            tx_status,
            miner_coinbase_output,
            // set it to an arbitrary value cause when we use it we always updated it.
            // Is used before sending the share to upstream in the main loop when we have a share.
            // Is upated in the message handler that si called earlier in the main loop.
            last_template_id: 0,
            jd,
            jdc_signature,
        }
    }

    /// Starts the processing of messages from the downstream mining node.
    ///
    /// This method is called after the initial `SetupConnection` handshake is complete
    /// and the downstream's status has been set to `Paired` or `SoloMinerPaired`.
    /// It sends a `SetupConnectionSuccess` message back to the downstream and then
    /// enters a loop to continuously receive and process incoming messages.
    /// If the downstream connection closes, it reports a `DownstreamShutdown` status.
    pub async fn start(
        self_mutex: &Arc<Mutex<Self>>,
        setup_connection_success: SetupConnectionSuccess,
    ) {
        // Ensure the downstream is in a paired state before starting message processing.
        if self_mutex
            .safe_lock(|self_| self_.status.is_paired())
            .unwrap()
        {
            let setup_connection_success: MiningDeviceMessages = setup_connection_success.into();

            {
                DownstreamMiningNode::send(
                    self_mutex,
                    setup_connection_success.try_into().unwrap(),
                )
                .await
                .unwrap();
            }
            let receiver = self_mutex
                .safe_lock(|self_| self_.receiver.clone())
                .unwrap();
            Self::set_channel_factory(self_mutex.clone());

            while let Ok(message) = receiver.recv().await {
                let incoming: StdFrame = message.try_into().unwrap();
                Self::next(self_mutex, incoming).await;
            }
            let tx_status = self_mutex.safe_lock(|s| s.tx_status.clone()).unwrap();
            let err = Error::DownstreamDown;
            let status = status::Status {
                state: State::DownstreamShutdown(err.into()),
            };
            tx_status.send(status).await.unwrap();
        } else {
            panic!()
        }
    }

    // Sets up the `PoolChannelFactory` for this downstream connection.
    //
    // In pooled mining mode, it waits for the `OpenExtendedMiningChannelSuccess`
    // message from the upstream, which provides the necessary information to
    // take the initialized factory from the upstream instance.
    // In solo mining mode, it creates a new `PoolChannelFactory` instance locally.
    fn set_channel_factory(self_mutex: Arc<Mutex<Self>>) {
        // Check if the downstream is in solo mining mode.
        if !self_mutex.safe_lock(|s| s.status.is_solo_miner()).unwrap() {
            // Safe unwrap already checked if it contains an upstream with `is_solo_miner`
            let upstream = self_mutex
                .safe_lock(|s| s.status.get_upstream().unwrap())
                .unwrap();
            // Spawn a task to wait for and take the channel factory from the upstream.
            let recv_factory = {
                let self_mutex = self_mutex.clone();
                tokio::task::spawn(async move {
                    let factory = UpstreamMiningNode::take_channel_factory(upstream).await;
                    self_mutex
                        .safe_lock(|s| {
                            s.status.set_channel(factory);
                        })
                        .unwrap();
                })
            };
            self_mutex
                .safe_lock(|s| {
                    s.task_collector
                        .safe_lock(|c| c.push(recv_factory.abort_handle()))
                        .unwrap()
                })
                .unwrap();
        }
    }

    /// Parses the received message from the downstream and dispatches it to the
    /// appropriate handler based on the `ParseMiningMessagesFromDownstream` trait.
    ///
    /// After processing, it calls `match_send_to` to handle the result from the handler.
    pub async fn next(self_mutex: &Arc<Mutex<Self>>, mut incoming: StdFrame) {
        // Extract message type and payload. Panics on missing header or type.
        let message_type = incoming.get_header().unwrap().msg_type();
        let payload = incoming.payload();

        let next_message_to_send = ParseMiningMessagesFromDownstream::handle_message_mining(
            self_mutex.clone(),
            message_type,
            payload,
        );

        // Process the result from the message handler
        Self::match_send_to(self_mutex.clone(), next_message_to_send, Some(incoming)).await;
    }

    /// Recursively handles the `SendTo` result from a message handler.
    ///
    /// This method determines how to proceed based on the instruction received
    /// from the handler:
    /// - `RelaySameMessageToRemote`: Relays the original incoming message to the upstream.
    /// - `RelayNewMessage`: Sends a newly constructed message (e.g., a processed share) to the
    ///   upstream. Includes logic for retrieving the correct job ID for shares.
    /// - `Multiple`: Processes a vector of `SendTo` instructions recursively.
    /// - `Respond`: Sends a message as a response back to the downstream.
    /// - `None`: Indicates no immediate action is required.
    #[async_recursion::async_recursion]
    async fn match_send_to(
        self_mutex: Arc<Mutex<Self>>,
        next_message_to_send: Result<SendTo<UpstreamMiningNode>, Error>,
        incoming: Option<StdFrame>,
    ) {
        match next_message_to_send {
            // If the handler requests to relay the same message upstream.
            Ok(SendTo::RelaySameMessageToRemote(upstream_mutex)) => {
                let sv2_frame: codec_sv2::Sv2Frame<AnyMessage, buffer_sv2::Slice> =
                    incoming.unwrap().map(|payload| payload.try_into().unwrap());

                // Send the message to the upstream.
                UpstreamMiningNode::send(&upstream_mutex, sv2_frame)
                    .await
                    .unwrap();
            }
            // If the handler requests to relay a new message, specifically SubmitSharesExtended.
            Ok(SendTo::RelayNewMessage(Mining::SubmitSharesExtended(mut share))) => {
                // This case is for pooled mining, where the JDC forwards a share.
                let upstream_mutex = self_mutex
                    .safe_lock(|s| s.status.get_upstream().unwrap())
                    .unwrap();

                // When re receive SetupConnectionSuccess we link the last_template_id with the
                // pool's job_id. The below return as soon as we have a pairable job id for the
                // template_id associated with this share.
                let last_template_id = self_mutex.safe_lock(|s| s.last_template_id).unwrap();
                let job_id_future =
                    UpstreamMiningNode::get_job_id(&upstream_mutex, last_template_id);

                // Wait for the job ID with a timeout. If it times out, don't send the share
                let job_id = match timeout(Duration::from_secs(10), job_id_future).await {
                    Ok(job_id) => job_id,
                    Err(_) => {
                        return;
                    }
                };

                // Set the job ID in the share message.
                share.job_id = job_id;
                debug!(
                    "Sending valid block solution upstream, with job_id {}",
                    job_id
                );
                let message = Mining::SubmitSharesExtended(share);
                let message: AnyMessage = AnyMessage::Mining(message);
                let sv2_frame: codec_sv2::Sv2Frame<AnyMessage, buffer_sv2::Slice> =
                    message.try_into().unwrap();

                // Send the share message to the upstream.
                UpstreamMiningNode::send(&upstream_mutex, sv2_frame)
                    .await
                    .unwrap();
            }
            // If the handler requests to relay a *new* message.
            Ok(SendTo::RelayNewMessage(message)) => {
                let message: AnyMessage = AnyMessage::Mining(message);
                let sv2_frame: codec_sv2::Sv2Frame<AnyMessage, buffer_sv2::Slice> =
                    message.try_into().unwrap();
                let upstream_mutex = self_mutex.safe_lock(|s| s.status.get_upstream().expect("We should return RelayNewMessage only if we are not in solo mining mode")).unwrap();
                UpstreamMiningNode::send(&upstream_mutex, sv2_frame)
                    .await
                    .unwrap();
            }
            // If the handler requests to send multiple messages.
            Ok(SendTo::Multiple(messages)) => {
                // Iterate through the messages and handle each one recursively.
                for message in messages {
                    // Note: We pass None for `incoming` as these are new messages, not the
                    // original.
                    Self::match_send_to(self_mutex.clone(), Ok(message), None).await;
                }
            }
            // If the handler requests to send a response back to the downstream.
            Ok(SendTo::Respond(message)) => {
                let message = MiningDeviceMessages::Mining(message);
                let sv2_frame: codec_sv2::Sv2Frame<MiningDeviceMessages, buffer_sv2::Slice> =
                    message.try_into().unwrap();

                // Send the response message downstream.
                Self::send(&self_mutex, sv2_frame).await.unwrap();
            }
            Ok(SendTo::None(None)) => (),
            Ok(m) => unreachable!("Unexpected message type: {:?}", m),
            Err(_) => todo!(),
        }
    }

    /// Send a message downstream
    pub async fn send(
        self_mutex: &Arc<Mutex<Self>>,
        sv2_frame: StdFrame,
    ) -> Result<(), SendError<StdFrame>> {
        let either_frame = sv2_frame.into();
        let sender = self_mutex.safe_lock(|self_| self_.sender.clone()).unwrap();
        match sender.send(either_frame).await {
            Ok(_) => Ok(()),
            Err(_) => {
                todo!()
            }
        }
    }

    /// Handles a `NewTemplate` message received from the template receiver.
    ///
    /// This method is called directly from the template receiver when a new block
    /// template is available. It updates the internal `PoolChannelFactory` with
    /// the new template and the pool's coinbase output (if a channel is open).
    /// It then generates the appropriate mining messages (`NewExtendedMiningJob`)
    /// using the channel factory and sends them downstream to the miner.
    /// Finally, it updates the `last_template_id` and sets the `IS_NEW_TEMPLATE_HANDLED`
    /// global flag to `true` (Release ordering) to signal that the downstream
    /// has finished processing the new template.
    pub async fn on_new_template(
        self_mutex: &Arc<Mutex<Self>>,
        mut new_template: NewTemplate<'static>,
        pool_outputs: &[u8],
    ) -> Result<(), Error> {
        // Check if a channel is open. If not, just set the flag and return.
        if !self_mutex.safe_lock(|s| s.status.have_channel()).unwrap() {
            super::IS_NEW_TEMPLATE_HANDLED.store(true, std::sync::atomic::Ordering::Release);
            return Ok(());
        }

        let mut deserialized_outputs: Vec<TxOut> = deserialize(pool_outputs).unwrap();

        // we know the first output is where the template revenue must
        // be allocated
        deserialized_outputs[0].value = Amount::from_sat(new_template.coinbase_tx_value_remaining);

        // Update the channel factory with the new template and pool outputs and get messages to
        // send downstream.
        let to_send = self_mutex
            .safe_lock(|s| {
                let channel = s.status.get_channel();
                channel.update_pool_outputs(deserialized_outputs);
                channel.on_new_template(&mut new_template)
            })
            .unwrap()?;

        // to_send is a HashMap<channel_id, messages_to_send>. Since this implementation
        // currently only supports one downstream connection and one channel, we can
        // take all messages from the map's values.
        let to_send = to_send.into_values();

        // Send each generated message downstream.
        for message in to_send {
            // If the message is NewExtendedMiningJob, update the JobDeclarator's coinbase prefix
            // and suffix.
            let message = if let Mining::NewExtendedMiningJob(job) = message {
                if let Some(jd) = self_mutex.safe_lock(|s| s.jd.clone()).unwrap() {
                    jd.safe_lock(|jd| {
                        jd.coinbase_tx_prefix = job.coinbase_tx_prefix.clone();
                        jd.coinbase_tx_suffix = job.coinbase_tx_suffix.clone();
                    })
                    .unwrap();
                }
                Mining::NewExtendedMiningJob(job)
            } else {
                message
            };
            let message = MiningDeviceMessages::Mining(message);
            let frame: StdFrame = message.try_into().unwrap();
            Self::send(self_mutex, frame).await.unwrap();
        }

        // Set the global flag to true to indicate that the downstream
        // has finished handling the NewTemplate message.
        super::IS_NEW_TEMPLATE_HANDLED.store(true, std::sync::atomic::Ordering::Release);
        Ok(())
    }

    /// Handles a `SetNewPrevHash` message received from the template receiver.
    ///
    /// This method is called directly from the template receiver when a new
    /// previous block hash is available. It updates the internal `PoolChannelFactory`
    /// with the new previous hash (if a channel is open), which is necessary for
    /// the factory to generate correct job IDs. It then generates the appropriate
    /// `SetNewPrevHash` message for the downstream miner and sends it.
    pub async fn on_set_new_prev_hash(
        self_mutex: &Arc<Mutex<Self>>,
        new_prev_hash: roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'static>,
    ) -> Result<(), Error> {
        // Check if a channel is open. If not, return immediately.
        if !self_mutex.safe_lock(|s| s.status.have_channel()).unwrap() {
            return Ok(());
        }

        // Update the channel factory with the new previous hash and get the corresponding job ID.
        let job_id = self_mutex
            .safe_lock(|s| {
                let channel = s.status.get_channel();
                channel.on_new_prev_hash_from_tp(&new_prev_hash)
            })
            .unwrap()?;

        // Get the extended channel IDs from the factory. Expect exactly one in this 1:1 setup.
        let channel_ids = self_mutex
            .safe_lock(|s| s.status.get_channel().get_extended_channels_ids())
            .unwrap();

        // Determine the channel ID. Panics if the number of channels is not 1.
        let channel_id = match channel_ids.len() {
            1 => channel_ids[0],
            _ => unreachable!(),
        };

        // Construct the SetNewPrevHash message for the downstream miner.
        let to_send = SetNewPrevHash {
            channel_id,
            job_id,
            prev_hash: new_prev_hash.prev_hash,
            min_ntime: new_prev_hash.header_timestamp,
            nbits: new_prev_hash.n_bits,
        };
        let message = MiningDeviceMessages::Mining(Mining::SetNewPrevHash(to_send));
        let frame = message.try_into().unwrap();
        Self::send(self_mutex, frame).await.unwrap();
        Ok(())
    }
}

/// It impl UpstreamMining cause the proxy act as an upstream node for the DownstreamMiningNode
impl ParseMiningMessagesFromDownstream<UpstreamMiningNode> for DownstreamMiningNode {
    // Returns the channel type supported between the downstream mining node and
    // this JDC instance. Only `Extended` channels are supported.
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::Extended
    }

    // Indicates whether work selection is enabled for this connection.
    // In this JDC implementation acting as an upstream for the downstream, work
    // selection is handled upstream (by the pool or JDC logic), not by the downstream.
    fn is_work_selection_enabled(&self) -> bool {
        false
    }

    // Checks if a downstream user identity is authorized to connect.
    fn is_downstream_authorized(
        _self_mutex: Arc<Mutex<Self>>,
        _user_identity: &Str0255,
    ) -> Result<bool, Error> {
        Ok(true)
    }

    // Handles an `OpenStandardMiningChannel` message received from the downstream.
    //
    // This method logs a warning and ignores the message because the JDC
    // is configured to only support `Extended` mining channels with downstream nodes.
    //
    // Returns `Ok(SendTo::None(None))` indicating no action is taken.
    fn handle_open_standard_mining_channel(
        &mut self,
        _: OpenStandardMiningChannel,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        warn!("Ignoring OpenStandardMiningChannel");
        Ok(SendTo::None(None))
    }

    // Handles an `OpenExtendedMiningChannel` message received from the downstream.
    //
    // This is the expected message from a downstream miner to open a mining channel.
    // - If the JDC is connected to a pool, it relays this message to the upstream pool to request
    //   an extended channel there.
    // - If the JDC is in solo mining mode, it creates a local `PoolChannelFactory` for this channel
    //   and responds with `OpenExtendedMiningChannelSuccess`.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(upstream_mutex))` if in pooled mining mode.
    // Returns `Ok(SendTo::Multiple(messages))` if in solo mining mode, containing
    // the `OpenExtendedMiningChannelSuccess` and potentially other initial messages.
    fn handle_open_extended_mining_channel(
        &mut self,
        m: OpenExtendedMiningChannel,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        info!(
            "Received OpenExtendedMiningChannel from: {} with id: {}",
            std::str::from_utf8(m.user_identity.as_ref()).unwrap_or("Unknown identity"),
            m.get_request_id_as_u32()
        );
        debug!("OpenExtendedMiningChannel: {}", m);

        // Check if the downstream is in solo mining mode.
        if !self.status.is_solo_miner() {
            // If not solo mining, it's pooled mining. Relay the message upstream.
            // Safe unwrap: is_solo_miner being false implies there's an upstream.
            Ok(SendTo::RelaySameMessageToRemote(
                self.status.get_upstream().unwrap(),
            ))
        } else {
            // If in solo mining mode, create a local channel factory.
            // The channel factory is created here to ensure it exists when a channel is opened.
            // hardcoded value
            let extranonce_len = 32;
            let jdc_signature_len = self.jdc_signature.len();
            let range_0 = std::ops::Range { start: 0, end: 0 };

            // JDC only allows for one single downstream, so we don't need any free bytes on range_1
            // we just allocate enough space for the JDC signature
            let range_1 = std::ops::Range {
                start: 0,
                end: jdc_signature_len,
            };
            let range_2 = std::ops::Range {
                start: jdc_signature_len,
                end: extranonce_len,
            };
            let ids = Arc::new(Mutex::new(roles_logic_sv2::utils::GroupId::new()));
            let coinbase_outputs = self.miner_coinbase_output.clone();

            // Create the ExtendedExtranonce structure.
            let extranonces = ExtendedExtranonce::new(
                range_0,
                range_1,
                range_2,
                Some(self.jdc_signature.as_bytes().to_vec()),
            )
            .map_err(|_| {
                roles_logic_sv2::Error::ExtendedExtranonceCreationFailed(
                    "Failed to create ExtendedExtranonce".into(),
                )
            })?;
            let creator = JobsCreators::new(extranonce_len as u8);
            // hardcoded value
            let share_per_min = 1.0;
            let kind = roles_logic_sv2::channel_logic::channel_factory::ExtendedChannelKind::Pool;

            // Create the PoolChannelFactory instance for solo mining.
            let channel_factory = PoolChannelFactory::new(
                ids,
                extranonces,
                creator,
                share_per_min,
                kind,
                coinbase_outputs,
            );

            // Set the created channel factory in the downstream's status.
            self.status.set_channel(channel_factory);

            // Process the new extended channel request in the locally created factory.
            let request_id = m.request_id;
            let hash_rate = m.nominal_hash_rate;
            let min_extranonce_size = m.min_extranonce_size;
            let messages_res = self.status.get_channel().new_extended_channel(
                request_id,
                hash_rate,
                min_extranonce_size,
            );

            // Based on the factory's response, generate messages to send back to the downstream.
            match messages_res {
                Ok(messages) => {
                    let messages = messages.into_iter().map(SendTo::Respond).collect();
                    Ok(SendTo::Multiple(messages))
                }
                Err(_) => Err(roles_logic_sv2::Error::ChannelIsNeitherExtendedNeitherInAPool),
            }
        }
    }

    // Handles an `UpdateChannel` message received from the downstream.
    //
    // - If in pooled mining mode, it relays this message upstream to the pool.
    // - If in solo mining mode, it updates the maximum target in the local `PoolChannelFactory`
    //   based on the nominal hash rate provided by the miner and responds with a `SetTarget`
    //   message to the downstream.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(upstream_mutex))` if in pooled mining mode.
    // Returns `Ok(SendTo::Respond(Mining::SetTarget(set_target)))` if in solo mining mode.
    fn handle_update_channel(
        &mut self,
        m: UpdateChannel,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        info!("Received UpdateChannel message");

        // Check if the downstream is in solo mining mode.
        if !self.status.is_solo_miner() {
            // If not solo mining, relay the message upstream.
            // Safe unwrap: is_solo_miner being false implies there's an upstream
            Ok(SendTo::RelaySameMessageToRemote(
                self.status.get_upstream().unwrap(),
            ))
        } else {
            // If in solo mining mode, calculate the target based on the miner's hash rate.
            let maximum_target =
                roles_logic_sv2::utils::hash_rate_to_target(m.nominal_hash_rate.into(), 10.0)?;

            // Update the target in the local channel factory.
            self.status
                .get_channel()
                .update_target_for_channel(m.channel_id, maximum_target.clone().into());

            // Construct the SetTarget message for the downstream.
            let set_target = SetTarget {
                channel_id: m.channel_id,
                maximum_target,
            };
            Ok(SendTo::Respond(Mining::SetTarget(set_target)))
        }
    }

    // Handles a `SubmitSharesStandard` message received from the downstream.
    //
    // Returns `Ok(SendTo::None(None))` indicating no action is taken.
    fn handle_submit_shares_standard(
        &mut self,
        _: SubmitSharesStandard,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        warn!("Ignoring SubmitSharesStandard");
        Ok(SendTo::None(None))
    }

    /// Handles a `SubmitSharesExtended` message received from the downstream.
    ///
    /// This method processes a submitted share from the miner using the internal
    /// `PoolChannelFactory` to validate it against the current job's target.
    /// - If the share meets the downstream target, it is processed further.
    /// - If the share meets the Bitcoin target (in solo mining), it's a potential block.
    /// - If the share meets the upstream pool's target (in pooled mining and withholding is off),
    ///   it is relayed upstream.
    /// - If the share is invalid, an error response is sent downstream.
    fn handle_submit_shares_extended(
        &mut self,
        m: SubmitSharesExtended,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        info!("Received SubmitSharesExtended message");
        debug!("SubmitSharesExtended {}", m);

        // Process the submitted share using the channel factory.
        match self
            .status
            .get_channel()
            .on_submit_shares_extended(m.clone())
            .unwrap()
        {
            // If the share does not meet the downstream target.
            OnNewShare::SendErrorDownstream(s) => {
                error!("Share does not meet the downstream target");
                Ok(SendTo::Respond(Mining::SubmitSharesError(s)))
            }
            // If the share is valid and should be sent upstream (pooled mining).
            OnNewShare::SendSubmitShareUpstream((m, Some(template_id))) => {
                if !self.status.is_solo_miner() {
                    match m {
                        Share::Extended(share) => {
                            // Update the last_template_id for correlating with upstream job ID.
                            let for_upstream = Mining::SubmitSharesExtended(share);
                            self.last_template_id = template_id;
                            Ok(SendTo::RelayNewMessage(for_upstream))
                        }
                        // We are in an extended channel shares are extended
                        Share::Standard(_) => unreachable!(),
                    }
                } else {
                    // This case should not happen in solo mining, as shares meeting Bitcoin target
                    // are handled separately.
                    Ok(SendTo::None(None))
                }
            }
            OnNewShare::RelaySubmitShareUpstream => unreachable!(),
            // If the share meets the Bitcoin target (potential block found).
            OnNewShare::ShareMeetBitcoinTarget((
                share,
                Some(template_id),
                coinbase,
                extranonce,
            )) => {
                match share {
                    Share::Extended(share) => {
                        // Get the solution sender channel
                        let solution_sender = self.solution_sender.clone();

                        // Construct the SubmitSolution message for the template receiver.
                        let solution = SubmitSolution {
                            template_id,
                            version: share.version,
                            header_timestamp: share.ntime,
                            header_nonce: share.nonce,
                            coinbase_tx: coinbase.try_into()?,
                        };

                        // Send the solution to the solution sender. Blocking send is used,
                        // expecting the channel to not be full. Panics on send failure.
                        solution_sender.send_blocking(solution).unwrap();

                        // If not in solo mining mode, send the solution to the Job Declarator
                        // to potentially push it as a block candidate to the JDS.
                        if !self.status.is_solo_miner() {
                            {
                                let jd = self.jd.clone();
                                let mut share = share.clone();
                                // Update the share's extranonce with the full calculated
                                // extranonce.
                                share.extranonce = extranonce.try_into().unwrap();
                                // Spawn a task to send the solution to the Job Declarator.
                                tokio::task::spawn(async move {
                                    JobDeclarator::on_solution(&jd.unwrap(), share).await
                                });
                            }
                        }

                        // If not withholding and not in solo mining mode, relay the share upstream.
                        // This is likely for block propagation to the pool.
                        // Safe unwrap: is_solo_miner being false implies there's an upstream.
                        if !self.withhold && !self.status.is_solo_miner() {
                            self.last_template_id = template_id;
                            let for_upstream = Mining::SubmitSharesExtended(share);
                            Ok(SendTo::RelayNewMessage(for_upstream))
                        } else {
                            // If withholding or in solo mining, no action is needed upstream.
                            Ok(SendTo::None(None))
                        }
                    }
                    // We are in an extended channel shares are extended
                    Share::Standard(_) => unreachable!(),
                }
            }
            // When we have a ShareMeetBitcoinTarget it means that the proxy know the bitcoin
            // target that means that the proxy must have JD capabilities that means that the
            // second tuple elements can not be None but must be Some(template_id)
            OnNewShare::ShareMeetBitcoinTarget(_) => unreachable!(),
            OnNewShare::SendSubmitShareUpstream(_) => unreachable!(),
            OnNewShare::ShareMeetDownstreamTarget => Ok(SendTo::None(None)),
        }
    }

    /// Handles a `SetCustomMiningJob` message received from the downstream.
    ///
    /// Returns `Ok(SendTo::None(None))` indicating no action is taken.
    fn handle_set_custom_mining_job(
        &mut self,
        _: SetCustomMiningJob,
    ) -> Result<SendTo<UpstreamMiningNode>, Error> {
        warn!("Ignoring SetCustomMiningJob");
        Ok(SendTo::None(None))
    }
}

impl ParseCommonMessagesFromDownstream for DownstreamMiningNode {
    /// Handles a `SetupConnection` message received from the downstream.
    ///
    /// Returns `Ok(SendToCommon::Respond(response.into()))` indicating that a
    /// `SetupConnectionSuccess` message should be sent back to the downstream
    fn handle_setup_connection(
        &mut self,
        m: SetupConnection,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, Error> {
        info!(
            "Received `SetupConnection`: version={}, flags={:b}",
            m.min_version, m.flags
        );
        let response = SetupConnectionSuccess {
            used_version: 2,
            // require extended channels
            flags: 0b0000_0000_0000_0010,
        };
        self.status.pair();
        Ok(SendToCommon::Respond(response.into()))
    }
}

use std::net::SocketAddr;
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::codec_sv2::binary_sv2::Str0255,
};
use tokio::{
    net::TcpListener,
    task::AbortHandle,
    time::{timeout, Duration},
};

/// Starts listening for incoming downstream mining node connections on the specified address.
///
/// This function sets up a TCP listener and continuously accepts incoming connections.
/// For each accepted connection, it performs a Noise handshake,
/// initializes a `DownstreamMiningNode` instance, and spawns an task to handle
/// that specific downstream connection's lifecycle and message processing.
///
/// NOTE: The current implementation is explicitly designed to handle only one downstream
/// connection at a time. If a second connection attempt is made while one is active,
/// it will be ignored and logged. This limitation needs refactoring to properly manage
/// the state and resources of multiple downstream nodes concurrently for a production environment.
///
/// FIX ME: There is a noted issue where the downstream connection is established
/// and fully processed (including the initial handshake and starting its main loop)
/// *before* the connection with the Template Provider is initiated. This order can lead
/// to the downstream miner receiving jobs before the JDC has received template information.
/// The connection order should either be mutually exclusive or carefully managed to ensure
/// template information is available before providing jobs to the downstream.
#[allow(clippy::too_many_arguments)]
pub async fn listen_for_downstream_mining(
    address: SocketAddr,
    upstream: Option<Arc<Mutex<UpstreamMiningNode>>>,
    withhold: bool,
    authority_public_key: Secp256k1PublicKey,
    authority_secret_key: Secp256k1SecretKey,
    cert_validity_sec: u64,
    task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    tx_status: async_channel::Sender<status::Status<'static>>,
    miner_coinbase_output: Vec<TxOut>,
    jd: Option<Arc<Mutex<JobDeclarator>>>,
    config: JobDeclaratorClientConfig,
    shutdown: Arc<Notify>,
    jdc_signature: String,
) {
    info!("Listening for downstream mining connections on {}", address);

    // Bind to the listener address.
    let listener = TcpListener::bind(address).await.unwrap();
    let mut has_downstream = false;

    // Loop indefinitely to accept incoming connections or handle shutdown.
    loop {
        tokio::select! {
            // Handle shutdown signal.
            _ = shutdown.notified() => {
                info!("Shutdown signal received. Stopping downstream mining listener.");
                break;
            }
            // Accept an incoming TCP connection.
            Ok((stream, _)) = listener.accept() => {
                // Check if a downstream connection is already active.
                if has_downstream {
                    error!("A downstream connection is already active. Ignoring additional connections.");
                    continue;
                }
                has_downstream = true;
                let task_collector = task_collector.clone();
                let miner_coinbase_output = miner_coinbase_output.clone();
                let jd = jd.clone();
                let upstream = upstream.clone();
                let timeout = config.timeout();
                let mut parts = config.tp_address().split(':');
                let ip_tp = parts.next().unwrap().to_string();
                let port_tp = parts.next().unwrap().parse::<u16>().unwrap();

                // Create a channel for sending miner solutions from this downstream to the template receiver.
                let (send_solution, recv_solution) = bounded(10);

                let responder = Responder::from_authority_kp(
                    &authority_public_key.into_bytes(),
                    &authority_secret_key.into_bytes(),
                    std::time::Duration::from_secs(cert_validity_sec),
                )
                .unwrap();
                let (receiver, sender) =
                    Connection::new(stream, HandshakeRole::Responder(responder))
                        .await
                        .expect("impossible to connect");

                let tx_status_downstream = status::Sender::Downstream(tx_status.clone());

                // Create a new DownstreamMiningNode instance for this connection.
                let node = DownstreamMiningNode::new(
                    receiver,
                    sender,
                    upstream.clone(),
                    send_solution,
                    withhold,
                    task_collector.clone(),
                    tx_status_downstream,
                    miner_coinbase_output,
                    jd.clone(),
                    jdc_signature.clone(),
                );

                // The first message from the downstream should be SetupConnection.
                // Receive and attempt to parse this initial message.
                let mut incoming: StdFrame = node.receiver.recv().await.unwrap().try_into().unwrap();
                let message_type = incoming.get_header().unwrap().msg_type();
                let payload = incoming.payload();
                let node = Arc::new(Mutex::new(node));

                // If connected to an upstream pool, set this downstream as the upstream's downstream.
                if let Some(upstream) = upstream {
                    upstream
                        .safe_lock(|s| s.downstream = Some(node.clone()))
                        .unwrap();
                }

                if let Ok(SendToCommon::Respond(message)) = DownstreamMiningNode::handle_message_common(
                    node.clone(),
                    message_type,
                    payload,
                ) {
                    let message = match message {
                        roles_logic_sv2::parsers::CommonMessages::SetupConnectionSuccess(m) => m,
                        _ => panic!(),
                    };

                    // Spawn a task to start the main message processing loop for this downstream.
                    let main_task = tokio::task::spawn({
                        let node = node.clone();
                        async move {
                            DownstreamMiningNode::start(&node, message).await;
                        }
                    });

                    node.safe_lock(|n| {
                        n.task_collector
                            .safe_lock(|c| {
                                c.push(main_task.abort_handle());
                            })
                            .unwrap()
                    })
                    .unwrap();


                    TemplateRx::connect(
                        SocketAddr::new(IpAddr::from_str(ip_tp.as_str()).unwrap(), port_tp),
                        recv_solution,
                        status::Sender::TemplateReceiver(tx_status.clone()),
                        jd,
                        node,
                        task_collector,
                        Arc::new(Mutex::new(PoolChangerTrigger::new(timeout))),
                        vec![],
                        config.tp_authority_public_key().cloned(),
                    )
                    .await;
                }
            }
        }
    }

    info!("Downstream mining listener has shut down.");
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/error.rs">
//! ## Error Module
//!
//! Defines [`Error`], the central error enum used throughout the Job Declarator Client (JDC).
//!
//! It unifies errors from:
//! - I/O operations
//! - Channels (send/recv)
//! - SV2 stack: Binary, Codec, Noise, Framing, RolesLogic
//! - Locking logic (PoisonError)
//! - Domain-specific issues
//!
//! This module ensures that all errors can be passed around consistently, including across async
//! boundaries.
use ext_config::ConfigError;
use std::fmt;
use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{self, binary_sv2, framing_sv2},
    mining_sv2::{ExtendedExtranonce, NewExtendedMiningJob, SetCustomMiningJob},
};

pub type ProxyResult<'a, T> = core::result::Result<T, Error<'a>>;

#[allow(dead_code)]
#[derive(Debug)]
pub enum ChannelSendError<'a> {
    SubmitSharesExtended(
        async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>,
    ),
    SetNewPrevHash(async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>),
    NewExtendedMiningJob(async_channel::SendError<NewExtendedMiningJob<'a>>),
    General(String),
    Extranonce(async_channel::SendError<(ExtendedExtranonce, u32)>),
    SetCustomMiningJob(
        async_channel::SendError<roles_logic_sv2::mining_sv2::SetCustomMiningJob<'a>>,
    ),
    NewTemplate(
        async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    ),
}

#[derive(Debug)]
pub enum Error<'a> {
    #[allow(dead_code)]
    VecToSlice32(Vec<u8>),
    /// Errors on bad CLI argument input.
    BadCliArgs,
    /// Errors on bad `config` TOML deserialize.
    BadConfigDeserialize(ConfigError),
    /// Errors from `binary_sv2` crate.
    BinarySv2(binary_sv2::Error),
    /// Errors on bad noise handshake.
    CodecNoise(codec_sv2::noise_sv2::Error),
    /// Errors from `framing_sv2` crate.
    FramingSv2(framing_sv2::Error),
    /// Errors on bad `TcpStream` connection.
    Io(std::io::Error),
    /// Errors on bad `String` to `int` conversion.
    ParseInt(std::num::ParseIntError),
    /// Errors from `roles_logic_sv2` crate.
    RolesSv2Logic(roles_logic_sv2::errors::Error),
    UpstreamIncoming(roles_logic_sv2::errors::Error),
    #[allow(dead_code)]
    SubprotocolMining(String),
    // Locking Errors
    PoisonLock,
    // Channel Receiver Error
    ChannelErrorReceiver(async_channel::RecvError),
    TokioChannelErrorRecv(tokio::sync::broadcast::error::RecvError),
    // Channel Sender Errors
    ChannelErrorSender(ChannelSendError<'a>),
    Infallible(std::convert::Infallible),
}

impl fmt::Display for Error<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        use Error::*;
        match self {
            BadCliArgs => write!(f, "Bad CLI arg input"),
            BadConfigDeserialize(ref e) => write!(f, "Bad `config` TOML deserialize: `{e:?}`"),
            BinarySv2(ref e) => write!(f, "Binary SV2 error: `{e:?}`"),
            CodecNoise(ref e) => write!(f, "Noise error: `{e:?}"),
            FramingSv2(ref e) => write!(f, "Framing SV2 error: `{e:?}`"),
            Io(ref e) => write!(f, "I/O error: `{e:?}"),
            ParseInt(ref e) => write!(f, "Bad convert from `String` to `int`: `{e:?}`"),
            RolesSv2Logic(ref e) => write!(f, "Roles SV2 Logic Error: `{e:?}`"),
            SubprotocolMining(ref e) => write!(f, "Subprotocol Mining Error: `{e:?}`"),
            UpstreamIncoming(ref e) => write!(f, "Upstream parse incoming error: `{e:?}`"),
            PoisonLock => write!(f, "Poison Lock error"),
            ChannelErrorReceiver(ref e) => write!(f, "Channel receive error: `{e:?}`"),
            TokioChannelErrorRecv(ref e) => write!(f, "Channel receive error: `{e:?}`"),
            ChannelErrorSender(ref e) => write!(f, "Channel send error: `{e:?}`"),
            VecToSlice32(ref e) => write!(f, "Standard Error: `{e:?}`"),
            Infallible(ref e) => write!(f, "Infallible Error:`{e:?}`"),
        }
    }
}

impl From<binary_sv2::Error> for Error<'_> {
    fn from(e: binary_sv2::Error) -> Self {
        Error::BinarySv2(e)
    }
}

impl From<codec_sv2::noise_sv2::Error> for Error<'_> {
    fn from(e: codec_sv2::noise_sv2::Error) -> Self {
        Error::CodecNoise(e)
    }
}

impl From<framing_sv2::Error> for Error<'_> {
    fn from(e: framing_sv2::Error) -> Self {
        Error::FramingSv2(e)
    }
}

impl From<std::io::Error> for Error<'_> {
    fn from(e: std::io::Error) -> Self {
        Error::Io(e)
    }
}

impl From<std::num::ParseIntError> for Error<'_> {
    fn from(e: std::num::ParseIntError) -> Self {
        Error::ParseInt(e)
    }
}

impl From<roles_logic_sv2::errors::Error> for Error<'_> {
    fn from(e: roles_logic_sv2::errors::Error) -> Self {
        Error::RolesSv2Logic(e)
    }
}

impl From<ConfigError> for Error<'_> {
    fn from(e: ConfigError) -> Self {
        Error::BadConfigDeserialize(e)
    }
}

impl From<async_channel::RecvError> for Error<'_> {
    fn from(e: async_channel::RecvError) -> Self {
        Error::ChannelErrorReceiver(e)
    }
}

impl From<tokio::sync::broadcast::error::RecvError> for Error<'_> {
    fn from(e: tokio::sync::broadcast::error::RecvError) -> Self {
        Error::TokioChannelErrorRecv(e)
    }
}

// *** LOCK ERRORS ***
// impl<'a> From<PoisonError<MutexGuard<'a, proxy::Bridge>>> for Error<'a> {
//     fn from(e: PoisonError<MutexGuard<'a, proxy::Bridge>>) -> Self {
//         Error::PoisonLock(
//             LockError::Bridge(e)
//         )
//     }
// }

// impl<'a> From<PoisonError<MutexGuard<'a, NextMiningNotify>>> for Error<'a> {
//     fn from(e: PoisonError<MutexGuard<'a, NextMiningNotify>>) -> Self {
//         Error::PoisonLock(
//             LockError::NextMiningNotify(e)
//         )
//     }
// }

// *** CHANNEL SENDER ERRORS ***
impl<'a> From<async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>>
    for Error<'a>
{
    fn from(
        e: async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>,
    ) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SubmitSharesExtended(e))
    }
}

impl<'a> From<async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>>
    for Error<'a>
{
    fn from(e: async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SetNewPrevHash(e))
    }
}

impl From<async_channel::SendError<(ExtendedExtranonce, u32)>> for Error<'_> {
    fn from(e: async_channel::SendError<(ExtendedExtranonce, u32)>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::Extranonce(e))
    }
}

impl<'a> From<async_channel::SendError<NewExtendedMiningJob<'a>>> for Error<'a> {
    fn from(e: async_channel::SendError<NewExtendedMiningJob<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::NewExtendedMiningJob(e))
    }
}

impl<'a> From<async_channel::SendError<SetCustomMiningJob<'a>>> for Error<'a> {
    fn from(e: async_channel::SendError<SetCustomMiningJob<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SetCustomMiningJob(e))
    }
}

impl<'a>
    From<
        async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    > for Error<'a>
{
    fn from(
        e: async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    ) -> Self {
        Error::ChannelErrorSender(ChannelSendError::NewTemplate(e))
    }
}

impl From<std::convert::Infallible> for Error<'_> {
    fn from(e: std::convert::Infallible) -> Self {
        Error::Infallible(e)
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/job_declarator/message_handler.rs">
//! Job Declarator: Message handler module
//!
//! Handles upstream Job Declaration Protocol messages by implementing the
//! `ParseJobDeclarationMessagesFromUpstream` trait.
use super::JobDeclarator;
use stratum_common::roles_logic_sv2::{
    codec_sv2::binary_sv2,
    handlers::{job_declaration::ParseJobDeclarationMessagesFromUpstream, SendTo_},
    job_declaration_sv2::{
        AllocateMiningJobTokenSuccess, DeclareMiningJobError, DeclareMiningJobSuccess,
        ProvideMissingTransactions, ProvideMissingTransactionsSuccess,
    },
    parsers::JobDeclaration,
};
use tracing::{debug, error, info};
pub type SendTo = SendTo_<JobDeclaration<'static>, ()>;
use stratum_common::roles_logic_sv2::errors::Error;

impl ParseJobDeclarationMessagesFromUpstream for JobDeclarator {
    /// Handles an `AllocateMiningJobTokenSuccess` message received from the JDS.
    ///
    /// This message indicates that the JDS has successfully allocated a mining job token
    /// in response to a previous `AllocateMiningJobToken` request. The allocated token
    /// is added to the `JobDeclarator`'s internal pool of available tokens.
    ///
    /// Returns `Ok(SendTo::None(None))` indicating that no immediate response
    /// is needed back to the JDS after successfully receiving and processing the token.
    fn handle_allocate_mining_job_token_success(
        &mut self,
        message: AllocateMiningJobTokenSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `AllocateMiningJobTokenSuccess` with id: {}",
            message.request_id
        );
        self.allocated_tokens.push(message.into_static());

        Ok(SendTo::None(None))
    }

    /// Handles a `DeclareMiningJobSuccess` message received from the JDS.
    ///
    /// Returns `Ok(SendTo::None(Some(message)))` wrapping the processed message
    /// for potential forwarding or further handling within the `JobDeclarator`'s
    /// message processing loop.
    fn handle_declare_mining_job_success(
        &mut self,
        message: DeclareMiningJobSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `DeclareMiningJobSuccess` with id {}",
            message.request_id
        );
        debug!("`DeclareMiningJobSuccess`: {}", message);
        let message = JobDeclaration::DeclareMiningJobSuccess(message.into_static());
        Ok(SendTo::None(Some(message)))
    }

    /// Handles a `DeclareMiningJobError` message received from the JDS.
    ///
    /// Returns `Ok(SendTo::None(None))` indicating that no immediate response is
    /// needed back to the JDS after receiving a job declaration error. The error
    /// has been logged.
    fn handle_declare_mining_job_error(
        &mut self,
        message: DeclareMiningJobError,
    ) -> Result<SendTo, Error> {
        error!(
            "Received `DeclareMiningJobError`, error code: {}",
            std::str::from_utf8(message.error_code.as_ref()).unwrap_or("unknown error code")
        );
        debug!("`DeclareMiningJobError`: {}", message);
        Ok(SendTo::None(None))
    }

    /// Handles a `ProvideMissingTransactions` message received from the JDS.
    ///
    /// This message is sent by the JDS to request the full transaction data for
    /// specific transactions that it has identified as missing based on previous
    /// communication (e.g., from an `IdentifyTransactions` exchange or a job declaration).
    ///
    /// The handler retrieves the full transaction list for the requested job (identified
    /// by `request_id`) from its `last_declare_mining_jobs_sent` window. It then filters
    /// this list to include only the transactions at the positions specified in the
    /// `unknown_tx_position_list` from the incoming message and constructs a
    /// `ProvideMissingTransactionsSuccess` message containing the requested transactions.
    ///
    /// Returns `Ok(SendTo::Respond(message_enum))` indicating that a response
    /// (`ProvideMissingTransactionsSuccess`) containing the requested transactions
    /// should be sent back to the JDS.
    fn handle_provide_missing_transactions(
        &mut self,
        message: ProvideMissingTransactions,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `ProvideMissingTransactions` with id: {}",
            message.request_id
        );
        debug!("`ProvideMissingTransactions`: {}", message);

        // Find the corresponding declared job in the window using the request ID.
        // Extract the full transaction list from the found job's details.
        let tx_list = self
            .last_declare_mining_jobs_sent
            .iter()
            .find_map(|entry| {
                if let Some((id, last_declare_job)) = entry {
                    if *id == message.request_id {
                        Some(last_declare_job.clone().tx_list.into_inner())
                    } else {
                        None
                    }
                } else {
                    None
                }
            })
            .ok_or(Error::UnknownRequestId(message.request_id))?;

        // Get the list of positions for missing transactions.
        let unknown_tx_position_list: Vec<u16> = message.unknown_tx_position_list.into_inner();

        // Filter the full transaction list to get the missing transactions based on positions.
        let missing_transactions: Vec<binary_sv2::B016M> = unknown_tx_position_list
            .iter()
            .filter_map(|&pos| tx_list.get(pos as usize).cloned())
            .collect();
        let request_id = message.request_id;
        let message_provide_missing_transactions = ProvideMissingTransactionsSuccess {
            request_id,
            transaction_list: binary_sv2::Seq064K::new(missing_transactions).unwrap(),
        };
        let message_enum =
            JobDeclaration::ProvideMissingTransactionsSuccess(message_provide_missing_transactions);
        Ok(SendTo::Respond(message_enum))
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/job_declarator/mod.rs">
//! ## Job Declarator Module
//!
//! It contains logic and constructs to connect to the Job Declarator Server (JDS) and process
//! messaging logic related to job declaration.
//!
//! It handles the lifecycle of declaring mining jobs to the JDS, including
//! allocating mining job tokens, sending job declarations based on templates,
//! processing responses from the JDS, and pushing block solutions.
//!
//! The module tightly couples with the `template_receiver` for new templates and
//! previous hash updates, and with the `upstream_sv2` module for communication
//! with the main pool instance (to send `SetCustomMiningJob(s)`).

pub mod message_handler;
use async_channel::{Receiver, Sender};
use std::{collections::HashMap, convert::TryInto};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        bitcoin::{
            consensus,
            consensus::{serialize, Decodable},
            hashes::Hash,
            Transaction, TxOut,
        },
        codec_sv2::{
            binary_sv2::{Seq0255, Seq064K, B016M, B064K, U256},
            HandshakeRole, Initiator, StandardEitherFrame, StandardSv2Frame,
        },
        handlers::SendTo_,
        job_declaration_sv2::{AllocateMiningJobTokenSuccess, PushSolution},
        mining_sv2::SubmitSharesExtended,
        parsers::{AnyMessage, JobDeclaration},
        template_distribution_sv2::SetNewPrevHash,
        utils::{deserialize_template_outputs, Mutex},
    },
};
use tokio::task::AbortHandle;
use tracing::{debug, error, info};

use async_recursion::async_recursion;
use nohash_hasher::BuildNoHashHasher;
use std::{net::SocketAddr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    handlers::job_declaration::ParseJobDeclarationMessagesFromUpstream,
    job_declaration_sv2::{AllocateMiningJobToken, DeclareMiningJob},
    template_distribution_sv2::NewTemplate,
    utils::Id,
};

pub type Message = AnyMessage<'static>;
pub type SendTo = SendTo_<JobDeclaration<'static>, ()>;
pub type StdFrame = StandardSv2Frame<Message>;

mod setup_connection;
use setup_connection::SetupConnectionHandler;

use super::{config::JobDeclaratorClientConfig, error::Error, upstream_sv2::Upstream};

/// Struct describing LastDeclareJob fields.
#[derive(Debug, Clone)]
pub struct LastDeclareJob {
    // The `DeclareMiningJob` message that was sent to the JDS.
    declare_job: DeclareMiningJob<'static>,
    // The `NewTemplate` message from which this job was derived.
    template: NewTemplate<'static>,
    // The `SetNewPrevHash` message associated with this job, if it's not a future template.
    prev_hash: Option<SetNewPrevHash<'static>>,
    // The pool's coinbase output(s) for this job
    coinbase_pool_output: Vec<u8>,
    // The list of transactions (as raw bytes) included in this job's template.
    tx_list: Seq064K<'static, B016M<'static>>,
}

/// Struct describing the JDC internal state and components.
#[derive(Debug)]
pub struct JobDeclarator {
    // Receiver channel for messages from the JDS.
    receiver: Receiver<StandardEitherFrame<AnyMessage<'static>>>,
    // Sender channel for messages to the JDS.
    sender: Sender<StandardEitherFrame<AnyMessage<'static>>>,
    // A pool of pre-allocated `AllocateMiningJobTokenSuccess` tokens received from the JDS.
    allocated_tokens: Vec<AllocateMiningJobTokenSuccess<'static>>,
    // A simple ID generator for tracking request IDs.
    req_ids: Id,
    // An array to store information about the last two `DeclareMiningJob` messages sent.
    // This is used to correlate `DeclareMiningJobSuccess` responses.
    last_declare_mining_jobs_sent: [Option<(u32, LastDeclareJob)>; 2],
    // The last received `SetNewPrevHash` message.
    last_set_new_prev_hash: Option<SetNewPrevHash<'static>>,
    // A counter to track how many `SetNewPrevHash` messages have been received
    /// since the last time a future job was promoted. Used to determine if
    /// a future job is still relevant when its `SetNewPrevHash` arrives.
    set_new_prev_hash_counter: u8,
    // A map storing information about future jobs (jobs derived from future templates)
    // received from the Template Provider, keyed by their template ID.
    // This information is kept until the corresponding `SetNewPrevHash` arrives,
    // at which point the future job can be promoted and declared to the upstream pool.
    #[allow(clippy::type_complexity)]
    future_jobs: HashMap<
        u64,
        (
            DeclareMiningJob<'static>,
            Seq0255<'static, U256<'static>>,
            NewTemplate<'static>,
            // pool's outputs
            Vec<u8>,
        ),
        BuildNoHashHasher<u64>,
    >,
    // `Upstream` instance, used for communicating with the main pool instance
    up: Arc<Mutex<Upstream>>,
    task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    // The prefix of the coinbase transaction,
    pub coinbase_tx_prefix: B064K<'static>,
    // The suffix of the coinbase transaction,
    pub coinbase_tx_suffix: B064K<'static>,
}

impl JobDeclarator {
    /// Instantiates a new `JobDeclarator` client, connects to the provided JDS address,
    /// performs the SV2 setup connection handshake, allocates initial mining job tokens,
    /// and starts the background task for processing messages from the JDS.
    pub async fn new(
        address: SocketAddr,
        authority_public_key: [u8; 32],
        config: JobDeclaratorClientConfig,
        up: Arc<Mutex<Upstream>>,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    ) -> Result<Arc<Mutex<Self>>, Error<'static>> {
        let stream = tokio::net::TcpStream::connect(address).await?;
        let initiator = Initiator::from_raw_k(authority_public_key)?;
        let (mut receiver, mut sender) =
            Connection::new(stream, HandshakeRole::Initiator(initiator))
                .await
                .expect("impossible to connect");

        info!(
            "JD Client: SETUP_CONNECTION address: {:?}",
            config.listening_address()
        );

        SetupConnectionHandler::setup(&mut receiver, &mut sender, *config.listening_address())
            .await
            .unwrap();

        info!("JD CONNECTED");

        let self_ = Arc::new(Mutex::new(JobDeclarator {
            receiver,
            sender,
            allocated_tokens: vec![],
            req_ids: Id::new(),
            last_declare_mining_jobs_sent: [None, None],
            last_set_new_prev_hash: None,
            future_jobs: HashMap::with_hasher(BuildNoHashHasher::default()),
            up,
            task_collector,
            coinbase_tx_prefix: vec![].try_into().unwrap(),
            coinbase_tx_suffix: vec![].try_into().unwrap(),
            set_new_prev_hash_counter: 0,
        }));

        Self::allocate_tokens(&self_, 2).await;
        Self::on_upstream_message(self_.clone());
        Ok(self_)
    }

    // Utility method to retrieve information about a previously sent `DeclareMiningJob`
    // from the `last_declare_mining_jobs_sent` window based on its request ID.
    fn get_last_declare_job_sent(
        self_mutex: &Arc<Mutex<Self>>,
        request_id: u32,
    ) -> Option<LastDeclareJob> {
        self_mutex
            .safe_lock(|s| {
                for (id, job) in s.last_declare_mining_jobs_sent.iter().flatten() {
                    if *id == request_id {
                        return Some(job.to_owned());
                    }
                }
                None
            })
            .unwrap()
    }

    // We maintain a window of 2 jobs. If more than 2 blocks are found,
    // the ordering will depend on the request ID. Only the 2 most recent request
    // IDs will be kept in memory, while the rest will be discarded.
    // More information can be found here: https://github.com/stratum-mining/stratum/pull/904#discussion_r1609469048
    fn update_last_declare_job_sent(
        self_mutex: &Arc<Mutex<Self>>,
        request_id: u32,
        j: LastDeclareJob,
    ) {
        self_mutex
            .safe_lock(|s| {
                if let Some(empty_index) = s
                    .last_declare_mining_jobs_sent
                    .iter()
                    .position(|entry| entry.is_none())
                {
                    s.last_declare_mining_jobs_sent[empty_index] = Some((request_id, j));
                } else if let Some((min_index, _)) = s
                    .last_declare_mining_jobs_sent
                    .iter()
                    .enumerate()
                    .filter_map(|(i, entry)| entry.as_ref().map(|(id, _)| (i, id)))
                    .min_by_key(|&(_, id)| id)
                {
                    s.last_declare_mining_jobs_sent[min_index] = Some((request_id, j));
                }
            })
            .unwrap();
    }

    /// This method retrieves an allocated mining job token from the internal pool.
    ///
    /// If the pool of allocated tokens is empty or low, this method triggers the
    /// allocation of more tokens from the JDS and waits until tokens are available
    /// before returning one. This ensures that tokens are always available when
    /// needed to declare new jobs.
    #[async_recursion]
    pub async fn get_last_token(
        self_mutex: &Arc<Mutex<Self>>,
    ) -> AllocateMiningJobTokenSuccess<'static> {
        // Check the current number of allocated tokens.
        let mut token_len = self_mutex.safe_lock(|s| s.allocated_tokens.len()).unwrap();
        match token_len {
            0 => {
                // If no tokens are available, spawn a task to allocate more (2 tokens).
                {
                    let task = {
                        let self_mutex = self_mutex.clone();
                        tokio::task::spawn(async move {
                            Self::allocate_tokens(&self_mutex, 2).await;
                        })
                    };
                    // Add the allocation task's handle to the collector.
                    self_mutex
                        .safe_lock(|s| {
                            s.task_collector
                                .safe_lock(|c| c.push(task.abort_handle()))
                                .unwrap()
                        })
                        .unwrap();
                }

                // Wait until at least one token becomes available to avoid infinite recursion.
                while token_len == 0 {
                    tokio::task::yield_now().await;
                    token_len = self_mutex.safe_lock(|s| s.allocated_tokens.len()).unwrap();
                }
                // Once tokens are available, recursively call get_last_token to retrieve one.
                Self::get_last_token(self_mutex).await
            }
            1 => {
                // If only one token is available, spawn a task to allocate one more
                // to maintain a buffer, but return the current token immediately.
                {
                    let task = {
                        let self_mutex = self_mutex.clone();
                        tokio::task::spawn(async move {
                            Self::allocate_tokens(&self_mutex, 1).await;
                        })
                    };
                    // Add the allocation task's handle to the collector.
                    self_mutex
                        .safe_lock(|s| {
                            s.task_collector
                                .safe_lock(|c| c.push(task.abort_handle()))
                                .unwrap()
                        })
                        .unwrap();
                }
                // There is a token, unwrap is safe
                self_mutex
                    .safe_lock(|s| s.allocated_tokens.pop())
                    .unwrap()
                    .unwrap()
            }
            // There are tokens, unwrap is safe
            _ => self_mutex
                .safe_lock(|s| s.allocated_tokens.pop())
                .unwrap()
                .unwrap(),
        }
    }

    /// Handles the event of a new template being received from the Template Receiver.
    ///
    /// This method constructs a `DeclareMiningJob` message based on the new template,
    /// an allocated mining job token, and the miner's coinbase transaction parts.
    /// It then updates the window of last sent jobs and sends the `DeclareMiningJob`
    /// message to the JDS.
    pub async fn on_new_template(
        self_mutex: &Arc<Mutex<Self>>,
        template: NewTemplate<'static>,
        token: Vec<u8>,
        tx_list_: Seq064K<'static, B016M<'static>>,
        excess_data: B064K<'static>,
        coinbase_pool_output: Vec<u8>,
    ) {
        // Get a new request ID and clone the sender for sending the message.
        let (id, sender) = self_mutex
            .safe_lock(|s| (s.req_ids.next(), s.sender.clone()))
            .unwrap();

        // Deserialize the transaction list to calculate short hashes and the list hash.
        let mut tx_list: Vec<Transaction> = Vec::new();
        let mut txids_as_u256: Vec<U256<'static>> = Vec::new();
        for tx in tx_list_.to_vec() {
            //TODO remove unwrap
            let tx: Transaction = consensus::deserialize(&tx).unwrap();
            let txid = tx.compute_txid();
            let byte_array: [u8; 32] = *txid.as_byte_array();
            let owned_vec: Vec<u8> = byte_array.into();
            let txid_as_u256 = U256::Owned(owned_vec);
            txids_as_u256.push(txid_as_u256);
            tx_list.push(tx);
        }
        let tx_ids = Seq064K::new(txids_as_u256).expect("Failed to create Seq064K");

        // Construct the DeclareMiningJob message.
        let declare_job = DeclareMiningJob {
            request_id: id,
            mining_job_token: token.try_into().unwrap(),
            version: template.version,
            coinbase_prefix: self_mutex
                .safe_lock(|s| s.coinbase_tx_prefix.clone())
                .unwrap(),
            coinbase_suffix: self_mutex
                .safe_lock(|s| s.coinbase_tx_suffix.clone())
                .unwrap(),
            tx_ids_list: tx_ids,
            excess_data, // request transaction data
        };

        // Determine the associated SetNewPrevHash message. This is only relevant
        // if the template is *not* a future template.
        let prev_hash = self_mutex
            .safe_lock(|s| s.last_set_new_prev_hash.clone())
            .unwrap()
            .filter(|_| !template.future_template);

        // Store information about this declared job in the window for tracking responses.
        let last_declare = LastDeclareJob {
            declare_job: declare_job.clone(),
            template,
            prev_hash,
            coinbase_pool_output,
            tx_list: tx_list_.clone(),
        };
        Self::update_last_declare_job_sent(self_mutex, id, last_declare);
        let frame: StdFrame =
            AnyMessage::JobDeclaration(JobDeclaration::DeclareMiningJob(declare_job))
                .try_into()
                .unwrap();
        sender.send(frame.into()).await.unwrap();
    }

    /// This method contains the core logic for processing incoming messages from the JDS.
    ///
    /// It runs in a background task and continuously receives messages from the JDS.
    /// It dispatches these messages to the appropriate handlers defined in the
    /// `ParseJobDeclarationMessagesFromUpstream` trait implementation.
    /// Based on the handler's response, it may process the message further
    /// (e.g., handling `DeclareMiningJobSuccess` or `DeclareMiningJobError`) or
    /// send a response back to the JDS if required.
    pub fn on_upstream_message(self_mutex: Arc<Mutex<Self>>) {
        let up = self_mutex.safe_lock(|s| s.up.clone()).unwrap();
        // Spawn the main task for receiving and processing JDS messages.
        let main_task = {
            let self_mutex = self_mutex.clone();
            tokio::task::spawn(async move {
                let receiver = self_mutex.safe_lock(|d| d.receiver.clone()).unwrap();
                loop {
                    let mut incoming: StdFrame = receiver.recv().await.unwrap().try_into().unwrap();
                    let message_type = incoming.get_header().unwrap().msg_type();
                    let payload = incoming.payload();
                    let next_message_to_send =
                        ParseJobDeclarationMessagesFromUpstream::handle_message_job_declaration(
                            self_mutex.clone(),
                            message_type,
                            payload,
                        );
                    // Process the result of the message handling.
                    match next_message_to_send {
                        // Handle a successful job declaration response.
                        Ok(SendTo::None(Some(JobDeclaration::DeclareMiningJobSuccess(m)))) => {
                            let new_token = m.new_mining_job_token;
                            // Retrieve the information about the original DeclareMiningJob using
                            // the request ID.
                            let last_declare = Self::get_last_declare_job_sent(&self_mutex, m.request_id).unwrap_or_else(|| panic!("Failed to get last declare job: job not found, Request Id: {:?}.", m.request_id));
                            debug!("LastDeclareJob.prev_hash: {:?}", last_declare.prev_hash);
                            let mut last_declare_mining_job_sent = last_declare.declare_job;
                            let is_future = last_declare.template.future_template;
                            let id = last_declare.template.template_id;
                            let merkle_path = last_declare.template.merkle_path.clone();
                            let template = last_declare.template;

                            // TODO: Signaling mechanism needed here to inform on_set_new_prev_hash
                            // that the token has been updated, so it can decide whether to send
                            // SetCustomJobs.
                            if is_future {
                                // If it was a future job, update its mining job token and store it
                                // in the future_jobs map.
                                last_declare_mining_job_sent.mining_job_token = new_token;
                                self_mutex
                                    .safe_lock(|s| {
                                        s.future_jobs.insert(
                                            id,
                                            (
                                                last_declare_mining_job_sent,
                                                merkle_path,
                                                template,
                                                last_declare.coinbase_pool_output,
                                            ),
                                        );
                                    })
                                    .unwrap();
                            } else {
                                // If it was a non-future job, it should have an associated
                                // SetNewPrevHash.
                                let set_new_prev_hash = last_declare.prev_hash;

                                let mut template_coinbase_outputs = deserialize_template_outputs(
                                    template.coinbase_tx_outputs.to_vec(),
                                    template.coinbase_tx_outputs_count,
                                )
                                .expect("Failed to deserialize template outputs");

                                let mut pool_coinbase_outputs = Vec::<TxOut>::consensus_decode(
                                    &mut last_declare.coinbase_pool_output.as_slice(),
                                )
                                .expect("Failed to deserialize pool outputs");
                                pool_coinbase_outputs.append(&mut template_coinbase_outputs);
                                let serialized_pool_outs = serialize(&pool_coinbase_outputs);
                                match set_new_prev_hash {
                                    // Send the SetCustomJobs message to the upstream pool.
                                    Some(p) => Upstream::set_custom_jobs(
                                        &up,
                                        last_declare_mining_job_sent,
                                        p,
                                        merkle_path,
                                        new_token,
                                        template.coinbase_tx_version,
                                        template.coinbase_prefix,
                                        template.coinbase_tx_input_sequence,
                                        serialized_pool_outs,
                                        template.coinbase_tx_locktime,
                                        template.template_id
                                        ).await.unwrap(),
                                    None => panic!("Invalid state we received a NewTemplate not future, without having received a set new prev hash")
                                }
                            }
                        }
                        Ok(SendTo::None(Some(JobDeclaration::DeclareMiningJobError(m)))) => {
                            error!("Job is not verified: {}", m);
                        }
                        Ok(SendTo::None(None)) => (),
                        Ok(SendTo::Respond(m)) => {
                            let sv2_frame: StdFrame =
                                AnyMessage::JobDeclaration(m).try_into().unwrap();
                            let sender =
                                self_mutex.safe_lock(|self_| self_.sender.clone()).unwrap();
                            sender.send(sv2_frame.into()).await.unwrap();
                        }
                        Ok(_) => unreachable!(),
                        Err(_) => todo!(),
                    }
                }
            })
        };
        self_mutex
            .safe_lock(|s| {
                s.task_collector
                    .safe_lock(|c| c.push(main_task.abort_handle()))
                    .unwrap()
            })
            .unwrap();
    }

    /// Handles the event of a `SetNewPrevHash` being received
    /// from the Template Receiver.
    ///
    /// This method updates the stored last `SetNewPrevHash` if the new one is
    /// more recent. It then checks if a corresponding future job is stored.
    /// If a matching future job is found and is still relevant (based on the
    /// `set_new_prev_hash_counter`), the future job is promoted, removed from
    /// the future jobs map, and a `SetCustomJobs` message is sent to the upstream
    /// pool to activate this job for mining.
    pub fn on_set_new_prev_hash(
        self_mutex: Arc<Mutex<Self>>,
        set_new_prev_hash: SetNewPrevHash<'static>,
    ) {
        // Spawn a task to handle the SetNewPrevHash
        tokio::task::spawn(async move {
            let id = set_new_prev_hash.template_id;
            // Update the last_set_new_prev_hash if the new one is more recent.
            let _ = self_mutex.safe_lock(|s| {
                debug!("Before update - last_set_new_prev_hash: {:?}, set_new_prev_hash_counter: {}",
                s.last_set_new_prev_hash, s.set_new_prev_hash_counter);
                let should_update = s
                    .last_set_new_prev_hash
                    .as_ref()
                    .map(|prev| set_new_prev_hash.template_id > prev.template_id)
                    .unwrap_or(true);

                if should_update {
                    s.last_set_new_prev_hash = Some(set_new_prev_hash.clone());
                    s.set_new_prev_hash_counter += 1;
                    debug!("After update - last_set_new_prev_hash updated to: {:?}, set_new_prev_hash_counter: {}",
                    s.last_set_new_prev_hash, s.set_new_prev_hash_counter);
                } else {
                    debug!("Received outdated SetNewPrevHash: {:?} compared to current: {:?}",
                    set_new_prev_hash, s.last_set_new_prev_hash);
                }
            });
            // Loop to find and promote the corresponding future job.
            let (job, up, merkle_path, template, pool_outs) = loop {
                match self_mutex
                    .safe_lock(|s| {
                        // Check if the received SetNewPrevHash is outdated based on the counter
                        if s.set_new_prev_hash_counter > 1
                            && s.last_set_new_prev_hash != Some(set_new_prev_hash.clone())
                        {
                            debug!(
                                "Declared job {} skipped due to set_new_prev_hash_counter",
                                id
                            );
                            s.set_new_prev_hash_counter -= 1;
                            Some(None)
                        } else {
                            // Attempt to remove and retrieve the future job matching the template
                            // ID.
                            s.future_jobs.remove(&id).map(
                                |(job, merkle_path, template, pool_outs)| {
                                    s.future_jobs =
                                        HashMap::with_hasher(BuildNoHashHasher::default());
                                    s.set_new_prev_hash_counter -= 1;
                                    Some((job, s.up.clone(), merkle_path, template, pool_outs))
                                },
                            )
                        }
                    })
                    .unwrap()
                {
                    Some(Some(future_job_tuple)) => break future_job_tuple,
                    Some(None) => return,
                    None => {}
                };
                tokio::task::yield_now().await;
            };
            // The token received from JDS for this job.
            let signed_token = job.mining_job_token.clone();
            // Prepare the pool's coinbase output by appending the template's outputs.
            let mut template_coinbase_outputs = deserialize_template_outputs(
                template.coinbase_tx_outputs.to_vec(),
                template.coinbase_tx_outputs_count,
            )
            .expect("Failed to deserialize template outputs");

            let mut pool_coinbase_outputs =
                Vec::<TxOut>::consensus_decode(&mut pool_outs.as_slice())
                    .expect("Failed to deserialize pool outputs");
            pool_coinbase_outputs.append(&mut template_coinbase_outputs);

            let serialized_pool_outs = serialize(&pool_coinbase_outputs);

            // Send the SetCustomJobs message to the upstream pool to activate this job.
            Upstream::set_custom_jobs(
                &up,
                job,
                set_new_prev_hash,
                merkle_path,
                signed_token,
                template.coinbase_tx_version,
                template.coinbase_prefix,
                template.coinbase_tx_input_sequence,
                serialized_pool_outs,
                template.coinbase_tx_locktime,
                template.template_id,
            )
            .await
            .unwrap();
        });
    }

    /// Sends `AllocateMiningJobToken` messages to the JDS to request new mining job tokens.
    ///
    /// This method is typically called when the internal pool of allocated tokens
    /// is low, ensuring that tokens are available for future job declarations.
    async fn allocate_tokens(self_mutex: &Arc<Mutex<Self>>, token_to_allocate: u32) {
        for i in 0..token_to_allocate {
            let message = JobDeclaration::AllocateMiningJobToken(AllocateMiningJobToken {
                user_identifier: "todo".to_string().try_into().unwrap(),
                request_id: i,
            });
            let sender = self_mutex.safe_lock(|s| s.sender.clone()).unwrap();
            // Safe unwrap message is build above and is valid, below can never panic
            let frame: StdFrame = AnyMessage::JobDeclaration(message).try_into().unwrap();
            // TODO join re
            sender.send(frame.into()).await.unwrap();
        }
    }

    /// Handles the event of a miner solution being received from the downstream.
    ///
    /// This method constructs a `PushSolution` message containing the necessary
    /// solution details and sends it to the JDS.
    pub async fn on_solution(
        self_mutex: &Arc<Mutex<Self>>,
        solution: SubmitSharesExtended<'static>,
    ) {
        // Retrieve the last received SetNewPrevHash message.
        let prev_hash = self_mutex
            .safe_lock(|s| s.last_set_new_prev_hash.clone())
            .unwrap()
            .expect("");

        // Construct the PushSolution message using details from the solution and the last prev
        // hash
        let solution = PushSolution {
            extranonce: solution.extranonce,
            prev_hash: prev_hash.prev_hash,
            ntime: solution.ntime,
            nonce: solution.nonce,
            nbits: prev_hash.n_bits,
            version: solution.version,
        };
        let frame: StdFrame = AnyMessage::JobDeclaration(JobDeclaration::PushSolution(solution))
            .try_into()
            .unwrap();
        let sender = self_mutex.safe_lock(|s| s.sender.clone()).unwrap();
        sender.send(frame.into()).await.unwrap();
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/job_declarator/setup_connection.rs">
//! Job Declarator: Setup Connection Handler Module
//!
//! Handles the logic for setting up a connection with an upstream Job Declarator (JDS).
//!
//! This includes building and sending a `SetupConnection` message, receiving the response,
//! and handling common SV2 connection-related messages.

use async_channel::{Receiver, Sender};
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{StandardEitherFrame, StandardSv2Frame},
    common_messages_sv2::{Protocol, Reconnect, SetupConnection},
    handlers::common::{ParseCommonMessagesFromUpstream, SendTo},
    parsers::AnyMessage,
    utils::Mutex,
    Error,
};
use tracing::info;

pub type Message = AnyMessage<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

/// Manages the process of sending and handling the `SetupConnection` handshake
/// for establishing a connection with a JDS.
pub struct SetupConnectionHandler {}

impl SetupConnectionHandler {
    // Builds a `SetupConnection` message using the given proxy address.
    fn get_setup_connection_message(proxy_address: SocketAddr) -> SetupConnection<'static> {
        let endpoint_host = proxy_address
            .ip()
            .to_string()
            .into_bytes()
            .try_into()
            .unwrap();
        let vendor = String::new().try_into().unwrap();
        let hardware_version = String::new().try_into().unwrap();
        let firmware = String::new().try_into().unwrap();
        let device_id = String::new().try_into().unwrap();
        let mut setup_connection = SetupConnection {
            protocol: Protocol::JobDeclarationProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0b0000_0000_0000_0000_0000_0000_0000_0000,
            endpoint_host,
            endpoint_port: proxy_address.port(),
            vendor,
            hardware_version,
            firmware,
            device_id,
        };
        setup_connection.allow_full_template_mode();
        setup_connection
    }

    /// This method sets up a job declarator connection.
    pub async fn setup(
        receiver: &mut Receiver<EitherFrame>,
        sender: &mut Sender<EitherFrame>,
        proxy_address: SocketAddr,
    ) -> Result<(), ()> {
        let setup_connection = Self::get_setup_connection_message(proxy_address);

        let sv2_frame: StdFrame = AnyMessage::Common(setup_connection.into())
            .try_into()
            .unwrap();
        let sv2_frame = sv2_frame.into();

        sender.send(sv2_frame).await.map_err(|_| ())?;

        let mut incoming: StdFrame = receiver.recv().await.unwrap().try_into().unwrap();

        let message_type = incoming.get_header().unwrap().msg_type();
        let payload = incoming.payload();
        ParseCommonMessagesFromUpstream::handle_message_common(
            Arc::new(Mutex::new(SetupConnectionHandler {})),
            message_type,
            payload,
        )
        .unwrap();
        Ok(())
    }
}

impl ParseCommonMessagesFromUpstream for SetupConnectionHandler {
    // Handles a `SetupConnectionSuccess` message received from the JDS.
    //
    // Returns `Ok(SendTo::None(None))` indicating that no immediate message needs
    // to be sent back to the JDS as a direct response to `SetupConnectionSuccess`.
    fn handle_setup_connection_success(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::SetupConnectionSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `SetupConnectionSuccess` from JDS: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendTo::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_reconnect(&mut self, _m: Reconnect) -> Result<SendTo, Error> {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/mod.rs">
//! ## Job Declarator Client
//!
//! The `JobDeclaratorClient` is a miner-side role responsible for:
//! - Creating new mining jobs from templates received via a Template Provider.
//! - Declaring custom jobs to a remote Job Declarator Server (JDS).
//! - Handling pool fallback by switching to backup pools or entering solo mining mode if needed.

pub mod config;
pub mod downstream;
pub mod error;
pub mod job_declarator;
pub mod status;
pub mod template_receiver;
pub mod upstream_sv2;

use std::{sync::atomic::AtomicBool, time::Duration};

use async_channel::unbounded;
use config::JobDeclaratorClientConfig;
use futures::{select, FutureExt};
use job_declarator::JobDeclarator;
use std::{
    net::{IpAddr, SocketAddr},
    str::FromStr,
    sync::Arc,
};
use stratum_common::roles_logic_sv2::utils::Mutex;
use tokio::{sync::Notify, task::AbortHandle};

use tracing::{error, info};

/// Is used by the template receiver and the downstream. When a NewTemplate is received the context
/// that is running the template receiver set this value to false and then the message is sent to
/// the context that is running the Downstream that do something and then set it back to true.
///
/// In the meantime if the context that is running the template receiver receives a SetNewPrevHash
/// it wait until the value of this global is true before doing anything.
///
/// Acquire and Release memory ordering is used.
///
/// Memory Ordering Explanation:
/// We use Acquire-Release ordering instead of SeqCst or Relaxed for the following reasons:
/// 1. Acquire in template receiver context ensures we see all operations before the Release store
///    the downstream.
/// 2. Within the same execution context (template receiver), a Relaxed store followed by an Acquire
///    load is sufficient. This is because operations within the same context execute in the order
///    they appear in the code.
/// 3. The combination of Release in downstream and Acquire in template receiver contexts
///    establishes a happens-before relationship, guaranteeing that we handle the SetNewPrevHash
///    message after that downstream have finished handling the NewTemplate.
/// 3. SeqCst is overkill we only need to synchronize two contexts, a globally agreed-upon order
///    between all the contexts is not necessary.
pub static IS_NEW_TEMPLATE_HANDLED: AtomicBool = AtomicBool::new(true);

/// Job Declarator Client (or JDC) is the role which is Miner-side, in charge of creating new
/// mining jobs from the templates received by the Template Provider to which it is connected. It
/// declares custom jobs to the JDS, in order to start working on them.
/// JDC is also responsible for putting in action the Pool-fallback mechanism, automatically
/// switching to backup Pools in case of declared custom jobs refused by JDS (which is Pool side).
/// As a solution of last-resort, it is able to switch to Solo Mining until new safe Pools appear
/// in the market.
#[derive(Debug, Clone)]
pub struct JobDeclaratorClient {
    // Configuration of the [`JobDeclaratorClient`].
    config: JobDeclaratorClientConfig,
    // Used for notifying the [`JobDeclaratorClient`] to shutdown gracefully.
    shutdown: Arc<Notify>,
}

impl JobDeclaratorClient {
    /// Instantiate a new `JobDeclaratorClient` instance.
    pub fn new(config: JobDeclaratorClientConfig) -> Self {
        Self {
            config,
            shutdown: Arc::new(Notify::new()),
        }
    }

    /// Starts the main operational loop of the Job Declarator Client.
    ///
    /// This involves connecting to configured upstream pools (or entering solo mining mode),
    /// setting up the Job Declarator Server (JDS) connection, listening for downstream connections,
    /// and managing the template receiving process.
    ///
    /// The method handles automatic pool fallback in case of disconnection or detected
    /// rogue behavior from the current upstream pool. It also manages graceful shutdown
    /// upon receiving a termination signal (e.g., CTRL+C) or encountering internal errors.
    ///
    /// Subsystems are spawned sequentially with dependencies: Pool  JDS  Downstream  Template
    /// Receiver (implicitly handled within Downstream or other components).
    pub async fn start(self) {
        let mut upstream_index = 0;

        // Channel used to manage failed tasks
        let (tx_status, rx_status) = unbounded();

        let task_collector = Arc::new(Mutex::new(vec![]));

        // Spawn a task to listen for the CTRL+C signal for graceful shutdown.
        tokio::spawn({
            let shutdown_signal = self.shutdown.clone();
            async move {
                if tokio::signal::ctrl_c().await.is_ok() {
                    info!("Interrupt received");
                    shutdown_signal.notify_one();
                }
            }
        });

        let config = self.config;
        'outer: loop {
            let task_collector = task_collector.clone();
            let tx_status = tx_status.clone();
            let config = config.clone();
            let shutdown = self.shutdown.clone();
            let root_handler;

            // Check if there is a configured upstream pool and jds at the current index.
            if let Some(upstream) = config.upstreams().get(upstream_index) {
                let tx_status = tx_status.clone();
                let task_collector = task_collector.clone();
                let upstream = upstream.clone();
                // Spawn the initialization process for connecting to a pool.
                root_handler = tokio::spawn(async move {
                    Self::initialize_jd(config, tx_status, task_collector, upstream, shutdown)
                        .await;
                });
            } else {
                // If no more upstream pools are configured, enter solo mining mode.
                let tx_status: async_channel::Sender<status::Status<'_>> = tx_status.clone();
                let task_collector = task_collector.clone();
                root_handler = tokio::spawn(async move {
                    Self::initialize_jd_as_solo_miner(
                        config,
                        tx_status.clone(),
                        task_collector.clone(),
                        shutdown,
                    )
                    .await;
                });
            }

            // Inner loop to monitor the status of the root handler and spawned tasks.
            loop {
                select! {
                    task_status = rx_status.recv().fuse() => {
                        if let Ok(task_status) = task_status {
                            match task_status.state {
                                // Should only be sent by the downstream listener
                                status::State::DownstreamShutdown(err) => {
                                    error!("SHUTDOWN from: {}", err);
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    task_collector
                                        .safe_lock(|s| {
                                            for handle in s {
                                                handle.abort();
                                            }
                                        })
                                        .unwrap();
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    break;
                                }
                                status::State::UpstreamShutdown(err) => {
                                    error!("SHUTDOWN from: {}", err);
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    task_collector
                                        .safe_lock(|s| {
                                            for handle in s {
                                                handle.abort();
                                            }
                                        })
                                        .unwrap();
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    break;
                                }
                                status::State::UpstreamRogue => {
                                    error!("Changing Pool");
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    task_collector
                                        .safe_lock(|s| {
                                            for handle in s {
                                                handle.abort();
                                            }
                                        })
                                        .unwrap();
                                    upstream_index += 1;
                                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                                    break;
                                }
                                status::State::Healthy(msg) => {
                                    info!("HEALTHY message: {}", msg);
                                }
                            }
                        } else {
                            info!("Received unknown task. Shutting down.");
                            task_collector
                                .safe_lock(|s| {
                                    for handle in s {
                                        handle.abort();
                                    }
                                })
                                .unwrap();
                            root_handler.abort();
                            break 'outer;
                        }
                    },
                    _ = self.shutdown.notified().fuse() => {
                        info!("Shutting down gracefully...");
                        task_collector
                            .safe_lock(|s| {
                                for handle in s {
                                    handle.abort();
                                }
                            })
                            .unwrap();
                        root_handler.abort();
                        break 'outer;
                    }
                };
            }
        }
    }

    // Initializes the Job Declarator Client to operate in solo mining mode.
    //
    // This function is called when no upstream pools are configured or available.
    // In solo mining mode, the JDC will generate its own mining jobs rather than
    // receiving them from a pool. It primarily sets up the downstream listener
    // to provide these solo mining jobs to connected miners.
    async fn initialize_jd_as_solo_miner(
        config: JobDeclaratorClientConfig,
        tx_status: async_channel::Sender<status::Status<'static>>,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
        shutdown: Arc<Notify>,
    ) {
        let miner_tx_out = config.get_txout();

        // Spawn the downstream listener task. In solo mode, `upstream` and `jd` are `None`.
        let downstream_handle = tokio::spawn(downstream::listen_for_downstream_mining(
            *config.listening_address(),
            None,
            config.withhold(),
            *config.authority_public_key(),
            *config.authority_secret_key(),
            config.cert_validity_sec(),
            task_collector.clone(),
            tx_status.clone(),
            miner_tx_out,
            None,
            config.clone(),
            shutdown,
            config.jdc_signature().to_string(),
        ));
        let _ = task_collector.safe_lock(|e| {
            e.push(downstream_handle.abort_handle());
        });
    }

    /// Initializes the Job Declarator Client by connecting to a configured upstream Pool
    /// and setting up the associated downstream listener and Job Declarator.
    ///
    /// This function is called when there is an available upstream pool in the configuration.
    /// It handles the connection to the SV2 upstream, sets up the Job Declarator for
    /// communication with the pool's JDS, and starts the downstream listener to relay
    /// jobs from the pool to the miner.
    async fn initialize_jd(
        config: JobDeclaratorClientConfig,
        tx_status: async_channel::Sender<status::Status<'static>>,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
        upstream_config: config::Upstream,
        shutdown: Arc<Notify>,
    ) {
        let timeout = config.timeout();

        // Parse and format the upstream pool connection address.
        let mut parts = upstream_config.pool_address.split(':');
        let address = parts
            .next()
            .unwrap_or_else(|| panic!("Invalid pool address {}", upstream_config.pool_address));
        let port = parts
            .next()
            .and_then(|p| p.parse::<u16>().ok())
            .unwrap_or_else(|| panic!("Invalid pool address {}", upstream_config.pool_address));
        let upstream_addr = SocketAddr::new(
            IpAddr::from_str(address).unwrap_or_else(|_| {
                panic!("Invalid pool address {}", upstream_config.pool_address)
            }),
            port,
        );

        // Instantiate and connect to the SV2 Upstream (Pool).
        let upstream = match upstream_sv2::Upstream::new(
            upstream_addr,
            upstream_config.authority_pubkey,
            status::Sender::Upstream(tx_status.clone()),
            task_collector.clone(),
            Arc::new(Mutex::new(PoolChangerTrigger::new(timeout))),
            config.jdc_signature().to_string(),
        )
        .await
        {
            Ok(upstream) => upstream,
            Err(e) => {
                error!("Failed to create upstream: {}", e);
                panic!()
            }
        };

        // Set up the SV2 connection with the upstream pool.
        match upstream_sv2::Upstream::setup_connection(
            upstream.clone(),
            config.min_supported_version(),
            config.max_supported_version(),
        )
        .await
        {
            Ok(_) => info!("Connected to Upstream!"),
            Err(e) => {
                error!("Failed to connect to Upstream EXITING! : {}", e);
                panic!()
            }
        }

        // Start the task to receive and parse incoming messages from the SV2 upstream.
        if let Err(e) = upstream_sv2::Upstream::parse_incoming(upstream.clone()) {
            error!("failed to create sv2 parser: {}", e);
            panic!()
        }

        // Parse and format the Job Declarator Server (JDS) address for this pool.
        let mut parts = upstream_config.jd_address.split(':');
        let ip_jd = parts.next().unwrap().to_string();
        let port_jd = parts.next().unwrap().parse::<u16>().unwrap();

        // Instantiate the Job Declarator component.
        let jd = match JobDeclarator::new(
            SocketAddr::new(IpAddr::from_str(ip_jd.as_str()).unwrap(), port_jd),
            upstream_config.authority_pubkey.into_bytes(),
            config.clone(),
            upstream.clone(),
            task_collector.clone(),
        )
        .await
        {
            Ok(c) => c,
            Err(e) => {
                let _ = tx_status
                    .send(status::Status {
                        state: status::State::UpstreamShutdown(e),
                    })
                    .await;
                return;
            }
        };

        // Spawn the downstream listener task, providing the upstream and JobDeclarator instances.
        let downstream_handle = tokio::spawn(downstream::listen_for_downstream_mining(
            *config.listening_address(),
            Some(upstream),
            config.withhold(),
            *config.authority_public_key(),
            *config.authority_secret_key(),
            config.cert_validity_sec(),
            task_collector.clone(),
            tx_status.clone(),
            vec![],
            Some(jd),
            config.clone(),
            shutdown,
            config.jdc_signature().to_string(),
        ));
        let _ = task_collector.safe_lock(|e| {
            e.push(downstream_handle.abort_handle());
        });
    }

    /// Closes JDC role and any open connection associated with it.
    ///
    /// Note that this method will result in a full exit of the  running
    /// jd-client and any open connection most be re-initiated upon new
    /// start.
    #[allow(dead_code)]
    pub fn shutdown(&self) {
        self.shutdown.notify_one();
    }
}

/// A trigger mechanism to detect if an upstream pool is unresponsive and initiate a pool change.
#[derive(Debug)]
pub struct PoolChangerTrigger {
    // The timeout duration after which the upstream is considered rogue if no activity is
    // detected.
    timeout: Duration,
    // The handle for the spawned task that monitors the timeout.
    task: Option<tokio::task::JoinHandle<()>>,
}

impl PoolChangerTrigger {
    /// Creates a new `PoolChangerTrigger` instance.
    pub fn new(timeout: Duration) -> Self {
        Self {
            timeout,
            task: None,
        }
    }

    /// Starts the pool changer trigger.
    ///
    /// This spawns a task that will wait for the configured timeout.
    /// If the timeout is reached before `stop` is called, it sends an `UpstreamRogue`
    /// status message to the provided sender, triggering a pool change in the main JDC loop.
    pub fn start(&mut self, sender: status::Sender) {
        let timeout = self.timeout;
        let task = tokio::task::spawn(async move {
            tokio::time::sleep(timeout).await;
            let _ = sender
                .send(status::Status {
                    state: status::State::UpstreamRogue,
                })
                .await;
        });
        self.task = Some(task);
    }

    /// Stops the pool changer trigger.
    pub fn stop(&mut self) {
        if let Some(task) = self.task.take() {
            task.abort();
        }
    }
}

#[cfg(test)]
mod tests {
    use ext_config::{Config, File, FileFormat};

    use crate::*;

    #[tokio::test]
    async fn test_shutdown() {
        let config_path = "config-examples/jdc-config-hosted-example.toml";
        let config: JobDeclaratorClientConfig = match Config::builder()
            .add_source(File::new(config_path, FileFormat::Toml))
            .build()
        {
            Ok(settings) => match settings.try_deserialize::<JobDeclaratorClientConfig>() {
                Ok(c) => c,
                Err(e) => {
                    dbg!(&e);
                    return;
                }
            },
            Err(e) => {
                dbg!(&e);
                return;
            }
        };
        let jdc = JobDeclaratorClient::new(config.clone());
        let cloned = jdc.clone();
        tokio::spawn(async move {
            cloned.start().await;
        });
        jdc.shutdown();
        let ip = config.listening_address().ip();
        let port = config.listening_address().port();
        let jdc_addr = format!("{ip}:{port}");
        assert!(std::net::TcpListener::bind(jdc_addr).is_ok());
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/status.rs">
//! ## Status Reporting System for JDC
//!
//! This module defines how internal components of the Job Declarator Client (JDC) report
//! health, errors, and shutdown conditions back to the main runtime loop in `lib/mod.rs`.
//!
//! At the core, tasks send a [`Status`] (wrapping a [`State`]) through a channel,
//! which is tagged with a [`Sender`] enum to indicate the origin of the message.
//!
//! This allows for centralized, consistent error handling across the application.

use super::error::{self, Error};

/// Identifies the component that originated a [`Status`] update.
///
/// Each sender is associated with a dedicated side of the status channel.
/// This lets the central loop distinguish between errors from different parts of the system.
#[derive(Debug)]
pub enum Sender {
    /// Downstream task (e.g. per-client connection handler)
    Downstream(async_channel::Sender<Status<'static>>),
    /// Listener for incoming downstream connections
    DownstreamListener(async_channel::Sender<Status<'static>>),
    /// Upstream task (e.g, connection to pool)
    Upstream(async_channel::Sender<Status<'static>>),
    /// Template Provider
    TemplateReceiver(async_channel::Sender<Status<'static>>),
}

impl Sender {
    /// The send method is used to send status of component to central status receiver.
    pub async fn send(
        &self,
        status: Status<'static>,
    ) -> Result<(), async_channel::SendError<Status<'_>>> {
        match self {
            Self::Downstream(inner) => inner.send(status).await,
            Self::DownstreamListener(inner) => inner.send(status).await,
            Self::Upstream(inner) => inner.send(status).await,
            Self::TemplateReceiver(inner) => inner.send(status).await,
        }
    }
}

impl Clone for Sender {
    fn clone(&self) -> Self {
        match self {
            Self::Downstream(inner) => Self::Downstream(inner.clone()),
            Self::DownstreamListener(inner) => Self::DownstreamListener(inner.clone()),
            Self::Upstream(inner) => Self::Upstream(inner.clone()),
            Self::TemplateReceiver(inner) => Self::TemplateReceiver(inner.clone()),
        }
    }
}

/// The kind of event or status being reported by a task.
#[derive(Debug)]
pub enum State<'a> {
    /// A downstream component (e.g. client) failed and should be shut down.
    DownstreamShutdown(Error<'a>),
    /// A upstream component failed and should be shut down.
    UpstreamShutdown(Error<'a>),
    /// A upstream component gone rogue.
    UpstreamRogue,
    /// A generic message to indicate health or non-critical errors.
    Healthy(String),
}

/// Wraps a status update, to be passed through a status channel.
#[derive(Debug)]
pub struct Status<'a> {
    /// State represent current state of the component.
    pub state: State<'a>,
}

/// Sends a [`Status`] message tagged with its [`Sender`] to the central loop.
///
/// This is the core logic used to determine which status variant should be sent
/// based on the error type and sender context.
async fn send_status(
    sender: &Sender,
    e: error::Error<'static>,
    outcome: error_handling::ErrorBranch,
) -> error_handling::ErrorBranch {
    match sender {
        Sender::Downstream(tx) => {
            tx.send(Status {
                state: State::Healthy(e.to_string()),
            })
            .await
            .unwrap_or(());
        }
        Sender::DownstreamListener(tx) => {
            tx.send(Status {
                state: State::DownstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
        Sender::Upstream(tx) => {
            tx.send(Status {
                state: State::UpstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
        Sender::TemplateReceiver(tx) => {
            tx.send(Status {
                state: State::UpstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
    }
    outcome
}

/// Centralized error dispatcher for the JDC.
///
/// Used by the `handle_result!` macro across the codebase.
/// Decides whether the task should `Continue` or `Break` based on the error type and source.
pub async fn handle_error(
    sender: &Sender,
    e: error::Error<'static>,
) -> error_handling::ErrorBranch {
    tracing::error!("Error: {:?}", &e);
    match e {
        Error::VecToSlice32(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad CLI argument input.
        Error::BadCliArgs => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `config` TOML deserialize.
        Error::BadConfigDeserialize(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Errors from `binary_sv2` crate.
        Error::BinarySv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad noise handshake.
        Error::CodecNoise(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors from `framing_sv2` crate.
        Error::FramingSv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `TcpStream` connection.
        Error::Io(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `String` to `int` conversion.
        Error::ParseInt(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors from `roles_logic_sv2` crate.
        Error::RolesSv2Logic(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        Error::UpstreamIncoming(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::SubprotocolMining(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Locking Errors
        Error::PoisonLock => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Channel Receiver Error
        Error::ChannelErrorReceiver(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::TokioChannelErrorRecv(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Channel Sender Errors
        Error::ChannelErrorSender(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::Infallible(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/template_receiver/message_handler.rs">
//! ## Template Receiver: Message Handler
//!
//! Implementation of the `ParseTemplateDistributionMessagesFromServer` trait for `TemplateRx`.
//!
//! This trait defines how the `TemplateRx` component handles various types of
//! template distribution messages received from a server, and how it responds
//! to each message type accordingly.
use super::TemplateRx;
use stratum_common::roles_logic_sv2::{
    errors::Error,
    handlers::template_distribution::{ParseTemplateDistributionMessagesFromServer, SendTo},
    parsers::TemplateDistribution,
    template_distribution_sv2::*,
};
use tracing::{debug, error, info};

impl ParseTemplateDistributionMessagesFromServer for TemplateRx {
    // Handles a `NewTemplate` message received from the Template Provider.
    //
    // Returns `Ok(SendTo::None(Some(new_template)))` indicating that no immediate
    // message needs to be sent back to the server as a direct response to `NewTemplate`,
    fn handle_new_template(&mut self, m: NewTemplate) -> Result<SendTo, Error> {
        info!(
            "Received NewTemplate with id: {}, is future: {}",
            m.template_id, m.future_template
        );
        debug!("NewTemplate: {}", m);
        let new_template = m.into_static();
        let new_template = TemplateDistribution::NewTemplate(new_template);
        Ok(SendTo::None(Some(new_template)))
    }

    // Handles a `SetNewPrevHash` message received from the Template Provider.
    //
    // Returns `Ok(SendTo::None(Some(new_prev_hash)))` indicating no immediate response
    // to the server but wrapping the processed `SetNewPrevHash` for forwarding to
    // other components like the `JobDeclarator` and downstream.
    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo, Error> {
        info!("Received SetNewPrevHash for template: {}", m.template_id);
        debug!("SetNewPrevHash: {}", m);
        let new_prev_hash = SetNewPrevHash {
            template_id: m.template_id,
            prev_hash: m.prev_hash.into_static(),
            header_timestamp: m.header_timestamp,
            n_bits: m.n_bits,
            target: m.target.into_static(),
        };
        let new_prev_hash = TemplateDistribution::SetNewPrevHash(new_prev_hash);
        self.pool_chaneger_trigger.safe_lock(|t| t.stop()).unwrap();
        Ok(SendTo::None(Some(new_prev_hash)))
    }

    // Handles a `RequestTransactionDataSuccess` message received from the Template Provider.
    //
    // Returns `Ok(SendTo::None(Some(tx_received)))` wrapping the processed message
    // for forwarding to components like the `JobDeclarator` to complete the job information.
    // Returns `Err(Error)` if an error occurs (currently not possible).
    fn handle_request_tx_data_success(
        &mut self,
        m: RequestTransactionDataSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received RequestTransactionDataSuccess for template: {}",
            m.template_id
        );
        debug!("RequestTransactionDataSuccess: {}", m);
        let m = RequestTransactionDataSuccess {
            transaction_list: m.transaction_list.into_static(),
            excess_data: m.excess_data.into_static(),
            template_id: m.template_id,
        };
        let tx_received = TemplateDistribution::RequestTransactionDataSuccess(m);
        Ok(SendTo::None(Some(tx_received)))
    }

    // Handles a `RequestTransactionDataError` message received from the server.
    //
    // Returns `Err(Error::NoValidTemplate)` if the error code is "template-id-not-found"
    // or an unrecognized error code, indicating that the requested template is invalid
    // or no longer available.
    fn handle_request_tx_data_error(
        &mut self,
        m: RequestTransactionDataError,
    ) -> Result<SendTo, Error> {
        error!(
            "Received RequestTransactionDataError for template: {}, error: {}",
            m.template_id,
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        let m = RequestTransactionDataError {
            template_id: m.template_id,
            error_code: m.error_code.into_static(),
        };
        let error_code_string =
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code");
        match error_code_string {
            "template-id-not-found" => Err(Error::NoValidTemplate(error_code_string.to_string())),
            "stale-template-id" => Ok(SendTo::None(Some(
                TemplateDistribution::RequestTransactionDataError(m),
            ))),
            _ => Err(Error::NoValidTemplate(error_code_string.to_string())),
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/template_receiver/mod.rs">
//! ## Template Receiver (JDC)
//! Contains the logic required for the Job Declarator Client (JDC) to connect to and communicate
//! with a Template Provider (TP).
//!
//! This includes establishing a secure connection, sending and receiving SV2 Template Distribution
//! protocol messages, handling template-related events, and coordinating with the job declarator
//! and downstream subsystem.
use super::{job_declarator::JobDeclarator, status, PoolChangerTrigger};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use key_utils::Secp256k1PublicKey;
use setup_connection::SetupConnectionHandler;
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self,
        bitcoin::{
            consensus::{deserialize, serialize, Encodable},
            Amount, TxOut,
        },
        codec_sv2::{HandshakeRole, Initiator, StandardEitherFrame, StandardSv2Frame},
        handlers::{template_distribution::ParseTemplateDistributionMessagesFromServer, SendTo_},
        job_declaration_sv2::AllocateMiningJobTokenSuccess,
        parsers::{AnyMessage, TemplateDistribution},
        template_distribution_sv2::{
            CoinbaseOutputConstraints, NewTemplate, RequestTransactionData, SubmitSolution,
        },
        utils::Mutex,
    },
};
use tokio::task::AbortHandle;
use tracing::{error, info, warn};

mod message_handler;
mod setup_connection;

pub type SendTo = SendTo_<roles_logic_sv2::parsers::TemplateDistribution<'static>, ()>;
pub type Message = AnyMessage<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

/// Represents a template receiver client
pub struct TemplateRx {
    // Receiver channel for incoming messages from the Template Provider.
    receiver: Receiver<EitherFrame>,
    // Sender channel for sending messages to the Template Provider.
    sender: Sender<EitherFrame>,
    // Sender for communicating status updates back to the main status loop
    // for error handling and state management.
    tx_status: status::Sender,
    // Present when connected to a pool, absent in solo mining mode.
    jd: Option<Arc<Mutex<super::job_declarator::JobDeclarator>>>,
    // used for sending template and job information to the downstream.
    down: Arc<Mutex<super::downstream::DownstreamMiningNode>>,
    task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    // Stores the last received `NewTemplate` message.
    new_template_message: Option<NewTemplate<'static>>,
    // Trigger mechanism to detect unresponsive upstream behavior and initiate a pool change.
    pool_chaneger_trigger: Arc<Mutex<PoolChangerTrigger>>,
    // The encoded miner's coinbase output(s) from the configuration.
    miner_coinbase_output: Vec<u8>,
}

impl TemplateRx {
    // The connect method connects to the Template Provider over TCP, performs the SV2 setup
    // connection handshake, and starts background tasks for handling incoming template messages
    // and forwarding miner solutions.
    //
    // This is the entry point for establishing communication with the Template Provider.
    #[allow(clippy::too_many_arguments)]
    pub async fn connect(
        address: SocketAddr,
        solution_receiver: Receiver<SubmitSolution<'static>>,
        tx_status: status::Sender,
        jd: Option<Arc<Mutex<super::job_declarator::JobDeclarator>>>,
        down: Arc<Mutex<super::downstream::DownstreamMiningNode>>,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
        pool_chaneger_trigger: Arc<Mutex<PoolChangerTrigger>>,
        miner_coinbase_outputs: Vec<TxOut>,
        authority_public_key: Option<Secp256k1PublicKey>,
    ) {
        let mut encoded_outputs = vec![];
        // If in solo mining mode (jd is None), encode only the first coinbase output
        // as per JDS behavior. Otherwise, encode all provided outputs.
        if jd.is_none() {
            miner_coinbase_outputs[0]
                .consensus_encode(&mut encoded_outputs)
                .expect("Invalid coinbase output in config");
        } else {
            miner_coinbase_outputs
                .consensus_encode(&mut encoded_outputs)
                .expect("Invalid coinbase output in config");
        }
        // Establish a TCP connection to the Template Provider address.
        let stream = tokio::net::TcpStream::connect(address).await.unwrap();

        let initiator = match authority_public_key {
            Some(pub_key) => Initiator::from_raw_k(pub_key.into_bytes()),
            None => Initiator::without_pk(),
        }
        .unwrap();
        let (mut receiver, mut sender) =
            Connection::new(stream, HandshakeRole::Initiator(initiator))
                .await
                .unwrap();

        info!("Template Receiver try to set up connection");
        // Perform the SV2 setup connection handshake with the Template Provider.
        SetupConnectionHandler::setup(&mut receiver, &mut sender, address)
            .await
            .unwrap();
        info!("Template Receiver connection set up");

        let self_mutex = Arc::new(Mutex::new(Self {
            receiver: receiver.clone(),
            sender: sender.clone(),
            tx_status,
            jd,
            down,
            task_collector: task_collector.clone(),
            new_template_message: None,
            pool_chaneger_trigger,
            miner_coinbase_output: encoded_outputs,
        }));

        // Spawn a task to handle incoming block solutions from the miner and forward them
        // to the Template Provider
        let task = tokio::task::spawn(Self::on_new_solution(self_mutex.clone(), solution_receiver));
        task_collector
            .safe_lock(|c| c.push(task.abort_handle()))
            .unwrap();

        // Start the main task for receiving and processing template-related messages
        // from the Template Provider.
        Self::start_templates(self_mutex);
    }

    /// This method is used to send message to template provider.
    pub async fn send(self_: &Arc<Mutex<Self>>, sv2_frame: StdFrame) {
        let either_frame = sv2_frame.into();
        let sender_to_tp = self_.safe_lock(|self_| self_.sender.clone()).unwrap();
        match sender_to_tp.send(either_frame).await {
            Ok(_) => (),
            Err(e) => panic!("{e:?}"),
        }
    }

    /// Sends a `CoinbaseOutputConstraints` message to the Template Provider.
    ///
    /// This informs the TP about the maximum size and sigops allowed in the miner's
    /// additional coinbase output data.
    pub async fn send_coinbase_output_constraints(
        self_mutex: &Arc<Mutex<Self>>,
        size: u32,
        sigops: u16,
    ) {
        let coinbase_output_data_size = AnyMessage::TemplateDistribution(
            TemplateDistribution::CoinbaseOutputConstraints(CoinbaseOutputConstraints {
                coinbase_output_max_additional_size: size,
                coinbase_output_max_additional_sigops: sigops,
            }),
        );
        let frame: StdFrame = coinbase_output_data_size.try_into().unwrap();
        Self::send(self_mutex, frame).await;
    }

    /// Sends a `RequestTransactionData` message to the Template Provider.
    ///
    /// This requests the full transaction data for a template identified by its ID.
    pub async fn send_tx_data_request(
        self_mutex: &Arc<Mutex<Self>>,
        new_template: NewTemplate<'static>,
    ) {
        let tx_data_request = AnyMessage::TemplateDistribution(
            TemplateDistribution::RequestTransactionData(RequestTransactionData {
                template_id: new_template.template_id,
            }),
        );
        let frame: StdFrame = tx_data_request.try_into().unwrap();
        Self::send(self_mutex, frame).await;
    }

    /// Retrieves the last allocated mining job token.
    ///
    /// If the JDC is connected to a pool, it fetches the token from the `JobDeclarator`.
    /// In solo mining mode, it generates a dummy token with constraints derived from
    /// the miner's configured coinbase output.
    async fn get_last_token(
        jd: Option<Arc<Mutex<JobDeclarator>>>,
        miner_coinbase_output: &[u8],
    ) -> AllocateMiningJobTokenSuccess<'static> {
        if let Some(jd) = jd {
            JobDeclarator::get_last_token(&jd).await
        } else {
            // This is when JDC is doing solo mining

            AllocateMiningJobTokenSuccess {
                request_id: 0,
                mining_job_token: vec![0; 32].try_into().unwrap(),
                coinbase_outputs: miner_coinbase_output.to_vec().try_into().unwrap(),
            }
        }
    }

    /// Contains the core logic for the Template Receiver's main operational loop.
    ///
    /// This function is responsible for:
    /// 1. Sending initial `CoinbaseOutputConstraints` to the Template Provider.
    /// 2. Continuously receiving and processing messages from the Template Provider.
    /// 3. Handling different Template Distribution messages (`NewTemplate`, `SetNewPrevHash`,
    ///    `RequestTransactionDataSuccess`, `RequestTransactionDataError`).
    /// 4. Requesting transaction data for new templates.
    /// 5. Coordinating the delivery of template and job information to the `JobDeclarator` (when
    ///    connected to a pool) and the `DownstreamMiningNode`.
    /// 6. Utilizing the `IS_NEW_TEMPLATE_HANDLED` global atomic for synchronization between the
    ///    template receiver and downstream when processing `NewTemplate` and `SetNewPrevHash`.
    ///
    /// FIX ME: Remove dependence from other modules in this. This gonna help in
    /// removing sequential component spawning.
    pub fn start_templates(self_mutex: Arc<Mutex<Self>>) {
        let jd = self_mutex.safe_lock(|s| s.jd.clone()).unwrap();
        let down = self_mutex.safe_lock(|s| s.down.clone()).unwrap();
        let tx_status = self_mutex.safe_lock(|s| s.tx_status.clone()).unwrap();
        let mut coinbase_output_constraints_sent = false;
        let mut last_token = None;
        let miner_coinbase_output = self_mutex
            .safe_lock(|s| s.miner_coinbase_output.clone())
            .unwrap();

        // Spawn the main task for handling incoming template messages.
        let main_task = {
            let self_mutex = self_mutex.clone();
            tokio::task::spawn(async move {
                // Send CoinbaseOutputConstraints to TP
                loop {
                    // Retrieve the last allocated mining job token if not already available.
                    if last_token.is_none() {
                        let jd = self_mutex.safe_lock(|s| s.jd.clone()).unwrap();
                        last_token =
                            Some(Self::get_last_token(jd, &miner_coinbase_output[..]).await);
                    }
                    // Send CoinbaseOutputConstraints to the Template Provider if not already sent.
                    if !coinbase_output_constraints_sent {
                        coinbase_output_constraints_sent = true;

                        let jds_coinbase_outputs =
                            last_token.clone().unwrap().coinbase_outputs.to_vec();
                        let deserialized_jds_coinbase_outputs: Vec<TxOut> =
                            deserialize(&jds_coinbase_outputs).expect("Invalid coinbase output");

                        let mut coinbase_output_max_additional_size = 0;
                        let mut coinbase_output_max_additional_sigops = 0;

                        for output in deserialized_jds_coinbase_outputs {
                            coinbase_output_max_additional_size += output.size();
                            coinbase_output_max_additional_sigops +=
                                output.script_pubkey.count_sigops() as u16;
                        }

                        Self::send_coinbase_output_constraints(
                            &self_mutex,
                            coinbase_output_max_additional_size as u32,
                            coinbase_output_max_additional_sigops,
                        )
                        .await;
                    }

                    // Receive Templates and SetPrevHash from TP to send to JD
                    let receiver = self_mutex
                        .clone()
                        .safe_lock(|s| s.receiver.clone())
                        .unwrap();
                    let received = handle_result!(tx_status.clone(), receiver.recv().await);
                    let mut frame: StdFrame =
                        handle_result!(tx_status.clone(), received.try_into());
                    let message_type = frame.get_header().unwrap().msg_type();
                    let payload = frame.payload();

                    // Process the received message using the template distribution message handler
                    let next_message_to_send =
                        ParseTemplateDistributionMessagesFromServer::handle_message_template_distribution(
                            self_mutex.clone(),
                            message_type,
                            payload,
                        );
                    match next_message_to_send {
                        Ok(SendTo::None(m)) => {
                            match m {
                                // Send the new template along with the token to the JD so that JD
                                // can declare the mining job
                                Some(TemplateDistribution::NewTemplate(m)) => {
                                    // Set the global flag to false (Release ordering) to signal
                                    // that a new template is being handled by the downstream.
                                    super::IS_NEW_TEMPLATE_HANDLED
                                        .store(false, std::sync::atomic::Ordering::Release);
                                    // Request transaction data for the new template.
                                    Self::send_tx_data_request(&self_mutex, m.clone()).await;
                                    self_mutex
                                        .safe_lock(|t| t.new_template_message = Some(m.clone()))
                                        .unwrap();
                                    // Get the pool's coinbase output from the last token.
                                    let token = last_token.clone().unwrap();
                                    let pool_outputs = token.coinbase_outputs.to_vec();

                                    // Notify the downstream mining node about the new template.
                                    super::downstream::DownstreamMiningNode::on_new_template(
                                        &down,
                                        m.clone(),
                                        &pool_outputs[..],
                                    )
                                    .await
                                    .unwrap();
                                }
                                // Handle SetNewPrevHash messages.
                                Some(TemplateDistribution::SetNewPrevHash(m)) => {
                                    info!("Received SetNewPrevHash, waiting for IS_NEW_TEMPLATE_HANDLED");
                                    // Wait until the IS_NEW_TEMPLATE_HANDLED flag is true,
                                    // indicating the downstream has finished processing the
                                    // previous NewTemplate.
                                    while !super::IS_NEW_TEMPLATE_HANDLED
                                        .load(std::sync::atomic::Ordering::Acquire)
                                    {
                                        tokio::task::yield_now().await;
                                    }
                                    info!("IS_NEW_TEMPLATE_HANDLED ok");
                                    // If connected to a pool, notify the Job Declarator about the
                                    // new prev hash.
                                    if let Some(jd) = jd.as_ref() {
                                        super::job_declarator::JobDeclarator::on_set_new_prev_hash(
                                            jd.clone(),
                                            m.clone(),
                                        );
                                    }
                                    // Notify the downstream mining node about the new prev hash.
                                    super::downstream::DownstreamMiningNode::on_set_new_prev_hash(
                                        &down, m,
                                    )
                                    .await
                                    .unwrap();
                                }
                                // Handle RequestTransactionDataSuccess messages.
                                Some(TemplateDistribution::RequestTransactionDataSuccess(m)) => {
                                    // safe to unwrap because this message is received after the new
                                    // template message
                                    let transactions_data = m.transaction_list;
                                    let excess_data = m.excess_data;

                                    // Retrieve the stored NewTemplate message (safe to unwrap as
                                    // this message follows a NewTemplate).
                                    let m = self_mutex
                                        .safe_lock(|t| t.new_template_message.clone())
                                        .unwrap()
                                        .unwrap();

                                    // Retrieve the last token and reset the stored token.
                                    let token = last_token.unwrap();
                                    last_token = None;

                                    // Extract mining token and pool coinbase output from the token.
                                    let mining_token = token.mining_job_token.to_vec();
                                    let pool_coinbase_outputs = token.coinbase_outputs.to_vec();

                                    let mut deserialized_outputs: Vec<TxOut> =
                                        deserialize(&pool_coinbase_outputs).unwrap();

                                    // we know the first output is where the template revenue must
                                    // be allocated
                                    deserialized_outputs[0].value =
                                        Amount::from_sat(m.coinbase_tx_value_remaining);

                                    let reserialized_outputs = serialize(&deserialized_outputs);

                                    // If connected to a pool, notify the Job Declarator with the
                                    // complete template information (including transactions).
                                    if let Some(jd) = jd.as_ref() {
                                        super::job_declarator::JobDeclarator::on_new_template(
                                            jd,
                                            m.clone(),
                                            mining_token,
                                            transactions_data,
                                            excess_data,
                                            reserialized_outputs,
                                        )
                                        .await;
                                    }
                                }
                                Some(TemplateDistribution::RequestTransactionDataError(_)) => {
                                    warn!("The prev_hash of the template requested to Template Provider no longer points to the latest tip. Continuing work on the updated template.")
                                }
                                _ => {
                                    error!("{:?}", frame);
                                    error!("{:?}", frame.payload());
                                    error!("{:?}", frame.get_header());
                                    std::process::exit(1);
                                }
                            }
                        }
                        Ok(m) => {
                            error!("{:?}", m);
                            error!("{:?}", frame);
                            error!("{:?}", frame.payload());
                            error!("{:?}", frame.get_header());
                            std::process::exit(1);
                        }
                        Err(e) => {
                            error!("{:?}", e);
                            error!("{:?}", frame);
                            error!("{:?}", frame.payload());
                            error!("{:?}", frame.get_header());
                            std::process::exit(1);
                        }
                    }
                }
            })
        };
        self_mutex
            .safe_lock(|s| {
                s.task_collector
                    .safe_lock(|c| c.push(main_task.abort_handle()))
                    .unwrap()
            })
            .unwrap();
    }

    /// Handles incoming `SubmitSolution` messages from the miner.
    ///
    /// This method continuously receives solutions from the provided receiver channel
    /// and forwards them as `SubmitSolution` messages to the Template Provider.
    async fn on_new_solution(self_: Arc<Mutex<Self>>, rx: Receiver<SubmitSolution<'static>>) {
        while let Ok(solution) = rx.recv().await {
            let sv2_frame: StdFrame =
                AnyMessage::TemplateDistribution(TemplateDistribution::SubmitSolution(solution))
                    .try_into()
                    .expect("Failed to convert solution to sv2 frame!");
            Self::send(&self_, sv2_frame).await
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/template_receiver/setup_connection.rs">
//! Template Receiver: Setup Connection Handler
//!
//! Handles setup connection logic using the Template Distribution Protocol in SV2.
//!
//! This includes sending a `SetupConnection` message and processing responses from the upstream.

use async_channel::{Receiver, Sender};
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{StandardEitherFrame, StandardSv2Frame},
    common_messages_sv2::{Protocol, Reconnect, SetupConnection},
    handlers::common::{ParseCommonMessagesFromUpstream, SendTo},
    parsers::AnyMessage,
    utils::Mutex,
    Error,
};
use tracing::info;

pub type Message = AnyMessage<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

// A handler responsible for managing the setup connection handshake.
pub struct SetupConnectionHandler {}

impl SetupConnectionHandler {
    /// Builds a `SetupConnection` message using the given socket address.
    fn get_setup_connection_message(address: SocketAddr) -> SetupConnection<'static> {
        let endpoint_host = address.ip().to_string().into_bytes().try_into().unwrap();
        let vendor = String::new().try_into().unwrap();
        let hardware_version = String::new().try_into().unwrap();
        let firmware = String::new().try_into().unwrap();
        let device_id = String::new().try_into().unwrap();
        SetupConnection {
            protocol: Protocol::TemplateDistributionProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0b0000_0000_0000_0000_0000_0000_0000_0000,
            endpoint_host,
            endpoint_port: address.port(),
            vendor,
            hardware_version,
            firmware,
            device_id,
        }
    }

    /// processes the setup connection lifecycle.
    pub async fn setup(
        receiver: &mut Receiver<EitherFrame>,
        sender: &mut Sender<EitherFrame>,
        address: SocketAddr,
    ) -> Result<(), ()> {
        let setup_connection = Self::get_setup_connection_message(address);

        let sv2_frame: StdFrame = AnyMessage::Common(setup_connection.into())
            .try_into()
            .unwrap();
        let sv2_frame = sv2_frame.into();
        sender.send(sv2_frame).await.map_err(|_| ())?;

        let mut incoming: StdFrame = receiver
            .recv()
            .await
            .expect("Connection to TP closed!")
            .try_into()
            .expect("Failed to parse incoming SetupConnectionResponse");
        let message_type = incoming.get_header().unwrap().msg_type();
        let payload = incoming.payload();
        ParseCommonMessagesFromUpstream::handle_message_common(
            Arc::new(Mutex::new(SetupConnectionHandler {})),
            message_type,
            payload,
        )
        .unwrap();
        Ok(())
    }
}

impl ParseCommonMessagesFromUpstream for SetupConnectionHandler {
    // Handles a `SetupConnectionSuccess` message received from the upstream.
    //
    // Returns `Ok(SendTo::None(None))` indicating that no immediate message needs
    // to be sent back to the server as a response to `SetupConnectionSuccess`.
    fn handle_setup_connection_success(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::SetupConnectionSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `SetupConnectionSuccess` from TP: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendTo::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_reconnect(&mut self, _m: Reconnect) -> Result<SendTo, Error> {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/upstream_sv2/mod.rs">
use stratum_common::roles_logic_sv2::{
    codec_sv2::{StandardEitherFrame, StandardSv2Frame},
    parsers::AnyMessage,
};

pub mod upstream;
pub use upstream::Upstream;

pub type Message = AnyMessage<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;
</file>

<file path="stratum-1.4.0/roles/jd-client/src/lib/upstream_sv2/upstream.rs">
//! ## Upstream Module
//!
//! The `Upstream` module provides the necessary constructs and methods for establishing and
//! managing connections from the Job Declarator Client (JDC) to upstream pools, as well as handling
//! related message processing.
//!
//! It includes trait implementations required for representing an upstream connection,
//! intercepting messages from upstream nodes (pools), and generating appropriate responses.
//!
//! This module acts as the client-side implementation for communicating with a pool
//! that supports the SV2 Mining Protocol.
//!
//! Trait implementations within this module:
//! - [`IsUpstream`]: Represents a generic upstream connection (possibly redundant in this specific
//!   structure).
//! - [`IsMiningUpstream`]: Represents an upstream connection specifically for the Mining Protocol
//!   (possibly redundant).
//! - [`ParseCommonMessagesFromUpstream`]: Handles the interpretation of common SV2 messages like
//!   `SetupConnectionSuccess` and `SetupConnectionError` received from the upstream pool during the
//!   initial handshake.
//! - [`ParseMiningMessagesFromUpstream<Downstream>`]: Processes messages specific to the SV2 Mining
//!   Protocol received from the upstream pool and determines how to handle them, potentially
//!   relaying them to a downstream mining node or updating internal state.

use super::super::downstream::DownstreamMiningNode as Downstream;

use super::super::{
    error::{
        Error::{CodecNoise, PoisonLock, UpstreamIncoming},
        ProxyResult,
    },
    status,
    upstream_sv2::{EitherFrame, Message, StdFrame},
    PoolChangerTrigger,
};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use key_utils::Secp256k1PublicKey;
use std::{
    collections::{HashMap, VecDeque},
    net::SocketAddr,
    sync::Arc,
    thread::sleep,
    time::Duration,
};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2,
    roles_logic_sv2::{
        channel_logic::channel_factory::PoolChannelFactory,
        codec_sv2::{
            self, binary_sv2,
            binary_sv2::{Seq0255, U256},
            framing_sv2, HandshakeRole, Initiator,
        },
        common_messages_sv2::{Protocol, Reconnect, SetupConnection},
        handlers::{
            common::{ParseCommonMessagesFromUpstream, SendTo as SendToCommon},
            mining::{ParseMiningMessagesFromUpstream, SendTo, SupportedChannelTypes},
        },
        job_declaration_sv2::DeclareMiningJob,
        mining_sv2::{ExtendedExtranonce, Extranonce, SetCustomMiningJob, SetGroupChannel},
        parsers::{AnyMessage, Mining, MiningDeviceMessages},
        utils::{Id, Mutex},
        Error as RolesLogicError,
    },
};
use tokio::{net::TcpStream, task, task::AbortHandle};
use tracing::{debug, error, info, warn};

// A fixed-capacity circular buffer used for storing mappings with a limited history.
//
// When a new element is inserted and the buffer is at capacity, the oldest element
// is automatically removed from the front.
#[derive(Debug)]
struct CircularBuffer {
    // The internal `VecDeque` storing the key-value pairs.
    buffer: VecDeque<(u64, u32)>,
    // The maximum number of elements the buffer can hold.
    capacity: usize,
}

impl CircularBuffer {
    // Creates a new `CircularBuffer` with the specified capacity.
    fn new(capacity: usize) -> Self {
        CircularBuffer {
            buffer: VecDeque::with_capacity(capacity),
            capacity,
        }
    }

    // Inserts a new key-value pair into the buffer.
    fn insert(&mut self, key: u64, value: u32) {
        if self.buffer.len() == self.capacity {
            self.buffer.pop_front();
        }
        self.buffer.push_back((key, value));
    }

    // Retrieves the value associated with a given key from the buffer.
    fn get(&self, id: u64) -> Option<u32> {
        self.buffer
            .iter()
            .find_map(|&(key, value)| if key == id { Some(value) } else { None })
    }
}

impl std::default::Default for CircularBuffer {
    fn default() -> Self {
        Self::new(10)
    }
}

// Maintains mappings between request IDs, template IDs, and upstream-assigned job IDs.
//
// This struct is used to correlate different identifiers received from the
// Template Provider and the upstream pool throughout the job declaration process.
#[derive(Debug, Default)]
struct TemplateToJobId {
    // A circular buffer mapping Template IDs (u64) to upstream-assigned Job IDs (u32).
    // This buffer has a limited capacity to store recent mappings.
    template_id_to_job_id: CircularBuffer,
    // A HashMap mapping Request IDs (u32) from `DeclareMiningJob` messages to Template IDs (u64).
    request_id_to_template_id: HashMap<u32, u64>,
}

impl TemplateToJobId {
    // Registers a mapping between a Request ID and a Template ID.
    //
    // This is typically done when a `DeclareMiningJob` is sent, associating the
    // request ID with the template ID it's based on.
    fn register_template_id(&mut self, template_id: u64, request_id: u32) {
        self.request_id_to_template_id
            .insert(request_id, template_id);
    }

    // Registers a mapping between a Template ID and an upstream-assigned Job ID.
    //
    // This is typically done when a `SetCustomMiningJobSuccess` message is received
    // from the upstream pool, which provides the upstream's job ID for a previously
    // declared job (associated with a template ID). This mapping is stored in
    // the circular buffer.
    fn register_job_id(&mut self, template_id: u64, job_id: u32) {
        self.template_id_to_job_id.insert(template_id, job_id);
    }

    // Retrieves the upstream-assigned Job ID for a given Template ID from the circular buffer.
    fn get_job_id(&mut self, template_id: u64) -> Option<u32> {
        self.template_id_to_job_id.get(template_id)
    }

    // Removes and returns the Template ID associated with a given Request ID.
    fn take_template_id(&mut self, request_id: u32) -> Option<u64> {
        self.request_id_to_template_id.remove(&request_id)
    }

    // Creates a new `TemplateToJobId` instance with a default-sized circular buffer.
    fn new() -> Self {
        Self::default()
    }
}

/// Upstream struct representing all possible requirement for upstream instantiation.
#[derive(Debug)]
pub struct Upstream {
    // The channel ID assigned by the upstream pool for this connection.
    // This is received in the `OpenExtendedMiningChannelSuccess` message.
    channel_id: Option<u32>,
    /// This allows the upstream threads to be able to communicate back to the main thread its
    /// current status.
    tx_status: status::Sender,
    // The size of the `extranonce1` provided by the upstream pool.
    // Currently hardcoded to 16, which is the only size the pool is expected to support. ->
    // Inaccuracy.
    #[allow(dead_code)]
    pub upstream_extranonce1_size: usize,
    // Receiver channel for incoming messages from the upstream pool.
    pub receiver: Receiver<EitherFrame>,
    // Sender channel for sending messages to the upstream pool.
    pub sender: Sender<EitherFrame>,
    /// `DownstreamMiningNode` instance, present when a downstream miner is connected.
    pub downstream: Option<Arc<Mutex<Downstream>>>,
    task_collector: Arc<Mutex<Vec<AbortHandle>>>,
    // Trigger mechanism to detect unresponsive upstream behavior and initiate a pool change.
    pool_chaneger_trigger: Arc<Mutex<PoolChangerTrigger>>,
    // Optional `PoolChannelFactory` instance. This factory is created upon receiving
    // `OpenExtendedMiningChannelSuccess` and is used by the template provider client
    // to check shares received from the downstream, simulating the upstream's
    // channel logic
    channel_factory: Option<PoolChannelFactory>,
    // Manager for mapping Template IDs, Request IDs, and upstream Job IDs.
    template_to_job_id: TemplateToJobId,
    // Simple ID generator for creating unique request IDs for messages sent to the upstream.
    req_ids: Id,
    // The JDC's signature, used in the `ExtendedExtranonce` calculation.
    jdc_signature: String,
}

impl Upstream {
    /// This method sends message to upstream.
    pub async fn send(self_: &Arc<Mutex<Self>>, sv2_frame: StdFrame) -> ProxyResult<'static, ()> {
        let sender = self_
            .safe_lock(|s| s.sender.clone())
            .map_err(|_| PoisonLock)?;
        let either_frame = sv2_frame.into();
        sender.send(either_frame).await.map_err(|e| {
            super::super::error::Error::ChannelErrorSender(
                super::super::error::ChannelSendError::General(e.to_string()),
            )
        })?;
        Ok(())
    }
    /// Instantiates a new `Upstream` connection to the specified SV2 pool address.
    ///
    /// This method establishes a TCP connection, performs the Noise handshake
    /// , and initializes the `Upstream` struct with
    /// the necessary communication channels and state managers. It includes
    /// retry logic for the initial TCP connection attempt.
    #[allow(clippy::too_many_arguments)]
    pub async fn new(
        address: SocketAddr,
        authority_public_key: Secp256k1PublicKey,
        tx_status: status::Sender,
        task_collector: Arc<Mutex<Vec<AbortHandle>>>,
        pool_chaneger_trigger: Arc<Mutex<PoolChangerTrigger>>,
        jdc_signature: String,
    ) -> ProxyResult<'static, Arc<Mutex<Self>>> {
        // Attempt to connect to the SV2 Upstream role (pool) with retry logic.
        let socket = loop {
            match TcpStream::connect(address).await {
                Ok(socket) => break socket,
                Err(e) => {
                    error!(
                        "Failed to connect to Upstream role at {}, retrying in 5s: {}",
                        address, e
                    );

                    sleep(Duration::from_secs(5));
                }
            }
        };

        let pub_key: Secp256k1PublicKey = authority_public_key;
        let initiator = Initiator::from_raw_k(pub_key.into_bytes())?;

        info!(
            "PROXY SERVER - ACCEPTING FROM UPSTREAM: {}",
            socket.peer_addr()?
        );

        // Channel to send and receive messages to the SV2 Upstream role
        let (receiver, sender) = Connection::new(socket, HandshakeRole::Initiator(initiator))
            .await
            .expect("Failed to create connection");

        Ok(Arc::new(Mutex::new(Self {
            channel_id: None,
            upstream_extranonce1_size: 16, /* 16 is the default since that is the only value the
                                            * pool supports currently */
            tx_status,
            receiver,
            sender,
            downstream: None,
            task_collector,
            pool_chaneger_trigger,
            channel_factory: None,
            template_to_job_id: TemplateToJobId::new(),
            req_ids: Id::new(),
            jdc_signature,
        })))
    }

    /// Setups the connection with the SV2 Upstream role (most typically a SV2 Pool).
    pub async fn setup_connection(
        self_: Arc<Mutex<Self>>,
        min_version: u16,
        max_version: u16,
    ) -> ProxyResult<'static, ()> {
        // Get the `SetupConnection` message with Mining Device information (currently hard coded)
        let setup_connection = Self::get_setup_connection_message(min_version, max_version, true)?;

        // Put the `SetupConnection` message in a `StdFrame` to be sent over the wire
        let sv2_frame: StdFrame = Message::Common(setup_connection.into()).try_into()?;
        // Send the `SetupConnection` frame to the SV2 Upstream role
        // Only one Upstream role is supported, panics if multiple connections are encountered
        Self::send(&self_, sv2_frame).await?;

        let recv = self_
            .safe_lock(|s| s.receiver.clone())
            .map_err(|_| PoisonLock)?;

        // Wait for the SV2 Upstream to respond with either a `SetupConnectionSuccess` or a
        // `SetupConnectionError` inside a SV2 binary message frame
        let mut incoming: StdFrame = match recv.recv().await {
            Ok(frame) => frame.try_into()?,
            Err(e) => {
                error!("Upstream connection closed: {}", e);
                return Err(CodecNoise(
                    codec_sv2::noise_sv2::Error::ExpectedIncomingHandshakeMessage,
                ));
            }
        };

        // Gets the binary frame message type from the message header
        let message_type = if let Some(header) = incoming.get_header() {
            header.msg_type()
        } else {
            return Err(framing_sv2::Error::ExpectedHandshakeFrame.into());
        };
        // Gets the message payload
        let payload = incoming.payload();

        // Handle the incoming message (should be either `SetupConnectionSuccess` or
        // `SetupConnectionError`)
        ParseCommonMessagesFromUpstream::handle_message_common(
            self_.clone(),
            message_type,
            payload,
        )?;
        Ok(())
    }

    // Constructs and sends a `SetCustomMiningJob` message to the upstream pool.
    //
    // This method is called after a job is declared to the JDS and validated
    // (receiving `DeclareMiningJobSuccess`). It takes the declared job details,
    // the latest `SetNewPrevHash` information, and the signed mining job token
    // to create the `SetCustomMiningJob` message. This message instructs the
    // upstream pool to make this specific job available to connected downstream.
    #[allow(clippy::too_many_arguments)]
    pub async fn set_custom_jobs(
        self_: &Arc<Mutex<Self>>,
        declare_mining_job: DeclareMiningJob<'static>,
        set_new_prev_hash: roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'static>,
        merkle_path: Seq0255<'static, U256<'static>>,
        signed_token: binary_sv2::B0255<'static>,
        coinbase_tx_version: u32,
        coinbase_prefix: binary_sv2::B0255<'static>,
        coinbase_tx_input_n_sequence: u32,
        coinbase_tx_outs: Vec<u8>,
        coinbase_tx_locktime: u32,
        template_id: u64,
    ) -> ProxyResult<'static, ()> {
        info!("Sending set custom mining job");

        // Get a new request ID for the SetCustomMiningJob message.
        let request_id = self_.safe_lock(|s| s.req_ids.next()).unwrap();

        // Wait until the channel ID is available (received in OpenExtendedMiningChannelSuccess).
        let channel_id = loop {
            if let Some(id) = self_.safe_lock(|s| s.channel_id).unwrap() {
                break id;
            };
            tokio::task::yield_now().await;
        };

        // Get the current timestamp for min_ntime.
        let updated_timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs() as u32;

        // Construct the SetCustomMiningJob message.
        let to_send = SetCustomMiningJob {
            channel_id,
            request_id,
            token: signed_token,
            version: declare_mining_job.version,
            prev_hash: set_new_prev_hash.prev_hash,
            min_ntime: updated_timestamp,
            nbits: set_new_prev_hash.n_bits,
            coinbase_tx_version,
            coinbase_prefix,
            coinbase_tx_input_n_sequence,
            coinbase_tx_outputs: coinbase_tx_outs.try_into().unwrap(),
            coinbase_tx_locktime,
            merkle_path,
        };
        let message = AnyMessage::Mining(Mining::SetCustomMiningJob(to_send));
        let frame: StdFrame = message.try_into().unwrap();

        // Register the mapping between the template ID and the request ID for this message.
        self_
            .safe_lock(|s| {
                s.template_to_job_id
                    .register_template_id(template_id, request_id)
            })
            .unwrap();
        Self::send(self_, frame).await
    }

    /// Parses incoming SV2 messages from the Upstream role and routes them to the
    /// appropriate handler for processing.
    ///
    /// This is the main loop for receiving and processing messages from the pool.
    /// It dispatches mining-specific messages to the `ParseMiningMessagesFromUpstream`
    /// trait implementation. Based on the handler's return value (`SendTo`), it
    /// either relays the message to the downstream mining node or performs other actions.
    /// Errors during message handling or receiving are reported via the status channel.
    #[allow(clippy::result_large_err)]
    pub fn parse_incoming(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        let (recv, tx_status) = self_
            .safe_lock(|s| (s.receiver.clone(), s.tx_status.clone()))
            .map_err(|_| PoisonLock)?;

        // Spawn the main task for receiving and processing upstream messages.
        let main_task = {
            let self_ = self_.clone();
            task::spawn(async move {
                loop {
                    // Waiting to receive a message from the SV2 Upstream role
                    let incoming = handle_result!(tx_status, recv.recv().await);
                    let mut incoming: StdFrame = handle_result!(tx_status, incoming.try_into());
                    // On message receive, get the message type from the message header and get the
                    // message payload
                    let message_type =
                        incoming
                            .get_header()
                            .ok_or(super::super::error::Error::FramingSv2(
                                framing_sv2::Error::ExpectedSv2Frame,
                            ));

                    let message_type = handle_result!(tx_status, message_type).msg_type();

                    let payload = incoming.payload();

                    // Gets the response message for the received SV2 Upstream role message
                    // `handle_message_mining` takes care of the SetupConnection +
                    // SetupConnection.Success
                    let next_message_to_send =
                        Upstream::handle_message_mining(self_.clone(), message_type, payload);

                    // Routes the incoming messages accordingly
                    match next_message_to_send {
                        // This is a transparent proxy it will only relay messages as received
                        Ok(SendTo::RelaySameMessageToRemote(downstream_mutex)) => {
                            let sv2_frame: codec_sv2::Sv2Frame<
                                MiningDeviceMessages,
                                buffer_sv2::Slice,
                            > = incoming.map(|payload| payload.try_into().unwrap());
                            Downstream::send(&downstream_mutex, sv2_frame)
                                .await
                                .unwrap();
                        }
                        // No need to handle impossible state just panic cause are impossible and we
                        // will never panic ;-) Verified: handle_message_mining only either panics,
                        // returns Ok(SendTo::None(None)) or Ok(SendTo::None(Some(m))), or returns
                        // Err This is a transparent proxy it will only
                        // relay messages as received
                        Ok(SendTo::None(_)) => (),
                        Ok(_) => unreachable!(),
                        Err(e) => {
                            let status = status::Status {
                                state: status::State::UpstreamShutdown(UpstreamIncoming(e)),
                            };
                            error!(
                                "TERMINATING: Error handling pool role message: {:?}",
                                status
                            );
                            if let Err(e) = tx_status.send(status).await {
                                error!("Status channel down: {:?}", e);
                            }

                            break;
                        }
                    }
                }
            })
        };
        self_
            .safe_lock(|s| {
                s.task_collector
                    .safe_lock(|c| c.push(main_task.abort_handle()))
                    .unwrap()
            })
            .unwrap();
        Ok(())
    }

    /// Creates the `SetupConnection` message to setup the connection with the SV2 Upstream role.
    /// TODO: The Mining Device information is hard coded here, need to receive from Downstream
    /// instead.
    #[allow(clippy::result_large_err)]
    fn get_setup_connection_message(
        min_version: u16,
        max_version: u16,
        is_work_selection_enabled: bool,
    ) -> ProxyResult<'static, SetupConnection<'static>> {
        let endpoint_host = "0.0.0.0".to_string().into_bytes().try_into()?;
        let vendor = String::new().try_into()?;
        let hardware_version = String::new().try_into()?;
        let firmware = String::new().try_into()?;
        let device_id = String::new().try_into()?;
        let flags = match is_work_selection_enabled {
            false => 0b0000_0000_0000_0000_0000_0000_0000_0100,
            true => 0b0000_0000_0000_0000_0000_0000_0000_0110,
        };
        Ok(SetupConnection {
            protocol: Protocol::MiningProtocol,
            min_version,
            max_version,
            flags,
            endpoint_host,
            endpoint_port: 50,
            vendor,
            hardware_version,
            firmware,
            device_id,
        })
    }

    /// This method provides the `PoolChannelFactory` once it has been created
    /// (upon receiving `OpenExtendedMiningChannelSuccess`).
    ///
    /// This method is used by other components (like the template provider client)
    /// that need access to the channel factory to perform share validation or
    /// other channel-related operations. It waits until the factory is available.
    pub async fn take_channel_factory(self_: Arc<Mutex<Self>>) -> PoolChannelFactory {
        // Wait until the channel_factory field is populated.
        while self_.safe_lock(|s| s.channel_factory.is_none()).unwrap() {
            tokio::task::yield_now().await;
        }
        self_
            .safe_lock(|s| {
                let mut factory = None;
                std::mem::swap(&mut s.channel_factory, &mut factory);
                factory.unwrap()
            })
            .unwrap()
    }

    /// This method retrieves the upstream-assigned job ID for a given template ID.
    ///
    /// This method checks the `template_to_job_id` mapper. If the mapping is not
    /// immediately available (because `SetCustomMiningJobSuccess` hasn't been
    /// processed yet), it waits until the job ID is registered.
    pub async fn get_job_id(self_: &Arc<Mutex<Self>>, template_id: u64) -> u32 {
        loop {
            if let Some(id) = self_
                .safe_lock(|s| s.template_to_job_id.get_job_id(template_id))
                .unwrap()
            {
                return id;
            }
            tokio::task::yield_now().await;
        }
    }
}

impl ParseCommonMessagesFromUpstream for Upstream {
    // Handles a `SetupConnectionSuccess` message received from the upstream pool.
    //
    // Returns `Ok(SendToCommon::None(None))` as no immediate response is required.
    fn handle_setup_connection_success(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::SetupConnectionSuccess,
    ) -> Result<SendToCommon, RolesLogicError> {
        info!(
            "Received `SetupConnectionSuccess` from Pool: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendToCommon::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }

    fn handle_reconnect(&mut self, _m: Reconnect) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }
}

/// Connection-wide SV2 Upstream role messages parser implemented by a downstream ("downstream"
/// here is relative to the SV2 Upstream role and is represented by this `Upstream` struct).
impl ParseMiningMessagesFromUpstream<Downstream> for Upstream {
    // Returns the channel type supported between the SV2 Upstream role (pool) and this
    // `Upstream` instance. For a JDC, this is always `Extended`
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::Extended
    }

    // Indicates whether work selection is enabled for this connection..
    fn is_work_selection_enabled(&self) -> bool {
        true
    }

    /// Handles an `OpenStandardMiningChannelSuccess` message.
    ///
    /// This method panics because standard mining channels are explicitly NOT
    /// used between the JDC and the SV2 Upstream role.
    /// Only Extended channels are expected.
    fn handle_open_standard_mining_channel_success(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::OpenStandardMiningChannelSuccess,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        panic!("Standard Mining Channels are not used in Translator Proxy")
    }

    // Handles an `OpenExtendedMiningChannelSuccess` message received from the upstream pool.
    //
    // This message confirms that an extended mining channel has been successfully opened.
    // It provides the assigned `channel_id`, `extranonce_prefix`, `extranonce_size`, and `target`.
    // This method uses this information to:
    // 1. Store the assigned `channel_id`.
    // 2. Create a `PoolChannelFactory` instance that simulates the upstream's channel logic for the
    //    template provider client to use in share validation.
    // 3. Relays the original `OpenExtendedMiningChannelSuccess` message to the downstream mining
    //    node if one is connected.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` if a downstream
    // is connected, indicating the message should be relayed.
    fn handle_open_extended_mining_channel_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::OpenExtendedMiningChannelSuccess,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received OpenExtendedMiningChannelSuccess with request id: {} and channel id: {}",
            m.request_id, m.channel_id
        );
        debug!("OpenStandardMiningChannelSuccess: {}", m);
        // --- Create the PoolChannelFactory  ---
        let ids = Arc::new(Mutex::new(roles_logic_sv2::utils::GroupId::new()));
        let jdc_signature_len = self.jdc_signature.len();
        let prefix_len = m.extranonce_prefix.to_vec().len();
        let self_len = 0;
        let total_len = prefix_len + m.extranonce_size as usize;
        let range_0 = 0..prefix_len;
        let range_1 = prefix_len..prefix_len + jdc_signature_len + self_len;
        let range_2 = prefix_len + jdc_signature_len + self_len..total_len;

        // Create an ExtendedExtranonce structure defining the layout of the extranonce.
        let extranonces = ExtendedExtranonce::new(
            range_0,
            range_1,
            range_2,
            Some(self.jdc_signature.as_bytes().to_vec()),
        )
        .map_err(|err| RolesLogicError::ExtendedExtranonceCreationFailed(format!("{err:?}")))?;

        // Job creator for the factory.
        let creator = roles_logic_sv2::job_creator::JobsCreators::new(total_len as u8);
        // Placeholder shares per minute
        let share_per_min = 1.0;

        let channel_kind =
            roles_logic_sv2::channel_logic::channel_factory::ExtendedChannelKind::ProxyJd {
                upstream_target: m.target.clone().into(),
            };

        // Create the PoolChannelFactory instance.
        let mut channel_factory = PoolChannelFactory::new(
            ids,
            extranonces,
            creator,
            share_per_min,
            channel_kind,
            vec![],
        );

        // Replicate the upstream's extended channel information within the factory.
        let extranonce: Extranonce = m
            .extranonce_prefix
            .into_static()
            .to_vec()
            .try_into()
            .unwrap();

        // Store the assigned channel ID.
        self.channel_id = Some(m.channel_id);
        channel_factory
            .replicate_upstream_extended_channel_only_jd(
                m.target.into_static(),
                extranonce,
                m.channel_id,
                m.extranonce_size,
            )
            .expect("Impossible to open downstream channel");
        self.channel_factory = Some(channel_factory);

        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles an `OpenMiningChannelError` message received from the upstream pool.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_open_mining_channel_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::OpenMiningChannelError,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received OpenExtendedMiningChannelError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles an `UpdateChannelError` message received from the upstream pool.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_update_channel_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::UpdateChannelError,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received UpdateChannelError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles a `CloseChannel` message received from the upstream pool.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_close_channel(
        &mut self,
        m: roles_logic_sv2::mining_sv2::CloseChannel,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!("Received CloseChannel for channel id: {}", m.channel_id);
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles a `SetExtranoncePrefix` message received from the upstream pool.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_set_extranonce_prefix(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SetExtranoncePrefix,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received SetExtranoncePrefix for channel id: {}",
            m.channel_id
        );
        debug!("SetExtranoncePrefix: {}", m);
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles a `SubmitSharesSuccess` message received from the upstream pool.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_submit_shares_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SubmitSharesSuccess,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        info!("Received SubmitSharesSuccess");
        debug!("SubmitSharesSuccess: {}", m);
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles a `SubmitSharesError` message received from the upstream pool.
    //
    // This message indicates that a share submitted by a miner was rejected by
    // the pool. The current implementation logs the error code and triggers
    // the `pool_changer_trigger`, which may initiate a pool fallback if multiple
    // share errors occur. It does NOT relay the error message downstream,
    // as the JDC handles pool fallback.
    //
    // Returns `Ok(SendTo::None(None))` as no message is relayed downstream in this case.
    fn handle_submit_shares_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SubmitSharesError,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received SubmitSharesError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        self.pool_chaneger_trigger
            .safe_lock(|t| t.start(self.tx_status.clone()))
            .unwrap();
        Ok(SendTo::None(None))
    }

    // Handles a `NewMiningJob` message.
    fn handle_new_mining_job(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::NewMiningJob,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        panic!("Standard Mining Channels are not used in Translator Proxy")
    }

    // Handles a `NewExtendedMiningJob` message received from the upstream pool.
    //
    // This message provides a new mining job using the extended format. However,
    // in this JDC implementation, the job information is primarily derived from
    // the Template Provider and Job Declarator. Therefore, this message from
    // the upstream pool is logged as a warning and ignored, as the JDC relies
    // on its declared jobs.
    //
    // Returns `Ok(SendTo::None(None))` indicating that the message is processed
    // but no action or response is needed.
    fn handle_new_extended_mining_job(
        &mut self,
        _: roles_logic_sv2::mining_sv2::NewExtendedMiningJob,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        warn!("Extended job received from upstream, proxy ignore it, and use the one declared by JOB DECLARATOR");
        Ok(SendTo::None(None))
    }

    // Handles a `SetNewPrevHash` message received from the upstream pool.
    //
    // This message indicates that the previous block hash has changed. Similar
    // to `NewExtendedMiningJob`, this message from the upstream pool is logged
    // as a warning and ignored, as the JDC relies on the `SetNewPrevHash` received
    // from the Template Provider which triggers the promotion of future jobs
    // declared via the JDS.
    //
    // Returns `Ok(SendTo::None(None))` indicating that the message is processed
    // but no action or response is needed.
    fn handle_set_new_prev_hash(
        &mut self,
        _: roles_logic_sv2::mining_sv2::SetNewPrevHash,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        warn!("SNPH received from upstream, proxy ignored it, and used the one declared by JDC");
        Ok(SendTo::None(None))
    }

    // Handles a `SetCustomMiningJobSuccess` message received from the upstream pool.
    //
    // This message confirms that a `SetCustomMiningJob` request previously sent
    // by the JDC has been successfully processed by the upstream pool. It provides
    // the upstream's assigned `job_id` for this job. This method logs the success
    // and registers the mapping between the original template ID (derived from
    // the `request_id`) and the upstream's `job_id` in the `template_to_job_id` mapper.
    //
    // Returns `Ok(SendTo::None(None))` as no message is relayed downstream for this event.
    fn handle_set_custom_mining_job_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SetCustomMiningJobSuccess,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        // TODO
        info!(
            "Received SetCustomMiningJobSuccess for channel id: {} for job id: {}",
            m.channel_id, m.job_id
        );
        debug!("SetCustomMiningJobSuccess: {}", m);
        if let Some(template_id) = self.template_to_job_id.take_template_id(m.request_id) {
            self.template_to_job_id
                .register_job_id(template_id, m.job_id);
            Ok(SendTo::None(None))
        } else {
            error!("Attention received a SetupConnectionSuccess with unknown request_id");
            Ok(SendTo::None(None))
        }
    }

    // Handles a `SetCustomMiningJobError` message received from the upstream pool.
    fn handle_set_custom_mining_job_error(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::SetCustomMiningJobError,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        todo!()
    }

    // Handles a `SetTarget` message received from the upstream pool.
    //
    // This message updates the mining target (difficulty) for a specific channel.
    // This method updates the target in the internal `PoolChannelFactory` and
    // in the downstream mining node's channel status to ensure miners are working
    // on the correct difficulty. It also relays the original message downstream.
    //
    // Returns `Ok(SendTo::RelaySameMessageToRemote(downstream_mutex))` to relay
    // the message downstream.
    fn handle_set_target(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SetTarget,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!("Received SetTarget for channel id: {}", m.channel_id);
        debug!("SetTarget: {}", m);
        if let Some(factory) = self.channel_factory.as_mut() {
            factory.update_target_for_channel(m.channel_id, m.maximum_target.clone().into());
            factory.set_target(&mut m.maximum_target.clone().into());
        }
        if let Some(downstream) = &self.downstream {
            let _ = downstream.safe_lock(|d| {
                let factory = d.status.get_channel();
                factory.set_target(&mut m.maximum_target.clone().into());
                factory.update_target_for_channel(m.channel_id, m.maximum_target.into());
            });
        }
        Ok(SendTo::RelaySameMessageToRemote(
            self.downstream.as_ref().unwrap().clone(),
        ))
    }

    // Handles a `SetGroupChannel` message received from the upstream pool. Not implemented.
    fn handle_set_group_channel(
        &mut self,
        _m: SetGroupChannel,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-client/src/main.rs">
//! Entry point for the Job Declarator Client (JDC).
//!
//! This binary parses CLI arguments, loads the TOML configuration file, and
//! starts the main runtime defined in `jd_client::JobDeclaratorClient`.
//!
//! The actual task orchestration and shutdown logic are managed in `lib/mod.rs`.

mod args;
use args::process_cli_args;
use config_helpers::logging::init_logging;
use jd_client::JobDeclaratorClient;
use tracing::error;

/// This will start:
/// 1. An Upstream, this will connect with the mining Pool
/// 2. A listener that will wait for a tproxy
/// 3. A JobDeclarator, this will connect with the job-declarator-server
/// 4. A TemplateRx, this will connect with bitcoind
///
/// Setup phase
/// 1. Upstream: ->SetupConnection, <-SetupConnectionSuccess
/// 2. Downstream: <-SetupConnection, ->SetupConnectionSuccess, <-OpenExtendedMiningChannel
/// 3. Upstream: ->OpenExtendedMiningChannel, <-OpenExtendedMiningChannelSuccess
/// 4. Downstream: ->OpenExtendedMiningChannelSuccess
///
/// Setup phase
/// 1. JobDeclarator: ->SetupConnection, <-SetupConnectionSuccess, ->AllocateMiningJobToken(x2),
///    <-AllocateMiningJobTokenSuccess (x2)
/// 2. TemplateRx: ->CoinbaseOutputDataSize
///
/// Main loop:
/// 1. TemplateRx: <-NewTemplate, SetNewPrevHash
/// 2. JobDeclarator: -> CommitMiningJob (JobDeclarator::on_new_template), <-CommitMiningJobSuccess
/// 3. Upstream: ->SetCustomMiningJob, Downstream: ->NewExtendedMiningJob, ->SetNewPrevHash
/// 4. Downstream: <-Share
/// 5. Upstream: ->Share
///
/// When we have a NewTemplate we send the NewExtendedMiningJob downstream and the CommitMiningJob
/// to the JDS altogether.
/// Then we receive CommitMiningJobSuccess and we use the new token to send SetCustomMiningJob to
/// the pool.
/// When we receive SetCustomMiningJobSuccess we set in Upstream job_id equal to the one received
/// in SetCustomMiningJobSuccess so that we still send shares upstream with the right job_id.
///
/// The above procedure, let us send NewExtendedMiningJob downstream right after a NewTemplate has
/// been received this will reduce the time that pass from a NewTemplate and the mining-device
/// starting to mine on the new job.
///
/// In the case a future NewTemplate the SetCustomMiningJob is sent only if the candidate become
/// the actual NewTemplate so that we do not send a lot of useless future Job to the pool. That
/// means that SetCustomMiningJob is sent only when a NewTemplate become "active"
///
/// The JobDeclarator always have 2 available token, that means that whenever a token is used to
/// commit a job with upstream we require a new one. Having always a token when needed means that
/// whenever we want to commit a mining job we can do that without waiting for upstream to provide
/// a new token.
///
/// Entrypoint for the Job Declarator Client binary.
///
/// Loads the configuration from TOML and initializes the main runtime
/// defined in `jd_client::JobDeclaratorClient`. Errors during startup are logged.
#[tokio::main]
async fn main() {
    let jdc_config = match process_cli_args() {
        Ok(p) => p,
        Err(e) => {
            error!("Job Declarator Client Config error: {}", e);
            return;
        }
    };

    init_logging(jdc_config.log_file());

    let jdc = JobDeclaratorClient::new(jdc_config);
    jdc.start().await;
}
</file>

<file path="stratum-1.4.0/roles/jd-server/Cargo.toml">
[package]
name = "jd_server"
version = "0.1.3"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "Job Declarator Server (JDS) role"
documentation = "https://github.com/stratum-mining/stratum"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


[lib]
name = "jd_server"
path = "src/lib/mod.rs"

[dependencies]
stratum-common = { path = "../../common", features = ["with_network_helpers"] }
async-channel = "1.5.1"
buffer_sv2 = { path = "../../utils/buffer" }
rand = "0.8.4"
tokio = { version = "1.44.1", features = ["full"] }
ext-config = { version = "0.14.0", features = ["toml"], package = "config" }
tracing = { version = "0.1" }
error_handling = { path = "../../utils/error-handling" }
nohash-hasher = "0.2.0"
serde_json = { version = "1.0", default-features = false, features = ["alloc","raw_value"] }
serde = { version = "1.0.89", features = ["derive", "alloc"], default-features = false }
hashbrown = { version = "0.11", default-features = false, features = ["ahash", "serde"] }
key-utils = { path = "../../utils/key-utils" }
rpc_sv2 = { path = "../roles-utils/rpc" }
hex = "0.4.3"
config-helpers = { path = "../roles-utils/config-helpers" }
clap = { version = "4.5.39", features = ["derive"] }
</file>

<file path="stratum-1.4.0/roles/jd-server/config-examples/jds-config-hosted-example.toml">
# If set to true, JDS require JDC to reveal the transactions they are going to mine on
full_template_mode_required = true

# SRI Pool config
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600

# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./jd-server.log"

# SRI Pool JD config
listen_jd_address = "0.0.0.0:34264"
# RPC config for mempool (it can be also the same TP if correctly configured)
core_rpc_url =  "http://75.119.150.111"
core_rpc_port = 48332
core_rpc_user =  "username"
core_rpc_pass =  "password"
# Time interval used for JDS mempool update 
[mempool_update_interval]
unit = "secs"
value = 1
</file>

<file path="stratum-1.4.0/roles/jd-server/config-examples/jds-config-local-example.toml">
# If set to true, JDS require JDC to reveal the transactions they are going to mine on
full_template_mode_required = true

# SRI Pool config
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600

# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./jd-server.log"

# SRI Pool JD config
listen_jd_address = "127.0.0.1:34264"
# RPC config for mempool (it can be also the same TP if correctly configured)
core_rpc_url =  "http://127.0.0.1"
core_rpc_port = 48332
core_rpc_user =  "username"
core_rpc_pass =  "password"
# Time interval used for JDS mempool update 
[mempool_update_interval]
unit = "secs"
value = 1
</file>

<file path="stratum-1.4.0/roles/jd-server/src/args.rs">
use std::path::PathBuf;

use clap::Parser;
use ext_config::{Config, File, FileFormat};
use jd_server::{
    config::JobDeclaratorServerConfig,
    error::JdsError,
    // error::{Error, ProxyResult},
};

use tracing::error;

/// CLI argument parser for the JDS binary.
///
/// Supports the following flags:
/// - `-c`, `--config`: specify a custom config file path
/// - `-h`, `--help`: print help and usage info
#[derive(Parser, Debug)]
#[command(author, version, about = "Job Declarator Server (JDS)", long_about = None)]
pub struct Args {
    #[arg(
        short = 'c',
        long = "config",
        help = "Path to the TOML configuration file",
        default_value = "jds-config.toml"
    )]
    pub config_path: std::path::PathBuf,
    #[arg(
        short = 'f',
        long = "log-file",
        help = "Path to the log file. If not set, logs will only be written to stdout."
    )]
    pub log_file: Option<PathBuf>,
}

/// Process CLI args and load configuration.
#[allow(clippy::result_large_err)]
pub fn process_cli_args() -> Result<JobDeclaratorServerConfig, JdsError> {
    // Parse CLI arguments
    let args = Args::parse();

    // Build configuration from the provided file path
    let config_path = args.config_path.to_str().ok_or_else(|| {
        error!("Invalid configuration path.");
        JdsError::BadCliArgs
    })?;

    let settings = Config::builder()
        .add_source(File::new(config_path, FileFormat::Toml))
        .build()
        .map_err(|e| {
            error!("Failed to build config: {}", e);
            JdsError::BadCliArgs
        })?;

    // Deserialize settings into JobDeclaratorServerConfig
    let mut config = settings
        .try_deserialize::<JobDeclaratorServerConfig>()
        .map_err(|e| {
            error!("Failed to deserialize config: {}", e);
            JdsError::BadCliArgs
        })?;

    config.set_log_file(args.log_file);

    Ok(config)
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/config.rs">
//! ## Configuration Module
//!
//! Defines [`JobDeclaratorServerConfig`], the configuration structure for the Job Declarator Server
//! (JDS).
//!
//! This module handles:
//! - Parsing TOML files via `serde`
//! - Accessing Bitcoin Core RPC parameters
//! - Managing cryptographic keys for Noise authentication
//! - Setting networking and coinbase logic
//!
//! Also defines a helper struct [`CoreRpc`] to group RPC parameters.

use config_helpers::CoinbaseOutput;
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use serde::Deserialize;
use std::{
    path::{Path, PathBuf},
    time::Duration,
};
use stratum_common::roles_logic_sv2::bitcoin::{Amount, TxOut};

#[derive(Debug, serde::Deserialize, Clone)]
pub struct JobDeclaratorServerConfig {
    #[serde(default = "default_true")]
    full_template_mode_required: bool,
    listen_jd_address: String,
    authority_public_key: Secp256k1PublicKey,
    authority_secret_key: Secp256k1SecretKey,
    cert_validity_sec: u64,
    #[serde(alias = "coinbase_output")] // only one is allowed, so don't make the user type the plural
    #[serde(deserialize_with = "config_helpers::deserialize_vec_exactly_1")]
    coinbase_outputs: Vec<CoinbaseOutput>,
    core_rpc_url: String,
    core_rpc_port: u16,
    core_rpc_user: String,
    core_rpc_pass: String,
    #[serde(deserialize_with = "config_helpers::duration_from_toml")]
    mempool_update_interval: Duration,
    log_file: Option<PathBuf>,
}

impl JobDeclaratorServerConfig {
    /// Creates a new instance of [`JobDeclaratorServerConfig`].
    ///
    /// # Panics
    ///
    /// Panics if `coinbase_outputs` is empty.
    pub fn new(
        listen_jd_address: String,
        authority_public_key: Secp256k1PublicKey,
        authority_secret_key: Secp256k1SecretKey,
        cert_validity_sec: u64,
        coinbase_outputs: Vec<CoinbaseOutput>,
        core_rpc: CoreRpc,
        mempool_update_interval: Duration,
    ) -> Self {
        assert!(
            !coinbase_outputs.is_empty(),
            "cannot have empty coinbase outputs array"
        );
        Self {
            full_template_mode_required: true,
            listen_jd_address,
            authority_public_key,
            authority_secret_key,
            cert_validity_sec,
            coinbase_outputs,
            core_rpc_url: core_rpc.url,
            core_rpc_port: core_rpc.port,
            core_rpc_user: core_rpc.user,
            core_rpc_pass: core_rpc.pass,
            mempool_update_interval,
            log_file: None,
        }
    }

    /// Returns the listening address of the Job Declarator Server.
    pub fn listen_jd_address(&self) -> &str {
        &self.listen_jd_address
    }

    /// Returns the public key of the authority.
    pub fn authority_public_key(&self) -> &Secp256k1PublicKey {
        &self.authority_public_key
    }

    /// Returns the secret key of the authority.
    pub fn authority_secret_key(&self) -> &Secp256k1SecretKey {
        &self.authority_secret_key
    }

    /// Returns the URL of the core RPC.
    pub fn core_rpc_url(&self) -> &str {
        &self.core_rpc_url
    }

    /// Returns the port of the core RPC.
    pub fn core_rpc_port(&self) -> u16 {
        self.core_rpc_port
    }

    /// Returns the user of the core RPC.
    pub fn core_rpc_user(&self) -> &str {
        &self.core_rpc_user
    }

    /// Returns the password of the core RPC.
    pub fn core_rpc_pass(&self) -> &str {
        &self.core_rpc_pass
    }

    /// Returns the coinbase outputs.
    pub fn coinbase_outputs(&self) -> &Vec<CoinbaseOutput> {
        &self.coinbase_outputs
    }

    /// Returns the certificate validity in seconds.
    pub fn cert_validity_sec(&self) -> u64 {
        self.cert_validity_sec
    }

    /// Returns whether [`Full Template`] is required. Otherwise, [`Coinbase Only`] mode will be
    /// used.
    ///
    /// [`Full Template`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#632-full-template-mode
    /// [`Coinbase Only`]: https://github.com/stratum-mining/sv2-spec/blob/main/06-Job-Declaration-Protocol.md#631-coinbase-only-mode
    pub fn full_template_mode_required(&self) -> bool {
        self.full_template_mode_required
    }

    /// Returns the mempool update interval.
    pub fn mempool_update_interval(&self) -> Duration {
        self.mempool_update_interval
    }

    /// Sets the listening address of Bitcoin core RPC.
    pub fn set_core_rpc_url(&mut self, url: String) {
        self.core_rpc_url = url;
    }

    /// Sets coinbase outputs.
    pub fn set_coinbase_outputs(&mut self, outputs: Vec<CoinbaseOutput>) {
        self.coinbase_outputs = outputs;
    }

    pub fn get_txout(&self) -> Vec<TxOut> {
        self.coinbase_outputs
            .iter()
            .map(|out| TxOut {
                value: Amount::from_sat(0),
                script_pubkey: out.script_pubkey().to_owned(),
            })
            .collect()
    }
    pub fn log_file(&self) -> Option<&Path> {
        self.log_file.as_deref()
    }
    pub fn set_log_file(&mut self, log_file: Option<PathBuf>) {
        if let Some(path) = log_file {
            self.log_file = Some(path);
        }
    }
}

fn default_true() -> bool {
    true
}

#[derive(Debug, Deserialize, Clone)]
pub struct CoreRpc {
    url: String,
    port: u16,
    user: String,
    pass: String,
}

impl CoreRpc {
    pub fn new(url: String, port: u16, user: String, pass: String) -> Self {
        Self {
            url,
            port,
            user,
            pass,
        }
    }
}

#[cfg(test)]
mod tests {
    use super::super::JobDeclaratorServer;
    use ext_config::{Config, ConfigError, File, FileFormat};
    use std::path::PathBuf;
    use stratum_common::roles_logic_sv2::bitcoin::{self, Amount, ScriptBuf, TxOut};

    use crate::config::JobDeclaratorServerConfig;

    const COINBASE_CONFIG_TEMPLATE: &'static str = r#"
        full_template_mode_required = true
        authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
        authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
        cert_validity_sec = 3600

        coinbase_outputs = %COINBASE_OUTPUTS%

        listen_jd_address = "127.0.0.1:34264"
        core_rpc_url =  "http://127.0.0.1"
        core_rpc_port = 48332
        core_rpc_user =  "username"
        core_rpc_pass =  "password"
        [mempool_update_interval]
        unit = "secs"
        value = 1
    "#;
    const TEST_PK_HEX: &'static str =
        "036adc3bdf21e6f9a0f0fb0066bf517e5b7909ed1563d6958a10993849a7554075";
    const TEST_INVALID_PK_HEX: &'static str =
        "036adc3bdf21e6f9a0f0fb0066bf517e5b7909ed1563d6958a10993849a7ffffff";

    fn load_config(path: &str) -> JobDeclaratorServerConfig {
        let config_path = PathBuf::from(path);
        assert!(
            config_path.exists(),
            "No config file found at {:?}",
            config_path
        );

        let config_path = config_path.to_str().unwrap();

        let settings = Config::builder()
            .add_source(File::new(config_path, FileFormat::Toml))
            .build()
            .expect("Failed to build config");

        settings.try_deserialize().expect("Failed to parse config")
    }

    fn load_coinbase_config_str(path: &str) -> Result<JobDeclaratorServerConfig, ConfigError> {
        let s = COINBASE_CONFIG_TEMPLATE.replace("%COINBASE_OUTPUTS%", path);
        let settings = Config::builder()
            .add_source(File::from_str(&s, FileFormat::Toml))
            .build()
            .expect("Failed to build config");

        settings.try_deserialize()
    }

    #[tokio::test]
    async fn test_offline_rpc_url() {
        let mut config = load_config("config-examples/jds-config-hosted-example.toml");
        config.set_core_rpc_url("http://127.0.0.1".to_string());
        let jd = JobDeclaratorServer::new(config);
        assert!(jd.start().await.is_err());
    }

    #[test]
    fn test_get_txout_non_empty() {
        let pk = TEST_PK_HEX
            .parse::<bitcoin::PublicKey>()
            .expect("Failed to parse public key");
        let config = load_coinbase_config_str(&format!(
            "[ {{ output_script_type = \"P2WPKH\", output_script_value = \"{pk}\" }} ]"
        ))
        .expect("Failed to parse config");

        let outputs = config.get_txout();
        let expected_script = ScriptBuf::from_hex(&format!(
            "0014{}",
            pk.wpubkey_hash().expect("compressed key")
        ))
        .expect("hex");
        let expected_transaction_output = TxOut {
            value: Amount::from_sat(0),
            script_pubkey: expected_script,
        };

        assert_eq!(outputs[0], expected_transaction_output);
    }

    #[test]
    fn test_get_txout_empty() {
        let error =
            load_coinbase_config_str("[]").expect_err("cannot parse config with empty list");
        assert_eq!(
            error.to_string(),
            "invalid length 0, expected a list with exactly one coinbase output, or a single descriptor string",
        );
    }

    #[test]
    fn test_get_txout_invalid_script_type() {
        // This error message was introduced in https://github.com/stratum-mining/stratum/pull/1720
        // as part of a change to allow Vec or non-Vec coinbase output lists to be parsed. We
        // have https://github.com/stratum-mining/stratum/issues/1793 to track improving it.
        let error = load_coinbase_config_str(&format!(
            "[ {{ output_script_type = \"INVALID\", output_script_value = \"{TEST_PK_HEX}\" }} ]"
        ))
        .expect_err("Cannot parse config with bad script type");
        assert_eq!(
            error.to_string(),
            "could not parse descriptor string (or old-style list format)",
        );
    }

    #[test]
    fn test_get_txout_invalid_value() {
        // This error message was introduced in https://github.com/stratum-mining/stratum/pull/1720
        // as part of a change to allow Vec or non-Vec coinbase output lists to be parsed. We
        // have https://github.com/stratum-mining/stratum/issues/1793 to track improving it.
        let error = load_coinbase_config_str(&format!(
            "[ {{ output_script_type = \"P2WPKH\", output_script_value = \"{TEST_INVALID_PK_HEX}\" }} ]"
        ))
        .expect_err("Cannot parse config with bad pubkeys");
        assert_eq!(
            error.to_string(),
            "could not parse descriptor string (or old-style list format)",
        );
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/error.rs">
//! ## Error Module
//!
//! Defines [`JdsError`], the central error enum used throughout the Job Declarator Server (JDS).
//!
//! It unifies errors from:
//! - I/O operations
//! - Channels (send/recv)
//! - SV2 stack: Binary, Codec, Noise, Framing, RolesLogic
//! - Mempool layer
//! - Locking logic (PoisonError)
//! - Domain-specific issues (e.g., missing job, invalid URL, reconstruction failures)
//!
//! This module ensures that all errors can be passed around consistently, including across async
//! boundaries.

use std::{
    convert::From,
    fmt::Debug,
    sync::{MutexGuard, PoisonError},
};

use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{self, binary_sv2, noise_sv2},
    parsers::Mining,
};

use crate::mempool::error::JdsMempoolError;

#[derive(std::fmt::Debug)]
pub enum JdsError {
    Io(std::io::Error),
    ChannelSend(Box<dyn std::marker::Send + Debug>),
    ChannelRecv(async_channel::RecvError),
    BinarySv2(binary_sv2::Error),
    Codec(codec_sv2::Error),
    Noise(noise_sv2::Error),
    RolesLogic(roles_logic_sv2::Error),
    Framing(codec_sv2::framing_sv2::Error),
    PoisonLock(String),
    Custom(String),
    Sv2ProtocolError((u32, Mining<'static>)),
    MempoolError(JdsMempoolError),
    ImpossibleToReconstructBlock(String),
    NoLastDeclaredJob,
    InvalidRPCUrl,
    BadCliArgs,
}

impl std::fmt::Display for JdsError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        use JdsError::*;
        match self {
            Io(ref e) => write!(f, "I/O error: `{e:?}"),
            ChannelSend(ref e) => write!(f, "Channel send failed: `{e:?}`"),
            ChannelRecv(ref e) => write!(f, "Channel recv failed: `{e:?}`"),
            BinarySv2(ref e) => write!(f, "Binary SV2 error: `{e:?}`"),
            Codec(ref e) => write!(f, "Codec SV2 error: `{e:?}"),
            Framing(ref e) => write!(f, "Framing SV2 error: `{e:?}`"),
            Noise(ref e) => write!(f, "Noise SV2 error: `{e:?}"),
            RolesLogic(ref e) => write!(f, "Roles Logic SV2 error: `{e:?}`"),
            PoisonLock(ref e) => write!(f, "Poison lock: {e:?}"),
            Custom(ref e) => write!(f, "Custom SV2 error: `{e:?}`"),
            Sv2ProtocolError(ref e) => {
                write!(f, "Received Sv2 Protocol Error from upstream: `{e:?}`")
            }
            MempoolError(ref e) => write!(f, "Mempool error: `{e:?}`"),
            ImpossibleToReconstructBlock(e) => {
                write!(f, "Error in reconstructing the block: {e:?}")
            }
            NoLastDeclaredJob => write!(f, "Last declared job not found"),
            InvalidRPCUrl => write!(f, "Invalid Template Provider RPC URL"),
            BadCliArgs => write!(f, "Bad CLI arg input"),
        }
    }
}

impl From<std::io::Error> for JdsError {
    fn from(e: std::io::Error) -> JdsError {
        JdsError::Io(e)
    }
}

impl From<async_channel::RecvError> for JdsError {
    fn from(e: async_channel::RecvError) -> JdsError {
        JdsError::ChannelRecv(e)
    }
}

impl From<binary_sv2::Error> for JdsError {
    fn from(e: binary_sv2::Error) -> JdsError {
        JdsError::BinarySv2(e)
    }
}

impl From<codec_sv2::Error> for JdsError {
    fn from(e: codec_sv2::Error) -> JdsError {
        JdsError::Codec(e)
    }
}

impl From<noise_sv2::Error> for JdsError {
    fn from(e: noise_sv2::Error) -> JdsError {
        JdsError::Noise(e)
    }
}

impl From<roles_logic_sv2::Error> for JdsError {
    fn from(e: roles_logic_sv2::Error) -> JdsError {
        JdsError::RolesLogic(e)
    }
}

impl<T: 'static + std::marker::Send + Debug> From<async_channel::SendError<T>> for JdsError {
    fn from(e: async_channel::SendError<T>) -> JdsError {
        JdsError::ChannelSend(Box::new(e))
    }
}

impl From<String> for JdsError {
    fn from(e: String) -> JdsError {
        JdsError::Custom(e)
    }
}
impl From<codec_sv2::framing_sv2::Error> for JdsError {
    fn from(e: codec_sv2::framing_sv2::Error) -> JdsError {
        JdsError::Framing(e)
    }
}

impl<T> From<PoisonError<MutexGuard<'_, T>>> for JdsError {
    fn from(e: PoisonError<MutexGuard<T>>) -> JdsError {
        JdsError::PoisonLock(e.to_string())
    }
}

impl From<(u32, Mining<'static>)> for JdsError {
    fn from(e: (u32, Mining<'static>)) -> Self {
        JdsError::Sv2ProtocolError(e)
    }
}

impl From<JdsMempoolError> for JdsError {
    fn from(error: JdsMempoolError) -> Self {
        JdsError::MempoolError(error)
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/job_declarator/message_handler.rs">
use std::{convert::TryInto, io::Cursor, sync::Arc};
use stratum_common::roles_logic_sv2::{
    bitcoin::{
        consensus::Decodable as BitcoinDecodable,
        hashes::{sha256d, Hash},
        Transaction, Txid,
    },
    codec_sv2::binary_sv2::{Decodable, Serialize, U256},
    handlers::{job_declaration::ParseJobDeclarationMessagesFromDownstream, SendTo_},
    job_declaration_sv2::{
        AllocateMiningJobToken, AllocateMiningJobTokenSuccess, DeclareMiningJob,
        DeclareMiningJobError, DeclareMiningJobSuccess, ProvideMissingTransactions,
        ProvideMissingTransactionsSuccess, PushSolution,
    },
    parsers::JobDeclaration,
    utils::Mutex,
};
pub type SendTo = SendTo_<JobDeclaration<'static>, ()>;
use crate::mempool::JDsMempool;

use super::{signed_token, TransactionState};
use stratum_common::roles_logic_sv2::{errors::Error, parsers::AnyMessage as AllMessages};
use tracing::{debug, info};

use super::JobDeclaratorDownstream;

impl JobDeclaratorDownstream {
    fn verify_job(&mut self, message: &DeclareMiningJob) -> bool {
        // Convert token from B0255 to u32
        let four_byte_array: [u8; 4] = message
            .mining_job_token
            .clone()
            .to_vec()
            .as_slice()
            .try_into()
            .unwrap();
        let token_u32 = u32::from_le_bytes(four_byte_array);
        // TODO Function to implement, it must be checked if the requested job has:
        // 1. right coinbase
        // 2. right version field
        // 3. right prev-hash
        // 4. right nbits
        self.token_to_job_map.contains_key(&(token_u32))
    }
}

impl ParseJobDeclarationMessagesFromDownstream for JobDeclaratorDownstream {
    fn handle_allocate_mining_job_token(
        &mut self,
        message: AllocateMiningJobToken,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `AllocateMiningJobToken` with id: {}",
            message.request_id
        );
        debug!("`AllocateMiningJobToken`: {:?}", message.request_id);
        let token = self.tokens.next();
        self.token_to_job_map.insert(token, None);
        let message_success = AllocateMiningJobTokenSuccess {
            request_id: message.request_id,
            mining_job_token: token.to_le_bytes().to_vec().try_into().unwrap(),
            coinbase_outputs: self.coinbase_output.clone().try_into().unwrap(),
        };
        let message_enum = JobDeclaration::AllocateMiningJobTokenSuccess(message_success);
        info!(
            "Sending AllocateMiningJobTokenSuccess to proxy {}",
            message_enum
        );
        Ok(SendTo::Respond(message_enum))
    }

    // Transactions that are present in the mempool are stored here, that is sent to the
    // mempool which use the rpc client to retrieve the whole data for each transaction.
    // The unknown transactions is a vector that contains the transactions that are not in the
    // jds mempool, and will be non-empty in the ProvideMissingTransactionsSuccess message
    fn handle_declare_mining_job(&mut self, message: DeclareMiningJob) -> Result<SendTo, Error> {
        info!(
            "Received `DeclareMiningJob` with id: {}",
            message.request_id
        );
        debug!("`DeclareMiningJob`: {}", message);
        if let Some(old_mining_job) = self.declared_mining_job.0.take() {
            clear_declared_mining_job(old_mining_job, &message, self.mempool.clone())?;
        }
        let mut known_transactions: Vec<Txid> = vec![];
        if self.verify_job(&message) {
            let txids = message.tx_ids_list.inner_as_ref();
            let mempool = self.mempool.safe_lock(|x| x.mempool.clone())?;
            let mut transactions_with_state = vec![TransactionState::Missing; txids.len()];
            let mut missing_txs: Vec<u16> = Vec::new();
            for (i, txid) in txids.iter().enumerate() {
                let hash = sha256d::Hash::from_slice(txid)?;
                let txid = Txid::from(hash);
                match mempool.contains_key(&txid) {
                    true => {
                        transactions_with_state[i] = TransactionState::PresentInMempool(txid);
                        known_transactions.push(txid);
                    }
                    false => {
                        missing_txs.push(i as u16);
                    }
                }
            }
            self.declared_mining_job = (
                Some(message.clone().into_static()),
                transactions_with_state,
                missing_txs.clone(),
            );
            // here we send the transactions that we want to be stored in jds mempool with full data

            self.add_txs_to_mempool
                .add_txs_to_mempool_inner
                .known_transactions
                .append(&mut known_transactions);
            let mut full_token = [0u8; 255];
            message.mining_job_token.to_bytes(&mut full_token)?;
            let mining_job_token = &mut full_token[..32];
            if missing_txs.is_empty() {
                let message_success = DeclareMiningJobSuccess {
                    request_id: message.request_id,
                    new_mining_job_token: signed_token(
                        U256::from_bytes(mining_job_token)?,
                        &self.public_key.clone(),
                        &self.private_key.clone(),
                    ),
                };
                let message_enum_success = JobDeclaration::DeclareMiningJobSuccess(message_success);
                Ok(SendTo::Respond(message_enum_success))
            } else {
                let message_provide_missing_transactions = ProvideMissingTransactions {
                    request_id: message.request_id,
                    unknown_tx_position_list: missing_txs.into(),
                };
                let message_enum_provide_missing_transactions =
                    JobDeclaration::ProvideMissingTransactions(
                        message_provide_missing_transactions,
                    );
                Ok(SendTo::Respond(message_enum_provide_missing_transactions))
            }
        } else {
            let message_error = DeclareMiningJobError {
                request_id: message.request_id,
                error_code: Vec::new().try_into().unwrap(),
                error_details: Vec::new().try_into().unwrap(),
            };
            let message_enum_error = JobDeclaration::DeclareMiningJobError(message_error);
            Ok(SendTo::Respond(message_enum_error))
        }
    }

    fn handle_provide_missing_transactions_success(
        &mut self,
        message: ProvideMissingTransactionsSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `ProvideMissingTransactionsSuccess` with id: {}",
            message.request_id
        );
        debug!("`ProvideMissingTransactionsSuccess`: {}", message);
        let (declared_mining_job, ref mut transactions_with_state, missing_indexes) =
            &mut self.declared_mining_job;
        let mut unknown_transactions: Vec<Transaction> = vec![];
        match declared_mining_job {
            Some(declared_job) => {
                let id = declared_job.request_id;
                // check request_id in order to ignore old ProvideMissingTransactionsSuccess (see
                // issue #860)
                if id == message.request_id {
                    for (i, tx) in message.transaction_list.inner_as_ref().iter().enumerate() {
                        let mut cursor = Cursor::new(tx);
                        let transaction =
                            Transaction::consensus_decode_from_finite_reader(&mut cursor)
                                .map_err(|e| Error::TxDecodingError(e.to_string()))?;
                        Vec::push(&mut unknown_transactions, transaction.clone());
                        let index =
                            *missing_indexes
                                .get(i)
                                .ok_or(Error::LogicErrorMessage(Box::new(
                                    AllMessages::JobDeclaration(
                                        JobDeclaration::ProvideMissingTransactionsSuccess(
                                            message.clone().into_static(),
                                        ),
                                    ),
                                )))? as usize;
                        // insert the missing transactions in the mempool
                        transactions_with_state[index] =
                            TransactionState::PresentInMempool(transaction.compute_txid());
                    }
                    self.add_txs_to_mempool
                        .add_txs_to_mempool_inner
                        .unknown_transactions
                        .append(&mut unknown_transactions);
                    // if there still a missing transaction return an error
                    for tx_with_state in transactions_with_state {
                        match tx_with_state {
                            TransactionState::PresentInMempool(_) => continue,
                            TransactionState::Missing => return Err(Error::JDSMissingTransactions),
                        }
                    }
                    let mut full_token = [0u8; 255];
                    declared_job
                        .mining_job_token
                        .clone()
                        .to_bytes(&mut full_token)?;
                    let mining_job_token = &mut full_token[..32];
                    let message_success = DeclareMiningJobSuccess {
                        request_id: message.request_id,
                        new_mining_job_token: signed_token(
                            U256::from_bytes(mining_job_token)?,
                            &self.public_key.clone(),
                            &self.private_key.clone(),
                        ),
                    };
                    let message_enum_success =
                        JobDeclaration::DeclareMiningJobSuccess(message_success);
                    return Ok(SendTo::Respond(message_enum_success));
                }
            }
            None => return Err(Error::NoValidJob),
        }
        Ok(SendTo::None(None))
    }

    fn handle_push_solution(&mut self, message: PushSolution<'_>) -> Result<SendTo, Error> {
        info!("Received PushSolution from JDC");
        debug!("`PushSolution`: {}", message);
        let m = JobDeclaration::PushSolution(message.clone().into_static());
        Ok(SendTo::None(Some(m)))
    }
}

fn clear_declared_mining_job(
    old_mining_job: DeclareMiningJob,
    new_mining_job: &DeclareMiningJob,
    mempool: Arc<Mutex<JDsMempool>>,
) -> Result<(), Error> {
    let old_transactions = old_mining_job.tx_ids_list.inner_as_ref();
    let new_transactions = new_mining_job.tx_ids_list.inner_as_ref();

    if old_transactions.is_empty() {
        info!("No transactions to remove from mempool");
        return Ok(());
    }

    let result = mempool.safe_lock(|mempool_| -> Result<(), Error> {
        let mempool_txs = mempool_.mempool.clone();

        for old_txid in old_transactions
            .iter()
            .filter(|&id| !new_transactions.contains(id))
        {
            if let Some(tx) = mempool_txs.get(*old_txid) {
                let txid = tx.as_ref().unwrap().0.compute_txid();
                match mempool_.mempool.get_mut(&txid) {
                    Some(Some((_transaction, counter))) => {
                        if *counter > 1 {
                            *counter -= 1;
                            debug!(
                                "Fat transaction {:?} counter decremented; job id {:?} dropped",
                                txid, old_mining_job.request_id
                            );
                        } else {
                            mempool_.mempool.remove(&txid);
                            debug!(
                                "Fat transaction {:?} with job id {:?} removed from mempool",
                                txid, old_mining_job.request_id
                            );
                        }
                    }
                    Some(None) => debug!(
                        "Thin transaction {:?} with job id {:?} removed from mempool",
                        txid, old_mining_job.request_id
                    ),
                    None => {}
                }
            } else {
                debug!(
                    "Transaction with id {:?} not found in mempool for old jobs",
                    old_txid
                );
            }
        }
        Ok(())
    })?;

    result.map_err(|err| Error::PoisonLock(err.to_string()))
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/job_declarator/mod.rs">
//! # Job Declarator Server - Protocol and Downstream Handling
//!
//! This module implements the core logic of the **Job Declarator Server (JDS)**.
//!
//! Responsibilities include:
//! - Listening for downstream client connections (JDCs)
//! - Handling the Job Declaration Protocol (AllocateMiningJobToken, DeclareMiningJob, PushSolution,
//!   etc.)
//! - Tracking job state and transaction presence
//! - Managing transaction flow into the local mempool
//! - Assembling and submitting full blocks to the upstream node
//!
//! Structure:
//! - [`JobDeclarator`] handles server-level responsibilities like accepting new TCP connections.
//! - [`JobDeclaratorDownstream`] manages the per-client state and protocol interaction.
//!
//! The design is one-task-per-downstream, with communication via channels and internal
//! synchronization.

pub mod message_handler;
use super::{
    error::JdsError, mempool::JDsMempool, status, EitherFrame, JobDeclaratorServerConfig, StdFrame,
};
use async_channel::{Receiver, Sender};
use core::panic;
use error_handling::handle_result;
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey, SignatureService};
use nohash_hasher::BuildNoHashHasher;
use std::{collections::HashMap, convert::TryInto, sync::Arc};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self,
        bitcoin::{consensus::encode::serialize, Block, Transaction, Txid},
        codec_sv2::{
            binary_sv2,
            binary_sv2::{B0255, U256},
            HandshakeRole, Responder,
        },
        common_messages_sv2::{
            Protocol, SetupConnection, SetupConnectionError, SetupConnectionSuccess,
        },
        handlers::job_declaration::{ParseJobDeclarationMessagesFromDownstream, SendTo},
        job_declaration_sv2::{DeclareMiningJob, PushSolution},
        parsers::{AnyMessage as JdsMessages, JobDeclaration},
        utils::{Id, Mutex},
    },
};
use tokio::{net::TcpListener, time::Duration};
use tracing::{debug, error, info};

/// Represents whether a transaction declared in a mining job is known to the JDS mempool
/// or still missing and needs to be fetched/provided.
#[derive(Clone, Debug)]
pub enum TransactionState {
    PresentInMempool(Txid),
    Missing,
}

/// Contains transaction identifiers and full transaction data that need to be
/// added or completed in the JDS mempool.
///
/// Used internally during the job declaration lifecycle.
#[derive(Clone, Debug)]
pub struct AddTrasactionsToMempoolInner {
    pub known_transactions: Vec<Txid>,
    pub unknown_transactions: Vec<Transaction>,
}

/// Wrapper struct enabling transaction updates to be sent via a channel to the mempool task.
#[derive(Clone, Debug)]
pub struct AddTrasactionsToMempool {
    pub add_txs_to_mempool_inner: AddTrasactionsToMempoolInner,
    pub sender_add_txs_to_mempool: Sender<AddTrasactionsToMempoolInner>,
}

/// Represents a single downstream connection to a JDC.
///
/// This struct tracks all state relevant to one connection, including:
/// - The declared mining job and missing transactions
/// - Mapping between tokens and job IDs
/// - Interaction with the mempool
///
/// It operates in its own async task and communicates with the rest of the system
/// via channels and locks.

#[derive(Debug)]
pub struct JobDeclaratorDownstream {
    #[allow(dead_code)]
    full_template_mode_required: bool,
    sender: Sender<EitherFrame>,
    receiver: Receiver<EitherFrame>,
    // TODO this should be computed for each new template so that fees are included
    #[allow(dead_code)]
    // TODO: use coinbase output
    coinbase_output: Vec<u8>,
    token_to_job_map: HashMap<u32, Option<u8>, BuildNoHashHasher<u32>>,
    tokens: Id,
    public_key: Secp256k1PublicKey,
    private_key: Secp256k1SecretKey,
    mempool: Arc<Mutex<JDsMempool>>,
    // Vec<u16> is the vector of missing transactions
    declared_mining_job: (
        Option<DeclareMiningJob<'static>>,
        Vec<TransactionState>,
        Vec<u16>,
    ),
    add_txs_to_mempool: AddTrasactionsToMempool,
}

impl JobDeclaratorDownstream {
    /// Creates a new downstream connection context.
    pub fn new(
        full_template_mode_required: bool,
        receiver: Receiver<EitherFrame>,
        sender: Sender<EitherFrame>,
        config: &JobDeclaratorServerConfig,
        mempool: Arc<Mutex<JDsMempool>>,
        sender_add_txs_to_mempool: Sender<AddTrasactionsToMempoolInner>,
    ) -> Self {
        // TODO: use next variables
        let token_to_job_map = HashMap::with_hasher(BuildNoHashHasher::default());
        let tokens = Id::new();
        let add_txs_to_mempool_inner = AddTrasactionsToMempoolInner {
            known_transactions: vec![],
            unknown_transactions: vec![],
        };
        let coinbase_output = serialize(&config.get_txout());

        Self {
            full_template_mode_required,
            receiver,
            sender,
            coinbase_output,
            token_to_job_map,
            tokens,
            public_key: *config.authority_public_key(),
            private_key: *config.authority_secret_key(),
            mempool,
            declared_mining_job: (None, Vec::new(), Vec::new()),
            add_txs_to_mempool: AddTrasactionsToMempool {
                add_txs_to_mempool_inner,
                sender_add_txs_to_mempool,
            },
        }
    }

    fn get_block_hex(
        self_mutex: Arc<Mutex<Self>>,
        message: PushSolution,
    ) -> Result<String, Box<JdsError>> {
        let (last_declare_, _, _) = self_mutex
            .clone()
            .safe_lock(|x| x.declared_mining_job.clone())
            .map_err(|e| Box::new(JdsError::PoisonLock(e.to_string())))?;
        let last_declare = last_declare_.ok_or(Box::new(JdsError::NoLastDeclaredJob))?;
        let transactions_list = Self::collect_txs_in_job(self_mutex)?;
        let block: Block =
            roles_logic_sv2::utils::BlockCreator::new(last_declare, transactions_list, message)
                .into();
        Ok(hex::encode(serialize(&block)))
    }

    fn collect_txs_in_job(self_mutex: Arc<Mutex<Self>>) -> Result<Vec<Transaction>, Box<JdsError>> {
        let (_, transactions_with_state, _) = self_mutex
            .clone()
            .safe_lock(|x| x.declared_mining_job.clone())
            .map_err(|e| Box::new(JdsError::PoisonLock(e.to_string())))?;
        let mempool = self_mutex
            .safe_lock(|x| x.mempool.clone())
            .map_err(|e| Box::new(JdsError::PoisonLock(e.to_string())))?;
        let mut transactions_list: Vec<Transaction> = Vec::new();
        for tx_with_state in transactions_with_state.iter().enumerate() {
            if let TransactionState::PresentInMempool(txid) = tx_with_state.1 {
                let tx = mempool
                    .safe_lock(|x| x.mempool.get(txid).cloned())
                    .map_err(|e| JdsError::PoisonLock(e.to_string()))?
                    .ok_or(Box::new(JdsError::ImpossibleToReconstructBlock(
                        "Txid not found in jds mempool".to_string(),
                    )))?
                    .ok_or(Box::new(JdsError::ImpossibleToReconstructBlock(
                        "Txid found in jds mempool but transactions not present".to_string(),
                    )))?;
                transactions_list.push(tx.0);
            } else {
                return Err(Box::new(JdsError::ImpossibleToReconstructBlock(
                    "Unknown transaction".to_string(),
                )));
            };
        }
        Ok(transactions_list)
    }

    async fn send_txs_to_mempool(self_mutex: Arc<Mutex<Self>>) {
        let add_txs_to_mempool = self_mutex
            .safe_lock(|a| a.add_txs_to_mempool.clone())
            .unwrap();
        let sender_add_txs_to_mempool = add_txs_to_mempool.sender_add_txs_to_mempool;
        let add_txs_to_mempool_inner = add_txs_to_mempool.add_txs_to_mempool_inner;
        let _ = sender_add_txs_to_mempool
            .send(add_txs_to_mempool_inner)
            .await;
        // the trasnactions sent to the mempool can be freed
        let _ = self_mutex.safe_lock(|a| {
            a.add_txs_to_mempool.add_txs_to_mempool_inner = AddTrasactionsToMempoolInner {
                known_transactions: vec![],
                unknown_transactions: vec![],
            };
        });
    }

    fn get_transactions_in_job(self_mutex: Arc<Mutex<Self>>) -> Vec<Txid> {
        let mut known_transactions: Vec<Txid> = Vec::new();
        let job_transactions = self_mutex
            .safe_lock(|a| a.declared_mining_job.1.clone())
            .unwrap();
        for transaction in job_transactions {
            match transaction {
                TransactionState::PresentInMempool(txid) => known_transactions.push(txid),
                TransactionState::Missing => {
                    continue;
                }
            }
        }
        known_transactions
    }

    /// Sends a single Job Declaration message back to the downstream client.
    ///
    /// Wraps the message into a `StdFrame` and sends it through the established channel.
    pub async fn send(
        self_mutex: Arc<Mutex<Self>>,
        message: roles_logic_sv2::parsers::JobDeclaration<'static>,
    ) -> Result<(), ()> {
        let sv2_frame: StdFrame = JdsMessages::JobDeclaration(message).try_into().unwrap();
        let sender = self_mutex.safe_lock(|self_| self_.sender.clone()).unwrap();
        sender.send(sv2_frame.into()).await.map_err(|_| ())?;
        Ok(())
    }

    /// Starts the message processing loop for this downstream connection.
    ///
    /// - Waits for incoming SV2 messages
    /// - Delegates message parsing to [`ParseJobDeclarationMessagesFromDownstream`]
    /// - Sends appropriate responses back to the client
    /// - Updates the JDS mempool as needed
    ///
    /// This loop runs until the client disconnects or a critical error is encountered.
    pub fn start(
        self_mutex: Arc<Mutex<Self>>,
        tx_status: status::Sender,
        new_block_sender: Sender<String>,
    ) {
        let recv = self_mutex.safe_lock(|s| s.receiver.clone()).unwrap();
        tokio::spawn(async move {
            loop {
                match recv.recv().await {
                    Ok(message) => {
                        let mut frame: StdFrame = handle_result!(tx_status, message.try_into());
                        let header = frame
                            .get_header()
                            .ok_or_else(|| JdsError::Custom(String::from("No header set")));
                        let header = handle_result!(tx_status, header);
                        let message_type = header.msg_type();
                        let payload = frame.payload();
                        let next_message_to_send =
                            ParseJobDeclarationMessagesFromDownstream::handle_message_job_declaration(
                                self_mutex.clone(),
                                message_type,
                                payload,
                            );
                        // How works the txs recognition and txs storing in JDS mempool
                        // when a DMJ arrives, the JDS compares the received transactions with the
                        // ids in the the JDS mempool. Then there are two scenarios
                        // 1. the JDS recognizes all the transactions. Then, just before a DMJS is
                        //    sent, the JDS mempool is triggered to fill in the JDS mempool the id
                        //    of declared job with the full transaction (with send_tx_to_mempool
                        //    method(), that eventually will ask the transactions to a bitcoin node
                        //    via RPC)
                        // 2. there are some unknown txids. Just before sending PMT, the JDS mempool
                        //    is triggered to fill the known txids with the full transactions. When
                        //    a PMTS arrives, just before sending a DMJS, the unknown full
                        //    transactions provided by the downstream are added to the JDS mempool
                        match next_message_to_send {
                            Ok(SendTo::Respond(m)) => {
                                match m {
                                    JobDeclaration::AllocateMiningJobToken(_) => {
                                        error!("Send unexpected message: AMJT");
                                    }
                                    JobDeclaration::AllocateMiningJobTokenSuccess(_) => {
                                        debug!("Send message: AMJTS");
                                    }
                                    JobDeclaration::DeclareMiningJob(_) => {
                                        error!("Send unexpected message: DMJ");
                                    }
                                    JobDeclaration::DeclareMiningJobError(_) => {
                                        debug!("Send nmessage: DMJE");
                                    }
                                    JobDeclaration::DeclareMiningJobSuccess(_) => {
                                        debug!("Send message: DMJS. Updating the JDS mempool.");
                                        Self::send_txs_to_mempool(self_mutex.clone()).await;
                                    }
                                    JobDeclaration::ProvideMissingTransactions(_) => {
                                        debug!("Send message: PMT. Updating the JDS mempool.");
                                        Self::send_txs_to_mempool(self_mutex.clone()).await;
                                    }
                                    JobDeclaration::ProvideMissingTransactionsSuccess(_) => {
                                        error!("Send unexpected PMTS");
                                    }
                                    JobDeclaration::PushSolution(_) => todo!(),
                                }
                                Self::send(self_mutex.clone(), m).await.unwrap();
                            }
                            Ok(SendTo::RelayNewMessage(message)) => {
                                error!("JD Server: unexpected relay new message {}", message);
                            }
                            Ok(SendTo::RelayNewMessageToRemote(remote, message)) => {
                                error!(
                                    "JD Server: unexpected relay new message to remote. Remote: {:?}, Message: {}",
                                    remote,
                                    message
                                );
                            }
                            Ok(SendTo::RelaySameMessageToRemote(remote)) => {
                                error!(
                                    "JD Server: unexpected relay same message to remote. Remote: {:?}",
                                    remote
                                );
                            }
                            Ok(SendTo::Multiple(multiple)) => {
                                error!("JD Server: unexpected multiple messages: {:?}", multiple);
                            }
                            Ok(SendTo::None(m)) => {
                                match m {
                                    Some(JobDeclaration::PushSolution(message)) => {
                                        match Self::collect_txs_in_job(self_mutex.clone()) {
                                            Ok(_) => {
                                                info!(
                                                    "All transactions in downstream job are recognized correctly by the JD Server"
                                                );
                                                let hexdata =
                                                    match JobDeclaratorDownstream::get_block_hex(
                                                        self_mutex.clone(),
                                                        message,
                                                    ) {
                                                        Ok(inner) => inner,
                                                        Err(e) => {
                                                            error!(
                                                            "Received solution but encountered error: {:?}",
                                                            e
                                                        );
                                                            recv.close();
                                                            //TODO should we brake it?
                                                            break;
                                                        }
                                                    };
                                                let _ = new_block_sender.send(hexdata).await;
                                            }
                                            Err(error) => {
                                                error!("Missing transactions: {:?}", error);
                                                // TODO print here the ip of the downstream
                                                let known_transactions =
                                                    JobDeclaratorDownstream::get_transactions_in_job(
                                                        self_mutex.clone()
                                                    );
                                                let retrieve_transactions =
                                                    AddTrasactionsToMempoolInner {
                                                        known_transactions,
                                                        unknown_transactions: Vec::new(),
                                                    };
                                                let mempool = self_mutex
                                                    .clone()
                                                    .safe_lock(|a| a.mempool.clone())
                                                    .unwrap();
                                                tokio::select! {
                                                    _ = JDsMempool::add_tx_data_to_mempool(mempool, retrieve_transactions) => {
                                                        match JobDeclaratorDownstream::get_block_hex(
                                                            self_mutex.clone(),
                                                            message.clone(),
                                                        ) {
                                                            Ok(hexdata) => {
                                                                let _ = new_block_sender.send(hexdata).await;
                                                            },
                                                            Err(e) => {
                                                                handle_result!(
                                                                    tx_status,
                                                                    Err(*e)
                                                                );
                                                            }
                                                        };
                                                    }
                                                    _ = tokio::time::sleep(Duration::from_secs(60)) => {}
                                                }
                                            }
                                        };
                                    }
                                    Some(JobDeclaration::DeclareMiningJob(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::DeclareMiningJobSuccess(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::DeclareMiningJobError(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::AllocateMiningJobToken(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::AllocateMiningJobTokenSuccess(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::ProvideMissingTransactions(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    Some(JobDeclaration::ProvideMissingTransactionsSuccess(_)) => {
                                        error!("JD Server received an unexpected message {:?}", m);
                                    }
                                    None => (),
                                }
                            }
                            Err(e) => {
                                error!("{:?}", e);
                                handle_result!(
                                    tx_status,
                                    Err(JdsError::Custom("Invalid message received".to_string()))
                                );
                                recv.close();
                                break;
                            }
                        }
                    }
                    Err(err) => {
                        handle_result!(tx_status, Err(JdsError::ChannelRecv(err)));
                        break;
                    }
                }
            }
        });
    }
}

pub fn signed_token(
    tx_hash_list_hash: U256,
    _pub_key: &Secp256k1PublicKey,
    prv_key: &Secp256k1SecretKey,
) -> B0255<'static> {
    let secp = SignatureService::default();

    let signature = secp.sign(tx_hash_list_hash.to_vec(), prv_key.0);

    // Sign message
    signature.as_ref().to_vec().try_into().unwrap()
}

fn _get_random_token() -> B0255<'static> {
    let inner: [u8; 32] = rand::random();
    inner.to_vec().try_into().unwrap()
}

/// The entry point of the Job Declarator Server.
///
/// Responsible for initializing server state and accepting incoming TCP connections
/// from downstream clients (JDCs). Each client gets a dedicated [`JobDeclaratorDownstream`]
/// instance.
///
/// Responsibilities:
/// - Listening on the configured address
/// - Performing the SV2 Noise handshake
/// - Handling `SetupConnection` messages
/// - Spawning the downstream message loop
pub struct JobDeclarator {}

impl JobDeclarator {
    /// Starts the Job Declarator server.
    ///
    /// - Accepts configuration and shared components (status sender, mempool, etc.).
    /// - Initializes internal state.
    /// - Begins listening for downstream connections via
    ///   [`JobDeclarator::accept_incoming_connection`].
    pub async fn start(
        config: JobDeclaratorServerConfig,
        status_tx: crate::status::Sender,
        mempool: Arc<Mutex<JDsMempool>>,
        new_block_sender: Sender<String>,
        sender_add_txs_to_mempool: Sender<AddTrasactionsToMempoolInner>,
    ) {
        let self_ = Arc::new(Mutex::new(Self {}));
        info!("JD INITIALIZED");
        Self::accept_incoming_connection(
            self_,
            config,
            status_tx,
            mempool,
            new_block_sender,
            sender_add_txs_to_mempool,
        )
        .await;
    }
    async fn accept_incoming_connection(
        _self_: Arc<Mutex<JobDeclarator>>,
        config: JobDeclaratorServerConfig,
        status_tx: crate::status::Sender,
        mempool: Arc<Mutex<JDsMempool>>,
        new_block_sender: Sender<String>,
        sender_add_txs_to_mempool: Sender<AddTrasactionsToMempoolInner>,
    ) {
        let listener = TcpListener::bind(config.listen_jd_address()).await.unwrap();

        while let Ok((stream, _)) = listener.accept().await {
            let responder = Responder::from_authority_kp(
                &config.authority_public_key().into_bytes(),
                &config.authority_secret_key().into_bytes(),
                std::time::Duration::from_secs(config.cert_validity_sec()),
            )
            .unwrap();

            let addr = stream.peer_addr();

            if let Ok((receiver, sender)) =
                Connection::new(stream, HandshakeRole::Responder(responder)).await
            {
                match receiver.recv().await {
                    Ok(EitherFrame::Sv2(mut sv2_message)) => {
                        debug!("Received SV2 message: {:?}", sv2_message);
                        let payload = sv2_message.payload();

                        if let Ok(setup_connection) =
                            binary_sv2::from_bytes::<SetupConnection>(payload)
                        {
                            let flag = setup_connection.flags;
                            let is_valid = SetupConnection::check_flags(
                                Protocol::JobDeclarationProtocol,
                                config.full_template_mode_required() as u32,
                                flag,
                            );

                            if is_valid {
                                let success_message = SetupConnectionSuccess {
                                    used_version: 2,
                                    flags: (setup_connection.flags & 1u32),
                                };
                                info!("Sending success message for proxy");
                                let sv2_frame: StdFrame = JdsMessages::Common(success_message.into())
        .try_into()
        .expect("Failed to convert setup connection response message to standard frame");

                                sender.send(sv2_frame.into()).await.unwrap();

                                let jddownstream = Arc::new(Mutex::new(
                                    JobDeclaratorDownstream::new(
                                        (setup_connection.flags & 1u32) != 0u32, /* this takes a
                                                                                  * bool instead
                                                                                  * of u32 */
                                        receiver.clone(),
                                        sender.clone(),
                                        &config,
                                        mempool.clone(),
                                        sender_add_txs_to_mempool.clone(), /* each downstream has its own sender (multi producer single consumer) */
                                    ),
                                ));

                                JobDeclaratorDownstream::start(
                                    jddownstream,
                                    status_tx.clone(),
                                    new_block_sender.clone(),
                                );
                            } else {
                                let error_message = SetupConnectionError {
                                    flags: flag,
                                    error_code: "unsupported-feature-flags"
                                        .to_string()
                                        .into_bytes()
                                        .try_into()
                                        .unwrap(),
                                };
                                info!("Sending error message for proxy");
                                let sv2_frame: StdFrame = JdsMessages::Common(error_message.into())
        .try_into()
        .expect("Failed to convert setup connection response message to standard frame");

                                sender.send(sv2_frame.into()).await.unwrap();
                            }
                        } else {
                            error!("Error parsing SetupConnection message");
                        }
                    }
                    Ok(EitherFrame::HandShake(handshake_message)) => {
                        error!(
                            "Unexpected handshake message from upstream: {:?} at {:?}",
                            handshake_message, addr
                        );
                    }
                    Err(e) => {
                        error!("Error receiving message: {:?}", e);
                    }
                }
            } else {
                error!("Cannot connect to {:?}", addr);
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/mempool/error.rs">
//! ## JDS Mempool Errors
//!
//! This module defines the error types and handling utilities related to the mempool logic in the
//! Job Declarator Server (JDS).
//!
//! These errors are mostly used when interacting with:
//! - the internal mempool data structure
//! - the RPC client that communicates with the Bitcoin node
//! - the synchronization/update routines
//!
//! It also includes a centralized error logging helper (`handle_error`) to standardize warnings
//! and diagnostics across components.

use rpc_sv2::mini_rpc_client::RpcError;
use std::{convert::From, sync::PoisonError};
use tracing::{error, warn};

/// Errors that may occur during JDS mempool operations.
#[derive(Debug)]
pub enum JdsMempoolError {
    /// The mempool was found to be empty (likely due to testnet/signet conditions).
    EmptyMempool,
    /// Failed to construct a valid RPC client (e.g. invalid URL, malformed credentials).
    NoClient,
    /// An RPC call to the Bitcoin node failed.
    Rpc(RpcError),
    /// A poisoned lock was encountered while accessing the mempool
    PoisonLock(String),
}

impl From<RpcError> for JdsMempoolError {
    fn from(value: RpcError) -> Self {
        JdsMempoolError::Rpc(value)
    }
}

impl<T> From<PoisonError<T>> for JdsMempoolError {
    fn from(value: PoisonError<T>) -> Self {
        JdsMempoolError::PoisonLock(value.to_string())
    }
}

/// Logs a structured diagnostic message for a given mempool error.
///
/// This function is used throughout the codebase to provide more meaningful context
/// in logs when mempool-related operations fail.
pub fn handle_error(err: &JdsMempoolError) {
    match err {
        JdsMempoolError::EmptyMempool => {
            warn!("{:?}", err);
            warn!("Template Provider is running, but its MEMPOOL is empty (possible reasons: you're testing in testnet, signet, or regtest)");
        }
        JdsMempoolError::NoClient => {
            error!("{:?}", err);
            error!("Unable to establish RPC connection with Template Provider (possible reasons: not fully synced, down)");
        }
        JdsMempoolError::Rpc(_) => {
            error!("{:?}", err);
            error!("Unable to establish RPC connection with Template Provider (possible reasons: not fully synced, down)");
        }
        JdsMempoolError::PoisonLock(_) => {
            error!("{:?}", err);
            error!("Poison lock error)");
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/mempool/mod.rs">
//! ## Mempool Management for the Job Declarator Server (JDS)
//!
//! This module defines the internal mempool of the JDS, responsible for keeping track of known
//! transactions and interacting with the Bitcoin node via RPC.
//!
//! Its core responsibilities are:
//! - Keeping a local copy of txids and (optionally) their full transaction data
//! - Pulling known transactions from the Bitcoin node on demand (via `getrawtransaction`)
//! - Accepting and tracking raw transactions received from clients
//! - Forwarding valid blocks to the Bitcoin node via `submitblock`
//!
//! Internally, `JDsMempool` uses a `HashMap<Txid, Option<(Transaction, u32)>>`:
//! - `None`: transaction only known by ID, data is missing
//! - `Some`: full transaction is known, `u32` is a reference counter for eviction
//!
//! Most methods are `Arc<Mutex<_>>`-wrapped and should be reviewed for locking efficiency.

pub mod error;
use super::job_declarator::AddTrasactionsToMempoolInner;
use crate::mempool::error::JdsMempoolError;
use async_channel::Receiver;
use hashbrown::HashMap;
use rpc_sv2::{mini_rpc_client, mini_rpc_client::RpcError};
use std::{str::FromStr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    bitcoin::{blockdata::transaction::Transaction, hash_types::Txid},
    utils::Mutex,
};

/// Wrapper around a known transaction and its hash.
#[derive(Clone, Debug)]
pub struct TransactionWithHash {
    pub id: Txid,
    pub tx: Option<(Transaction, u32)>, // Full data and ref count
}

/// Internal representation of the JDS mempool.
#[derive(Clone, Debug)]
pub struct JDsMempool {
    /// Local map of known txids and their associated data (if available).
    pub mempool: HashMap<Txid, Option<(Transaction, u32)>>,
    /// Auth for RPC connection to the node.
    auth: mini_rpc_client::Auth,
    /// URI of the Bitcoin node.
    url: rpc_sv2::Uri,
    /// Receiver for new block solutions coming from JDC.
    new_block_receiver: Receiver<String>,
}

impl JDsMempool {
    /// Returns a MiniRpcClient if the URL looks valid.
    pub fn get_client(&self) -> Option<mini_rpc_client::MiniRpcClient> {
        let url = self.url.to_string();
        if url.contains("http") {
            let client = mini_rpc_client::MiniRpcClient::new(self.url.clone(), self.auth.clone());
            Some(client)
        } else {
            None
        }
    }

    /// This function is used only for debug purposes and should not be used
    /// in production code.
    #[cfg(debug_assertions)]
    pub fn _get_transaction_list(self_: Arc<Mutex<Self>>) -> Vec<Txid> {
        let tx_list = self_.safe_lock(|x| x.mempool.clone()).unwrap();
        let tx_list_: Vec<Txid> = tx_list.iter().map(|n| *n.0).collect();
        tx_list_
    }

    /// Instantiates a new empty mempool for JDS.
    pub fn new(
        url: rpc_sv2::Uri,
        username: String,
        password: String,
        new_block_receiver: Receiver<String>,
    ) -> Self {
        let auth = mini_rpc_client::Auth::new(username, password);
        let empty_mempool: HashMap<Txid, Option<(Transaction, u32)>> = HashMap::new();
        JDsMempool {
            mempool: empty_mempool,
            auth,
            url,
            new_block_receiver,
        }
    }

    /// Simple RPC ping to verify connection to Bitcoin node.
    pub async fn health(self_: Arc<Mutex<Self>>) -> Result<(), JdsMempoolError> {
        let client = self_
            .safe_lock(|a| a.get_client())?
            .ok_or(JdsMempoolError::NoClient)?;
        client.health().await.map_err(JdsMempoolError::Rpc)
    }

    /// Inserts transactions into the mempool:
    /// - known txids are fetched from the Bitcoin node
    /// - unknown txs are directly inserted
    pub async fn add_tx_data_to_mempool(
        self_: Arc<Mutex<Self>>,
        add_txs_to_mempool_inner: AddTrasactionsToMempoolInner,
    ) -> Result<(), JdsMempoolError> {
        let txids = add_txs_to_mempool_inner.known_transactions;
        let transactions = add_txs_to_mempool_inner.unknown_transactions;
        let client = self_
            .safe_lock(|a| a.get_client())?
            .ok_or(JdsMempoolError::NoClient)?;
        // fill in the mempool the transactions id in the mempool with the full transactions
        // retrieved from the jd client
        for txid in txids {
            if let Some(None) = self_
                .safe_lock(|a| a.mempool.get(&txid).cloned())
                .map_err(|e| JdsMempoolError::PoisonLock(e.to_string()))?
            {
                let transaction = client
                    .get_raw_transaction(&txid.to_string(), None)
                    .await
                    .map_err(JdsMempoolError::Rpc)?;
                let _ = self_.safe_lock(|a| {
                    a.mempool
                        .entry(transaction.compute_txid())
                        .and_modify(|entry| {
                            if let Some((_, count)) = entry {
                                *count += 1;
                            } else {
                                *entry = Some((transaction.clone(), 1));
                            }
                        })
                        .or_insert(Some((transaction, 1)));
                });
            }
        }

        // fill in the mempool the transactions given in input
        for transaction in transactions {
            let _ = self_.safe_lock(|a| {
                a.mempool
                    .entry(transaction.compute_txid())
                    .and_modify(|entry| {
                        if let Some((_, count)) = entry {
                            *count += 1;
                        } else {
                            *entry = Some((transaction.clone(), 1));
                        }
                    })
                    .or_insert(Some((transaction, 1)));
            });
        }
        Ok(())
    }

    /// Periodically synchronizes the mempool with the Bitcoin node.
    /// This only inserts thin entries (`None` as value), not full transactions.
    pub async fn update_mempool(self_: Arc<Mutex<Self>>) -> Result<(), JdsMempoolError> {
        let client = self_
            .safe_lock(|x| x.get_client())?
            .ok_or(JdsMempoolError::NoClient)?;

        let mempool = client.get_raw_mempool().await?;

        let raw_mempool_txids: Result<Vec<Txid>, _> = mempool
            .into_iter()
            .map(|id| {
                Txid::from_str(&id)
                    .map_err(|err| JdsMempoolError::Rpc(RpcError::Deserialization(err.to_string())))
            })
            .collect();

        let raw_mempool_txids = raw_mempool_txids?;

        // Holding the lock till the light mempool updation is complete.
        let is_mempool_empty = self_.safe_lock(|x| {
            raw_mempool_txids.iter().for_each(|txid| {
                x.mempool.entry(*txid).or_insert(None);
            });
            x.mempool.is_empty()
        })?;

        if is_mempool_empty {
            Err(JdsMempoolError::EmptyMempool)
        } else {
            Ok(())
        }
    }

    /// Listens for block submissions (hex-encoded) and propagates them to the Bitcoin node.
    pub async fn on_submit(self_: Arc<Mutex<Self>>) -> Result<(), JdsMempoolError> {
        let new_block_receiver: Receiver<String> =
            self_.safe_lock(|x| x.new_block_receiver.clone())?;
        let client = self_
            .safe_lock(|x| x.get_client())?
            .ok_or(JdsMempoolError::NoClient)?;

        while let Ok(block_hex) = new_block_receiver.recv().await {
            match mini_rpc_client::MiniRpcClient::submit_block(&client, block_hex).await {
                Ok(_) => return Ok(()),
                Err(e) => JdsMempoolError::Rpc(e),
            };
        }
        Ok(())
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/mod.rs">
//! ## JDS Core Runtime Module
//!
//! This module serves as the central coordination layer of the Job Declarator Server (JDS).
//!
//! It connects all core components:
//! - `mempool`: a local cache of Bitcoin transactions, synchronized via RPC.
//! - `job_declarator`: protocol logic for handling downstream job declaration clients.
//! - `status`: a simple health/error propagation mechanism.
//! - `config`: configuration loader and accessor.
//!
//! The [`JobDeclaratorServer`] struct represents the entrypoint to the system's async runtime.
//! It is launched from `main.rs` and responsible for:
//! - validating config
//! - initializing the mempool
//! - spawning all background tasks
//! - handling graceful shutdowns and task health reporting
//!
//! All components communicate asynchronously using `async_channel`.

pub mod config;
pub mod error;
pub mod job_declarator;
pub mod mempool;
pub mod status;
use async_channel::{bounded, unbounded, Receiver, Sender};
use config::JobDeclaratorServerConfig;
use error::JdsError;
use error_handling::handle_result;
use job_declarator::JobDeclarator;
use mempool::error::JdsMempoolError;
pub use rpc_sv2::Uri;
use std::{ops::Sub, str::FromStr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    codec_sv2::{StandardEitherFrame, StandardSv2Frame},
    parsers::AnyMessage as JdsMessages,
    utils::Mutex,
};
use tokio::{select, task};
use tracing::{error, info, warn};

/// Type alias for incoming SV2 messages.
pub type Message = JdsMessages<'static>;

/// SV2 frame carrying a parsed JDS message.
pub type StdFrame = StandardSv2Frame<Message>;

/// SV2 frame that can be either a standard message or handshake frame.
pub type EitherFrame = StandardEitherFrame<Message>;

/// The core runtime orchestrator for the JDS system.
///
/// Starts all essential services (mempool polling, block submission, job declaration protocol)
/// and monitors for shutdown conditions or task failures via a `status` channel.
#[derive(Debug, Clone)]
pub struct JobDeclaratorServer {
    config: JobDeclaratorServerConfig,
}

impl JobDeclaratorServer {
    /// Constructs a new instance using the given TOML configuration.
    pub fn new(config: JobDeclaratorServerConfig) -> Self {
        Self { config }
    }

    /// Starts the Job Declarator Server runtime.
    ///
    /// This method spawns the following:
    /// - a task for polling the Bitcoin Core mempool
    /// - a task for processing new block submissions from downstream clients
    /// - a task for listening to incoming downstream connections
    /// - a task for integrating transaction data into the local mempool
    ///
    /// It concludes with a `select!` loop that reacts to:
    /// - SIGINT (`tokio::signal::ctrl_c()`)
    /// - messages from the `status` channel
    ///
    /// When a critical error or interrupt is received, the server shuts down cleanly.
    pub async fn start(&self) -> Result<(), JdsError> {
        let mut config = self.config.clone();
        // Normalize URL to avoid trailing slashes.
        if config.core_rpc_url().ends_with('/') {
            config.set_core_rpc_url(config.core_rpc_url().trim_end_matches('/').to_string());
        }
        let url = config.core_rpc_url().to_string() + ":" + &config.core_rpc_port().to_string();
        let username = config.core_rpc_user();
        let password = config.core_rpc_pass();
        // Channel for sending new blocks to the Bitcoin node
        let (new_block_sender, new_block_receiver): (Sender<String>, Receiver<String>) =
            bounded(10);
        let url = Uri::from_str(&url.clone()).expect("Invalid core rpc url");
        // Shared mempool instance
        let mempool = Arc::new(Mutex::new(mempool::JDsMempool::new(
            url,
            username.to_string(),
            password.to_string(),
            new_block_receiver,
        )));
        let mempool_update_interval = config.mempool_update_interval();
        let mempool_cloned_ = mempool.clone();
        let mempool_cloned_1 = mempool.clone();
        // Pre-flight check: can we reach the RPC node
        if let Err(e) = mempool::JDsMempool::health(mempool_cloned_1.clone()).await {
            error!("JDS Connection with bitcoin core failed {:?}", e);
            return Err(JdsError::MempoolError(e));
        }
        let (status_tx, status_rx) = unbounded();
        let sender = status::Sender::Downstream(status_tx.clone());
        let mut last_empty_mempool_warning =
            std::time::Instant::now().sub(std::time::Duration::from_secs(60));

        let sender_update_mempool = sender.clone();
        // ========== Task: Periodically update the mempool via RPC ========== //
        task::spawn(async move {
            loop {
                let update_mempool_result: Result<(), mempool::error::JdsMempoolError> =
                    mempool::JDsMempool::update_mempool(mempool_cloned_.clone()).await;
                if let Err(err) = update_mempool_result {
                    match err {
                        JdsMempoolError::EmptyMempool => {
                            if last_empty_mempool_warning.elapsed().as_secs() >= 60 {
                                warn!("{:?}", err);
                                warn!("Template Provider is running, but its mempool is empty (possible reasons: you're testing in testnet, signet, or regtest)");
                                last_empty_mempool_warning = std::time::Instant::now();
                            }
                        }
                        JdsMempoolError::NoClient => {
                            mempool::error::handle_error(&err);
                            handle_result!(sender_update_mempool, Err(err));
                        }
                        JdsMempoolError::Rpc(_) => {
                            mempool::error::handle_error(&err);
                            handle_result!(sender_update_mempool, Err(err));
                        }
                        JdsMempoolError::PoisonLock(_) => {
                            mempool::error::handle_error(&err);
                            handle_result!(sender_update_mempool, Err(err));
                        }
                    }
                }
                tokio::time::sleep(mempool_update_interval).await;
                // DO NOT REMOVE THIS LINE
                //let _transactions =
                // mempool::JDsMempool::_get_transaction_list(mempool_cloned_.clone());
            }
        });

        // ========== Task: Listen for SubmitSolution events ========== //
        let mempool_cloned = mempool.clone();
        let sender_submit_solution = sender.clone();
        task::spawn(async move {
            loop {
                let result = mempool::JDsMempool::on_submit(mempool_cloned.clone()).await;
                if let Err(err) = result {
                    match err {
                        JdsMempoolError::EmptyMempool => {
                            if last_empty_mempool_warning.elapsed().as_secs() >= 60 {
                                warn!("{:?}", err);
                                warn!("Template Provider is running, but its mempool is empty (possible reasons: you're testing in testnet, signet, or regtest)");
                                last_empty_mempool_warning = std::time::Instant::now();
                            }
                        }
                        _ => {
                            // TODO here there should be a better error managmenet
                            mempool::error::handle_error(&err);
                            handle_result!(sender_submit_solution, Err(err));
                        }
                    }
                }
            }
        });

        // ========== Task: Launch Job Declarator server ========== //
        let cloned = config.clone();
        let mempool_cloned = mempool.clone();
        let (sender_add_txs_to_mempool, receiver_add_txs_to_mempool) = unbounded();
        task::spawn(async move {
            JobDeclarator::start(
                cloned,
                sender,
                mempool_cloned,
                new_block_sender,
                sender_add_txs_to_mempool,
            )
            .await
        });

        // ========== Task: Add transactions to mempool when received ========== //
        task::spawn(async move {
            loop {
                if let Ok(add_transactions_to_mempool) = receiver_add_txs_to_mempool.recv().await {
                    let mempool_cloned = mempool.clone();
                    task::spawn(async move {
                        match mempool::JDsMempool::add_tx_data_to_mempool(
                            mempool_cloned,
                            add_transactions_to_mempool,
                        )
                        .await
                        {
                            Ok(_) => (),
                            Err(err) => {
                                // TODO
                                // here there should be a better error management
                                mempool::error::handle_error(&err);
                            }
                        }
                    });
                }
            }
        });

        // ========== Central Runtime Loop: Shutdown and Error Reactions ========== //
        loop {
            let task_status = select! {
                task_status = status_rx.recv() => task_status,
                interrupt_signal = tokio::signal::ctrl_c() => {
                    match interrupt_signal {
                        Ok(()) => {
                            info!("Interrupt received");
                        },
                        Err(err) => {
                            error!("Unable to listen for interrupt signal: {}", err);
                            // we also shut down in case of error
                        },
                    }
                    break;
                }
            };
            let task_status: status::Status = task_status.unwrap();

            match task_status.state {
                // Should only be sent by the downstream listener
                status::State::DownstreamShutdown(err) => {
                    error!(
                        "SHUTDOWN from Downstream: {}\nTry to restart the downstream listener",
                        err
                    );
                }
                status::State::TemplateProviderShutdown(err) => {
                    error!("SHUTDOWN from Upstream: {}\nTry to reconnecting or connecting to a new upstream", err);
                    break;
                }
                status::State::Healthy(msg) => {
                    info!("HEALTHY message: {}", msg);
                }
                status::State::DownstreamInstanceDropped(downstream_id) => {
                    warn!("Dropping downstream instance {} from jds", downstream_id);
                }
            }
        }
        Ok(())
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/lib/status.rs">
//! ## Status Reporting System for JDS
//!
//! This module defines how internal components of the Job Declarator Server (JDS) report
//! health, errors, and shutdown conditions back to the main runtime loop in `lib/mod.rs`.
//!
//! At the core, tasks send a [`Status`] (wrapping a [`State`]) through a channel,
//! which is tagged with a [`Sender`] enum to indicate the origin of the message.
//!
//! This allows for centralized, consistent error handling across the application.

use stratum_common::roles_logic_sv2::parsers::Mining;

use super::error::JdsError;

/// Identifies the component that originated a [`Status`] update.
///
/// Each sender is associated with a dedicated side of the status channel.
/// This lets the central loop distinguish between errors from different parts of the system.
#[derive(Debug)]
pub enum Sender {
    /// Downstream task (e.g. per-client connection handler)
    Downstream(async_channel::Sender<Status>),
    /// Listener for incoming downstream connections
    DownstreamListener(async_channel::Sender<Status>),
    /// Template Provider (Bitcoin Core RPC)
    Upstream(async_channel::Sender<Status>),
}

impl Clone for Sender {
    fn clone(&self) -> Self {
        match self {
            Self::Downstream(inner) => Self::Downstream(inner.clone()),
            Self::DownstreamListener(inner) => Self::DownstreamListener(inner.clone()),
            Self::Upstream(inner) => Self::Upstream(inner.clone()),
        }
    }
}

/// The kind of event or status being reported by a task.
#[derive(Debug)]
pub enum State {
    /// A downstream component (e.g. client) failed and should be shut down.
    DownstreamShutdown(JdsError),
    /// The Template Provider (upstream Bitcoin Core) failed.
    TemplateProviderShutdown(JdsError),
    /// A specific downstream instance was dropped (e.g., due to protocol error).
    DownstreamInstanceDropped(u32),
    /// A generic message to indicate health or non-critical errors.
    Healthy(String),
}

/// Wraps a status update, to be passed through a status channel.
#[derive(Debug)]
pub struct Status {
    pub state: State,
}

/// Sends a [`Status`] message tagged with its [`Sender`] to the central loop.
///
/// This is the core logic used to determine which status variant should be sent
/// based on the error type and sender context.
async fn send_status(
    sender: &Sender,
    e: JdsError,
    outcome: error_handling::ErrorBranch,
) -> error_handling::ErrorBranch {
    match sender {
        Sender::Downstream(tx) => match e {
            JdsError::Sv2ProtocolError((id, Mining::OpenMiningChannelError(_))) => {
                tx.send(Status {
                    state: State::DownstreamInstanceDropped(id),
                })
                .await
                .unwrap_or(());
            }
            JdsError::ChannelRecv(_) => {
                tx.send(Status {
                    state: State::DownstreamShutdown(e),
                })
                .await
                .unwrap_or(());
            }
            JdsError::MempoolError(_) => {
                tx.send(Status {
                    state: State::TemplateProviderShutdown(e),
                })
                .await
                .unwrap_or(());
            }
            _ => {
                let string_err = e.to_string();
                tx.send(Status {
                    state: State::Healthy(string_err),
                })
                .await
                .unwrap_or(());
            }
        },
        Sender::DownstreamListener(tx) => {
            tx.send(Status {
                state: State::DownstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
        Sender::Upstream(tx) => {
            tx.send(Status {
                state: State::TemplateProviderShutdown(e),
            })
            .await
            .unwrap_or(());
        }
    }
    outcome
}

/// Centralized error dispatcher for the JDS.
///
/// Used by the `handle_result!` macro across the codebase.
/// Decides whether the task should `Continue` or `Break` based on the error type and source.
pub async fn handle_error(sender: &Sender, e: JdsError) -> error_handling::ErrorBranch {
    tracing::debug!("Error: {:?}", &e);
    match e {
        JdsError::Io(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::ChannelSend(_) => {
            //This should be a continue because if we fail to send to 1 downstream we should
            // continue processing the other downstreams in the loop we are in.
            // Otherwise if a downstream fails to send to then subsequent downstreams in
            // the map won't get send called on them
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        JdsError::ChannelRecv(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        JdsError::BinarySv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::Codec(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::Noise(_) => send_status(sender, e, error_handling::ErrorBranch::Continue).await,
        JdsError::RolesLogic(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::Custom(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::Framing(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::PoisonLock(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::Sv2ProtocolError(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        JdsError::MempoolError(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        JdsError::ImpossibleToReconstructBlock(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        JdsError::NoLastDeclaredJob => {
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        JdsError::InvalidRPCUrl => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        JdsError::BadCliArgs => send_status(sender, e, error_handling::ErrorBranch::Break).await,
    }
}

#[cfg(test)]
mod tests {
    use std::{convert::TryInto, io::Error};

    use super::*;
    use async_channel::{bounded, RecvError};
    use stratum_common::roles_logic_sv2::{
        self,
        codec_sv2::{self, binary_sv2, noise_sv2},
        mining_sv2::OpenMiningChannelError,
    };

    #[tokio::test]
    async fn test_send_status_downstream_listener_shutdown() {
        let (tx, rx) = bounded(1);
        let sender = Sender::DownstreamListener(tx);
        let error = JdsError::ChannelRecv(async_channel::RecvError);

        send_status(&sender, error, error_handling::ErrorBranch::Continue).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::DownstreamShutdown(e) => {
                    assert_eq!(e.to_string(), "Channel recv failed: `RecvError`")
                }
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_send_status_upstream_shutdown() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Upstream(tx);
        let error = JdsError::MempoolError(crate::mempool::error::JdsMempoolError::EmptyMempool);
        let error_string = error.to_string();
        send_status(&sender, error, error_handling::ErrorBranch::Continue).await;

        match rx.recv().await {
            Ok(status) => match status.state {
                State::TemplateProviderShutdown(e) => assert_eq!(e.to_string(), error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_io_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::Io(Error::new(std::io::ErrorKind::Interrupted, "IO error"));
        let error_string = error.to_string();

        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_channel_send_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::ChannelSend(Box::new("error"));
        let error_string = error.to_string();

        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_channel_receive_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::ChannelRecv(RecvError);
        let error_string = error.to_string();

        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::DownstreamShutdown(e) => assert_eq!(e.to_string(), error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_binary_sv2_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::BinarySv2(binary_sv2::Error::IoError);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_codec_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::Codec(codec_sv2::Error::InvalidStepForInitiator);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_noise_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::Noise(noise_sv2::Error::HandshakeNotFinalized);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_roles_logic_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::RolesLogic(roles_logic_sv2::Error::BadPayloadSize);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_custom_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::Custom("error".to_string());
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_framing_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::Framing(codec_sv2::framing_sv2::Error::ExpectedHandshakeFrame);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_poison_lock_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::PoisonLock("error".to_string());
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_impossible_to_reconstruct_block_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::ImpossibleToReconstructBlock("Impossible".to_string());
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_no_last_declared_job_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::NoLastDeclaredJob;
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::Healthy(e) => assert_eq!(e, error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_last_mempool_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let error = JdsError::MempoolError(crate::mempool::error::JdsMempoolError::EmptyMempool);
        let error_string = error.to_string();
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::TemplateProviderShutdown(e) => assert_eq!(e.to_string(), error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }

    #[tokio::test]
    async fn test_handle_error_sv2_protocol_error() {
        let (tx, rx) = bounded(1);
        let sender = Sender::Downstream(tx);
        let inner: [u8; 32] = rand::random();
        let value = inner.to_vec().try_into().unwrap();
        let error = JdsError::Sv2ProtocolError((
            12,
            Mining::OpenMiningChannelError(OpenMiningChannelError {
                request_id: 1,
                error_code: value,
            }),
        ));
        let error_string = "12";
        handle_error(&sender, error).await;
        match rx.recv().await {
            Ok(status) => match status.state {
                State::DownstreamInstanceDropped(e) => assert_eq!(e.to_string(), error_string),
                _ => panic!("Unexpected state received"),
            },
            Err(_) => panic!("Failed to receive status"),
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/jd-server/src/main.rs">
//! Entry point for the Job Declarator Server (JDS).
//!
//! This binary parses CLI arguments, loads the TOML configuration file, and
//! starts the main runtime defined in `jd_server::JobDeclaratorServer`.
//!
//! The actual task orchestration and shutdown logic are managed in `lib/mod.rs`.
mod args;
use args::process_cli_args;
use config_helpers::logging::init_logging;
use jd_server::JobDeclaratorServer;
use tracing::error;

/// Entrypoint for the Job Declarator Server binary.
///
/// Loads the configuration from TOML and initializes the main runtime
/// defined in `jd_server::JobDeclaratorServer`. Errors during startup are logged.
#[tokio::main]
async fn main() {
    let config = match process_cli_args() {
        Ok(cfg) => cfg,
        Err(e) => {
            error!("Failed to process CLI arguments: {}", e);
            return;
        }
    };
    init_logging(config.log_file());
    let _ = JobDeclaratorServer::new(config).start().await;
}
</file>

<file path="stratum-1.4.0/roles/pool/Cargo.toml">
[package]
name = "pool_sv2"
version = "0.1.3"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "SV2 pool role"
documentation = "https://docs.rs/pool_sv2"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


[lib]
name = "pool_sv2"
path = "src/lib/mod.rs"

[dependencies]
async-channel = "1.5.1"
stratum-common = { path = "../../common", features = ["with_network_helpers"] }
buffer_sv2 = { path = "../../utils/buffer" }
rand = "0.8.4"
serde = { version = "1.0.89", features = ["derive", "alloc"], default-features = false }
secp256k1 = { version = "0.28.2", default-features = false, features = ["alloc", "rand", "rand-std"] }
tokio = { version = "1.44.1", features = ["full"] }
ext-config = { version = "0.14.0", features = ["toml"], package = "config" }
tracing = { version = "0.1" }
async-recursion = "1.0.0"
error_handling = { path = "../../utils/error-handling" }
nohash-hasher = "0.2.0"
key-utils = { path = "../../utils/key-utils" }
config-helpers = { path = "../roles-utils/config-helpers" }
clap = { version = "4.5.39", features = ["derive"] }

[dev-dependencies]
hex = "0.4.3"
integration_tests_sv2 = { path = "../../test/integration-tests" }
</file>

<file path="stratum-1.4.0/roles/pool/config-examples/pool-config-hosted-tp-example.toml">
# SRI Pool config
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600
test_only_listen_adress_plain = "0.0.0.0:34250"
listen_address = "0.0.0.0:34254"

# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Pool signature (string to be included in coinbase tx)
pool_signature = "Stratum V2 SRI Pool"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./pool.log"

# Template Provider config
# Local TP (this is pointing to localhost so you must run a TP locally for this configuration to work)
#tp_address = "127.0.0.1:8442"
# Hosted testnet TP 
tp_address = "75.119.150.111:8442"
tp_authority_public_key = "9bwHCYnjhbHm4AS3pWg9MtAH83mzWohoJJJDELYBqZhDNqszDLc"
shares_per_minute = 1.0
share_batch_size = 10
</file>

<file path="stratum-1.4.0/roles/pool/config-examples/pool-config-local-tp-example.toml">
# SRI Pool config
authority_public_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
authority_secret_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
cert_validity_sec = 3600
test_only_listen_adress_plain =  "0.0.0.0:34250"
listen_address = "0.0.0.0:34254"

# Coinbase outputs are specified as descriptors. A full list of descriptors is available at
#     https://github.com/bitcoin/bips/blob/master/bip-0380.mediawiki#appendix-b-index-of-script-expressions
# Although the `musig` descriptor is not yet supported and the legacy `combo` descriptor never
# will be. If you have an address, embed it in a descriptor like `addr(<address here>)`.
coinbase_output = "addr(tb1qa0sm0hxzj0x25rh8gw5xlzwlsfvvyz8u96w3p8)"

# Pool signature (string to be included in coinbase tx)
pool_signature = "Stratum V2 SRI Pool"

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./pool.log"


# Template Provider config
# Local TP (this is pointing to localhost so you must run a TP locally for this configuration to work)
tp_address = "127.0.0.1:8442"
shares_per_minute = 1.0
share_batch_size = 10
</file>

<file path="stratum-1.4.0/roles/pool/README.md">
# SRI Pool

SRI Pool is designed to communicate with Downstream role (most typically a Translator Proxy or a Mining Proxy) running SV2 protocol to exploit features introduced by its sub-protocols.

The most typical high level configuration is:

```
<--- Most Downstream ----------------------------------------- Most Upstream --->

+---------------------------------------------------+  +------------------------+
|                     Mining Farm                   |  |      Remote Pool       |
|                                                   |  |                        |
|  +-------------------+     +------------------+   |  |   +-----------------+  |
|  | SV1 Mining Device | <-> | Translator Proxy | <------> | SV2 Pool Server |  |
|  +-------------------+     +------------------+   |  |   +-----------------+  |
|                                                   |  |                        |
+---------------------------------------------------+  +------------------------+

```

## Setup

### Configuration File

`pool-config-hosted-tp-example.toml` and `pool-config-local-tp-example.toml` are examples of configuration files.

The configuration file contains the following information:

1. The SRI Pool information which includes the SRI Pool authority public key
   (`authority_public_key`), the SRI Pool authority secret key (`authority_secret_key`).
2. The address which it will use to listen to new connection from downstream roles (`listen_address`)
3. The list of uncompressed pubkeys for coinbase payout (`coinbase_outputs`)
4. A string that serves as signature on the coinbase tx (`pool_signature`).
5. The Template Provider address (`tp_address`).
6. Optionally, you may want to verify that your TP connection is authentic. You may get `tp_authority_public_key` from the logs of your TP, for example:

```
# 2024-02-13T14:59:24Z Template Provider authority key: EguTM8URcZDQVeEBsM4B5vg9weqEUnufA8pm85fG4bZd
```

### Run

There are two files found in `roles/pool/config-examples`

1. `pool-config-hosted-tp-example.toml` runs on our community hosted server.
2. `pool-config-example-tp-example.toml` runs with your local config.

Run the Pool:

```bash
cd roles/pool/config-examples
cargo run -- -c pool-config-hosted-tp-example.toml
```
</file>

<file path="stratum-1.4.0/roles/pool/src/args.rs">
//! CLI argument parsing for the Pool binary.
//!
//! Defines the `Args` struct and a function to process CLI arguments into a PoolConfig.

use clap::Parser;
use ext_config::{Config, File, FileFormat};
use pool_sv2::config::PoolConfig;
use std::path::PathBuf;

/// Holds the parsed CLI arguments for the Pool binary.
#[derive(Parser, Debug)]
#[command(author, version, about = "Pool CLI", long_about = None)]
pub struct Args {
    #[arg(
        short = 'c',
        long = "config",
        help = "Path to the TOML configuration file",
        default_value = "pool-config.toml"
    )]
    pub config_path: PathBuf,
    #[arg(
        short = 'f',
        long = "log-file",
        help = "Path to the log file. If not set, logs will only be written to stdout."
    )]
    pub log_file: Option<PathBuf>,
}

/// Parses CLI arguments and loads the PoolConfig from the specified file.
pub fn process_cli_args() -> PoolConfig {
    let args = Args::parse();
    let config_path = args.config_path.to_str().expect("Invalid config path");
    let mut config: PoolConfig = Config::builder()
        .add_source(File::new(config_path, FileFormat::Toml))
        .build()
        .and_then(|settings| settings.try_deserialize::<PoolConfig>())
        .expect("Failed to load or deserialize config");

    config.set_log_dir(args.log_file);

    config
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/config.rs">
//! ## Configuration Module
//!
//! Defines [`PoolConfig`], the configuration structure for the Pool, along with its supporting
//! types.
//!
//! This module handles:
//! - Initializing [`PoolConfig`]
//! - Managing [`TemplateProviderConfig`], [`AuthorityConfig`], [`CoinbaseOutput`], and
//!   [`ConnectionConfig`]
//! - Validating and converting coinbase outputs
use std::path::{Path, PathBuf};

use config_helpers::CoinbaseOutput;
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};

/// Configuration for the Pool, including connection, authority, and coinbase settings.
#[derive(Clone, Debug, serde::Deserialize)]
pub struct PoolConfig {
    listen_address: String,
    tp_address: String,
    tp_authority_public_key: Option<Secp256k1PublicKey>,
    authority_public_key: Secp256k1PublicKey,
    authority_secret_key: Secp256k1SecretKey,
    cert_validity_sec: u64,
    #[serde(alias = "coinbase_output")] // only one is allowed, so don't make the user type the plural
    #[serde(deserialize_with = "config_helpers::deserialize_vec_exactly_1")]
    coinbase_outputs: Vec<CoinbaseOutput>,
    pool_signature: String,
    shares_per_minute: f32,
    share_batch_size: usize,
    log_file: Option<PathBuf>,
}

impl PoolConfig {
    /// Creates a new instance of the [`PoolConfig`].
    ///
    /// # Panics
    ///
    /// Panics if `coinbase_outputs` is empty.
    pub fn new(
        pool_connection: ConnectionConfig,
        template_provider: TemplateProviderConfig,
        authority_config: AuthorityConfig,
        coinbase_outputs: Vec<CoinbaseOutput>,
        shares_per_minute: f32,
        share_batch_size: usize,
    ) -> Self {
        assert!(
            !coinbase_outputs.is_empty(),
            "set of coinbase outputs must be nonempty"
        );
        Self {
            listen_address: pool_connection.listen_address,
            tp_address: template_provider.address,
            tp_authority_public_key: template_provider.authority_public_key,
            authority_public_key: authority_config.public_key,
            authority_secret_key: authority_config.secret_key,
            cert_validity_sec: pool_connection.cert_validity_sec,
            coinbase_outputs,
            pool_signature: pool_connection.signature,
            shares_per_minute,
            share_batch_size,
            log_file: None,
        }
    }

    /// Returns the coinbase outputs.
    pub fn coinbase_outputs(&self) -> &Vec<CoinbaseOutput> {
        self.coinbase_outputs.as_ref()
    }

    /// Returns Pool listenining address.
    pub fn listen_address(&self) -> &String {
        &self.listen_address
    }

    /// Returns the authority public key.
    pub fn authority_public_key(&self) -> &Secp256k1PublicKey {
        &self.authority_public_key
    }

    /// Returns the authority secret key.
    pub fn authority_secret_key(&self) -> &Secp256k1SecretKey {
        &self.authority_secret_key
    }

    /// Returns the certificate validity in seconds.
    pub fn cert_validity_sec(&self) -> u64 {
        self.cert_validity_sec
    }

    /// Returns the Pool signature.
    pub fn pool_signature(&self) -> &String {
        &self.pool_signature
    }

    /// Return the Template Provider authority public key.
    pub fn tp_authority_public_key(&self) -> Option<&Secp256k1PublicKey> {
        self.tp_authority_public_key.as_ref()
    }

    /// Returns the Template Provider address.
    pub fn tp_address(&self) -> &String {
        &self.tp_address
    }

    /// Returns the share batch size.
    pub fn share_batch_size(&self) -> usize {
        self.share_batch_size
    }

    /// Sets the coinbase outputs.
    pub fn set_coinbase_outputs(&mut self, coinbase_outputs: Vec<CoinbaseOutput>) {
        self.coinbase_outputs = coinbase_outputs;
    }

    /// Returns the shares per minute.
    pub fn shares_per_minute(&self) -> f32 {
        self.shares_per_minute
    }

    /// Change TP address.
    pub fn set_tp_address(&mut self, tp_address: String) {
        self.tp_address = tp_address;
    }

    /// Sets the log directory.
    pub fn set_log_dir(&mut self, log_dir: Option<PathBuf>) {
        if let Some(dir) = log_dir {
            self.log_file = Some(dir);
        }
    }
    /// Returns the log directory.
    pub fn log_dir(&self) -> Option<&Path> {
        self.log_file.as_deref()
    }
}

/// Configuration for connecting to a Template Provider.
pub struct TemplateProviderConfig {
    address: String,
    authority_public_key: Option<Secp256k1PublicKey>,
}

impl TemplateProviderConfig {
    pub fn new(address: String, authority_public_key: Option<Secp256k1PublicKey>) -> Self {
        Self {
            address,
            authority_public_key,
        }
    }
}

/// Pool's authority public and secret keys.
pub struct AuthorityConfig {
    pub public_key: Secp256k1PublicKey,
    pub secret_key: Secp256k1SecretKey,
}

impl AuthorityConfig {
    pub fn new(public_key: Secp256k1PublicKey, secret_key: Secp256k1SecretKey) -> Self {
        Self {
            public_key,
            secret_key,
        }
    }
}

/// Connection settings for the Pool listener.
pub struct ConnectionConfig {
    listen_address: String,
    cert_validity_sec: u64,
    signature: String,
}

impl ConnectionConfig {
    pub fn new(listen_address: String, cert_validity_sec: u64, signature: String) -> Self {
        Self {
            listen_address,
            cert_validity_sec,
            signature,
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/error.rs">
//! ## Error Module
//!
//! Defines [`PoolError`], the main error type used across the Pool.
//!
//! Centralizes errors from:
//! - I/O operations
//! - Channel send/receive
//! - SV2 stack: Binary, Codec, Noise, Framing, Roles Logic
//! - Locking (PoisonError)
//!
//! Ensures all errors are easy to pass around, including across async boundaries.

use std::{
    convert::From,
    fmt::Debug,
    sync::{MutexGuard, PoisonError},
};

use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{self, binary_sv2, noise_sv2},
    parsers::Mining,
    vardiff::error::VardiffError,
};

/// Represents various errors that can occur in the pool implementation.
#[derive(std::fmt::Debug)]
pub enum PoolError {
    /// I/O-related error.
    Io(std::io::Error),
    /// Error when sending a message through a channel.
    ChannelSend(Box<dyn std::marker::Send + Debug>),
    /// Error when receiving a message from an asynchronous channel.
    ChannelRecv(async_channel::RecvError),
    /// Error from the `binary_sv2` crate.
    BinarySv2(binary_sv2::Error),
    /// Error from the `codec_sv2` crate.
    Codec(codec_sv2::Error),
    /// Error related to parsing a coinbase output specification.
    CoinbaseOutput(config_helpers::CoinbaseOutputError),
    /// Error from the `noise_sv2` crate.
    Noise(noise_sv2::Error),
    /// Error from the `roles_logic_sv2` crate.
    RolesLogic(roles_logic_sv2::Error),
    /// Error related to SV2 message framing.
    Framing(codec_sv2::framing_sv2::Error),
    /// Error due to a poisoned lock, typically from a failed mutex operation.
    PoisonLock(String),
    /// Error indicating that a component has shut down unexpectedly.
    ComponentShutdown(String),
    /// Custom error message.
    Custom(String),
    /// Error related to the SV2 protocol, including an error code and a `Mining` message.
    Sv2ProtocolError((u32, Mining<'static>)),
    Vardiff(VardiffError),
}

impl From<VardiffError> for PoolError {
    fn from(value: VardiffError) -> Self {
        PoolError::Vardiff(value)
    }
}

impl std::fmt::Display for PoolError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        use PoolError::*;
        match self {
            Io(ref e) => write!(f, "I/O error: `{e:?}"),
            ChannelSend(ref e) => write!(f, "Channel send failed: `{e:?}`"),
            ChannelRecv(ref e) => write!(f, "Channel recv failed: `{e:?}`"),
            BinarySv2(ref e) => write!(f, "Binary SV2 error: `{e:?}`"),
            Codec(ref e) => write!(f, "Codec SV2 error: `{e:?}"),
            CoinbaseOutput(ref e) => write!(f, "Coinbase output error: `{e:?}"),
            Framing(ref e) => write!(f, "Framing SV2 error: `{e:?}`"),
            Noise(ref e) => write!(f, "Noise SV2 error: `{e:?}"),
            RolesLogic(ref e) => write!(f, "Roles Logic SV2 error: `{e:?}`"),
            PoisonLock(ref e) => write!(f, "Poison lock: {e:?}"),
            ComponentShutdown(ref e) => write!(f, "Component shutdown: {e:?}"),
            Custom(ref e) => write!(f, "Custom SV2 error: `{e:?}`"),
            Sv2ProtocolError(ref e) => {
                write!(f, "Received Sv2 Protocol Error from upstream: `{e:?}`")
            }
            PoolError::Vardiff(ref e) => {
                write!(f, "Received Vardiff Error : {e:?}")
            }
        }
    }
}

pub type PoolResult<T> = Result<T, PoolError>;

impl From<std::io::Error> for PoolError {
    fn from(e: std::io::Error) -> PoolError {
        PoolError::Io(e)
    }
}

impl From<async_channel::RecvError> for PoolError {
    fn from(e: async_channel::RecvError) -> PoolError {
        PoolError::ChannelRecv(e)
    }
}

impl From<binary_sv2::Error> for PoolError {
    fn from(e: binary_sv2::Error) -> PoolError {
        PoolError::BinarySv2(e)
    }
}

impl From<codec_sv2::Error> for PoolError {
    fn from(e: codec_sv2::Error) -> PoolError {
        PoolError::Codec(e)
    }
}

impl From<config_helpers::CoinbaseOutputError> for PoolError {
    fn from(e: config_helpers::CoinbaseOutputError) -> PoolError {
        PoolError::CoinbaseOutput(e)
    }
}

impl From<noise_sv2::Error> for PoolError {
    fn from(e: noise_sv2::Error) -> PoolError {
        PoolError::Noise(e)
    }
}

impl From<roles_logic_sv2::Error> for PoolError {
    fn from(e: roles_logic_sv2::Error) -> PoolError {
        PoolError::RolesLogic(e)
    }
}

impl<T: 'static + std::marker::Send + Debug> From<async_channel::SendError<T>> for PoolError {
    fn from(e: async_channel::SendError<T>) -> PoolError {
        PoolError::ChannelSend(Box::new(e))
    }
}

impl From<String> for PoolError {
    fn from(e: String) -> PoolError {
        PoolError::Custom(e)
    }
}
impl From<codec_sv2::framing_sv2::Error> for PoolError {
    fn from(e: codec_sv2::framing_sv2::Error) -> PoolError {
        PoolError::Framing(e)
    }
}

impl<T> From<PoisonError<MutexGuard<'_, T>>> for PoolError {
    fn from(e: PoisonError<MutexGuard<T>>) -> PoolError {
        PoolError::PoisonLock(e.to_string())
    }
}

impl From<(u32, Mining<'static>)> for PoolError {
    fn from(e: (u32, Mining<'static>)) -> Self {
        PoolError::Sv2ProtocolError(e)
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/mining_pool/message_handler.rs">
//! # Downstream Message Handler Implementation
//!
//! Implements the `ParseMiningMessagesFromDownstream` trait for the [`Downstream`] struct.
//!
//! This module defines how the pool logic (specifically, a `Downstream` connection instance)
//! reacts to various mining-related messages received from a connected downstream miner.

use super::super::mining_pool::Downstream;
use std::{
    convert::TryInto,
    sync::{Arc, RwLock},
};
use stratum_common::roles_logic_sv2::{
    bitcoin::{consensus::Decodable, transaction::TxOut, Amount},
    channels::server::{
        error::{ExtendedChannelError, StandardChannelError},
        extended::ExtendedChannel,
        group::GroupChannel,
        jobs::job_store::DefaultJobStore,
        share_accounting::{ShareValidationError, ShareValidationResult},
        standard::StandardChannel,
    },
    codec_sv2::binary_sv2::Str0255,
    errors::Error,
    handlers::mining::{ParseMiningMessagesFromDownstream, SendTo, SupportedChannelTypes},
    mining_sv2::*,
    parsers::Mining,
    template_distribution_sv2::SubmitSolution,
    utils::Mutex,
    VardiffState,
};
use tracing::{error, info};

impl ParseMiningMessagesFromDownstream<()> for Downstream {
    // Specifies the types of mining channels supported by this pool implementation.
    //
    // Currently always returns `SupportedChannelTypes::GroupAndExtended`, indicating
    // support for both standard (grouped under a single ID) and extended channels.
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::GroupAndExtended
    }

    // Indicates whether downstream miners are allowed to select their own work (jobs).
    fn is_work_selection_enabled(&self) -> bool {
        true
    }

    // Checks if a downstream miner is authorized based on its user identity.
    fn is_downstream_authorized(
        _self_mutex: Arc<Mutex<Self>>,
        _user_identity: &Str0255,
    ) -> Result<bool, Error> {
        Ok(true)
    }

    // Handles an `OpenStandardMiningChannel` message from the downstream miner.
    //
    // This attempts to add a new standard mining channel (or associate the request
    // with an existing group ID) using the `PoolChannelFactory`.
    //
    // Returns
    // - `Ok(SendTo::Multiple)` - Containing success messages (`OpenStandardMiningChannelSuccess`,
    //   `SetTarget`, `SetExtranoncePrefix`) generated by the factory upon successful channel setup.
    // - `Err(Error)` - If the channel factory fails to add the channel (e.g., invalid parameters,
    //   internal error, lock poisoning).
    fn handle_open_standard_mining_channel(
        &mut self,
        incoming: OpenStandardMiningChannel,
    ) -> Result<SendTo<()>, Error> {
        let request_id = incoming.get_request_id_as_u32();
        let user_identity = std::str::from_utf8(incoming.user_identity.as_ref())
            .map(|s| s.to_string())
            .map_err(|e| Error::InvalidUserIdentity(e.to_string()))?;

        info!("Received OpenStandardMiningChannel: {}", incoming);

        let last_future_template = self.last_future_template.clone();
        let last_set_new_prev_hash_tdp = self.last_new_prev_hash.clone();

        // note: the fact that we're parsing a Vec<TxOut> from the config file is a bit of a hack
        // so while we don't clean that up, we only set the value of the first output
        let mut pool_coinbase_outputs = self.empty_pool_coinbase_outputs.clone();
        pool_coinbase_outputs[0].value =
            Amount::from_sat(last_future_template.coinbase_tx_value_remaining);

        if !self.requires_standard_jobs && self.group_channel.is_none() {
            // we only create one group channel for all standard channels

            let group_channel_id = self.channel_id_factory.next();
            let job_store = Box::new(DefaultJobStore::new());

            let mut group_channel = GroupChannel::new(group_channel_id, job_store);
            group_channel
                .on_new_template(last_future_template.clone(), pool_coinbase_outputs.clone())
                .map_err(Error::FailedToProcessNewTemplateGroupChannel)?;

            group_channel
                .on_set_new_prev_hash(last_set_new_prev_hash_tdp.clone())
                .map_err(Error::FailedToProcessSetNewPrevHashGroupChannel)?;

            self.group_channel = Some(Arc::new(RwLock::new(group_channel)));
        }

        let nominal_hash_rate = incoming.nominal_hash_rate;
        let requested_max_target = incoming.max_target.into_static();

        let extranonce_prefix = self
            .extranonce_prefix_factory_standard
            .safe_lock(|factory| factory.next_prefix_standard())
            .map_err(|e| Error::PoisonLock(e.to_string()))
            .and_then(|res| res.map_err(Error::ExtranoncePrefixFactoryError))?
            .to_vec();

        let channel_id = self.channel_id_factory.next();
        let job_store = Box::new(DefaultJobStore::new());
        let mut standard_channel = match StandardChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix.clone(),
            requested_max_target.into(),
            nominal_hash_rate,
            self.share_batch_size,
            self.shares_per_minute,
            job_store,
        ) {
            Ok(channel) => channel,
            Err(e) => match e {
                StandardChannelError::InvalidNominalHashrate => {
                    error!("OpenMiningChannelError: invalid-nominal-hashrate");
                    let open_standard_mining_channel_error = OpenMiningChannelError {
                        request_id,
                        error_code: "invalid-nominal-hashrate"
                            .to_string()
                            .try_into()
                            .expect("error code must be valid string"),
                    };
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        open_standard_mining_channel_error,
                    )));
                }
                StandardChannelError::RequestedMaxTargetOutOfRange => {
                    error!("OpenMiningChannelError: max-target-out-of-range");
                    let open_standard_mining_channel_error = OpenMiningChannelError {
                        request_id,
                        error_code: "max-target-out-of-range"
                            .to_string()
                            .try_into()
                            .expect("error code must be valid string"),
                    };
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        open_standard_mining_channel_error,
                    )));
                }
                _ => {
                    error!("error in handle_open_standard_mining_channel: {:?}", e);
                    return Err(Error::FailedToCreateStandardChannel(e));
                }
            },
        };

        let mut messages = vec![];

        let group_channel_id = if let Some(group_channel_guard) = &self.group_channel {
            let group_channel = group_channel_guard
                .read()
                .map_err(|e| Error::PoisonLock(e.to_string()))?;
            group_channel.get_group_channel_id()
        } else {
            0
        };

        let open_standard_mining_channel_success = OpenStandardMiningChannelSuccess {
            request_id: incoming.request_id,
            channel_id,
            target: standard_channel.get_target().clone().into(),
            extranonce_prefix: standard_channel
                .get_extranonce_prefix()
                .clone()
                .try_into()
                .expect("extranonce_prefix must be valid"),
            group_channel_id,
        }
        .into_static();

        messages.push(Mining::OpenStandardMiningChannelSuccess(
            open_standard_mining_channel_success,
        ));

        // create a future standard job based on the last future template
        standard_channel
            .on_new_template(last_future_template.clone(), pool_coinbase_outputs)
            .map_err(Error::FailedToCreateStandardChannel)?;

        let future_standard_job_id = standard_channel
            .get_future_template_to_job_id()
            .get(&last_future_template.template_id)
            .expect("future job id must exist");
        let future_standard_job = standard_channel
            .get_future_jobs()
            .get(future_standard_job_id)
            .expect("future job must exist");
        let future_standard_job_message =
            future_standard_job.get_job_message().clone().into_static();

        messages.push(Mining::NewMiningJob(future_standard_job_message));

        // SetNewPrevHash message activates the future job
        let prev_hash = last_set_new_prev_hash_tdp.prev_hash.clone();
        let header_timestamp = last_set_new_prev_hash_tdp.header_timestamp;
        let n_bits = last_set_new_prev_hash_tdp.n_bits;
        let set_new_prev_hash_mining = SetNewPrevHash {
            channel_id,
            job_id: *future_standard_job_id,
            prev_hash,
            min_ntime: header_timestamp,
            nbits: n_bits,
        };
        standard_channel
            .on_set_new_prev_hash(last_set_new_prev_hash_tdp.clone())
            .map_err(Error::FailedToCreateStandardChannel)?;
        messages.push(Mining::SetNewPrevHash(set_new_prev_hash_mining));

        let messages = messages.into_iter().map(SendTo::Respond).collect();

        let vardiff = VardiffState::new()?;

        self.standard_channels
            .insert(channel_id, Arc::new(RwLock::new(standard_channel)));

        self.vardiff
            .insert(channel_id, Arc::new(RwLock::new(Box::new(vardiff))));

        if let Some(group_channel_guard) = &self.group_channel {
            let mut group_channel = group_channel_guard
                .write()
                .map_err(|e| Error::PoisonLock(e.to_string()))?;
            group_channel.add_standard_channel_id(channel_id);
        }

        Ok(SendTo::Multiple(messages))
    }

    // Handles an `OpenExtendedMiningChannel` message from the downstream miner.
    //
    // This attempts to create a new dedicated extended mining channel using the
    // `PoolChannelFactory`.
    //
    // Returns
    // - `Ok(SendTo::Multiple)` - Containing success messages (`OpenExtendedMiningChannelSuccess`,
    //   `SetTarget`, `SetExtranoncePrefix`) generated by the factory.
    // - `Err(Error)` - If the factory fails (e.g., pool doesn't support extended channels, invalid
    //   parameters, lock poisoning)
    fn handle_open_extended_mining_channel(
        &mut self,
        m: OpenExtendedMiningChannel,
    ) -> Result<SendTo<()>, Error> {
        let request_id = m.get_request_id_as_u32();
        let user_identity = std::str::from_utf8(m.user_identity.as_ref())
            .map(|s| s.to_string())
            .map_err(|e| Error::InvalidUserIdentity(e.to_string()))?;

        info!("Received OpenExtendedMiningChannel: {}", m);

        let nominal_hash_rate = m.nominal_hash_rate;
        let requested_max_target = m.max_target.into_static();
        let requested_min_rollable_extranonce_size = m.min_extranonce_size;

        let extranonce_prefix = match self
            .extranonce_prefix_factory_extended
            .safe_lock(|factory| {
                factory.next_prefix_extended(requested_min_rollable_extranonce_size.into())
            })
            .map_err(|e| Error::PoisonLock(e.to_string()))
            .and_then(|res| res.map_err(Error::ExtranoncePrefixFactoryError))
        {
            Ok(extranonce_prefix) => extranonce_prefix.to_vec(),
            Err(_) => {
                error!("OpenMiningChannelError: min-extranonce-size-too-large");
                let open_extended_mining_channel_error = OpenMiningChannelError {
                    request_id,
                    error_code: "min-extranonce-size-too-large"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                    open_extended_mining_channel_error,
                )));
            }
        };

        let channel_id = self.channel_id_factory.next();
        let job_store = Box::new(DefaultJobStore::new());
        let mut extended_channel = match ExtendedChannel::new(
            channel_id,
            user_identity,
            extranonce_prefix,
            requested_max_target.into(),
            nominal_hash_rate,
            true, // version rolling always allowed
            requested_min_rollable_extranonce_size,
            self.share_batch_size,
            self.shares_per_minute,
            job_store,
        ) {
            Ok(channel) => channel,
            Err(e) => match e {
                ExtendedChannelError::InvalidNominalHashrate => {
                    error!("OpenMiningChannelError: invalid-nominal-hashrate");
                    let open_extended_mining_channel_error = OpenMiningChannelError {
                        request_id,
                        error_code: "invalid-nominal-hashrate"
                            .to_string()
                            .try_into()
                            .expect("error code must be valid string"),
                    };
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        open_extended_mining_channel_error,
                    )));
                }
                ExtendedChannelError::RequestedMaxTargetOutOfRange => {
                    error!("OpenMiningChannelError: max-target-out-of-range");
                    let open_extended_mining_channel_error = OpenMiningChannelError {
                        request_id,
                        error_code: "max-target-out-of-range"
                            .to_string()
                            .try_into()
                            .expect("error code must be valid string"),
                    };
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        open_extended_mining_channel_error,
                    )));
                }
                ExtendedChannelError::RequestedMinExtranonceSizeTooLarge => {
                    error!("OpenMiningChannelError: min-extranonce-size-too-large");
                    let open_extended_mining_channel_error = OpenMiningChannelError {
                        request_id,
                        error_code: "min-extranonce-size-too-large"
                            .to_string()
                            .try_into()
                            .expect("error code must be valid string"),
                    };
                    return Ok(SendTo::Respond(Mining::OpenMiningChannelError(
                        open_extended_mining_channel_error,
                    )));
                }
                _ => {
                    error!("error in handle_open_extended_mining_channel: {:?}", e);
                    return Err(Error::FailedToCreateExtendedChannel(e));
                }
            },
        };

        let mut messages = vec![];

        let open_extended_mining_channel_success = OpenExtendedMiningChannelSuccess {
            request_id,
            channel_id,
            target: extended_channel.get_target().clone().into(),
            extranonce_prefix: extended_channel
                .get_extranonce_prefix()
                .clone()
                .try_into()?,
            extranonce_size: extended_channel.get_rollable_extranonce_size(),
        }
        .into_static();

        messages.push(Mining::OpenExtendedMiningChannelSuccess(
            open_extended_mining_channel_success,
        ));

        let last_future_template = self.last_future_template.clone();

        // note: the fact that we're parsing a Vec<TxOut> from the config file is a bit of a hack
        // so while we don't clean that up, we only set the value of the first output
        let mut pool_coinbase_outputs = self.empty_pool_coinbase_outputs.clone();
        pool_coinbase_outputs[0].value =
            Amount::from_sat(last_future_template.coinbase_tx_value_remaining);

        // create a future extended job based on the last future template
        extended_channel
            .on_new_template(last_future_template.clone(), pool_coinbase_outputs)
            .map_err(Error::FailedToCreateExtendedChannel)?;

        let future_extended_job_id = extended_channel
            .get_future_template_to_job_id()
            .get(&last_future_template.template_id)
            .expect("future job id must exist");
        let future_extended_job = extended_channel
            .get_future_jobs()
            .get(future_extended_job_id)
            .expect("future job must exist");

        let future_extended_job_message =
            future_extended_job.get_job_message().clone().into_static();

        // send this future job as new job message
        // to be immediately activated with the subsequent SetNewPrevHash message
        messages.push(Mining::NewExtendedMiningJob(future_extended_job_message));

        // SetNewPrevHash message activates the future job
        let last_set_new_prev_hash_tdp = self.last_new_prev_hash.clone();
        let prev_hash = last_set_new_prev_hash_tdp.prev_hash.clone();
        let header_timestamp = last_set_new_prev_hash_tdp.header_timestamp;
        let n_bits = last_set_new_prev_hash_tdp.n_bits;
        let set_new_prev_hash_mining = SetNewPrevHash {
            channel_id,
            job_id: *future_extended_job_id,
            prev_hash,
            min_ntime: header_timestamp,
            nbits: n_bits,
        };
        extended_channel
            .on_set_new_prev_hash(self.last_new_prev_hash.clone())
            .map_err(Error::FailedToCreateExtendedChannel)?;
        messages.push(Mining::SetNewPrevHash(set_new_prev_hash_mining));

        let messages = messages.into_iter().map(SendTo::Respond).collect();

        let vardiff = VardiffState::new()?;

        self.extended_channels
            .insert(channel_id, Arc::new(RwLock::new(extended_channel)));
        self.vardiff
            .insert(channel_id, Arc::new(RwLock::new(Box::new(vardiff))));

        Ok(SendTo::Multiple(messages))
    }

    // Handles an `UpdateChannel` message from the downstream miner.
    //
    // This updates the target difficulty for the specified channel based on the
    // newly provided nominal hash rate.
    //
    // Returns
    // - `Ok(SendTo::Respond)` - Containing a `SetTarget` message with the calculated new maximum
    //   target difficulty.
    // - `Err(Error)` - If calculating the target fails or the channel factory interaction fails.
    fn handle_update_channel(&mut self, m: UpdateChannel) -> Result<SendTo<()>, Error> {
        info!("Received UpdateChannel message: {}", m);

        let channel_id = m.channel_id;
        let new_nominal_hash_rate = m.nominal_hash_rate;
        let requested_maximum_target = m.maximum_target.into_static();

        let is_standard_channel = self.standard_channels.contains_key(&channel_id);
        let is_extended_channel = self.extended_channels.contains_key(&channel_id);

        if is_standard_channel {
            let mut standard_channel = self
                .standard_channels
                .get(&channel_id)
                .expect("standard channel must exist")
                .write()
                .map_err(|e| Error::PoisonLock(e.to_string()))?;
            let res = standard_channel
                .update_channel(new_nominal_hash_rate, Some(requested_maximum_target.into()));
            match res {
                Ok(_) => {}
                Err(e) => {
                    error!("UpdateChannelError: {:?}", e);
                    match e {
                        StandardChannelError::InvalidNominalHashrate => {
                            error!("UpdateChannelError: invalid-nominal-hashrate");
                            let update_channel_error = UpdateChannelError {
                                channel_id,
                                error_code: "invalid-nominal-hashrate"
                                    .to_string()
                                    .try_into()
                                    .expect("error code must be valid string"),
                            };
                            return Ok(SendTo::Respond(Mining::UpdateChannelError(
                                update_channel_error,
                            )));
                        }
                        StandardChannelError::RequestedMaxTargetOutOfRange => {
                            error!("UpdateChannelError: requested-max-target-out-of-range");
                            let update_channel_error = UpdateChannelError {
                                channel_id,
                                error_code: "requested-max-target-out-of-range"
                                    .to_string()
                                    .try_into()
                                    .expect("error code must be valid string"),
                            };
                            return Ok(SendTo::Respond(Mining::UpdateChannelError(
                                update_channel_error,
                            )));
                        }
                        _ => {
                            return Err(Error::FailedToUpdateStandardChannel(e));
                        }
                    }
                }
            }
            let new_target = standard_channel.get_target();
            let set_target = SetTarget {
                channel_id,
                maximum_target: new_target.clone().into(),
            };
            Ok(SendTo::Respond(Mining::SetTarget(set_target)))
        } else if is_extended_channel {
            let mut extended_channel = self
                .extended_channels
                .get(&channel_id)
                .expect("extended channel must exist")
                .write()
                .map_err(|e| Error::PoisonLock(e.to_string()))?;
            let res = extended_channel
                .update_channel(new_nominal_hash_rate, Some(requested_maximum_target.into()));
            match res {
                Ok(_) => {}
                Err(e) => {
                    error!("UpdateChannelError: {:?}", e);
                    match e {
                        ExtendedChannelError::InvalidNominalHashrate => {
                            error!("UpdateChannelError: invalid-nominal-hashrate");
                            let update_channel_error = UpdateChannelError {
                                channel_id,
                                error_code: "invalid-nominal-hashrate"
                                    .to_string()
                                    .try_into()
                                    .expect("error code must be valid string"),
                            };
                            return Ok(SendTo::Respond(Mining::UpdateChannelError(
                                update_channel_error,
                            )));
                        }
                        ExtendedChannelError::RequestedMaxTargetOutOfRange => {
                            error!("UpdateChannelError: max-target-out-of-range");
                            let update_channel_error = UpdateChannelError {
                                channel_id,
                                error_code: "max-target-out-of-range"
                                    .to_string()
                                    .try_into()
                                    .expect("error code must be valid string"),
                            };
                            return Ok(SendTo::Respond(Mining::UpdateChannelError(
                                update_channel_error,
                            )));
                        }
                        _ => {
                            return Err(Error::FailedToUpdateExtendedChannel(e));
                        }
                    }
                }
            }
            let new_target = extended_channel.get_target();
            let set_target = SetTarget {
                channel_id,
                maximum_target: new_target.clone().into(),
            };
            return Ok(SendTo::Respond(Mining::SetTarget(set_target)));
        } else {
            error!("UpdateChannelError: invalid-channel-id");
            let update_channel_error = UpdateChannelError {
                channel_id,
                error_code: "invalid-channel-id"
                    .to_string()
                    .try_into()
                    .expect("error code must be valid string"),
            };
            return Ok(SendTo::Respond(Mining::UpdateChannelError(
                update_channel_error,
            )));
        }
    }

    // Handles a `SubmitSharesStandard` message from the downstream miner.
    //
    // Validates the submitted share using the `PoolChannelFactory`. If the share is valid
    // and meets the network target, it constructs a `SubmitSolution` message and sends it
    // upstream via the `solution_sender`. Responds to the miner with `SubmitSharesSuccess`
    // or `SubmitSharesError`.
    //
    // Returns
    // - `Ok(SendTo::Respond)` - Containing either `SubmitSharesSuccess` or `SubmitSharesError`.
    // - `Err(Error)` - If the channel factory interaction fails or constructing the solution fails.
    fn handle_submit_shares_standard(
        &mut self,
        m: SubmitSharesStandard,
    ) -> Result<SendTo<()>, Error> {
        info!("Received: {}", m);

        let channel_id = m.channel_id;
        if !self.standard_channels.contains_key(&channel_id) {
            let submit_shares_error = SubmitSharesError {
                channel_id,
                sequence_number: m.sequence_number,
                error_code: "invalid-channel-id"
                    .to_string()
                    .try_into()
                    .expect("error code must be valid string"),
            };
            error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-channel-id ", channel_id, m.sequence_number);
            return Ok(SendTo::Respond(Mining::SubmitSharesError(
                submit_shares_error,
            )));
        }

        let mut standard_channel = self
            .standard_channels
            .get(&channel_id)
            .expect("standard channel must exist")
            .write()
            .map_err(|e| Error::PoisonLock(e.to_string()))?;

        let mut vardiff = self
            .vardiff
            .get(&channel_id)
            .expect("Vardiff must exist")
            .write()
            .map_err(|e| Error::PoisonLock(e.to_string()))?;

        let res = standard_channel.validate_share(m.clone());
        vardiff.increment_shares_since_last_update();
        match res {
            Ok(ShareValidationResult::Valid) => {
                info!(
                    "SubmitSharesStandard: valid share | channel_id: {}, sequence_number: {} ",
                    channel_id, m.sequence_number
                );
                Ok(SendTo::None(None))
            }
            Ok(ShareValidationResult::ValidWithAcknowledgement(
                last_sequence_number,
                new_submits_accepted_count,
                new_shares_sum,
            )) => {
                let success = SubmitSharesSuccess {
                    channel_id,
                    last_sequence_number,
                    new_submits_accepted_count,
                    new_shares_sum,
                };
                info!("SubmitSharesStandard: {} ", success);
                Ok(SendTo::Respond(Mining::SubmitSharesSuccess(success)))
            }
            Ok(ShareValidationResult::BlockFound(template_id, coinbase)) => {
                info!("SubmitSharesStandard:  Block Found!!! ");
                // if we have a template id (i.e.: this was not a custom job)
                // we can propagate the solution to the TP
                if let Some(template_id) = template_id {
                    info!("SubmitSharesStandard: Propagating solution to the Template Provider.");
                    let solution = SubmitSolution {
                        template_id,
                        version: m.version,
                        header_timestamp: m.ntime,
                        header_nonce: m.nonce,
                        coinbase_tx: coinbase.try_into()?,
                    };
                    if self.solution_sender.try_send(solution.clone()).is_err() {
                        return Err(Error::FailedToSendSolution);
                    }
                }
                let share_accounting = standard_channel.get_share_accounting();
                let success = SubmitSharesSuccess {
                    channel_id,
                    last_sequence_number: share_accounting.get_last_share_sequence_number(),
                    new_submits_accepted_count: share_accounting.get_shares_accepted(),
                    new_shares_sum: share_accounting.get_share_work_sum(),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesSuccess(success)))
            }
            Err(ShareValidationError::Invalid) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "invalid-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::Stale) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: stale-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "stale-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::InvalidJobId) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-job-id ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "invalid-job-id"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::DoesNotMeetTarget) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: difficulty-too-low ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "difficulty-too-low"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::DuplicateShare) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: duplicate-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "duplicate-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            _ => {
                // any other error variations should never happen
                unreachable!()
            }
        }
    }

    // Handles a `SubmitSharesExtended` message from the downstream miner.
    //
    // Similar logic to `handle_submit_shares_standard`, but for extended shares.
    // Validates the share, checks against targets, sends solutions to Template Provider if
    // applicable, and responds with `SubmitSharesSuccess` or `SubmitSharesError`.
    //
    // Returns
    // - `Ok(SendTo::Respond)` - Containing either `SubmitSharesSuccess` or `SubmitSharesError`.
    // - `Err(Error)` - If the channel factory interaction fails or constructing the solution fails.
    fn handle_submit_shares_extended(
        &mut self,
        m: SubmitSharesExtended,
    ) -> Result<SendTo<()>, Error> {
        info!("Received: {}", m);

        let channel_id = m.channel_id;
        if !self.extended_channels.contains_key(&channel_id) {
            let error = SubmitSharesError {
                channel_id,
                sequence_number: m.sequence_number,
                error_code: "invalid-channel-id"
                    .to_string()
                    .try_into()
                    .expect("error code must be valid string"),
            };
            error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-channel-id ", channel_id, m.sequence_number);
            return Ok(SendTo::Respond(Mining::SubmitSharesError(error)));
        }

        let mut extended_channel = self
            .extended_channels
            .get(&channel_id)
            .expect("extended channel must exist")
            .write()
            .map_err(|e| Error::PoisonLock(e.to_string()))?;

        let mut vardiff = self
            .vardiff
            .get(&channel_id)
            .expect("Vardiff must exist")
            .write()
            .map_err(|e| Error::PoisonLock(e.to_string()))?;

        let res = extended_channel.validate_share(m.clone());
        vardiff.increment_shares_since_last_update();
        match res {
            Ok(ShareValidationResult::Valid) => {
                info!(
                    "SubmitSharesExtended: valid share | channel_id: {}, sequence_number: {} ",
                    channel_id, m.sequence_number
                );
                Ok(SendTo::None(None))
            }
            Ok(ShareValidationResult::ValidWithAcknowledgement(
                last_sequence_number,
                new_submits_accepted_count,
                new_shares_sum,
            )) => {
                let success = SubmitSharesSuccess {
                    channel_id,
                    last_sequence_number,
                    new_submits_accepted_count,
                    new_shares_sum,
                };
                info!("SubmitSharesExtended: {} ", success);
                Ok(SendTo::Respond(Mining::SubmitSharesSuccess(success)))
            }
            Ok(ShareValidationResult::BlockFound(template_id, coinbase)) => {
                info!("SubmitSharesExtended:  Block Found!!! ");
                // if we have a template id (i.e.: this was not a custom job)
                // we can propagate the solution to the TP
                if let Some(template_id) = template_id {
                    info!("SubmitSharesExtended: Propagating solution to the Template Provider.");
                    let solution = SubmitSolution {
                        template_id,
                        version: m.version,
                        header_timestamp: m.ntime,
                        header_nonce: m.nonce,
                        coinbase_tx: coinbase.try_into()?,
                    };
                    if self.solution_sender.try_send(solution.clone()).is_err() {
                        return Err(Error::FailedToSendSolution);
                    }
                }
                let share_accounting = extended_channel.get_share_accounting();
                let success = SubmitSharesSuccess {
                    channel_id,
                    last_sequence_number: share_accounting.get_last_share_sequence_number(),
                    new_submits_accepted_count: share_accounting.get_shares_accepted(),
                    new_shares_sum: share_accounting.get_share_work_sum(),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesSuccess(success)))
            }
            Err(ShareValidationError::Invalid) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "invalid-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::Stale) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: stale-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "stale-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::InvalidJobId) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: invalid-job-id ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "invalid-job-id"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::DoesNotMeetTarget) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: difficulty-too-low ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "difficulty-too-low"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            Err(ShareValidationError::DuplicateShare) => {
                error!("SubmitSharesError: channel_id: {}, sequence_number: {}, error_code: duplicate-share ", channel_id, m.sequence_number);
                let error = SubmitSharesError {
                    channel_id: m.channel_id,
                    sequence_number: m.sequence_number,
                    error_code: "duplicate-share"
                        .to_string()
                        .try_into()
                        .expect("error code must be valid string"),
                };
                Ok(SendTo::Respond(Mining::SubmitSharesError(error)))
            }
            _ => {
                // any other error variations should never happen
                unreachable!()
            }
        }
    }

    // Handles a `SetCustomMiningJob` message from the downstream miner.
    //
    // This informs the `PoolChannelFactory` about the custom job details provided
    // by the miner for a specific channel.
    //
    // Returns
    // - `Ok(SendTo::Respond)` - Containing a `SetCustomMiningJobSuccess` message acknowledging the
    //   custom job setup.
    // - `Err(Error)` - If the channel factory interaction fails.
    fn handle_set_custom_mining_job(&mut self, m: SetCustomMiningJob) -> Result<SendTo<()>, Error> {
        info!("Received SetCustomMiningJob: {}", m);

        // this is a naive implementation, but ideally we should check the SetCustomMiningJob
        // message parameters, especially:
        // - the mining_job_token
        // - the amount of the pool payout output

        let custom_job_coinbase_outputs = Vec::<TxOut>::consensus_decode(
            &mut m.coinbase_tx_outputs.inner_as_ref().to_vec().as_slice(),
        )
        .map_err(|_| Error::FailedToDeserializeCoinbaseOutputs)?;

        // check that all script_pubkeys from self.empty_pool_coinbase_outputs are present in the
        // custom job coinbase outputs
        let missing_script = self.empty_pool_coinbase_outputs.iter().find(|pool_output| {
            !custom_job_coinbase_outputs
                .iter()
                .any(|custom_output| custom_output.script_pubkey == pool_output.script_pubkey)
        });

        if missing_script.is_some() {
            error!("SetCustomMiningJobError: pool-payout-script-missing");

            let error = SetCustomMiningJobError {
                request_id: m.request_id,
                channel_id: m.channel_id,
                error_code: "pool-payout-script-missing"
                    .to_string()
                    .try_into()
                    .expect("error code must be valid string"),
            };

            return Ok(SendTo::Respond(Mining::SetCustomMiningJobError(error)));
        }

        let channel_id = m.channel_id;
        if !self.extended_channels.contains_key(&channel_id) {
            error!("SetCustomMiningJobError: invalid-channel-id");
            let error = SetCustomMiningJobError {
                request_id: m.request_id,
                channel_id,
                error_code: "invalid-channel-id"
                    .to_string()
                    .try_into()
                    .expect("error code must be valid string"),
            };
            return Ok(SendTo::Respond(Mining::SetCustomMiningJobError(error)));
        }

        let mut extended_channel = self
            .extended_channels
            .get(&channel_id)
            .expect("extended channel must exist")
            .write()
            .map_err(|e| Error::PoisonLock(e.to_string()))?;

        let job_id = extended_channel
            .on_set_custom_mining_job(m.clone().into_static())
            .map_err(Error::FailedToSetCustomMiningJob)?;

        let success = SetCustomMiningJobSuccess {
            channel_id,
            request_id: m.request_id,
            job_id,
        };
        Ok(SendTo::Respond(Mining::SetCustomMiningJobSuccess(success)))
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/mining_pool/mod.rs">
//! ## Mining Pool
//!
//! The core functionality for a mining pool, including
//! management of downstream miners, job templates, and solution submissions.
//!
//! The [`Pool`] struct maintains the state of active downstream connections, handles
//! the acceptance of new connections, distributes new mining jobs, and processes
//! solutions submitted by miners.
//!
//! The [`Downstream`] struct represents a single connected miner, managing its
//! communication channels, incoming messages, and assigned mining jobs.
//!
//! Key functionalities include:
//! - Secure handshake and connection setup for downstream miners
//! - Broadcasting new mining templates and previous hash updates
//! - Handling mining shares submitted by downstreams
//!
//! Components:
//! - `Pool`: Central manager for all downstream connections and job updates.
//! - `Downstream`: Represents a miner and handles its connection lifecycle.
//! - `PoolChannelFactory`: Manages the creation and tracking of mining channels.
use crate::config::PoolConfig;

use super::{
    error::{PoolError, PoolResult},
    status,
};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use key_utils::SignatureService;
use nohash_hasher::BuildNoHashHasher;
use secp256k1;
use std::{
    collections::HashMap,
    convert::TryInto,
    net::SocketAddr,
    sync::{Arc, RwLock},
    time::Duration,
};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self,
        bitcoin::{Amount, TxOut},
        channels::server::{
            extended::ExtendedChannel, group::GroupChannel, standard::StandardChannel,
        },
        codec_sv2::{
            self, binary_sv2::U256, HandshakeRole, Responder, StandardEitherFrame, StandardSv2Frame,
        },
        errors::Error,
        handlers::mining::{ParseMiningMessagesFromDownstream, SendTo},
        mining_sv2::{
            ExtendedExtranonce, SetNewPrevHash as SetNewPrevHashMp, SetTarget, Target,
            MAX_EXTRANONCE_LEN,
        },
        parsers::{AnyMessage, Mining},
        template_distribution_sv2::{
            NewTemplate, SetNewPrevHash as SetNewPrevHashTdp, SubmitSolution,
        },
        utils::{Id as IdFactory, Mutex},
    },
};

use roles_logic_sv2::Vardiff;

use tokio::{net::TcpListener, task};
use tracing::{debug, error, info, warn};

pub mod setup_connection;
use setup_connection::SetupConnectionHandler;

pub mod message_handler;
/// Represents a generic SV2 message with a static lifetime.
pub type Message = AnyMessage<'static>;
/// A standard SV2 frame containing a message.
pub type StdFrame = StandardSv2Frame<Message>;
/// A standard SV2 frame that can contain either type of frame.
pub type EitherFrame = StandardEitherFrame<Message>;

/// Parses the coinbase output configurations from the [`PoolConfig`] and converts them
/// into `bitcoin::TxOut` objects required by the pool logic.
///
/// It iterates through the configured outputs, attempts to convert them into the
/// internal `CoinbaseOutput_` representation and then into `bitcoin::ScriptBuf`.
/// Sets the value to 0 sats as per SV2 pool requirements (actual value determined later)
pub fn get_coinbase_output(config: &PoolConfig) -> Vec<TxOut> {
    config
        .coinbase_outputs()
        .iter()
        .map(|out| TxOut {
            value: Amount::from_sat(0),
            script_pubkey: out.script_pubkey().to_owned(),
        })
        .collect()
}

/// Represents a single connection to a downstream miner.
///
/// Encapsulates the state and communication channels for one miner. An instance
/// is created for each accepted TCP connection after the Noise handshake and SV2
/// setup messages are successfully exchanged. Each `Downstream` runs its own message
/// receiving loop in a separate Tokio task.
#[derive(Debug)]
pub struct Downstream {
    // The unique identifier for this downstream connection's channel or group.
    id: u32,
    // Channel receiver for incoming SV2 frames from the network connection task.
    receiver: Receiver<EitherFrame>,
    // Channel sender for outgoing SV2 frames to the network connection task.
    sender: Sender<EitherFrame>,
    // Whether the downstream requires standard jobs.
    requires_standard_jobs: bool,
    // Sender channel to forward valid `SubmitSolution` messages received from this
    // downstream miner to the main [`Pool`] task, which then sends them upstream.
    solution_sender: Sender<SubmitSolution<'static>>,
    channel_id_factory: IdFactory,
    extranonce_prefix_factory_extended: Arc<Mutex<ExtendedExtranonce>>,
    extranonce_prefix_factory_standard: Arc<Mutex<ExtendedExtranonce>>,
    // A map of all extended channels, keyed by their ID.
    extended_channels: HashMap<u32, Arc<RwLock<ExtendedChannel<'static>>>>,
    // A map of all standard channels, keyed by their ID.
    standard_channels: HashMap<u32, Arc<RwLock<StandardChannel<'static>>>>,
    vardiff: HashMap<u32, Arc<RwLock<Box<dyn Vardiff>>>>,
    // naive approach:
    // we create one group channel for the connection
    // and add all standard channels to this same single group channel
    // (that is, only if SetupConnection.REQUIRES_STANDARD_JOBS flag is set)
    group_channel: Option<Arc<RwLock<GroupChannel<'static>>>>,
    share_batch_size: usize,
    shares_per_minute: f32,
    last_future_template: NewTemplate<'static>,
    last_new_prev_hash: SetNewPrevHashTdp<'static>,
    empty_pool_coinbase_outputs: Vec<TxOut>,
}

/// The central state manager for the mining pool.
///
/// Holds all active downstream connections and manages the overall pool logic.
/// It receives job updates (templates, prev_hashes) from template receiver and distributes
/// them to the appropriate downstreams. It also receives solutions from downstreams
/// and forwards them upstream.
pub struct Pool {
    // A map storing all active downstream connections.
    // Keyed by the downstream's channel/group ID (`u32`).
    downstreams: HashMap<u32, Arc<Mutex<Downstream>>, BuildNoHashHasher<u32>>,
    // Sender channel to forward solutions received from any downstream connection
    // to the upstream Template Provider connection task.
    solution_sender: Sender<SubmitSolution<'static>>,
    // Flag indicating whether at least one `NewTemplate` has been received and processed.
    // Might be used to ensure initial jobs are sent before accepting solutions??.
    new_template_processed: bool,
    downstream_id_factory: IdFactory,
    // Sender channel for reporting status updates and errors to the main monitoring loop.
    status_tx: status::Sender,
    extranonce_prefix_factory_extended: Arc<Mutex<ExtendedExtranonce>>,
    extranonce_prefix_factory_standard: Arc<Mutex<ExtendedExtranonce>>,
    share_batch_size: usize,
    last_future_template: Option<NewTemplate<'static>>,
    last_new_prev_hash: Option<SetNewPrevHashTdp<'static>>,
}

impl Downstream {
    /// Creates a new `Downstream` instance representing a miner connection.
    ///
    /// This function orchestrates the setup of a new downstream connection after the
    /// underlying TCP and Noise handshake are complete. It handles the initial SV2
    /// message exchange (`SetupConnection`), assigns a channel ID using the `channel_factory`,
    /// stores the connection, and spawns a dedicated Tokio task (`Downstream::run_receiver`)
    /// to handle incoming messages from this specific miner.
    #[allow(clippy::too_many_arguments)]
    pub async fn new(
        mut receiver: Receiver<EitherFrame>,
        mut sender: Sender<EitherFrame>,
        solution_sender: Sender<SubmitSolution<'static>>,
        pool: Arc<Mutex<Pool>>,
        status_tx: status::Sender,
        address: SocketAddr,
        shares_per_minute: f32,
        empty_pool_coinbase_outputs: Vec<TxOut>,
    ) -> PoolResult<Arc<Mutex<Self>>> {
        // Handle the SV2 SetupConnection message exchange.
        let setup_connection = Arc::new(Mutex::new(SetupConnectionHandler::new()));
        let requires_standard_jobs =
            SetupConnectionHandler::setup(setup_connection, &mut receiver, &mut sender, address)
                .await?;

        let id = pool.safe_lock(|p| p.downstream_id_factory.next())?;

        let channel_id_factory = IdFactory::new();

        // extranonce prefix factories are shared across all downstreams
        // that avoids extranonce_prefix collision across different downstreams
        let extranonce_prefix_factory_extended =
            pool.safe_lock(|p| p.extranonce_prefix_factory_extended.clone())?;
        let extranonce_prefix_factory_standard =
            pool.safe_lock(|p| p.extranonce_prefix_factory_standard.clone())?;

        let share_batch_size = pool.safe_lock(|p| p.share_batch_size)?;

        // prevents undefined behavior if some client connects
        // before the first template and prev hash are cached
        let mut has_cached_prev_hash = false;
        while !has_cached_prev_hash {
            pool.safe_lock(|p| {
                if p.last_new_prev_hash.is_some() {
                    has_cached_prev_hash = true;
                }
            })?;
            tokio::task::yield_now().await;
        }

        let last_future_template = pool.safe_lock(|p| {
            p.last_future_template
                .clone()
                .expect("last_future_template must be Some")
        })?;

        let last_new_prev_hash = pool.safe_lock(|p| {
            p.last_new_prev_hash
                .clone()
                .expect("last_new_prev_hash must be Some")
        })?;

        // note: the fact that we're parsing a Vec<TxOut> from the config file is a bit of a hack
        // so while we don't clean that up, we only set the value of the first output
        let mut pool_coinbase_outputs = empty_pool_coinbase_outputs.clone();
        pool_coinbase_outputs[0].value =
            Amount::from_sat(last_future_template.coinbase_tx_value_remaining);

        // Create the Downstream instance, wrapped for shared access.
        let self_ = Arc::new(Mutex::new(Downstream {
            id,
            receiver,
            sender: sender.clone(),
            requires_standard_jobs,
            solution_sender,
            channel_id_factory,
            extended_channels: HashMap::new(),
            standard_channels: HashMap::new(),
            vardiff: HashMap::new(),
            group_channel: None,
            extranonce_prefix_factory_extended,
            extranonce_prefix_factory_standard,
            share_batch_size,
            shares_per_minute,
            last_future_template,
            last_new_prev_hash,
            empty_pool_coinbase_outputs,
        }));

        tokio::spawn(spawn_vardiff_loop(self_.clone(), sender.clone(), id));

        let cloned = self_.clone();

        // Spawn a dedicated task to continuously receive and process messages from this downstream.
        task::spawn(async move {
            debug!("Starting up downstream receiver");
            let receiver_res = cloned
                .safe_lock(|d| d.receiver.clone())
                .map_err(|e| PoolError::PoisonLock(e.to_string()));
            let receiver = match receiver_res {
                Ok(recv) => recv,
                Err(e) => {
                    if let Err(e) = status_tx
                        .send(status::Status {
                            state: status::State::Healthy(format!(
                                "Downstream connection dropped: {e}"
                            )),
                        })
                        .await
                    {
                        error!("Encountered Error but status channel is down: {}", e);
                    }

                    return;
                }
            };
            loop {
                match receiver.recv().await {
                    Ok(received) => {
                        let received: Result<StdFrame, _> = received
                            .try_into()
                            .map_err(|e| PoolError::Codec(codec_sv2::Error::FramingSv2Error(e)));
                        let std_frame = handle_result!(status_tx, received);
                        // Process the valid standard frame using the `next` handler.
                        handle_result!(
                            status_tx,
                            Downstream::next(cloned.clone(), std_frame).await
                        );
                    }
                    _ => {
                        // Attempt to remove the downstream from the main pool's map.
                        let res = pool
                            .safe_lock(|p| p.downstreams.remove(&id))
                            .map_err(|e| PoolError::PoisonLock(e.to_string()));
                        handle_result!(status_tx, res);
                        error!("Downstream {} disconnected", id);

                        break;
                    }
                }
            }
            warn!("Downstream connection dropped");
            sender.close();
        });
        Ok(self_)
    }

    /// Processes a single incoming message (`StdFrame`) received from the downstream miner.
    ///
    /// It extracts the message type and payload, then uses the `roles_logic_sv2`
    /// (`ParseMiningMessagesFromDownstream`) to determine the appropriate
    /// response. Finally, it dispatches any necessary response(s) using
    /// `Downstream::match_send_to`.
    pub async fn next(self_mutex: Arc<Mutex<Self>>, mut incoming: StdFrame) -> PoolResult<()> {
        // Extract message type and payload.
        let message_type = incoming
            .get_header()
            .ok_or_else(|| PoolError::Custom(String::from("No header set")))?
            .msg_type();
        let payload = incoming.payload();
        debug!(
            "Received downstream message type: {:?}, payload: {:?}",
            message_type, payload
        );

        // Use the message handler implementation to parse the message and determine the response.
        let next_message_to_send = ParseMiningMessagesFromDownstream::handle_message_mining(
            self_mutex.clone(),
            message_type,
            payload,
        );

        // Send the determined response(s) back to the miner.
        Self::match_send_to(self_mutex, next_message_to_send).await
    }

    /// Dispatches messages back to the downstream miner based on the `SendTo` directive.
    ///
    /// Handles different scenarios: sending a single response, sending multiple messages,
    /// or doing nothing. It recursively calls itself for `SendTo::Multiple`.
    /// If an `OpenMiningChannelError` is encountered, it sends the error message and
    /// then returns a specific `PoolError` to signal that this downstream connection
    /// should be dropped by the caller (the receiver loop).
    #[async_recursion::async_recursion]
    async fn match_send_to(
        self_: Arc<Mutex<Self>>,
        send_to: Result<SendTo<()>, Error>,
    ) -> PoolResult<()> {
        match send_to {
            Ok(SendTo::Respond(message)) => {
                debug!("Sending to downstream: {}", message);
                // returning an error will send the error to the main thread,
                // and the main thread will drop the downstream from the pool
                if let &Mining::OpenMiningChannelError(_) = &message {
                    Self::send(self_.clone(), message.clone()).await?;
                    let downstream_id = self_.safe_lock(|d| d.id)?;
                    return Err(PoolError::Sv2ProtocolError((
                        downstream_id,
                        message.clone(),
                    )));
                } else {
                    Self::send(self_, message.clone()).await?;
                }
            }
            Ok(SendTo::Multiple(messages)) => {
                debug!("Sending multiple messages to downstream");
                // Recursively call match_send_to for each message in the sequence.
                for message in messages {
                    debug!("Sending downstream message: {:?}", message);
                    Self::match_send_to(self_.clone(), Ok(message)).await?;
                }
            }
            Ok(SendTo::None(_)) => {}
            Ok(m) => {
                error!("Unexpected SendTo: {:?}", m);
                panic!();
            }
            Err(Error::UnexpectedMessage(_message_type)) => todo!(),
            Err(e) => {
                error!("Error: {:?}", e);
                todo!()
            }
        }
        Ok(())
    }

    /// This method is used to send message to downstream.
    async fn send(
        self_mutex: Arc<Mutex<Self>>,
        message: roles_logic_sv2::parsers::Mining<'static>,
    ) -> PoolResult<()> {
        //let message = if let Mining::NewExtendedMiningJob(job) = message {
        //    Mining::NewExtendedMiningJob(extended_job_to_non_segwit(job, 32)?)
        //} else {
        //    message
        //};
        let sv2_frame: StdFrame = AnyMessage::Mining(message).try_into()?;
        let sender = self_mutex.safe_lock(|self_| self_.sender.clone())?;
        sender.send(sv2_frame.into()).await?;
        Ok(())
    }
}

// Verifies token for a custom job which is the signed tx_hash_list_hash by Job Declarator Server
//TODO: implement the use of this function in main.rs
#[allow(dead_code)]
pub fn verify_token(
    tx_hash_list_hash: U256,
    signature: secp256k1::schnorr::Signature,
    pub_key: key_utils::Secp256k1PublicKey,
) -> Result<(), secp256k1::Error> {
    let message: Vec<u8> = tx_hash_list_hash.to_vec();

    let secp = SignatureService::default();

    let is_verified = secp.verify(tx_hash_list_hash.to_vec(), signature, pub_key.0);

    // debug
    debug!("Message: {}", std::str::from_utf8(&message).unwrap());
    debug!("Verified signature {:?}", is_verified);
    is_verified
}

impl Pool {
    /// Binds to the configured listen address and starts accepting incoming TCP connections.
    ///
    /// Runs in a loop, accepting connections, performing the Noise handshake, and then
    /// calling `Pool::accept_incoming_connection_` to handle the SV2 setup and downstream
    /// creation for each successful connection.
    async fn accept_incoming_connection(
        self_: Arc<Mutex<Pool>>,
        config: PoolConfig,
        mut recv_stop_signal: tokio::sync::watch::Receiver<()>,
        shares_per_minute: f32,
        pool_coinbase_outputs: Vec<TxOut>,
    ) -> PoolResult<()> {
        let status_tx = self_.safe_lock(|s| s.status_tx.clone())?;
        // Bind the TCP listener to the address specified in the config.
        let listener = TcpListener::bind(&config.listen_address()).await?;
        info!("Pool is running on: {}", config.listen_address());
        // Spawn the main accept loop in a separate task.
        task::spawn(async move {
            loop {
                tokio::select! {
                    // Listen for the shutdown signal.
                    _ = recv_stop_signal.changed() => {
                        info!("Pool is stopping the server after stop shutdown signal received");
                        break;
                    },
                    // Accept new incoming TCP connections.
                    result = listener.accept() => {
                        match result {
                            Ok((stream, _)) => {
                                let address = stream.peer_addr().unwrap();
                                info!("New connection from {:?}", stream.peer_addr().map_err(PoolError::Io));
                                // Create a Noise protocol Responder using the pool's authority keys.
                                let responder = Responder::from_authority_kp(
                                    &config.authority_public_key().into_bytes(),
                                    &config.authority_secret_key().into_bytes(),
                                    std::time::Duration::from_secs(config.cert_validity_sec()),
                                );

                                match responder {
                                    Ok(resp) => {
                                        if let Ok((receiver, sender)) = Connection::new::<Message>(stream, HandshakeRole::Responder(resp)).await {
                                            handle_result!(
                                                status_tx,
                                                Self::accept_incoming_connection_(
                                                    self_.clone(),
                                                    receiver,
                                                    sender,
                                                    address,
                                                    shares_per_minute,
                                                    pool_coinbase_outputs.clone()
                                                ).await
                                            );
                                        }
                                    }
                                    Err(_) => {
                                        return;
                                    }
                                }
                            }
                            Err(e) => {
                                error!("Error accepting connection: {:?}", e);
                            }
                        }
                    }
                }
            }
        });
        Ok(())
    }

    /// Handles the post-handshake setup for a newly connected miner.
    ///
    /// Called by `accept_incoming_connection` after TCP and Noise handshake succeed.
    /// It creates the `Downstream` instance (which includes SV2 setup), and adds the
    /// new downstream to the pool's central `downstreams` map.
    async fn accept_incoming_connection_(
        self_: Arc<Mutex<Pool>>,
        receiver: Receiver<EitherFrame>,
        sender: Sender<EitherFrame>,
        address: SocketAddr,
        shares_per_minute: f32,
        pool_coinbase_outputs: Vec<TxOut>,
    ) -> PoolResult<()> {
        let solution_sender = self_.safe_lock(|p| p.solution_sender.clone())?;
        let status_tx = self_.safe_lock(|s| s.status_tx.clone())?;

        // Create the Downstream instance
        let downstream = Downstream::new(
            receiver,
            sender,
            solution_sender,
            self_.clone(),
            // convert Listener variant to Downstream variant
            status_tx.listener_to_connection(),
            address,
            shares_per_minute,
            pool_coinbase_outputs,
        )
        .await?;

        // Extract the assigned ID after successful creation.
        let channel_id = downstream.safe_lock(|d| d.id)?;

        // Add the new downstream to the central map.
        self_.safe_lock(|p| {
            p.downstreams.insert(channel_id, downstream);
        })?;
        Ok(())
    }

    /// Task to handle incoming `SetNewPrevHash` messages from the upstream source.
    ///
    /// Runs in a loop, receiving messages from the `rx` channel. For each message,
    /// it updates the pool's `last_prev_hash_template_id`, uses the `channel_factory`
    /// to generate the appropriate `SetNewPrevHash` message for downstream miners,
    /// and broadcasts it to all connected downstreams. Sends an acknowledgement signal
    /// on `sender_message_received_signal` after processing each message.
    async fn on_new_prev_hash(
        self_: Arc<Mutex<Self>>,
        rx: Receiver<SetNewPrevHashTdp<'static>>,
        sender_message_received_signal: Sender<()>,
    ) -> PoolResult<()> {
        let status_tx = self_
            .safe_lock(|s| s.status_tx.clone())
            .map_err(|e| PoolError::PoisonLock(e.to_string()))?;
        while let Ok(new_prev_hash) = rx.recv().await {
            debug!("New prev hash received: {}", new_prev_hash);
            let res = self_
                .safe_lock(|s| {
                    s.last_new_prev_hash = Some(new_prev_hash.clone());
                })
                .map_err(|e| PoolError::PoisonLock(e.to_string()));
            handle_result!(status_tx, res);

            let downstreams = self_
                .safe_lock(|s| s.downstreams.clone())
                .map_err(|e| PoolError::PoisonLock(e.to_string()));

            let downstreams = handle_result!(status_tx, downstreams);

            for (_downstream_id, downstream) in downstreams {
                downstream.safe_lock(|d| {
                    d.last_new_prev_hash = new_prev_hash.clone();
                })?;

                let mining_set_new_prev_hash_messages = downstream.safe_lock(|d| {
                    let mut messages = Vec::new();

                    // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                    // if no, we need to send the SetNewPrevHashMp to the group channel
                    if let Some(group_channel_guard) = &d.group_channel {
                        let mut group_channel = group_channel_guard
                            .write()
                            .map_err(|e| Error::PoisonLock(e.to_string()))?;

                        group_channel
                            .on_set_new_prev_hash(new_prev_hash.clone())
                            .map_err(Error::FailedToProcessSetNewPrevHashGroupChannel)?;

                        let group_channel_id = group_channel.get_group_channel_id();
                        let activated_group_job_id = group_channel
                            .get_active_job()
                            .expect("active job must exist")
                            .get_job_id();

                        let set_new_prev_hash_message = SetNewPrevHashMp {
                            channel_id: group_channel_id,
                            job_id: activated_group_job_id,
                            prev_hash: new_prev_hash.prev_hash.clone(),
                            min_ntime: new_prev_hash.header_timestamp,
                            nbits: new_prev_hash.n_bits,
                        };
                        messages.push(set_new_prev_hash_message.into_static());
                    }

                    for (standard_channel_id, standard_channel_lock) in d.standard_channels.iter() {
                        let mut standard_channel = standard_channel_lock
                            .write()
                            .map_err(|e| Error::PoisonLock(e.to_string()))?;

                        // process the SetNewPrevHashTdp for the standard channel
                        // regardless of the REQUIRES_STANDARD_JOBS flag
                        // because this is the only way we can verify shares later
                        standard_channel
                            .on_set_new_prev_hash(new_prev_hash.clone())
                            .map_err(Error::FailedToProcessSetNewPrevHashStandardChannel)?;

                        // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                        // if yes, there's no group channel, so we need to send the SetNewPrevHashMp
                        // to each standard channel
                        if d.group_channel.is_none() {
                            let activated_standard_job_id = standard_channel
                                .get_active_job()
                                .expect("active job must exist")
                                .get_job_id();
                            let set_new_prev_hash_message = SetNewPrevHashMp {
                                channel_id: *standard_channel_id,
                                job_id: activated_standard_job_id,
                                prev_hash: new_prev_hash.prev_hash.clone(),
                                min_ntime: new_prev_hash.header_timestamp,
                                nbits: new_prev_hash.n_bits,
                            };
                            messages.push(set_new_prev_hash_message.into_static());
                        }
                    }

                    for (extended_channel_id, extended_channel_lock) in d.extended_channels.iter() {
                        let mut extended_channel = extended_channel_lock
                            .write()
                            .map_err(|e| Error::PoisonLock(e.to_string()))?;
                        extended_channel
                            .on_set_new_prev_hash(new_prev_hash.clone())
                            .map_err(Error::FailedToProcessSetNewPrevHashExtendedChannel)?;
                        let activated_extended_job_id = extended_channel
                            .get_active_job()
                            .expect("active job must exist")
                            .get_job_id();
                        let set_new_prev_hash_message = SetNewPrevHashMp {
                            channel_id: *extended_channel_id,
                            job_id: activated_extended_job_id,
                            prev_hash: new_prev_hash.prev_hash.clone(),
                            min_ntime: new_prev_hash.header_timestamp,
                            nbits: new_prev_hash.n_bits,
                        };
                        messages.push(set_new_prev_hash_message.into_static());
                    }
                    Ok::<_, PoolError>(messages)
                })??;

                for message in mining_set_new_prev_hash_messages {
                    let res = Downstream::match_send_to(
                        downstream.clone(),
                        Ok(SendTo::Respond(Mining::SetNewPrevHash(message))),
                    )
                    .await;
                    handle_result!(status_tx, res);
                }
            }
            handle_result!(status_tx, sender_message_received_signal.send(()).await);
        }
        Ok(())
    }

    /// Task to handle incoming `NewTemplate` messages from the upstream source.
    ///
    /// Runs in a loop, receiving messages from the `rx` channel. For each template,
    /// it uses the `channel_factory` to generate the appropriate mining job messages
    /// (e.g., `NewMiningJob`, `SetExtranoncePrefix`) for each relevant downstream channel.
    /// It then sends these specific messages to the corresponding downstream miners.
    /// Sets the `new_template_processed` flag and sends an acknowledgement signal
    /// on `sender_message_received_signal` after processing.
    async fn on_new_template(
        self_: Arc<Mutex<Self>>,
        rx: Receiver<NewTemplate<'static>>,
        sender_message_received_signal: Sender<()>,
    ) -> PoolResult<()> {
        let status_tx = self_.safe_lock(|s| s.status_tx.clone())?;
        while let Ok(new_template) = rx.recv().await {
            info!(
                "New template received, creating a new mining job(s): {}",
                new_template
            );

            let downstreams = self_
                .safe_lock(|s| s.downstreams.clone())
                .map_err(|e| PoolError::PoisonLock(e.to_string()));
            let downstreams = handle_result!(status_tx, downstreams);

            for (_downstream_id, downstream) in downstreams {
                if new_template.future_template {
                    downstream.safe_lock(|d| {
                        d.last_future_template = new_template.clone();
                    })?;
                }

                let standard_job_messages = downstream.safe_lock(|d| {
                    let mut messages = Vec::new();

                    // note: the fact that we're parsing a Vec<TxOut> from the config file is a
                    // bit of a hack so while we don't clean that up, we
                    // only set the value of the first output
                    let mut pool_coinbase_outputs = d.empty_pool_coinbase_outputs.clone();
                    pool_coinbase_outputs[0].value =
                        Amount::from_sat(new_template.coinbase_tx_value_remaining);

                    match new_template.future_template {
                        true => {
                            for (_standard_channel_id, standard_channel_lock) in
                                d.standard_channels.iter()
                            {
                                let mut standard_channel = standard_channel_lock
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;

                                // process the NewTemplate for the standard channel
                                // regardless of the REQUIRES_STANDARD_JOBS flag
                                // because this is the only way we can verify shares later
                                standard_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(Error::FailedToProcessNewTemplateStandardChannel)?;

                                // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                                // if yes, there's no group channel, so we need to send the future
                                // job to each standard channel
                                // if no, there's a group channel and there's no standard job to
                                // send
                                if d.group_channel.is_none() {
                                    let standard_job_id = standard_channel
                                        .get_future_template_to_job_id()
                                        .get(&new_template.template_id)
                                        .expect("job_id must exist");
                                    let standard_job = standard_channel
                                        .get_future_jobs()
                                        .get(standard_job_id)
                                        .expect("standard job must exist");
                                    let standard_job_message = standard_job.get_job_message();
                                    messages.push(standard_job_message.clone().into_static());
                                }
                            }
                        }
                        false => {
                            for (_standard_channel_id, standard_channel_lock) in
                                d.standard_channels.iter()
                            {
                                let mut standard_channel = standard_channel_lock
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;

                                // process the NewTemplate for the standard channel
                                // regardless of the REQUIRES_STANDARD_JOBS flag
                                // because this is the only way we can verify shares later
                                standard_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(Error::FailedToProcessNewTemplateStandardChannel)?;

                                // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                                // if yes, there's no group channel, so we need to send the
                                // non-future job to each standard channel
                                // if no, there is a group channel, so there's no standard job to
                                // send
                                if d.group_channel.is_none() {
                                    let standard_job = standard_channel
                                        .get_active_job()
                                        .expect("standard job must exist");
                                    let standard_job_message = standard_job.get_job_message();
                                    messages.push(standard_job_message.clone().into_static());
                                }
                            }
                        }
                    }
                    Ok::<_, PoolError>(messages)
                })??;

                for standard_job_message in standard_job_messages {
                    let res = Downstream::match_send_to(
                        downstream.clone(),
                        Ok(SendTo::Respond(Mining::NewMiningJob(standard_job_message))),
                    )
                    .await;
                    handle_result!(status_tx, res);
                }

                let extended_job_messages = downstream.safe_lock(|d| {
                    // note: the fact that we're parsing a Vec<TxOut> from the config file is a bit
                    // of a hack so while we don't clean that up, we only set
                    // the value of the first output
                    let mut pool_coinbase_outputs = d.empty_pool_coinbase_outputs.clone();
                    pool_coinbase_outputs[0].value =
                        Amount::from_sat(new_template.coinbase_tx_value_remaining);

                    let mut messages = Vec::new();
                    match new_template.future_template {
                        true => {
                            // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                            // if yes, we don't care about Group Channel
                            // if no, we need to send the future job to the Group Channel
                            if let Some(group_channel_guard) = &d.group_channel {
                                let mut group_channel = group_channel_guard
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;
                                group_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(|e| {
                                        Error::FailedToProcessNewTemplateGroupChannel(e)
                                    })?;
                                let future_job_id = group_channel
                                    .get_future_template_to_job_id()
                                    .get(&new_template.template_id)
                                    .expect("job_id must exist");
                                let future_job = group_channel
                                    .get_future_jobs()
                                    .get(future_job_id)
                                    .expect("future job must exist");
                                let future_job_message = future_job.get_job_message();
                                messages.push(future_job_message.clone().into_static());
                            }

                            // also send the future job to each extended channel
                            for (_extended_channel_id, extended_channel_lock) in
                                d.extended_channels.iter()
                            {
                                let mut extended_channel = extended_channel_lock
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;

                                extended_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(|e| {
                                        Error::FailedToProcessNewTemplateExtendedChannel(e)
                                    })?;

                                let extended_job_id = extended_channel
                                    .get_future_template_to_job_id()
                                    .get(&new_template.template_id)
                                    .expect("job_id must exist");

                                let extended_job = extended_channel
                                    .get_future_jobs()
                                    .get(extended_job_id)
                                    .expect("extended job must exist");

                                let extended_job_message = extended_job.get_job_message();
                                messages.push(extended_job_message.clone().into_static());
                            }
                        }
                        false => {
                            // did SetupConnection have the REQUIRES_STANDARD_JOBS flag set?
                            // if yes, we don't care about Group Channel
                            // if no, we need to send the non-future job to the Group Channel
                            if let Some(group_channel_guard) = &d.group_channel {
                                let mut group_channel = group_channel_guard
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;
                                group_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(|e| {
                                        Error::FailedToProcessNewTemplateGroupChannel(e)
                                    })?;
                                let active_job = group_channel
                                    .get_active_job()
                                    .expect("active job must exist");
                                let active_job_message = active_job.get_job_message();
                                messages.push(active_job_message.clone().into_static());
                            }

                            // also send the non-future job to each extended channel
                            for (_extended_channel_id, extended_channel_lock) in
                                d.extended_channels.iter()
                            {
                                let mut extended_channel = extended_channel_lock
                                    .write()
                                    .map_err(|e| Error::PoisonLock(e.to_string()))?;

                                extended_channel
                                    .on_new_template(
                                        new_template.clone(),
                                        pool_coinbase_outputs.clone(),
                                    )
                                    .map_err(|e| {
                                        Error::FailedToProcessNewTemplateExtendedChannel(e)
                                    })?;

                                let extended_job = extended_channel
                                    .get_active_job()
                                    .expect("active job must exist");

                                let extended_job_message = extended_job.get_job_message();
                                messages.push(extended_job_message.clone().into_static());
                            }
                        }
                    }
                    Ok::<_, PoolError>(messages)
                })??;

                for extended_job_message in extended_job_messages {
                    let res = Downstream::match_send_to(
                        downstream.clone(),
                        Ok(SendTo::Respond(Mining::NewExtendedMiningJob(
                            extended_job_message,
                        ))),
                    )
                    .await;
                    handle_result!(status_tx, res);
                }
            }

            if new_template.future_template {
                let res = self_
                    .safe_lock(|s| {
                        s.last_future_template = Some(new_template);
                        s.new_template_processed = true;
                    })
                    .map_err(|e| PoolError::PoisonLock(e.to_string()));
                handle_result!(status_tx, res);
            }

            handle_result!(status_tx, sender_message_received_signal.send(()).await);
        }
        Ok(())
    }

    /// Starts the main pool logic, including the connection listener and message handling tasks.
    ///
    /// Initializes the `PoolChannelFactory` and the `Pool` state struct. Spawns three key
    /// background tasks:
    /// 1. `accept_incoming_connection`: Listens for and handles new downstream connections.
    /// 2. `on_new_prev_hash`: Processes previous hash updates from upstream.
    /// 3. `on_new_template`: Processes new job templates from upstream.
    #[allow(clippy::too_many_arguments)]
    pub async fn start(
        config: PoolConfig,
        new_template_rx: Receiver<NewTemplate<'static>>,
        new_prev_hash_rx: Receiver<SetNewPrevHashTdp<'static>>,
        solution_sender: Sender<SubmitSolution<'static>>,
        sender_message_received_signal: Sender<()>,
        status_tx: status::Sender,
        shares_per_minute: f32,
        recv_stop_signal: tokio::sync::watch::Receiver<()>,
    ) -> Result<Arc<Mutex<Self>>, PoolError> {
        let range_0 = std::ops::Range { start: 0, end: 0 };

        let pool_signature_len = config.pool_signature().len();
        let range_1_end = pool_signature_len + 8;
        let range_1 = std::ops::Range {
            start: 0,
            end: range_1_end,
        };
        let range_2 = std::ops::Range {
            start: range_1_end,
            end: MAX_EXTRANONCE_LEN,
        };

        let pool_coinbase_outputs = get_coinbase_output(&config);
        info!("PUB KEY: {:?}", pool_coinbase_outputs);
        let extranonce_prefix_factory_extended = ExtendedExtranonce::new(
            range_0.clone(),
            range_1.clone(),
            range_2.clone(),
            Some(config.pool_signature().as_bytes().to_vec()),
        )
        .expect("Failed to create ExtendedExtranonce with valid ranges");

        let extranonce_prefix_factory_standard = ExtendedExtranonce::new(
            range_0,
            range_1,
            range_2,
            Some(config.pool_signature().as_bytes().to_vec()),
        )
        .expect("Failed to create ExtendedExtranonce with valid ranges");

        // --- Initialize Pool State ---
        let pool = Arc::new(Mutex::new(Pool {
            downstreams: HashMap::with_hasher(BuildNoHashHasher::default()),
            solution_sender,
            new_template_processed: false,
            downstream_id_factory: IdFactory::new(),
            status_tx: status_tx.clone(),
            extranonce_prefix_factory_extended: Arc::new(Mutex::new(
                extranonce_prefix_factory_extended,
            )),
            extranonce_prefix_factory_standard: Arc::new(Mutex::new(
                extranonce_prefix_factory_standard,
            )),
            share_batch_size: config.share_batch_size(),
            last_future_template: None,
            last_new_prev_hash: None,
        }));

        let cloned = pool.clone();
        let cloned2 = pool.clone();
        let cloned3 = pool.clone();

        let pool_coinbase_outputs = get_coinbase_output(&config);

        info!("Starting up Pool server");
        let status_tx_clone = status_tx.clone();
        // Task to handle multiple downstream connection.
        if let Err(e) = Self::accept_incoming_connection(
            cloned,
            config,
            recv_stop_signal,
            shares_per_minute,
            pool_coinbase_outputs,
        )
        .await
        {
            error!("Pool stopped accepting connections due to: {}", &e);
            let _ = status_tx_clone
                .send(status::Status {
                    state: status::State::DownstreamShutdown(PoolError::ComponentShutdown(
                        "Pool stopped accepting connections".to_string(),
                    )),
                })
                .await;

            return Err(e);
        }

        let cloned = sender_message_received_signal.clone();
        let status_tx_clone = status_tx.clone();
        // Task to handle new prev hash message from template provider.
        task::spawn(async move {
            if let Err(e) = Self::on_new_prev_hash(cloned2, new_prev_hash_rx, cloned).await {
                error!("{}", e);
            }
            // on_new_prev_hash shutdown
            if status_tx_clone
                .send(status::Status {
                    state: status::State::DownstreamShutdown(PoolError::ComponentShutdown(
                        "Downstream no longer accepting new prevhash".to_string(),
                    )),
                })
                .await
                .is_err()
            {
                error!("Downstream shutdown and Status Channel dropped");
            }
        });

        let status_tx_clone = status_tx;
        // Task to handle new template message from template provider.
        task::spawn(async move {
            if let Err(e) =
                Self::on_new_template(pool, new_template_rx, sender_message_received_signal).await
            {
                error!("{}", e);
            }
            // on_new_template shutdown
            if status_tx_clone
                .send(status::Status {
                    state: status::State::DownstreamShutdown(PoolError::ComponentShutdown(
                        "Downstream no longer accepting templates".to_string(),
                    )),
                })
                .await
                .is_err()
            {
                error!("Downstream shutdown and Status Channel dropped");
            }
        });
        Ok(cloned3)
    }

    /// Removes a downstream connection from the pool's active map.
    ///
    /// Called when a downstream disconnects or needs to be removed for other reasons
    /// (e.g., protocol error signaled via `PoolError::Sv2ProtocolError`).
    ///
    /// **Note:** There's a potential race condition. If job distribution tasks clone the
    /// `downstreams` map just before this removal happens, they might still attempt
    /// to send a message to the removed downstream. This attempt will likely fail
    /// harmlessly when `Downstream::send` tries to use the closed channel.
    pub fn remove_downstream(&mut self, downstream_id: u32) {
        self.downstreams.remove(&downstream_id);
    }
}

async fn send_set_target_downstream(
    sender: Sender<EitherFrame>,
    channel_id: u32,
    target: Target,
) -> Result<(), PoolError> {
    debug!("Attempting to send `SetTarget` for channel_id={channel_id}");

    let target_message = SetTarget {
        channel_id,
        maximum_target: target.into(),
    };

    let mining_msg = Mining::SetTarget(target_message);

    info!("Sending SetTarget message to downstream: {}", mining_msg);

    let sv2_frame: StdFrame = AnyMessage::Mining(mining_msg).try_into()?;

    sender.send(sv2_frame.into()).await?;

    Ok(())
}

fn run_vardiff_on_extended_channel(
    channel_id: u32,
    channel: Arc<RwLock<ExtendedChannel<'static>>>,
    vardiff_state: Arc<RwLock<Box<dyn Vardiff>>>,
    updates: &mut Vec<(u32, Target)>,
) {
    let Ok(mut channel_state) = channel.write() else {
        debug!("Failed to lock extended channel {channel_id}");
        return;
    };

    let Ok(mut vardiff_state) = vardiff_state.write() else {
        debug!("Failed to lock vardiff state for extended channel {channel_id}");
        return;
    };

    let hashrate = channel_state.get_nominal_hashrate();
    let target = channel_state.get_target();
    let shares_per_minute = channel_state.get_shares_per_minute();

    let Ok(new_hashrate_opt) = vardiff_state.try_vardiff(hashrate, target, shares_per_minute)
    else {
        debug!("Vardiff computation failed for extended channel {channel_id}");
        return;
    };

    if let Some(new_hashrate) = new_hashrate_opt {
        if let Ok(()) = channel_state.update_channel(new_hashrate, None) {
            let updated_target = channel_state.get_target();
            updates.push((channel_id, updated_target.clone()));

            debug!(
                "Updated target for extended channel_id={channel_id} to {:?}",
                updated_target
            );
        } else {
            warn!("Failed to update extended channel {channel_id}");
        }
    }
}

fn run_vardiff_on_standard_channel(
    channel_id: u32,
    channel: Arc<RwLock<StandardChannel<'static>>>,
    vardiff_state: &Arc<RwLock<Box<dyn Vardiff>>>,
    updates: &mut Vec<(u32, Target)>,
) {
    let Ok(mut channel_state) = channel.write() else {
        debug!("Failed to lock standard channel {channel_id}");
        return;
    };

    let Ok(mut vardiff_state) = vardiff_state.write() else {
        debug!("Failed to lock vardiff state for standard channel {channel_id}");
        return;
    };

    let hashrate = channel_state.get_nominal_hashrate();
    let target = channel_state.get_target();
    let shares_per_minute = channel_state.get_shares_per_minute();

    let Ok(new_hashrate_opt) = vardiff_state.try_vardiff(hashrate, target, shares_per_minute)
    else {
        debug!("Vardiff computation failed for standard channel {channel_id}");
        return;
    };

    if let Some(new_hashrate) = new_hashrate_opt {
        if let Ok(()) = channel_state.update_channel(new_hashrate, None) {
            let updated_target = channel_state.get_target();
            updates.push((channel_id, updated_target.clone()));

            debug!(
                "Updated target for standard channel_id={channel_id} to {:?}",
                updated_target
            );
        } else {
            warn!("Failed to update standard channel {channel_id}");
        }
    }
}

/// This method implements the pool's variable difficulty logic for a single downstream.
/// A downstream can have multiple active channels connected to the pool.
/// Every 60 seconds, this method updates the difficulty state for each channel belonging to the
/// downstream.
async fn spawn_vardiff_loop(
    downstream: Arc<Mutex<Downstream>>,
    sender: Sender<EitherFrame>,
    downstream_id: u32,
) {
    info!("Spawning vardiff adjustment loop for downstream: {downstream_id}");

    'vardiff_loop: loop {
        if sender.is_closed() {
            debug!("Downstream {downstream_id} closed, stopping vardiff loop");
            break;
        }

        tokio::time::sleep(Duration::from_secs(60)).await;

        debug!("Starting vardiff updates for downstream: {downstream_id}");
        let mut updates = Vec::new();

        _ = downstream.safe_lock(|d| {
            for (channel_id, vardiff_state) in &d.vardiff {
                if let Some(channel) = d.extended_channels.get(channel_id) {
                    run_vardiff_on_extended_channel(
                        *channel_id,
                        channel.clone(),
                        vardiff_state.clone(),
                        &mut updates,
                    );
                }

                if let Some(channel) = d.standard_channels.get(channel_id) {
                    run_vardiff_on_standard_channel(
                        *channel_id,
                        channel.clone(),
                        vardiff_state,
                        &mut updates,
                    );
                }
            }
        });

        for (channel_id, target) in updates {
            if let Err(e) = send_set_target_downstream(sender.clone(), channel_id, target).await {
                error!(
                    "Failed to send SetTarget message downstream for channel {channel_id}: {:?}",
                    e
                );
                break 'vardiff_loop;
            }
        }
    }
}

#[cfg(test)]
mod test {
    use ext_config::{Config, File, FileFormat};
    use std::convert::TryInto;
    use stratum_common::roles_logic_sv2::{
        bitcoin::{
            self, absolute::LockTime, consensus, transaction::Version, Transaction, Witness,
        },
        codec_sv2::binary_sv2::{B0255, B064K},
    };
    use tracing::error;

    use super::PoolConfig;

    // this test is used to verify the `coinbase_tx_prefix` and `coinbase_tx_suffix` values tested
    // against in message generator
    // `stratum/test/message-generator/test/pool-sri-test-extended.json`
    #[test]
    fn test_coinbase_outputs_from_config() {
        let config_path = "./config-examples/pool-config-local-tp-example.toml";

        // Load config
        let config: PoolConfig = match Config::builder()
            .add_source(File::new(config_path, FileFormat::Toml))
            .build()
        {
            Ok(settings) => match settings.try_deserialize::<PoolConfig>() {
                Ok(c) => c,
                Err(e) => {
                    error!("Failed to deserialize config: {}", e);
                    return;
                }
            },
            Err(e) => {
                error!("Failed to build config: {}", e);
                return;
            }
        };

        // template from message generator test (mock TP template)
        let _extranonce_len = 3;
        let coinbase_prefix = vec![3, 76, 163, 38, 0];
        let _version = 536870912;
        let coinbase_tx_version = 2;
        let coinbase_tx_input_sequence = 4294967295;
        let _coinbase_tx_value_remaining: u64 = 625000000;
        let _coinbase_tx_outputs_count = 0;
        let coinbase_tx_locktime = 0;
        let coinbase_tx_outputs: Vec<bitcoin::TxOut> = super::get_coinbase_output(&config);
        // extranonce len set to max_extranonce_size in `ChannelFactory::new_extended_channel()`
        let extranonce_len = 32;

        // build coinbase TX from 'job_creator::coinbase()'

        let mut bip34_bytes = get_bip_34_bytes(coinbase_prefix.try_into().unwrap());
        let script_prefix_length = bip34_bytes.len() + config.pool_signature().len();
        bip34_bytes.extend_from_slice(config.pool_signature().as_bytes());
        bip34_bytes.extend_from_slice(&vec![0; extranonce_len as usize]);
        let witness = match bip34_bytes.len() {
            0 => Witness::from(vec![] as Vec<Vec<u8>>),
            _ => Witness::from(vec![vec![0; 32]]),
        };

        let tx_in = bitcoin::TxIn {
            previous_output: bitcoin::OutPoint::null(),
            script_sig: bip34_bytes.into(),
            sequence: bitcoin::Sequence(coinbase_tx_input_sequence),
            witness,
        };
        let coinbase = Transaction {
            version: Version::non_standard(coinbase_tx_version),
            lock_time: LockTime::from_consensus(coinbase_tx_locktime),
            input: vec![tx_in],
            output: coinbase_tx_outputs,
        };

        let coinbase_tx_prefix = coinbase_tx_prefix(&coinbase, script_prefix_length);
        let coinbase_tx_suffix =
            coinbase_tx_suffix(&coinbase, extranonce_len, script_prefix_length);
        assert!(
            coinbase_tx_prefix
                == [
                    2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255, 255, 255, 56, 3, 76, 163, 38,
                    0, 83, 116, 114, 97, 116, 117, 109, 32, 86, 50, 32, 83, 82, 73, 32, 80, 111,
                    111, 108
                ]
                .to_vec()
                .try_into()
                .unwrap(),
            "coinbase_tx_prefix incorrect"
        );
        assert!(
            coinbase_tx_suffix
                == [
                    255, 255, 255, 255, 1, 0, 0, 0, 0, 0, 0, 0, 0, 22, 0, 20, 235, 225, 183, 220,
                    194, 147, 204, 170, 14, 231, 67, 168, 111, 137, 223, 130, 88, 194, 8, 252, 1,
                    32, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                ]
                .to_vec()
                .try_into()
                .unwrap(),
            "coinbase_tx_suffix incorrect"
        );
    }

    // copied from roles-logic-sv2::job_creator
    fn coinbase_tx_prefix(coinbase: &Transaction, script_prefix_len: usize) -> B064K<'static> {
        let encoded = consensus::serialize(coinbase);
        // If script_prefix_len is not 0 we are not in a test enviornment and the coinbase have the
        // 0 witness
        let segwit_bytes = match script_prefix_len {
            0 => 0,
            _ => 2,
        };
        let index = 4    // tx version
            + segwit_bytes
            + 1  // number of inputs TODO can be also 3
            + 32 // prev OutPoint
            + 4  // index
            + 1  // bytes in script TODO can be also 3
            + script_prefix_len; // bip34_bytes
        let r = encoded[0..index].to_vec();
        r.try_into().unwrap()
    }

    // copied from roles-logic-sv2::job_creator
    fn coinbase_tx_suffix(
        coinbase: &Transaction,
        extranonce_len: u8,
        script_prefix_len: usize,
    ) -> B064K<'static> {
        let encoded = consensus::serialize(coinbase);
        // If script_prefix_len is not 0 we are not in a test enviornment and the coinbase have the
        // 0 witness
        let segwit_bytes = match script_prefix_len {
            0 => 0,
            _ => 2,
        };
        let r = encoded[4    // tx version
        + segwit_bytes
        + 1  // number of inputs TODO can be also 3
        + 32 // prev OutPoint
        + 4  // index
        + 1  // bytes in script TODO can be also 3
        + script_prefix_len  // bip34_bytes
        + (extranonce_len as usize)..]
            .to_vec();
        r.try_into().unwrap()
    }

    fn get_bip_34_bytes(coinbase_prefix: B0255<'static>) -> Vec<u8> {
        let script_prefix = &coinbase_prefix.to_vec()[..];
        // add 1 cause 0 is push 1 2 is 1 is push 2 ecc ecc
        // add 1 cause in the len there is also the op code itself
        let bip34_len = script_prefix[0] as usize + 2;
        if bip34_len == script_prefix.len() {
            script_prefix[0..bip34_len].to_vec()
        } else {
            panic!("bip34 length does not match script prefix")
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/mining_pool/setup_connection.rs">
//! ## Setup Connection Handler Module
//! Handles the setup connection handshake with the Downstream.
//!
//! [`SetupConnectionHandler`] builds and receives a `SetupConnection` message,
//! processes the response, and implements `ParseCommonMessagesFromDownstream` for
//! handling common downstream messages.
use super::super::{
    error::{PoolError, PoolResult},
    mining_pool::{EitherFrame, StdFrame},
};
use async_channel::{Receiver, Sender};
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    self,
    common_messages_sv2::{has_requires_std_job, SetupConnection, SetupConnectionSuccess},
    errors::Error,
    handlers::common::ParseCommonMessagesFromDownstream,
    parsers::{AnyMessage, CommonMessages},
    utils::Mutex,
};
use tracing::{debug, error, info};

/// Handles the `SetupConnection` message for downstream connections.
pub struct SetupConnectionHandler {
    // Whether only block headers are required for this connection.
    header_only: Option<bool>,
}

impl Default for SetupConnectionHandler {
    fn default() -> Self {
        Self::new()
    }
}

impl SetupConnectionHandler {
    /// Creates a new `SetupConnectionHandler` instance.
    pub fn new() -> Self {
        Self { header_only: None }
    }

    /// Handles the `SetupConnection` message from a downstream connection.
    pub async fn setup(
        self_: Arc<Mutex<Self>>,
        receiver: &mut Receiver<EitherFrame>,
        sender: &mut Sender<EitherFrame>,
        address: SocketAddr,
    ) -> PoolResult<bool> {
        // read stdFrame from receiver

        let mut incoming: StdFrame = match receiver.recv().await {
            Ok(EitherFrame::Sv2(s)) => {
                debug!("Got sv2 message: {:?}", s);
                s
            }
            Ok(EitherFrame::HandShake(s)) => {
                error!(
                    "Got unexpected handshake message from upstream: {:?} at {}",
                    s, address
                );
                panic!()
            }
            Err(e) => {
                error!("Error receiving message: {:?}", e);
                return Err(Error::NoDownstreamsConnected.into());
            }
        };

        let message_type = incoming
            .get_header()
            .ok_or_else(|| PoolError::Custom(String::from("No header set")))?
            .msg_type();
        let payload = incoming.payload();
        let response = ParseCommonMessagesFromDownstream::handle_message_common(
            self_.clone(),
            message_type,
            payload,
        )?;

        let message = response.into_message().ok_or(PoolError::RolesLogic(
            roles_logic_sv2::Error::NoDownstreamsConnected,
        ))?;

        let sv2_frame: StdFrame = AnyMessage::Common(message.clone()).try_into()?;
        let sv2_frame = sv2_frame.into();
        sender.send(sv2_frame).await?;
        self_.safe_lock(|s| s.header_only)?;

        match message {
            CommonMessages::SetupConnectionSuccess(m) => {
                debug!("Sent back SetupConnectionSuccess: {:?}", m);
                Ok(has_requires_std_job(m.flags))
            }
            _ => panic!(),
        }
    }
}

impl ParseCommonMessagesFromDownstream for SetupConnectionHandler {
    // Handles the specific SetupConnection message received from the downstream.
    //
    // Returns
    // - `Ok(SendTo::RelayNewMessageToRemote)` - Containing either `SetupConnectionSuccess`.
    fn handle_setup_connection(
        &mut self,
        incoming: SetupConnection,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, Error> {
        info!(
            "Received `SetupConnection`: version={}, flags={:b}",
            incoming.min_version, incoming.flags
        );
        use roles_logic_sv2::handlers::common::SendTo;
        let header_only = incoming.requires_standard_job();
        debug!("Handling setup connection: header_only: {}", header_only);
        self.header_only = Some(header_only);
        Ok(SendTo::RelayNewMessageToRemote(
            Arc::new(Mutex::new(())),
            CommonMessages::SetupConnectionSuccess(SetupConnectionSuccess {
                flags: incoming.flags,
                used_version: 2,
            }),
        ))
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/mod.rs">
//! # Pool Module
//! Core logic for running the mining pool server.
//!
//! Responsibilities:
//! - Spawns the Template Receiver client.
//! - Starts the Pool server for downstream miners.
//! - Monitors Pool status and handles shutdowns.
pub mod config;
pub mod error;
pub mod mining_pool;
pub mod status;
pub mod template_receiver;
use async_channel::{bounded, unbounded};
use config::PoolConfig;
use error::PoolError;
use mining_pool::{get_coinbase_output, Pool};
use std::sync::{Arc, Mutex};
use template_receiver::TemplateRx;
use tokio::select;
use tracing::{error, info, warn};

/// Represents the PoolSv2 instance, which manages the pool's operations.
///
/// This struct holds the pool configuration and provides functionality to start
/// and manage the pool, handling both upstream (Template Provider) and downstream connections.
#[derive(Debug, Clone)]
pub struct PoolSv2 {
    config: PoolConfig,
    status_tx: Arc<Mutex<Option<async_channel::Sender<status::Status>>>>,
}

impl PoolSv2 {
    /// Creates a new PoolSv2 instance with the given configuration.
    pub fn new(config: PoolConfig) -> PoolSv2 {
        PoolSv2 {
            config,
            status_tx: Arc::new(Mutex::new(None)),
        }
    }

    /// Starts the Pool-SV2 server and manages upstream and downstream connections.
    ///
    /// - Initializes a Template Receiver client to connect with the Template Provider.
    /// - Sets up a server for downstream miners to connect.
    /// - Creates multiple channels for communication between components.
    /// - Monitors system health and handles shutdown conditions.
    pub async fn start(&self) -> Result<(), PoolError> {
        let config = self.config.clone();
        // Channels for internal communication between Template Receiver and downstream miners.
        let (status_tx, status_rx) = unbounded(); // Monitors status of both upstream and downstream.

        if let Ok(mut s_tx) = self.status_tx.lock() {
            *s_tx = Some(status_tx.clone());
        } else {
            error!("Failed to access Pool status lock");
            return Err(PoolError::Custom(
                "Failed to access Pool status lock".to_string(),
            ));
        }
        // Watch channel used to signal the downstream Pool listener to stop.
        let (send_stop_signal, recv_stop_signal) = tokio::sync::watch::channel(());

        // Bounded channels for specific data flow between TemplateRx and Pool.
        let (s_new_t, r_new_t) = bounded(10); // New template updates.
        let (s_prev_hash, r_prev_hash) = bounded(10); // Previous hash updates.
        let (s_solution, r_solution) = bounded(10); // Share solution submissions from downstream.

        // This channel does something weird, it sends zero sized data from downstream upon
        // retrieval of any message from template receiver, and make the template receiver
        // wait until it receivers confirmation from downstream. Can be removed.
        let (s_message_recv_signal, r_message_recv_signal) = bounded(10);

        // Prepare coinbase output information required by TemplateRx.
        let coinbase_output_result = get_coinbase_output(&config);
        let coinbase_output_len = coinbase_output_result
            .iter()
            .map(|output| output.size() as u32)
            .sum();
        let tp_authority_public_key = config.tp_authority_public_key().cloned();
        let coinbase_output_sigops = coinbase_output_result
            .iter()
            .map(|output| output.script_pubkey.count_sigops() as u16)
            .sum::<u16>();

        // --- Spawn Template Receiver Task ---
        let tp_address = config.tp_address().clone();
        let cloned_status_tx = status_tx.clone();
        tokio::spawn(async move {
            let _ = TemplateRx::connect(
                tp_address.parse().unwrap(),
                s_new_t,
                s_prev_hash,
                r_solution,
                r_message_recv_signal,
                status::Sender::Upstream(cloned_status_tx),
                coinbase_output_len,
                coinbase_output_sigops,
                tp_authority_public_key,
            )
            .await;
        });

        // --- Start Downstream Pool Listener ---
        let pool = Pool::start(
            config.clone(),
            r_new_t,
            r_prev_hash,
            s_solution,
            s_message_recv_signal,
            status::Sender::DownstreamListener(status_tx),
            config.shares_per_minute(),
            recv_stop_signal,
        )
        .await?;
        // Monitor the status of Template Receiver and downstream connections.
        // Start the error handling loop
        // See `./status.rs` and `utils/error_handling` for information on how this operates
        // --- Spawn Status Monitoring and Shutdown Handling Loop ---
        tokio::spawn(async move {
            loop {
                let task_status = select! {
                    task_status = status_rx.recv() => task_status,
                    interrupt_signal = tokio::signal::ctrl_c() => {
                        match interrupt_signal {
                            Ok(()) => {
                                info!("Interrupt received");
                            },
                            Err(err) => {
                                error!("Unable to listen for interrupt signal: {}", err);
                                // we also shut down in case of error
                            },
                        }
                        break;
                    }
                };
                let task_status: status::Status = task_status.unwrap();

                match task_status.state {
                    status::State::Shutdown => {
                        info!("Received shutdown signal");
                        let _ = send_stop_signal.send(());
                        break;
                    }
                    // Should only be sent by the downstream listener
                    status::State::DownstreamShutdown(err) => {
                        error!(
                            "SHUTDOWN from Downstream: {}\nTry to restart the downstream listener",
                            err
                        );
                        let _ = send_stop_signal.send(());
                        break;
                    }
                    status::State::TemplateProviderShutdown(err) => {
                        error!("SHUTDOWN from Upstream: {}\nTry to reconnecting or connecting to a new upstream", err);
                        let _ = send_stop_signal.send(());
                        break;
                    }
                    status::State::Healthy(msg) => {
                        info!("HEALTHY message: {}", msg);
                    }
                    status::State::DownstreamInstanceDropped(downstream_id) => {
                        warn!("Dropping downstream instance {} from pool", downstream_id);
                        if pool
                            .safe_lock(|p| p.remove_downstream(downstream_id))
                            .is_err()
                        {
                            let _ = send_stop_signal.send(());
                            break;
                        }
                    }
                }
            }
        });
        Ok(())
    }

    /// Initiates a graceful shutdown of the running pool instance.
    ///
    /// It attempts to acquire the lock on the `status_tx` mutex. If successful
    /// and the pool is running (i.e., `status_tx` contains a `Some(sender)`),
    /// it sends a `State::Shutdown` message via the status channel.
    pub fn shutdown(&self) {
        info!("Attempting to shutdown pool");
        if let Ok(status_tx) = &self.status_tx.lock() {
            if let Some(status_tx) = status_tx.as_ref().cloned() {
                info!("Pool is running, sending shutdown signal");
                tokio::spawn(async move {
                    if let Err(e) = status_tx
                        .send(status::Status {
                            state: status::State::Shutdown,
                        })
                        .await
                    {
                        error!("Failed to send shutdown signal to status loop: {:?}", e);
                    } else {
                        info!("Sent shutdown signal to Pool");
                    }
                });
            } else {
                info!("Pool is not running.");
            }
        } else {
            error!("Failed to access Pool status lock");
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use ext_config::{Config, File, FileFormat};

    #[tokio::test]
    async fn shutdown_pool() {
        let template_provider = integration_tests_sv2::start_template_provider(None);
        let config_path = "config-examples/pool-config-local-tp-example.toml";
        let mut config: PoolConfig = match Config::builder()
            .add_source(File::new(config_path, FileFormat::Toml))
            .build()
        {
            Ok(settings) => match settings.try_deserialize::<PoolConfig>() {
                Ok(c) => c,
                Err(e) => {
                    error!("Failed to deserialize config: {}", e);
                    return;
                }
            },
            Err(e) => {
                error!("Failed to build config: {}", e);
                return;
            }
        };
        config.set_tp_address(template_provider.1.to_string());
        let pool_0 = PoolSv2::new(config.clone());
        let pool_1 = PoolSv2::new(config);
        assert!(pool_0.start().await.is_ok());
        tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
        assert!(pool_1.start().await.is_err());
        pool_0.shutdown();
        tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
        assert!(pool_1.start().await.is_ok());
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/status.rs">
//! ## Pool Status Reporting
//!
//! This module handles status updates from Pool components.
//!
//! Tasks send a [`Status`] through a channel, tagged with a [`Sender`] to show where it came from.
//! Centralizes and simplifies error handling across the system.

/// Identifies which component sent a status update.
use stratum_common::roles_logic_sv2::{self, parsers::Mining};

use super::error::PoolError;

/// Each sending side of the status channel
/// should be wrapped with this enum to allow
/// the main thread to know which component sent the message
#[derive(Debug)]
pub enum Sender {
    Downstream(async_channel::Sender<Status>),
    DownstreamListener(async_channel::Sender<Status>),
    Upstream(async_channel::Sender<Status>),
}

impl Sender {
    /// used to clone the sending side of the status channel used by the TCP Listener
    /// into individual Sender's for each Downstream instance
    pub fn listener_to_connection(&self) -> Self {
        match self {
            // should only be used to clone the DownstreamListener(Sender) into Downstream(Sender)s
            Self::DownstreamListener(inner) => Self::Downstream(inner.clone()),
            _ => unreachable!(),
        }
    }

    /// Sends a status message.
    pub async fn send(&self, status: Status) -> Result<(), async_channel::SendError<Status>> {
        match self {
            Self::Downstream(inner) => inner.send(status).await,
            Self::DownstreamListener(inner) => inner.send(status).await,
            Self::Upstream(inner) => inner.send(status).await,
        }
    }
}

impl Clone for Sender {
    fn clone(&self) -> Self {
        match self {
            Self::Downstream(inner) => Self::Downstream(inner.clone()),
            Self::DownstreamListener(inner) => Self::DownstreamListener(inner.clone()),
            Self::Upstream(inner) => Self::Upstream(inner.clone()),
        }
    }
}

/// Represents the possible connection states for both upstream (Template Provider)
/// and downstream.
#[derive(Debug)]
pub enum State {
    /// Indicates that the downstream connection has shut down due to an error.
    DownstreamShutdown(PoolError),
    /// Indicates that the upstream connection (Template Provider) has shut down due to an error.
    TemplateProviderShutdown(PoolError),
    /// Indicates that a specific downstream miner instance has disconnected.
    /// The `u32` value represents the ID of the disconnected instance.
    DownstreamInstanceDropped(u32),
    /// Represents a healthy state with an accompanying status message.
    Healthy(String),
    Shutdown,
}

/// Status message sent to the main thread's status loop for monitoring connection states.
#[derive(Debug)]
pub struct Status {
    /// The current connection state of the pool.
    pub state: State,
}

// This function is used to discern which component experienced the event.
// With this knowledge we can wrap the status message with information (`State` variants) so
// the main status loop can decide what should happen
async fn send_status(
    sender: &Sender,
    e: PoolError,
    outcome: error_handling::ErrorBranch,
) -> error_handling::ErrorBranch {
    match sender {
        Sender::Downstream(tx) => match e {
            PoolError::Sv2ProtocolError((id, Mining::OpenMiningChannelError(_))) => {
                tx.send(Status {
                    state: State::DownstreamInstanceDropped(id),
                })
                .await
                .unwrap_or(());
            }
            _ => {
                let string_err = e.to_string();
                tx.send(Status {
                    state: State::Healthy(string_err),
                })
                .await
                .unwrap_or(());
            }
        },
        Sender::DownstreamListener(tx) => match e {
            PoolError::RolesLogic(roles_logic_sv2::Error::NoDownstreamsConnected) => {
                tx.send(Status {
                    state: State::Healthy("No Downstreams Connected".to_string()),
                })
                .await
                .unwrap_or(());
            }
            _ => {
                tx.send(Status {
                    state: State::DownstreamShutdown(e),
                })
                .await
                .unwrap_or(());
            }
        },
        Sender::Upstream(tx) => {
            tx.send(Status {
                state: State::TemplateProviderShutdown(e),
            })
            .await
            .unwrap_or(());
        }
    }
    outcome
}

/// This function is called by `error_handling::handle_result!`
// todo: as described in issue #777, we should replace every generic *(_) with specific errors and
// cover every possible combination
pub async fn handle_error(sender: &Sender, e: PoolError) -> error_handling::ErrorBranch {
    tracing::debug!("Error: {:?}", &e);
    match e {
        PoolError::Io(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        PoolError::ChannelSend(_) => {
            //This should be a continue because if we fail to send to 1 downstream we should
            // continue processing the other downstreams in the loop we are in.
            // Otherwise if a downstream fails to send to then subsequent downstreams in
            // the map won't get send called on them
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        PoolError::ChannelRecv(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::BinarySv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        PoolError::Codec(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        PoolError::CoinbaseOutput(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::Noise(_) => send_status(sender, e, error_handling::ErrorBranch::Continue).await,
        PoolError::RolesLogic(roles_logic_sv2::Error::NoDownstreamsConnected) => {
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        PoolError::RolesLogic(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::Custom(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        PoolError::Framing(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        PoolError::PoisonLock(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::ComponentShutdown(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::Sv2ProtocolError(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        PoolError::Vardiff(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/template_receiver/message_handler.rs">
//! Implements `ParseTemplateDistributionMessagesFromServer` for [`TemplateRx`].
//!
//! Handles incoming template distribution messages from the Template Provider and forwards them
//! as needed.
use super::TemplateRx;
use std::sync::Arc;
use stratum_common::roles_logic_sv2::{
    errors::Error,
    handlers::template_distribution::{ParseTemplateDistributionMessagesFromServer, SendTo},
    parsers::TemplateDistribution,
    template_distribution_sv2::*,
    utils::Mutex,
};
use tracing::{debug, error, info};

impl ParseTemplateDistributionMessagesFromServer for TemplateRx {
    // Handles a `NewTemplate` message and returns `RelayNewMessageToRemote`.
    fn handle_new_template(&mut self, m: NewTemplate) -> Result<SendTo, Error> {
        info!(
            "Received NewTemplate with id: {}, is future: {}",
            m.template_id, m.future_template
        );
        debug!("NewTemplate: {}", m);
        let new_template = TemplateDistribution::NewTemplate(m.into_static());
        Ok(SendTo::RelayNewMessageToRemote(
            Arc::new(Mutex::new(())),
            new_template,
        ))
    }

    // Handles a `SetNewPrevHash` and return `RelayNewMessageToRemote`
    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo, Error> {
        info!("Received SetNewPrevHash for template: {}", m.template_id);
        debug!("SetNewPrevHash: {}", m);
        let new_prev_hash = TemplateDistribution::SetNewPrevHash(m.into_static());
        Ok(SendTo::RelayNewMessageToRemote(
            Arc::new(Mutex::new(())),
            new_prev_hash,
        ))
    }

    // Handles a `RequestTransactionDataSuccess` message and ignores it.
    //
    // This method is called when a `RequestTransactionDataSuccess` message is received.
    // This message is typically intended for Job Declarators, not the Template Receiver,
    // so it is logged and then ignored.
    fn handle_request_tx_data_success(
        &mut self,
        m: RequestTransactionDataSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received RequestTransactionDataSuccess for template: {}",
            m.template_id
        );
        debug!("RequestTransactionDataSuccess: {}", m);
        // Just ignore tx data messages this are meant for the declarators
        Ok(SendTo::None(None))
    }

    /// Handles a `RequestTransactionDataError` message and ignores it.
    ///
    /// This method is called when a `RequestTransactionDataError` message is received.
    /// This message is typically intended for Job Declarators, not the Template Receiver,
    /// so it is logged and then ignored.
    fn handle_request_tx_data_error(
        &mut self,
        m: RequestTransactionDataError,
    ) -> Result<SendTo, Error> {
        error!(
            "Received RequestTransactionDataError for template: {}, error: {}",
            m.template_id,
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        // Just ignore tx data messages this are meant for the declaretors
        Ok(SendTo::None(None))
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/template_receiver/mod.rs">
//! ## Template Receiver Module
//! [`TemplateRx`] manages the connection to the Template Provider.
//!
//! It is responsible for:
//! - Receiving and forwarding messages like `NewTemplate` and `SetNewPrevHash` to other subsystems.
//! - Receiving solutions from other subsystems and forwarding `SubmitSolution` messages to the
//!   template provider.
//! - Managing the underlying network connection and message flow.ike `SetNewPrevhash` and
//!   `newTemplate` and send it other subsystem.
use super::{
    error::{PoolError, PoolResult},
    mining_pool::{EitherFrame, StdFrame},
    status,
};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use key_utils::Secp256k1PublicKey;
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self, codec_sv2,
        codec_sv2::{HandshakeRole, Initiator},
        handlers::template_distribution::ParseTemplateDistributionMessagesFromServer,
        parsers::{AnyMessage, TemplateDistribution},
        template_distribution_sv2::{
            CoinbaseOutputConstraints, NewTemplate, SetNewPrevHash, SubmitSolution,
        },
        utils::Mutex,
    },
};
use tokio::{net::TcpStream, task};
use tracing::{info, warn};

mod message_handler;
mod setup_connection;
use setup_connection::SetupConnectionHandler;

/// Manages communication with the template provider and relays relevant messages downstream.
///
/// This struct maintains connection channels to the Template Provider and handles:
/// - Receiving and forwarding template-related messages to downstream.
/// - Intercepting and forwarding solution submission messages from downstream.
/// - Ensuring proper message flow between components.
pub struct TemplateRx {
    // Receiver for incoming messages from the template provider.
    receiver: Receiver<EitherFrame>,
    // Sender for outgoing messages to the template provider.
    sender: Sender<EitherFrame>,
    // Signal channel to indicate that a message has been received and processed.
    message_received_signal: Receiver<()>,
    // Sender for forwarding `NewTemplate` messages to other subsystems.
    new_template_sender: Sender<NewTemplate<'static>>,
    // Sender for forwarding `SetNewPrevHash` messages to other subsystems.
    new_prev_hash_sender: Sender<SetNewPrevHash<'static>>,
    // Sender for reporting status updates.
    status_tx: status::Sender,
}

impl TemplateRx {
    //// Establishes a connection with the template provider and sets up communication channels.
    ///
    /// This function handles connection retries in case of initial failure. Once connected,
    /// it performs the SV2 handshake using the `SetupConnectionHandler`. It then sends the
    /// `CoinbaseOutputConstraints` message to inform the template provider about the pool's
    /// constraints. Finally, it spawns two asynchronous tasks: one to handle incoming messages
    /// from the Template Provider (`start`) and another to handle outgoing solution submissions
    /// from downstream (`on_new_solution`).
    #[allow(clippy::too_many_arguments)]
    pub async fn connect(
        address: SocketAddr,
        templ_sender: Sender<NewTemplate<'static>>,
        prev_h_sender: Sender<SetNewPrevHash<'static>>,
        solution_receiver: Receiver<SubmitSolution<'static>>,
        message_received_signal: Receiver<()>,
        status_tx: status::Sender,
        coinbase_out_len: u32,
        coinbase_out_sigops: u16,
        expected_tp_authority_public_key: Option<Secp256k1PublicKey>,
    ) -> PoolResult<()> {
        // Attempt to establish a TCP connection to the template provider, retrying on failure.
        let stream = loop {
            match TcpStream::connect(address).await {
                Ok(stream) => break stream,
                Err(err) => {
                    warn!("Failed to connect to {}: {}. Retrying...", address, err);
                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                }
            }
        };
        info!("Connected to template distribution server at {}", address);

        // Initialize the Noise protocol initiator for secure communication.
        let initiator = match expected_tp_authority_public_key {
            Some(expected_tp_authority_public_key) => {
                Initiator::from_raw_k(expected_tp_authority_public_key.into_bytes())
            }
            None => Initiator::without_pk(),
        }?;

        let (mut receiver, mut sender) =
            Connection::new(stream, HandshakeRole::Initiator(initiator))
                .await
                .unwrap();
        // Perform the SV2 SetupConnection handshake.
        SetupConnectionHandler::setup(&mut receiver, &mut sender, address).await?;

        // Create the TemplateRx instance with the established channels.
        let self_ = Arc::new(Mutex::new(Self {
            receiver,
            sender,
            new_template_sender: templ_sender,
            new_prev_hash_sender: prev_h_sender,
            message_received_signal,
            status_tx,
        }));
        let cloned = self_.clone();

        // Define and send the CoinbaseOutputConstraints message.
        let coinbase_output_constraints = CoinbaseOutputConstraints {
            coinbase_output_max_additional_size: coinbase_out_len,
            coinbase_output_max_additional_sigops: coinbase_out_sigops,
        };
        let frame = AnyMessage::TemplateDistribution(
            TemplateDistribution::CoinbaseOutputConstraints(coinbase_output_constraints),
        )
        .try_into()?;

        Self::send(self_.clone(), frame).await?;

        // Spawn a task to handle incoming messages from the template provider.
        task::spawn(async { Self::start(cloned).await });
        // Spawn a task to handle outgoing solution submissions to the template provider.
        task::spawn(async { Self::on_new_solution(self_, solution_receiver).await });

        Ok(())
    }

    /// Listens for messages from the Template Provider and relays them downstream.
    ///
    /// This task runs in a loop, receiving messages from the template provider,
    /// parsing them as Template Distribution messages, and forwarding relevant messages
    /// (`NewTemplate`, `SetNewPrevHash`) to the appropriate internal channels. It also
    /// handles signaling after processing a message.
    pub async fn start(self_: Arc<Mutex<Self>>) {
        let (recv_msg_signal, receiver, new_template_sender, new_prev_hash_sender, status_tx) =
            self_
                .safe_lock(|s| {
                    (
                        s.message_received_signal.clone(),
                        s.receiver.clone(),
                        s.new_template_sender.clone(),
                        s.new_prev_hash_sender.clone(),
                        s.status_tx.clone(),
                    )
                })
                .unwrap();
        loop {
            let message_from_tp = handle_result!(status_tx, receiver.recv().await);
            let mut message_from_tp: StdFrame = handle_result!(
                status_tx,
                message_from_tp
                    .try_into()
                    .map_err(|e| PoolError::Codec(codec_sv2::Error::FramingSv2Error(e)))
            );
            let message_type_res = message_from_tp
                .get_header()
                .ok_or_else(|| PoolError::Custom(String::from("No header set")));
            let message_type = handle_result!(status_tx, message_type_res).msg_type();
            let payload = message_from_tp.payload();
            let msg = handle_result!(
                status_tx,
                ParseTemplateDistributionMessagesFromServer::handle_message_template_distribution(
                    self_.clone(),
                    message_type,
                    payload,
                )
            );
            match msg {
                roles_logic_sv2::handlers::SendTo_::RelayNewMessageToRemote(_, m) => match m {
                    TemplateDistribution::CoinbaseOutputConstraints(_) => todo!(),
                    TemplateDistribution::NewTemplate(m) => {
                        let res = new_template_sender.send(m).await;
                        handle_result!(status_tx, res);
                        handle_result!(status_tx, recv_msg_signal.recv().await);
                    }
                    TemplateDistribution::RequestTransactionData(_) => todo!(),
                    TemplateDistribution::RequestTransactionDataError(_) => todo!(),
                    TemplateDistribution::RequestTransactionDataSuccess(_) => todo!(),
                    TemplateDistribution::SetNewPrevHash(m) => {
                        let res = new_prev_hash_sender.send(m).await;
                        handle_result!(status_tx, res);
                        handle_result!(status_tx, recv_msg_signal.recv().await);
                    }
                    TemplateDistribution::SubmitSolution(_) => todo!(),
                },
                roles_logic_sv2::handlers::SendTo_::None(None) => (),
                _ => {
                    info!("Error: {:?}", msg);
                    std::process::abort();
                }
            }
        }
    }

    /// Sends a message to the template provider.
    pub async fn send(self_: Arc<Mutex<Self>>, sv2_frame: StdFrame) -> PoolResult<()> {
        let either_frame = sv2_frame.into();
        let sender = self_
            .safe_lock(|self_| self_.sender.clone())
            .map_err(|e| PoolError::PoisonLock(e.to_string()))?;
        sender.send(either_frame).await?;
        Ok(())
    }

    // Handles solution submission messages from downstream.
    //
    // This task listens on a dedicated receiver channel for `SubmitSolution`
    // messages. When a solution is received, it formats it into an SV2 `StdFrame` and
    // sends it to the template provider using the `send()` function.
    async fn on_new_solution(self_: Arc<Mutex<Self>>, rx: Receiver<SubmitSolution<'static>>) {
        let status_tx = self_.safe_lock(|s| s.status_tx.clone()).unwrap();
        while let Ok(solution) = rx.recv().await {
            info!("Sending Solution to TP: {}", &solution);
            let sv2_frame_res: Result<StdFrame, _> =
                AnyMessage::TemplateDistribution(TemplateDistribution::SubmitSolution(solution))
                    .try_into();
            match sv2_frame_res {
                Ok(frame) => {
                    handle_result!(status_tx, Self::send(self_.clone(), frame).await);
                }
                Err(_e) => {
                    // return submit error
                    todo!()
                }
            };
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/lib/template_receiver/setup_connection.rs">
//! Handles the setup connection handshake with the Template Provider.
//!
//! [`SetupConnectionHandler`] builds and sends a `SetupConnection` message,
//! processes the response, and implements `ParseCommonMessagesFromUpstream` for
//! handling common upstream messages.
use super::super::{
    error::{PoolError, PoolResult},
    mining_pool::{EitherFrame, StdFrame},
};
use async_channel::{Receiver, Sender};
use std::{convert::TryInto, net::SocketAddr, sync::Arc};
use stratum_common::roles_logic_sv2::{
    self, codec_sv2,
    common_messages_sv2::{Protocol, Reconnect, SetupConnection, SetupConnectionError},
    errors::Error,
    handlers::common::{ParseCommonMessagesFromUpstream, SendTo},
    parsers::{AnyMessage, CommonMessages},
    utils::Mutex,
};
use tracing::{error, info};

/// Handles the connection setup process with the Template Provider.
pub struct SetupConnectionHandler {}

impl SetupConnectionHandler {
    // Creates a `SetupConnection` message for the given network address.
    #[allow(clippy::result_large_err)]
    fn get_setup_connection_message(address: SocketAddr) -> PoolResult<SetupConnection<'static>> {
        let endpoint_host = address.ip().to_string().into_bytes().try_into()?;
        let vendor = String::new().try_into()?;
        let hardware_version = String::new().try_into()?;
        let firmware = String::new().try_into()?;
        let device_id = String::new().try_into()?;
        Ok(SetupConnection {
            protocol: Protocol::TemplateDistributionProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0b0000_0000_0000_0000_0000_0000_0000_0000,
            endpoint_host,
            endpoint_port: address.port(),
            vendor,
            hardware_version,
            firmware,
            device_id,
        })
    }

    /// Establishes a connection with the Template Provider by sending a `SetupConnection` message
    /// and validating the response.
    pub async fn setup(
        receiver: &mut Receiver<EitherFrame>,
        sender: &mut Sender<EitherFrame>,
        address: SocketAddr,
    ) -> PoolResult<()> {
        let setup_connection = Self::get_setup_connection_message(address)?;

        let sv2_frame: StdFrame = AnyMessage::Common(setup_connection.into()).try_into()?;
        let sv2_frame = sv2_frame.into();
        sender.send(sv2_frame).await?;

        let mut incoming: StdFrame = receiver
            .recv()
            .await?
            .try_into()
            .map_err(|e| PoolError::Codec(codec_sv2::Error::FramingSv2Error(e)))?;
        let message_type = incoming
            .get_header()
            .ok_or_else(|| PoolError::Custom(String::from("No header set")))?
            .msg_type();
        let payload = incoming.payload();

        ParseCommonMessagesFromUpstream::handle_message_common(
            Arc::new(Mutex::new(SetupConnectionHandler {})),
            message_type,
            payload,
        )?;
        Ok(())
    }
}

impl ParseCommonMessagesFromUpstream for SetupConnectionHandler {
    // Handles a successful setup connection response from the template provider.
    fn handle_setup_connection_success(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::SetupConnectionSuccess,
    ) -> Result<SendTo, Error> {
        info!(
            "Received `SetupConnectionSuccess` from TP: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendTo::None(None))
    }

    // Handles an error response during the setup connection process.
    fn handle_setup_connection_error(&mut self, m: SetupConnectionError) -> Result<SendTo, Error> {
        error!(
            "Received `SetupConnectionError` from TP with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        let flags = m.flags;
        let message = SetupConnectionError {
            flags,
            // this error code is currently a hack because there is a lifetime problem with
            // `error_code`.
            error_code: "unsupported-feature-flags"
                .to_string()
                .into_bytes()
                .try_into()
                .unwrap(),
        };
        Ok(SendTo::RelayNewMessage(
            CommonMessages::SetupConnectionError(message),
        ))
    }

    // Handles a channel endpoint change notification from the template provider.
    fn handle_channel_endpoint_changed(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<SendTo, Error> {
        info!(
            "Received ChannelEndpointChanged with channel id: {}",
            m.channel_id
        );
        Err(Error::UnexpectedMessage(
            roles_logic_sv2::common_messages_sv2::MESSAGE_TYPE_CHANNEL_ENDPOINT_CHANGED,
        ))
    }

    // Handles a reconnect request from the template provider (not implemented yet).
    fn handle_reconnect(&mut self, _m: Reconnect) -> Result<SendTo, Error> {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/roles/pool/src/main.rs">
//! Pool CLI entry point.
//!
//! This binary parses CLI arguments, loads the TOML configuration,
//! and starts the main runtime via `pool_sv2::start`.
//!
//! Task orchestration and shutdown are handled in `lib/mod.rs`.

pub use pool_sv2::{config, status, PoolSv2};
use tokio::select;
use tracing::{error, info};

mod args;
use args::process_cli_args;
use config_helpers::logging::init_logging;

/// Initializes logging, parses arguments, loads configuration, and starts the Pool runtime.
#[tokio::main]
async fn main() {
    let config = process_cli_args();
    init_logging(config.log_dir());
    let _ = PoolSv2::new(config).start().await;
    select! {
        interrupt_signal = tokio::signal::ctrl_c() => {
            match interrupt_signal {
                Ok(()) => {
                    info!("Pool(bin): Caught interrupt signal. Shutting down...");
                    return;
                },
                Err(err) => {
                    error!("Pool(bin): Unable to listen for interrupt signal: {}", err);
                    return;
                },
            }
        }
    };
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/Cargo.toml">
[package]
name = "config-helpers"
authors = ["The Stratum V2 Developers"]
version = "0.1.0"
edition = "2018"
description = "Helpers for working with Stratum V2 configuration files"
documentation = ""
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[dependencies]
serde = { version = "1.0.89", features = ["derive","alloc"], default-features = false }
miniscript = { version = "12.3.4", default-features = false, features = [ "no-std" ] }
roles_logic_sv2 = { path = "../../../protocols/v2/roles-logic-sv2" }
tracing-subscriber = { version = "0.3", features = ["env-filter"] }
tracing = { version = "0.1" }
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/README.md">
# config-helpers

This crate provide utils to handle Stratum V2 Roles configurations.

## Install

```cargo add config-helpers```
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/errors.rs">
use core::fmt;

use miniscript::bitcoin::{address, hex};

/// Error enum
#[derive(Debug)]
pub enum Error {
    /// Error parsing a Bitcoin address
    Address(address::ParseError),
    // TODO rust-miniscript 13 will have functions to do these checks for us so we don't
    // need to pollute our own error enum with this fiddly stuff
    /// addr() descriptor did not have exactly 1 child
    AddrDescriptorNChildren(usize),
    /// raw() descriptor child did not have 0 children
    AddrDescriptorGrandchild,
    /// raw() descriptor did not have exactly 1 child
    RawDescriptorNChildren(usize),
    /// addr() descriptor child did not have 0 children
    RawDescriptorGrandchild,
    /// Error parsing a raw descriptor as hex.
    Hex(hex::HexToBytesError),
    /// Invalid `output_script_value` for script type. It must be a valid public key/script
    InvalidOutputScript,
    /// Unknown script type in config
    UnknownOutputScriptType,
    /// Error from the `miniscript` crate.
    Miniscript(miniscript::Error),
}

impl fmt::Display for Error {
    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {
        use Error::*;
        match self {
            Address(ref e) => write!(f, "Bitcoin address: {e}"),
            AddrDescriptorNChildren(0) => write!(f, "Found addr() descriptor with no address"),
            AddrDescriptorNChildren(n) => write!(f, "Found addr() descriptor with {n} children; must be exactly one valid address"),
            AddrDescriptorGrandchild => write!(f, "Found descriptor of the form addr(X(y)); X must be a valid address and have no subexpression"),
            RawDescriptorNChildren(0) => write!(f, "Found raw() descriptor with no hex-encoded script"),
            RawDescriptorNChildren(n) => write!(f, "Found raw() descriptor with {n} children; must be exactly one hex-encoded script"),
            RawDescriptorGrandchild => write!(f, "Found descriptor of the form raw(X(y)); X must be a hex-encoded script and have no subexpression"),
            Hex(ref e) => write!(f, "Decoding hex-formatted script: {e}"),
            UnknownOutputScriptType => write!(f, "Unknown script type in config"),
            InvalidOutputScript => write!(f, "Invalid output_script_value for your script type. It must be a valid public key/script"),
            Miniscript(ref e) => write!(f, "Miniscript: {e}"),
        }
    }
}

impl From<address::ParseError> for Error {
    fn from(e: address::ParseError) -> Self {
        Error::Address(e)
    }
}

impl From<hex::HexToBytesError> for Error {
    fn from(e: hex::HexToBytesError) -> Self {
        Error::Hex(e)
    }
}

impl From<miniscript::Error> for Error {
    fn from(e: miniscript::Error) -> Self {
        Error::Miniscript(e)
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/mod.rs">
mod errors;
mod serde_types;

use miniscript::{
    bitcoin::{address::NetworkUnchecked, hex::FromHex as _, Address, Network, Script, ScriptBuf},
    DefiniteDescriptorKey, Descriptor,
};

pub use errors::Error;

/// Coinbase output transaction.
///
/// Typically used for parsing coinbase outputs defined in SRI role configuration files.
#[derive(Debug, serde::Deserialize, Clone)]
#[serde(try_from = "serde_types::SerdeCoinbaseOutput")]
pub struct CoinbaseOutput {
    script_pubkey: ScriptBuf,
    ok_for_mainnet: bool,
}

impl CoinbaseOutput {
    /// Creates a new [`CoinbaseOutput`] from a descriptor string.
    pub fn from_descriptor(mut s: &str) -> Result<Self, Error> {
        // Taproot descriptors cannot be parsed with `expression::Tree::from_str` and
        // need special handling. So we special-case them early and just pass to
        // rust-miniscript. In Miniscript 13 we will not need to do this.
        if s.starts_with("tr") {
            let desc = s.parse::<Descriptor<DefiniteDescriptorKey>>()?;
            return Ok(Self {
                script_pubkey: desc.script_pubkey(),
                // Descriptors don't have a way to specify a network, so we assume
                // they are OK to be used on mainnet.
                ok_for_mainnet: true,
            });
        }

        // Manually verify the checksum. FIXME in Miniscript 13 we will not need
        // to do this, since `expression::Tree::from_str` will do the checksum
        // validation for us. (And yield a much less horrible error type.)
        if let Some((desc_str, checksum_str)) = s.rsplit_once('#') {
            let expected_sum = miniscript::descriptor::checksum::desc_checksum(desc_str)?;
            if checksum_str != expected_sum {
                return Err(miniscript::Error::BadDescriptor(format!(
                    "Invalid checksum '{checksum_str}', expected '{expected_sum}'"
                ))
                .into());
            }
            s = desc_str;
        }

        let tree = miniscript::expression::Tree::from_str(s)?;
        match tree.name {
            "addr" => {
                // In rust-miniscript 13 these can be replaced with a call to
                // TreeIterItem::verify_toplevel which will these checks for us
                // in a uniform way.
                if tree.args.len() != 1 {
                    return Err(Error::AddrDescriptorNChildren(tree.args.len()));
                }
                if !tree.args[0].args.is_empty() {
                    return Err(Error::AddrDescriptorGrandchild);
                }

                let addr = tree.args[0].name.parse::<Address<NetworkUnchecked>>()?;
                Ok(Self {
                    script_pubkey: addr.assume_checked_ref().script_pubkey(),
                    ok_for_mainnet: addr.is_valid_for_network(Network::Bitcoin),
                })
            }
            "raw" => {
                // In rust-miniscript 13 these can be replaced with a call to
                // TreeIterItem::verify_toplevel which will these checks for us
                // in a uniform way.
                if tree.args.len() != 1 {
                    return Err(Error::RawDescriptorNChildren(tree.args.len()));
                }
                if !tree.args[0].args.is_empty() {
                    return Err(Error::RawDescriptorGrandchild);
                }

                let bytes = Vec::<u8>::from_hex(tree.args[0].name)?;
                Ok(Self {
                    script_pubkey: ScriptBuf::from(bytes),
                    // Users of hex scriptpubkeys are on their own.
                    ok_for_mainnet: true,
                })
            }
            _ => {
                let desc = s.parse::<Descriptor<DefiniteDescriptorKey>>()?;
                Ok(Self {
                    script_pubkey: desc.script_pubkey(),
                    // Descriptors don't have a way to specify a network, so we assume
                    // they are OK to be used on mainnet.
                    ok_for_mainnet: true,
                })
            }
        }
    }

    /// Whether this coinbase output is okay for use on mainnet.
    ///
    /// This is a "best effort" check and currently only returns false if the user
    /// provides an addr() descriptor in which they specified a testnet or regtest
    /// address.
    pub fn ok_for_mainnet(&self) -> bool {
        self.ok_for_mainnet
    }

    /// The `scriptPubKey` associated with the coinbase output
    pub fn script_pubkey(&self) -> &Script {
        &self.script_pubkey
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn fixed_vector_addr() {
        // Valid
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2)#wdnlkpe8")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "76a91477bff20c60e522dfaa3350c39b030a5d004e839a88ac",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(3J98t1WpEZ73CNmQviecrnyiWrnqRhWNLy)#rsjl0crt")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "a914b472a266d0bd89c13706a4132ccfb16f7c3b9fcb87",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "addr(bc1qw508d6qejxtdg4y5r3zarvary0c5xw7kv8f3t4)#uyjndxcw"
            )
            .unwrap()
            .script_pubkey()
            .to_hex_string(),
            "0014751e76e8199196d454941c45d1b3a323f1433bd6",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "addr(bc1qrp33g0q5c5txsp9arysrx4k6zdkfs4nce4xj0gdcccefvpysxf3qccfmv3)#8kzm8txf"
            )
            .unwrap()
            .script_pubkey()
            .to_hex_string(),
            "00201863143c14c5166804bd19203356da136c985678cd4d27a1b8c6329604903262",
        );
        // no checksum is ok
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2)")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "76a91477bff20c60e522dfaa3350c39b030a5d004e839a88ac",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2,)")
                .unwrap_err()
                .to_string(),
            "Found addr() descriptor with 2 children; must be exactly one valid address",
        );

        // Invalid
        // But empty checksum is not (in Miniscript 13 these error messages will be cleaner)
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2)#")
                .unwrap_err()
                .to_string(),
            "Miniscript: Invalid descriptor: Invalid checksum '', expected 'wdnlkpe8'",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2)#wdnlkpe7")
                .unwrap_err()
                .to_string(),
            "Miniscript: Invalid descriptor: Invalid checksum 'wdnlkpe7', expected 'wdnlkpe8'",
        );
        // Bad base58ck checksum even though the descriptor checksum is OK. Note that rust-bitcoin
        // 0.32 interprets bad bech32 checksums as "base58 errors" because it doessn't know
        // what encoding an invalid string is supposed to have. See https://github.com/rust-bitcoin/rust-bitcoin/issues/3044
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN3)#5v55uzec")
                .unwrap_err()
                .to_string(),
            "Bitcoin address: base58 error",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "addr(bc1qw508d6qejxtdg4y5r3zarvary0c5xw7kv8f3t3)#wfr7lfxf"
            )
            .unwrap_err()
            .to_string(),
            "Bitcoin address: base58 error",
        );
        // Flagrantly bad stuff -- should probably PR these upstream to rust-miniscript.
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr()")
                .unwrap_err()
                .to_string(),
            "Bitcoin address: base58 error",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(It's a mad mad world!?! )")
                .unwrap_err()
                .to_string(),
            "Miniscript: unprintable character 0xf0",
        );
        // This error is just wrong lol. Fixed in Miniscript 13.
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(It's a mad mad world!?! )#abcdefg")
                .unwrap_err()
                .to_string(),
            "Miniscript: Invalid descriptor: Invalid character in checksum: ''",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(It's a mad mad world!?!)#hmeprl29")
                .unwrap_err()
                .to_string(),
            "Bitcoin address: base58 error",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("addr(It's a mad mad world!?!)#")
                .unwrap_err()
                .to_string(),
            "Miniscript: Invalid descriptor: Invalid checksum '', expected 'hmeprl29'",
        );
    }

    #[test]
    fn fixed_vector_combo() {
        // We do not support combo descriptors. Nobody should.
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "combo(0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798)"
            )
            .unwrap_err()
            .to_string(),
            "Miniscript: unexpected combo(1 args) while parsing Miniscript"
        );
    }

    #[test]
    fn fixed_vector_musig() {
        // We do not support musig descriptors. One day.
        assert_eq!(
            CoinbaseOutput::from_descriptor("musig(0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798,03fff97bd5755eeea420453a14355235d382f6472f8568a18b2f057a1460297556)").unwrap_err().to_string(),
            "Miniscript: unexpected musig(2 args) while parsing Miniscript"
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("tr(musig(0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798,03fff97bd5755eeea420453a14355235d382f6472f8568a18b2f057a1460297556))").unwrap_err().to_string(),
            "Miniscript: expected )",
        );
    }

    #[test]
    fn fixed_vector_raw() {
        // Empty raw descriptors are OK; correspond to the empty script.
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw()")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw(deadbeef)")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "deadbeef",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw(DEADBEEF)")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "deadbeef",
        );
        // Should we allow this? We do, so I guess we should test it and make sure we don't stop..
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw(DEADbeef)")
                .unwrap()
                .script_pubkey()
                .to_hex_string(),
            "deadbeef",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw(0)")
                .unwrap_err()
                .to_string(),
            "Decoding hex-formatted script: odd length, failed to create bytes from hex",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("raw(0,1)")
                .unwrap_err()
                .to_string(),
            "Found raw() descriptor with 2 children; must be exactly one hex-encoded script",
        );
    }

    #[test]
    fn fixed_vector_miniscript() {
        assert_eq!(
            CoinbaseOutput::from_descriptor("sh(wsh(multi(2,0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798,03fff97bd5755eeea420453a14355235d382f6472f8568a18b2f057a1460297556)))#qpcmf2lu").unwrap().script_pubkey().to_hex_string(),
            "a9141cb55de50b72c67709ab16307d69557e6bb1a98787",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "tr(0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798)"
            )
            .unwrap()
            .script_pubkey()
            .to_hex_string(),
            "5120da4710964f7852695de2da025290e24af6d8c281de5a0b902b7135fd9fd74d21",
        );
        assert_eq!(
            CoinbaseOutput::from_descriptor("tr(0279be667ef9dcbbac55a06295ce870b07029bfcdb2dce28d959f2815b16f81798,{pk(03fff97bd5755eeea420453a14355235d382f6472f8568a18b2f057a1460297556),{multi_a(2,026a245bf6dc698504c89a20cfded60853152b695336c28063b61c65cbd269e6b4,0231ecbfac95d972f0b8f81ec6e01e9c621d91a4b48d5f9d12d7e95febe9f34d64),multi_a(2,026a245bf6dc698504c89a20cfded60853152b695336c28063b61c65cbd269e6b4,0231ecbfac95d972f0b8f81ec6e01e9c621d91a4b48d5f9d12d7e95febe9f34d64)}})")
            .unwrap()
            .script_pubkey()
            .to_hex_string(),
            "5120493bdae0d225af5cb88c4cb2a1e1e89e391153ba7699c91ebee2fd082ed1636c",
        );
    }

    #[test]
    fn fixed_vector_keys() {
        // xpub
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8NqtwybGhePY2gZ29ESFjqJoCu1Rupje8YtGqsefD265TMg7usUDFdp6W1EGMcet8)").unwrap().script_pubkey().to_hex_string(),
            "76a9143442193e1bb70916e914552172cd4e2dbc9df81188ac",
        );
        // xpub with non-hardened path
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8NqtwybGhePY2gZ29ESFjqJoCu1Rupje8YtGqsefD265TMg7usUDFdp6W1EGMcet8/1/2/3)").unwrap().script_pubkey().to_hex_string(),
            "76a914f2d2e1401c88353c2298d1a928d4ed827ff46ff688ac",
        );
        // xpub with hardened path (not allowed)
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8NqtwybGhePY2gZ29ESFjqJoCu1Rupje8YtGqsefD265TMg7usUDFdp6W1EGMcet8/1'/2/3)").unwrap_err().to_string(),
            "Miniscript: unexpected cannot parse multi-path keys, keys with a wildcard or keys with hardened derivation steps as a DerivedDescriptorKey",
        );
        // no wildcards allowed (at least for now; gmax thinks it would be cool if we would
        // instantiate it with the blockheight or something, but need to work out UX)
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8NqtwybGhePY2gZ29ESFjqJoCu1Rupje8YtGqsefD265TMg7usUDFdp6W1EGMcet8/*)").unwrap_err().to_string(),
            "Miniscript: unexpected cannot parse multi-path keys, keys with a wildcard or keys with hardened derivation steps as a DerivedDescriptorKey",
        );
        // No multipath descriptors allowed; this is not a wallet with change
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xpub661MyMwAqRbcFtXgS5sYJABqqG9YLmC4Q1Rdap9gSE8NqtwybGhePY2gZ29ESFjqJoCu1Rupje8YtGqsefD265TMg7usUDFdp6W1EGMcet8/<0;1>)").unwrap_err().to_string(),
            "Miniscript: unexpected cannot parse multi-path keys, keys with a wildcard or keys with hardened derivation steps as a DerivedDescriptorKey",
        );
        // Private keys are not allowed, or xprvs.
        assert_eq!(
            CoinbaseOutput::from_descriptor(
                "pkh(L4rK1yDtCWekvXuE6oXD9jCYfFNV2cWRpVuPLBcCU2z8TrisoyY1)"
            )
            .unwrap_err()
            .to_string(),
            "Miniscript: unexpected Key too short (<66 char), doesn't match any format",
        );
        // This is a confusing error message which should be fixed in Miniscript 13.
        assert_eq!(
            CoinbaseOutput::from_descriptor("pkh(xprv9s21ZrQH143K3QTDL4LXw2F7HEK3wJUD2nW2nRk4stbPy6cq3jPPqjiChkVvvNKmPGJxWUtg6LnF5kejMRNNU3TGtRBeJgk33yuGBxrMPHi)").unwrap_err().to_string(),
            "Miniscript: unexpected Public keys must be 64/66/130 characters in size",
        );
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/coinbase_output/serde_types.rs">
use core::convert::TryFrom;
use miniscript::bitcoin::{
    secp256k1::{All, Secp256k1},
    PublicKey, ScriptBuf, ScriptHash, WScriptHash, XOnlyPublicKey,
};

use super::Error;

#[derive(serde::Deserialize)]
pub(super) struct LegacyCoinbaseOutput {
    /// Specifies type of the script used in the output.
    ///
    /// Supported values include:
    /// - `"P2PK"`: Pay-to-Public-Key
    /// - `"P2PKH"`: Pay-to-Public-Key-Hash
    /// - `"P2SH"`: Pay-to-Script-Hash
    /// - `"P2WPKH"`: Pay-to-Witness-Public-Key-Hash:w

    /// - `"P2WSH"`: Pay-to-Witness-Script-Hash
    /// - `"P2TR"`: Pay-to-Taproot
    pub(super) output_script_type: String,

    /// Value associated with the script, typically a public key or script hash.
    ///
    /// This field's interpretation depends on the `output_script_type`:
    /// - For `"P2PK"`: The raw public key.
    /// - For `"P2PKH"`: A public key hash.
    /// - For `"P2WPKH"`: A witness public key hash.
    /// - For `"P2SH"`: A script hash.
    /// - For `"P2WSH"`: A witness script hash.
    /// - For `"P2TR"`: An x-only public key.
    pub(super) output_script_value: String,
}

impl TryFrom<LegacyCoinbaseOutput> for super::CoinbaseOutput {
    type Error = super::Error;
    fn try_from(value: LegacyCoinbaseOutput) -> Result<Self, Self::Error> {
        let script_pubkey = match value.output_script_type.as_str() {
            "TEST" => {
                let pub_key_hash = value
                    .output_script_value
                    .parse::<PublicKey>()
                    .map_err(|_| Error::InvalidOutputScript)?
                    .pubkey_hash();
                ScriptBuf::new_p2pkh(&pub_key_hash)
            }
            "P2PK" => {
                let pub_key = value
                    .output_script_value
                    .parse::<PublicKey>()
                    .map_err(|_| Error::InvalidOutputScript)?;
                ScriptBuf::new_p2pk(&pub_key)
            }
            "P2PKH" => {
                let pub_key_hash = value
                    .output_script_value
                    .parse::<PublicKey>()
                    .map_err(|_| Error::InvalidOutputScript)?
                    .pubkey_hash();
                ScriptBuf::new_p2pkh(&pub_key_hash)
            }
            "P2WPKH" => {
                let w_pub_key_hash = value
                    .output_script_value
                    .parse::<PublicKey>()
                    .map_err(|_| Error::InvalidOutputScript)?
                    .wpubkey_hash()
                    .unwrap();
                ScriptBuf::new_p2wpkh(&w_pub_key_hash)
            }
            "P2SH" => {
                let script_hashed = value
                    .output_script_value
                    .parse::<ScriptHash>()
                    .map_err(|_| Error::InvalidOutputScript)?;
                ScriptBuf::new_p2sh(&script_hashed)
            }
            "P2WSH" => {
                let w_script_hashed = value
                    .output_script_value
                    .parse::<WScriptHash>()
                    .map_err(|_| Error::InvalidOutputScript)?;
                ScriptBuf::new_p2wsh(&w_script_hashed)
            }
            "P2TR" => {
                // From the bip
                //
                // Conceptually, every Taproot output corresponds to a combination of
                // a single public key condition (the internal key),
                // and zero or more general conditions encoded in scripts organized in a tree.
                let pub_key = value
                    .output_script_value
                    .parse::<XOnlyPublicKey>()
                    .map_err(|_| Error::InvalidOutputScript)?;
                ScriptBuf::new_p2tr::<All>(&Secp256k1::<All>::new(), pub_key, None)
            }
            _ => return Err(Error::UnknownOutputScriptType),
        };
        Ok(Self {
            script_pubkey,
            // legacy encoding gives no way to specify testnet or mainnet
            ok_for_mainnet: true,
        })
    }
}

/// A coinbase output script as it appears in a configuration file.
///
/// Private to avoid exposing the enum constructors.
#[derive(serde::Deserialize)]
#[serde(untagged)] // decode as whichever variant makes sense for the input
enum SerdeCoinbaseOutputInner {
    Legacy(LegacyCoinbaseOutput),
    Descriptor(String),
}

/// A structure representing a coinbase output script as it appears in a
/// configuration file.
///
/// Can only be constructed via serde, and supports no operations except conversion
/// to a [`super::CoinbaseOutput`] via [`TryFrom`].
#[derive(serde::Deserialize)]
#[serde(transparent)]
pub struct SerdeCoinbaseOutput {
    inner: SerdeCoinbaseOutputInner,
}

impl TryFrom<SerdeCoinbaseOutput> for super::CoinbaseOutput {
    type Error = super::Error;
    fn try_from(value: SerdeCoinbaseOutput) -> Result<Self, Self::Error> {
        match value.inner {
            SerdeCoinbaseOutputInner::Legacy(legacy) => Self::try_from(legacy),
            SerdeCoinbaseOutputInner::Descriptor(ref s) => Self::from_descriptor(s),
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/lib.rs">
use serde::{Deserialize, Deserializer};

mod coinbase_output;
pub use coinbase_output::{CoinbaseOutput, Error as CoinbaseOutputError};

pub mod logging;

mod toml;
pub use toml::duration_from_toml;

/// Deserialize an object, allowing it to be encoded either directly or as
/// a singleton vector.
///
/// Returns a singleton vector in either case.
///
/// This function was created as part of https://github.com/stratum-mining/stratum/pull/1720
/// as a first step in transitioning the `coinbase_outputs` field of various configuration
/// files away from a vector toward a single object. As part of a larger refactoring, it
/// should be changed to return a single [`CoinbaseOutput`] directly.
pub fn deserialize_vec_exactly_1<'de, D>(d: D) -> Result<Vec<CoinbaseOutput>, D::Error>
where
    D: Deserializer<'de>,
{
    use serde::de::Error as _;

    // Serde will attempt `Single` first, then `Vec` if that fails.
    #[derive(Deserialize)]
    #[serde(untagged)]
    enum Helper {
        Single(CoinbaseOutput),
        Vec(Vec<CoinbaseOutput>),
    }

    match Helper::deserialize(d) {
        Err(_) => {
            // The errors yielded by serde are meaningless to the user and
            // expose the name of the private `Helper` type. Override them
            // with something equally useless but less confusing.
            Err(D::Error::custom(
                "could not parse descriptor string (or old-style list format)",
            ))
        }
        Ok(Helper::Single(ret)) => Ok(vec![ret]),
        Ok(Helper::Vec(ret)) => {
            if ret.len() != 1 {
                return Err(D::Error::invalid_length(
                    ret.len(),
                    &"a list with exactly one coinbase output, or a single descriptor string",
                ));
            }
            Ok(ret)
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/logging.rs">
use std::{fs::OpenOptions, io, path::Path, str::FromStr};
use tracing::level_filters::LevelFilter;
use tracing_subscriber::{fmt, prelude::*, EnvFilter, Registry};

/// Initialize logging to stdout and optionally to a file.
///
/// If `log_file` is Some, logs will be written to both stdout and the file.
/// If `log_level` is not provided or is invalid, it defaults to "info".
pub fn init_logging(log_file: Option<&Path>) {
    let rust_log = std::env::var("RUST_LOG").unwrap_or_else(|_| "info".to_string());
    let log_level_filter = LevelFilter::from_str(&rust_log).unwrap_or(LevelFilter::INFO);
    let env_filter = EnvFilter::new(log_level_filter.to_string());

    let subscriber: Box<dyn tracing::Subscriber + Send + Sync> = match log_file {
        Some(path) => {
            // Log to both file and stdout
            let path = path.to_owned();
            let file_layer = fmt::layer().with_writer(move || {
                OpenOptions::new()
                    .create(true)
                    .append(true)
                    .open(&path)
                    .expect("Failed to open log file")
            });
            let stdout_layer = fmt::layer().with_writer(io::stdout);
            Box::new(
                Registry::default()
                    .with(env_filter)
                    .with(stdout_layer)
                    .with(file_layer),
            )
        }
        None => {
            // Log only to stdout
            let stdout_layer = fmt::layer().with_writer(io::stdout);
            Box::new(Registry::default().with(env_filter).with(stdout_layer))
        }
    };

    tracing::subscriber::set_global_default(subscriber).expect("Failed to set global subscriber");
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/config-helpers/src/toml.rs">
use std::time::Duration;

/// Deserialize a duration from a TOML string.
pub fn duration_from_toml<'de, D>(deserializer: D) -> Result<Duration, D::Error>
where
    D: serde::Deserializer<'de>,
{
    use serde::Deserialize;

    #[derive(serde::Deserialize)]
    struct Helper {
        unit: String,
        value: u64,
    }

    let helper = Helper::deserialize(deserializer)?;
    match helper.unit.as_str() {
        "seconds" => Ok(Duration::from_secs(helper.value)),
        "secs" => Ok(Duration::from_secs(helper.value)),
        "s" => Ok(Duration::from_secs(helper.value)),
        "milliseconds" => Ok(Duration::from_millis(helper.value)),
        "millis" => Ok(Duration::from_millis(helper.value)),
        "ms" => Ok(Duration::from_millis(helper.value)),
        "microseconds" => Ok(Duration::from_micros(helper.value)),
        "micros" => Ok(Duration::from_micros(helper.value)),
        "us" => Ok(Duration::from_micros(helper.value)),
        "nanoseconds" => Ok(Duration::from_nanos(helper.value)),
        "nanos" => Ok(Duration::from_nanos(helper.value)),
        "ns" => Ok(Duration::from_nanos(helper.value)),
        // ... add other units as needed
        _ => Err(serde::de::Error::custom("Unsupported duration unit")),
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/network-helpers/Cargo.toml">
[package]
name = "network_helpers_sv2"
version = "4.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "Networking utils for SV2 roles"
documentation = "https://docs.rs/network_helpers_sv2"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
async-std = { version = "1.8.0", optional = true }
async-channel = { version = "1.8.0", optional = true }
tokio = { version = "1.44.1", features = ["full"] }
codec_sv2 = { path = "../../../protocols/v2/codec-sv2", version = "^2.0.0", features=["noise_sv2"], optional = true }
sv1_api = { path = "../../../protocols/v1/", version = "^1.0.0", optional = true }
tracing = { version = "0.1" }
futures = "0.3.28"
tokio-util = { version = "0.7.10", default-features = false, features = ["codec"], optional = true }
serde_json = { version = "1.0.138", default-features = false, optional = true }

[features]
default = ["async-channel", "codec_sv2"]
with_buffer_pool = ["codec_sv2/with_buffer_pool"]
sv1 = ["sv1_api", "tokio-util", "serde_json"]

[package.metadata.docs.rs]
features = ["with_buffer_pool", "sv1"]
</file>

<file path="stratum-1.4.0/roles/roles-utils/network-helpers/src/lib.rs">
pub mod noise_connection;
pub mod plain_connection;
#[cfg(feature = "sv1")]
pub mod sv1_connection;

use async_channel::{RecvError, SendError};
use codec_sv2::Error as CodecError;

pub use codec_sv2;

#[derive(Debug)]
pub enum Error {
    HandshakeRemoteInvalidMessage,
    CodecError(CodecError),
    RecvError,
    SendError,
    // This means that a socket that was supposed to be opened have been closed, likley by the
    // peer
    SocketClosed,
}

impl From<CodecError> for Error {
    fn from(e: CodecError) -> Self {
        Error::CodecError(e)
    }
}
impl From<RecvError> for Error {
    fn from(_: RecvError) -> Self {
        Error::RecvError
    }
}
impl<T> From<SendError<T>> for Error {
    fn from(_: SendError<T>) -> Self {
        Error::SendError
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/network-helpers/src/noise_connection.rs">
#![allow(clippy::new_ret_no_self)]
use crate::Error;
use async_channel::{unbounded, Receiver, Sender};
use codec_sv2::{
    binary_sv2::{Deserialize, GetSize, Serialize},
    noise_sv2::{ELLSWIFT_ENCODING_SIZE, INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE},
    HandShakeFrame, HandshakeRole, StandardEitherFrame, StandardNoiseDecoder, State,
};
use std::{convert::TryInto, sync::Arc};
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{
        tcp::{OwnedReadHalf, OwnedWriteHalf},
        TcpStream,
    },
    task,
};
use tracing::{debug, error};

pub struct Connection;

struct ConnectionState<Message> {
    sender_incoming: Sender<StandardEitherFrame<Message>>,
    receiver_incoming: Receiver<StandardEitherFrame<Message>>,
    sender_outgoing: Sender<StandardEitherFrame<Message>>,
    receiver_outgoing: Receiver<StandardEitherFrame<Message>>,
}

impl<Message> ConnectionState<Message> {
    fn close_all(&self) {
        self.sender_incoming.close();
        self.receiver_incoming.close();
        self.sender_outgoing.close();
        self.receiver_outgoing.close();
    }
}

async fn send_message<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
    writer: &mut OwnedWriteHalf,
    msg: StandardEitherFrame<Message>,
    state: &mut State,
    encoder: &mut codec_sv2::NoiseEncoder<Message>,
) -> Result<(), Error> {
    let buffer = encoder.encode(msg, state)?;
    writer
        .write_all(buffer.as_ref())
        .await
        .map_err(|_| Error::SocketClosed)?;
    Ok(())
}

async fn receive_message<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
    reader: &mut OwnedReadHalf,
    state: &mut State,
    decoder: &mut StandardNoiseDecoder<Message>,
) -> Result<StandardEitherFrame<Message>, Error> {
    let writable = decoder.writable();
    reader
        .read_exact(writable)
        .await
        .map_err(|_| Error::SocketClosed)?;
    decoder.next_frame(state).map_err(Error::CodecError)
}

impl Connection {
    pub async fn new<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
        stream: TcpStream,
        role: HandshakeRole,
    ) -> Result<
        (
            Receiver<StandardEitherFrame<Message>>,
            Sender<StandardEitherFrame<Message>>,
        ),
        Error,
    > {
        let address = stream.peer_addr().map_err(|_| Error::SocketClosed)?;
        let (mut reader, mut writer) = stream.into_split();
        let mut decoder = StandardNoiseDecoder::<Message>::new();
        let mut encoder = codec_sv2::NoiseEncoder::<Message>::new();
        let mut state = codec_sv2::State::initialized(role.clone());

        // Handshake Phase
        match role {
            HandshakeRole::Initiator(_) => {
                debug!("Initializing as downstream for {}", address);
                let mut responder_state = codec_sv2::State::not_initialized(&role);
                let first_msg = state.step_0()?;
                send_message(&mut writer, first_msg.into(), &mut state, &mut encoder).await?;
                debug!("First handshake message sent");

                loop {
                    match receive_message(&mut reader, &mut responder_state, &mut decoder).await {
                        Ok(second_msg) => {
                            debug!("Second handshake message received");
                            let handshake_frame: HandShakeFrame = second_msg
                                .try_into()
                                .map_err(|_| Error::HandshakeRemoteInvalidMessage)?;
                            let payload: [u8; INITIATOR_EXPECTED_HANDSHAKE_MESSAGE_SIZE] =
                                handshake_frame
                                    .get_payload_when_handshaking()
                                    .try_into()
                                    .map_err(|_| Error::HandshakeRemoteInvalidMessage)?;
                            let transport_state = state.step_2(payload)?;
                            state = transport_state;
                            break;
                        }
                        Err(Error::CodecError(codec_sv2::Error::MissingBytes(_))) => {
                            debug!("Waiting for more bytes during handshake");
                        }
                        Err(e) => {
                            error!("Handshake failed with upstream: {:?}", e);
                            return Err(e);
                        }
                    }
                }
            }
            HandshakeRole::Responder(_) => {
                debug!("Initializing as upstream for {}", address);
                let mut initiator_state = codec_sv2::State::not_initialized(&role);

                loop {
                    match receive_message(&mut reader, &mut initiator_state, &mut decoder).await {
                        Ok(first_msg) => {
                            debug!("First handshake message received");
                            let handshake_frame: HandShakeFrame = first_msg
                                .try_into()
                                .map_err(|_| Error::HandshakeRemoteInvalidMessage)?;
                            let payload: [u8; ELLSWIFT_ENCODING_SIZE] = handshake_frame
                                .get_payload_when_handshaking()
                                .try_into()
                                .map_err(|_| Error::HandshakeRemoteInvalidMessage)?;
                            let (second_msg, transport_state) = state.step_1(payload)?;
                            send_message(&mut writer, second_msg.into(), &mut state, &mut encoder)
                                .await?;
                            debug!("Second handshake message sent");
                            state = transport_state;
                            break;
                        }
                        Err(Error::CodecError(codec_sv2::Error::MissingBytes(_))) => {
                            debug!("Waiting for more bytes during handshake");
                        }
                        Err(e) => {
                            error!("Handshake failed with downstream: {:?}", e);
                            return Err(e);
                        }
                    }
                }
            }
        };

        debug!("Handshake completed with state: {:?}", state);

        let (sender_incoming, receiver_incoming) = unbounded();
        let (sender_outgoing, receiver_outgoing) = unbounded();

        let conn_state = Arc::new(ConnectionState {
            sender_incoming,
            receiver_incoming: receiver_incoming.clone(),
            sender_outgoing: sender_outgoing.clone(),
            receiver_outgoing,
        });

        // Spawn Reader
        let read_state = state.clone();
        Self::spawn_reader(reader, read_state, address, conn_state.clone());

        // Spawn Writer
        let write_state = state;
        Self::spawn_writer(writer, write_state, address, conn_state);

        Ok((receiver_incoming, sender_outgoing))
    }

    fn spawn_reader<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
        mut reader: OwnedReadHalf,
        mut reader_state: State,
        address: std::net::SocketAddr,
        conn_state: Arc<ConnectionState<Message>>,
    ) -> task::JoinHandle<()> {
        let sender_incoming = conn_state.sender_incoming.clone();
        task::spawn(async move {
            let mut decoder = StandardNoiseDecoder::<Message>::new();
            loop {
                match receive_message(&mut reader, &mut reader_state, &mut decoder).await {
                    Ok(frame) => {
                        if sender_incoming.send(frame).await.is_err() {
                            error!("Shutting down reader for {}", address);
                            conn_state.close_all();
                            break;
                        }
                    }
                    Err(Error::CodecError(codec_sv2::Error::MissingBytes(_))) => {
                        debug!("Waiting for more bytes while reading stream");
                    }
                    Err(e) => {
                        error!("Reader shutting down due to error: {:?}", e);
                        conn_state.close_all();
                        break;
                    }
                }
            }
        })
    }

    fn spawn_writer<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
        mut writer: OwnedWriteHalf,
        mut write_state: State,
        address: std::net::SocketAddr,
        conn_state: Arc<ConnectionState<Message>>,
    ) -> task::JoinHandle<()> {
        let receiver_outgoing = conn_state.receiver_outgoing.clone();
        task::spawn(async move {
            let mut encoder = codec_sv2::NoiseEncoder::<Message>::new();
            while let Ok(frame) = receiver_outgoing.recv().await {
                if let Err(e) =
                    send_message(&mut writer, frame, &mut write_state, &mut encoder).await
                {
                    error!("Error while writing to client {}: {:?}", address, e);
                    let _ = writer.shutdown().await;
                    conn_state.close_all();
                    break;
                }
            }
            let _ = writer.shutdown().await;
            conn_state.close_all();
        })
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/network-helpers/src/plain_connection.rs">
use async_channel::{bounded, Receiver, Sender};
use codec_sv2::binary_sv2::{Deserialize, Serialize};
use core::convert::TryInto;
use tokio::{
    io::{AsyncReadExt, AsyncWriteExt},
    net::{TcpListener, TcpStream},
    task,
};

use codec_sv2::{binary_sv2::GetSize, Error::MissingBytes, StandardDecoder, StandardEitherFrame};
use tracing::{error, trace};

#[derive(Debug)]
pub struct PlainConnection {}

impl PlainConnection {
    ///
    ///
    /// # Arguments
    ///
    /// * `strict` - true - will disconnect a connection that sends a message that can't be
    ///   translated, false - will ignore messages that can't be translated
    #[allow(clippy::new_ret_no_self)]
    pub async fn new<'a, Message: Serialize + Deserialize<'a> + GetSize + Send + 'static>(
        stream: TcpStream,
    ) -> (
        Receiver<StandardEitherFrame<Message>>,
        Sender<StandardEitherFrame<Message>>,
    ) {
        const NOISE_HANDSHAKE_SIZE_HINT: usize = 3363412;

        let (mut reader, mut writer) = stream.into_split();

        let (sender_incoming, receiver_incoming): (
            Sender<StandardEitherFrame<Message>>,
            Receiver<StandardEitherFrame<Message>>,
        ) = bounded(10); // TODO caller should provide this param
        let (sender_outgoing, receiver_outgoing): (
            Sender<StandardEitherFrame<Message>>,
            Receiver<StandardEitherFrame<Message>>,
        ) = bounded(10); // TODO caller should provide this param

        // RECEIVE AND PARSE INCOMING MESSAGES FROM TCP STREAM
        task::spawn(async move {
            let mut decoder = StandardDecoder::<Message>::new();

            loop {
                let writable = decoder.writable();
                match reader.read_exact(writable).await {
                    Ok(_) => {
                        match decoder.next_frame() {
                            Ok(frame) => {
                                if let Err(e) = sender_incoming.send(frame.into()).await {
                                    error!("Failed to send incoming message: {}", e);
                                    task::yield_now().await;
                                    break;
                                }
                            }
                            Err(MissingBytes(size)) => {
                                // Only disconnect if we get noise handshake message - this
                                // shouldn't
                                // happen in plain_connection
                                if size == NOISE_HANDSHAKE_SIZE_HINT {
                                    error!("Got noise message on unencrypted connection - disconnecting");
                                    break;
                                } else {
                                    trace!("MissingBytes({}) on incoming message - ignoring", size);
                                }
                            }
                            Err(e) => {
                                error!("Failed to read from stream: {}", e);
                                sender_incoming.close();
                                task::yield_now().await;
                                break;
                            }
                        }
                    }
                    Err(e) => {
                        // Just fail and force to reinitialize everything
                        error!("Failed to read from stream: {}", e);
                        sender_incoming.close();
                        task::yield_now().await;
                        break;
                    }
                }
            }
        });

        // ENCODE AND SEND INCOMING MESSAGES TO TCP STREAM
        task::spawn(async move {
            let mut encoder = codec_sv2::Encoder::<Message>::new();

            loop {
                let received = receiver_outgoing.recv().await;
                match received {
                    Ok(frame) => {
                        let b = encoder.encode(frame.try_into().unwrap()).unwrap();

                        match (writer).write_all(b).await {
                            Ok(_) => (),
                            Err(_) => {
                                let _ = writer.shutdown().await;
                            }
                        }
                    }
                    Err(_) => {
                        // Just fail and force to reinitilize everything
                        let _ = writer.shutdown().await;
                        error!("Failed to read from stream - terminating connection");
                        task::yield_now().await;
                        break;
                    }
                };
            }
        });

        (receiver_incoming, sender_outgoing)
    }
}

pub async fn plain_listen(address: &str, sender: Sender<TcpStream>) {
    let listener = TcpListener::bind(address).await.unwrap();
    loop {
        if let Ok((stream, _)) = listener.accept().await {
            let _ = sender.send(stream).await;
        }
    }
}
pub async fn plain_connect(address: &str) -> Result<TcpStream, ()> {
    let stream = TcpStream::connect(address).await.map_err(|_| ())?;
    Ok(stream)
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/network-helpers/src/sv1_connection.rs">
use async_channel::{unbounded, Receiver, Sender};
use futures::StreamExt;
use sv1_api::json_rpc;
use tokio::{
    io::{AsyncWriteExt, BufReader, BufWriter},
    net::TcpStream,
};
use tokio_util::codec::{FramedRead, LinesCodec};

/// Represents a connection between two roles communicating using SV1 protocol.
///
/// This struct can be used to read and write messages to the other side of the connection.  The
/// channel is unidirectional, i.e., each [`ConnectionSV1`] instance handles the connection either
/// from the upstream perspective or the downstream perspective. In order to communicate in both
/// directions, you will need two instances of this struct.
#[derive(Debug)]
pub struct ConnectionSV1 {
    receiver: Receiver<json_rpc::Message>,
    sender: Sender<json_rpc::Message>,
}

struct ConnectionState {
    receiver_outgoing: Receiver<json_rpc::Message>,
    sender_outgoing: Sender<json_rpc::Message>,
    receiver_incoming: Receiver<json_rpc::Message>,
    sender_incoming: Sender<json_rpc::Message>,
}

impl ConnectionState {
    fn new(
        receiver_outgoing: Receiver<json_rpc::Message>,
        sender_outgoing: Sender<json_rpc::Message>,
        receiver_incoming: Receiver<json_rpc::Message>,
        sender_incoming: Sender<json_rpc::Message>,
    ) -> Self {
        Self {
            receiver_incoming,
            receiver_outgoing,
            sender_incoming,
            sender_outgoing,
        }
    }

    fn close(&self) {
        self.receiver_incoming.close();
        self.receiver_outgoing.close();
        self.sender_incoming.close();
        self.sender_outgoing.close();
    }
}

const MAX_LINE_LENGTH: usize = 1 << 16;

impl ConnectionSV1 {
    pub async fn new(stream: TcpStream) -> Self {
        let (read_half, write_half) = stream.into_split();
        let (sender_incoming, receiver_incoming) = unbounded();
        let (sender_outgoing, receiver_outgoing) = unbounded();

        let buffer_read_half = BufReader::new(read_half);
        let buffer_write_half = BufWriter::new(write_half);

        let connection_state = ConnectionState::new(
            receiver_outgoing.clone(),
            sender_outgoing.clone(),
            receiver_incoming.clone(),
            sender_incoming.clone(),
        );

        tokio::spawn(async move {
            tokio::select! {
                _ = Self::run_reader(buffer_read_half, sender_incoming.clone()) => {
                    tracing::info!("Reader task exited. Closing writer sender.");
                    connection_state.close();
                }
                _ = Self::run_writer(buffer_write_half, receiver_outgoing.clone()) => {
                    tracing::info!("Writer task exited.Closing reader sender.");
                    connection_state.close();
                }
            }
        });

        Self {
            receiver: receiver_incoming,
            sender: sender_outgoing,
        }
    }

    async fn run_reader(
        reader: BufReader<tokio::net::tcp::OwnedReadHalf>,
        sender: Sender<json_rpc::Message>,
    ) {
        let mut lines = FramedRead::new(reader, LinesCodec::new_with_max_length(MAX_LINE_LENGTH));
        while let Some(result) = lines.next().await {
            match result {
                Ok(line) => match serde_json::from_str::<json_rpc::Message>(&line) {
                    Ok(msg) => {
                        if sender.send(msg).await.is_err() {
                            tracing::warn!("Receiver dropped, stopping reader");
                            break;
                        }
                    }
                    Err(e) => {
                        tracing::error!("Failed to deserialize message: {e:?}");
                    }
                },
                Err(e) => {
                    tracing::error!("Error reading from stream: {e:?}");
                    break;
                }
            }
        }
    }

    async fn run_writer(
        mut writer: BufWriter<tokio::net::tcp::OwnedWriteHalf>,
        receiver: Receiver<json_rpc::Message>,
    ) {
        while let Ok(msg) = receiver.recv().await {
            match serde_json::to_string(&msg) {
                Ok(line) => {
                    let data = format!("{line}\n");
                    if writer.write_all(data.as_bytes()).await.is_err() {
                        tracing::error!("Failed to write to stream");
                        break;
                    }
                    if writer.flush().await.is_err() {
                        tracing::error!("Failed to flush writer.");
                        break;
                    }
                }
                Err(e) => {
                    tracing::error!("Failed to serialize message: {e:?}");
                    break;
                }
            }
        }
    }

    /// Send a message to the other side of the connection.
    pub async fn send(&self, msg: json_rpc::Message) -> bool {
        self.sender.send(msg).await.is_ok()
    }

    /// Receive a message from the other side of the connection.
    pub async fn receive(&self) -> Option<json_rpc::Message> {
        self.receiver.recv().await.ok()
    }

    /// Get a clone of the receiver channel.
    pub fn receiver(&self) -> Receiver<json_rpc::Message> {
        self.receiver.clone()
    }

    /// Get a clone of the sender channel.
    pub fn sender(&self) -> Sender<json_rpc::Message> {
        self.sender.clone()
    }
}

#[cfg(test)]
mod tests {
    use tokio::net::TcpListener;

    use super::*;

    #[tokio::test]
    async fn test_sv1_connection() {
        let listener = TcpListener::bind("127.0.0.1:0").await.unwrap();
        let addr = listener.local_addr().unwrap();
        let downstream_stream = TcpStream::connect(addr).await.unwrap();
        let (upstream_stream, _) = listener.accept().await.unwrap();

        let upstream_connection = ConnectionSV1::new(upstream_stream).await;
        let downstream_connection = ConnectionSV1::new(downstream_stream).await;
        let message = json_rpc::Message::StandardRequest(json_rpc::StandardRequest {
            id: 1,
            method: "test".to_string(),
            params: serde_json::Value::Null,
        });
        assert!(downstream_connection.send(message).await);
        let received_on_upstream = upstream_connection.receive().await.unwrap();
        match received_on_upstream {
            json_rpc::Message::StandardRequest(received) => {
                assert_eq!(received.id, 1);
                assert_eq!(received.method, "test".to_string());
                assert_eq!(received.params, serde_json::Value::Null);
            }
            _ => {
                panic!("Unexpected message type");
            }
        }
        let upstream_response = json_rpc::Message::OkResponse(json_rpc::Response {
            id: 1,
            result: serde_json::Value::String("response".to_string()),
            error: None,
        });
        assert!(upstream_connection.send(upstream_response).await);
        let received_upstream = downstream_connection.receive().await.unwrap();
        match received_upstream {
            json_rpc::Message::OkResponse(received) => {
                assert_eq!(received.id, 1);
                assert_eq!(
                    received.result,
                    serde_json::Value::String("response".to_string())
                );
            }
            _ => {
                panic!("Unexpected message type");
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/roles-utils/rpc/Cargo.toml">
[package]
name = "rpc_sv2"
version = "1.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "SV2 JD Server RPC"
documentation = "https://docs.rs/rpc_sv2"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
stratum-common = { path = "../../../common", version = "3.0.0" }
serde = { version = "1.0.89", features = ["derive", "alloc"], default-features = false }
serde_json = { version = "1.0", default-features = false, features = ["alloc","raw_value"] }
hex = "0.4.3"
base64 = "0.21.5"
hyper = { version = "1.1.0", features = ["full"] }
hyper-util = { version = "0.1", features = ["full"] }
http-body-util = "0.1"
</file>

<file path="stratum-1.4.0/roles/roles-utils/rpc/src/lib.rs">
pub mod mini_rpc_client;
use serde::{Deserialize, Serialize};

#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct Hash([u8; 32]);

#[allow(dead_code)]
#[derive(Clone, Deserialize)]
pub struct Amount(f64);

#[derive(Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct BlockHash(Hash);

pub use hyper::Uri;
</file>

<file path="stratum-1.4.0/roles/roles-utils/rpc/src/mini_rpc_client.rs">
// TODO
//  - manage id in RpcResult messages
use base64::Engine;
use hex::decode;
use http_body_util::{BodyExt, Full};
use hyper::{
    body::Bytes,
    header::{AUTHORIZATION, CONTENT_TYPE},
    Request,
};
use hyper_util::{
    client::legacy::{connect::HttpConnector, Client},
    rt::TokioExecutor,
};
use serde::{Deserialize, Serialize};
use serde_json::json;
use stratum_common::roles_logic_sv2::bitcoin::{
    consensus::encode::deserialize as consensus_decode, Transaction,
};

use super::BlockHash;

#[derive(Clone, Debug)]
pub struct MiniRpcClient {
    client: Client<HttpConnector, Full<Bytes>>,
    url: hyper::Uri,
    auth: Auth,
}

impl MiniRpcClient {
    pub fn new(url: hyper::Uri, auth: Auth) -> MiniRpcClient {
        let client: Client<_, Full<Bytes>> = Client::builder(TokioExecutor::new()).build_http();
        MiniRpcClient { client, url, auth }
    }

    pub async fn get_raw_transaction(
        &self,
        txid: &String,
        block_hash: Option<&BlockHash>,
    ) -> Result<Transaction, RpcError> {
        let response = match block_hash {
            Some(hash) => {
                self.send_json_rpc_request("getrawtransaction", json!([txid, false, hash]))
            }
            None => self.send_json_rpc_request("getrawtransaction", json!([txid, false])),
        }
        .await;
        match response {
            Ok(result_hex) => {
                let result_deserialized: JsonRpcResult<String> = serde_json::from_str(&result_hex)
                    .map_err(|e| {
                        RpcError::Deserialization(e.to_string()) // TODO manage message ids
                    })?;
                let transaction_hex: String = result_deserialized
                    .result
                    .ok_or_else(|| RpcError::Other("Result not found".to_string()))?;
                let transaction_bytes = decode(transaction_hex).expect("Decoding failed");
                Ok(consensus_decode(&transaction_bytes).expect("Deserialization failed"))
            }
            Err(error) => Err(error),
        }
    }

    pub async fn get_raw_mempool(&self) -> Result<Vec<String>, RpcError> {
        let response = self.send_json_rpc_request("getrawmempool", json!([])).await;
        match response {
            Ok(result_hex) => {
                let result_deserialized: JsonRpcResult<Vec<String>> =
                    serde_json::from_str(&result_hex).map_err(|e| {
                        RpcError::Deserialization(e.to_string()) // TODO manage message ids
                    })?;
                let mempool: Vec<String> = result_deserialized
                    .result
                    .ok_or_else(|| RpcError::Other("Result not found".to_string()))?;
                Ok(mempool)
            }
            Err(error) => Err(error),
        }
    }

    pub async fn submit_block(&self, block_hex: String) -> Result<(), RpcError> {
        let response = self
            .send_json_rpc_request("submitblock", json!([block_hex]))
            .await;

        match response {
            Ok(_) => Ok(()),
            Err(error) => Err(error),
        }
    }

    /// Checks the health of the RPC connection by sending a request to the blockchain info
    /// endpoint
    pub async fn health(&self) -> Result<(), RpcError> {
        let response = self
            .send_json_rpc_request("getblockchaininfo", json!([]))
            .await;
        match response {
            Ok(_) => Ok(()),
            Err(error) => Err(error),
        }
    }

    async fn send_json_rpc_request(
        &self,
        method: &str,
        params: serde_json::Value,
    ) -> Result<String, RpcError> {
        let client = &self.client;
        let (username, password) = self.auth.clone().get_user_pass();
        let request = JsonRpcRequest {
            jsonrpc: "2.0".to_string(),
            method: method.to_string(),
            params,
            id: 1, //TODO manage message ids
        };

        let request_body = match serde_json::to_string(&request) {
            Ok(body) => body,
            Err(e) => return Err(RpcError::Serialization(e.to_string())),
        };

        let req = Request::builder()
            .method("POST")
            .uri(self.url.clone())
            .header(CONTENT_TYPE, "application/json")
            .header(
                AUTHORIZATION,
                format!(
                    "Basic {}",
                    base64::engine::general_purpose::STANDARD
                        .encode(format!("{username}:{password}"))
                ),
            )
            .body(Full::<Bytes>::from(request_body))
            .map_err(|e| RpcError::Http(e.to_string()))?;

        let response = client
            .request(req)
            .await
            .map_err(|e| RpcError::Http(e.to_string()))?;

        let status = response.status();
        let body = response
            .into_body()
            .collect()
            .await
            .map_err(|e| RpcError::Http(e.to_string()))?
            .to_bytes()
            .to_vec();

        if status.is_success() {
            String::from_utf8(body).map_err(|e| {
                RpcError::Deserialization(e.to_string()) // TODO manage message ids
            })
        } else {
            let error_result: Result<JsonRpcResult<_>, _> = serde_json::from_slice(&body);
            match error_result {
                Ok(error_response) => Err(error_response.into()),
                Err(e) => Err(RpcError::Deserialization(e.to_string())),
            }
        }
    }
}

#[derive(Clone, Debug)]
pub struct Auth {
    username: String,
    password: String,
}

impl Auth {
    pub fn get_user_pass(self) -> (String, String) {
        (self.username, self.password)
    }
    pub fn new(username: String, password: String) -> Auth {
        Auth { username, password }
    }
}

#[derive(Debug, Serialize)]
struct JsonRpcRequest {
    jsonrpc: String,
    method: String,
    params: serde_json::Value,
    id: u64,
}

#[derive(Debug, Deserialize)]
pub struct JsonRpcResult<T> {
    result: Option<T>,
    pub error: Option<JsonRpcError>,
    pub id: u64,
}

#[derive(Debug, Deserialize, Clone)]
pub struct JsonRpcError {
    pub code: i32,
    pub message: String,
}

#[derive(Debug, Deserialize)]
pub enum RpcError {
    // TODO this type is slightly incorrect, as the JsonRpcError evaluates a generic that is meant
    // for the result field of JsonRpcResult struct. This should be corrected
    JsonRpc(JsonRpcResult<JsonRpcError>),
    Deserialization(String),
    Serialization(String),
    Http(String),
    Other(String),
}

impl From<JsonRpcResult<JsonRpcError>> for RpcError {
    fn from(error: JsonRpcResult<JsonRpcError>) -> Self {
        Self::JsonRpc(error)
    }
}
</file>

<file path="stratum-1.4.0/roles/tarpaulin.toml">
[default]
features = "with_buffer_pool default"
run-types = [ "Lib" ]
timeout = "120s"
fail-under = 0

[report]
out = ["Xml"]
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/Cargo.toml">
[package]
name = "mining_device_sv1"
version = "0.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
publish = false
documentation = "https://github.com/stratum-mining/stratum"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
name = "mining_device_sv1"
path = "src/lib.rs"

[dependencies]
stratum-common = { path = "../../../common" }    
async-channel = "1.5.1"
serde = { version = "1.0.89", default-features = false, features = ["derive", "alloc"] }
serde_json = { version = "1.0.64", default-features = false, features = ["alloc"] }
v1 = { path="../../../protocols/v1", package="sv1_api" }
num-bigint = "0.4.3"
num-traits = "0.2.15"
tracing = "0.1.41"
tracing-subscriber = "0.3.19"
tokio = { version = "1.44.1", features = ["full"] }
primitive-types = "0.13.1"
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/src/client.rs">
use crate::{job::Job, miner::Miner};
use async_channel::{unbounded, Receiver, Sender};
use num_bigint::BigUint;
use num_traits::FromPrimitive;
use primitive_types::U256;
use std::{
    convert::TryInto,
    net::SocketAddr,
    ops::Div,
    sync::Arc,
    time::{self, Duration},
};
use stratum_common::roles_logic_sv2::utils::Mutex;
use tokio::{
    io::{AsyncBufReadExt, AsyncWriteExt, BufReader},
    net::TcpStream,
    task,
    time::sleep,
};
use tracing::{error, info, warn};
use v1::{
    client_to_server,
    error::Error,
    json_rpc, server_to_client,
    utils::{Extranonce, HexU32Be},
    ClientStatus, IsClient,
};

/// Represents the Mining Device client which is connected to a Upstream node (either a SV1 Pool
/// server or a SV1 <-> SV2 Translator Proxy server).
#[derive(Debug, Clone)]
pub struct Client {
    client_id: u32,
    extranonce1: Option<Extranonce<'static>>,
    extranonce2_size: Option<usize>,
    version_rolling_mask: Option<HexU32Be>,
    version_rolling_min_bit: Option<HexU32Be>,
    pub(crate) status: ClientStatus,
    sented_authorize_request: Vec<(u64, String)>, // (id, user_name)
    authorized: Vec<String>,
    /// Receives incoming messages from the SV1 Upstream node.
    receiver_incoming: Receiver<String>,
    /// Sends outgoing messages to the SV1 Upstream node.
    sender_outgoing: Sender<String>,
    /// Representation of the Mining Devices
    miner: Arc<Mutex<Miner>>,
}

impl Client {
    /// Outgoing channels are used to send messages to the Upstream
    /// Incoming channels are used to receive messages from the Upstream
    /// There are three separate channels, the first two are responsible for receiving and sending
    /// messages to the Upstream, and the third is responsible for pass valid job submissions to
    /// the first set of channels:
    /// 1. `(sender_incoming, receiver_incoming)`: `sender_incoming` listens on the socket where
    ///    messages are being sent from the Upstream node. From the socket, it reads the incoming
    ///    bytes from the Upstream into a `BufReader`. The incoming bytes represent a message from
    ///    the Upstream, and each new line is a new message. When it gets this line (a message) from
    ///    the Upstream, it sends them to the `receiver_incoming` which is listening in a loop. The
    ///    message line received by the `receiver_incoming` are then parsed by the `Client` in the
    ///    `parse_message` method to be handled.
    /// 2. `(sender_outgoing, receiver_outgoing)`: When the `parse_message` method on the `Client`
    ///    is called, it handles the message and formats the a new message to be sent to the
    ///    Upstream in response. It sends the response message via the `sender_outgoing` to the
    ///    `receiver_outgoing` which is waiting to receive a message in its own task. When the
    ///    `receiver_outgoing` receives the response message from the the `sender_outgoing`, it
    ///    writes this message to the socket connected to the Upstream via `write_all`.
    /// 3. `(sender_share, receiver_share)`: A new thread is spawned to mock the act of a Miner
    ///    hashing over a candidate block without blocking the rest of the program. Since this in
    ///    its own thread, we need a channel to communicate with it, which is `(sender_share,
    ///    receiver_share)`. In this thread, on each new share, `sender_share` sends the pertinent
    ///    information to create a `mining.submit` message to the `receiver_share` that is waiting
    ///    to receive this information in a separate task. In this task, once `receiver_share` gets
    ///    the information from `sender_share`, it is formatted as a `v1::client_to_server::Submit`
    ///    and then serialized into a json message that is sent to the Upstream via
    ///    `sender_outgoing`.
    pub async fn connect(
        client_id: u32,
        upstream_addr: SocketAddr,
        single_submit: bool,
        custom_target: Option<[u8; 32]>,
    ) {
        let stream = loop {
            if let Ok(stream) = TcpStream::connect(upstream_addr).await {
                break stream;
            }
            info!(
                "SV1 Miner: Failed to connect to upstream at {} Retrying in 1 second.",
                upstream_addr
            );
            sleep(Duration::from_secs(1)).await;
        };
        let (reader, mut writer) = stream.into_split();

        // `sender_incoming` listens on socket for incoming messages from the Upstream and sends
        // messages to the `receiver_incoming` to be parsed and handled by the `Client`
        let (sender_incoming, receiver_incoming) = unbounded();
        // `sender_outgoing` sends the message parsed by the `Client` to the `receiver_outgoing`
        // which writes the messages to the socket to the Upstream
        let (sender_outgoing, receiver_outgoing) = unbounded();
        // `sender_share` sends job share results to the `receiver_share` where the job share
        // results are formated into a "mining.submit" messages that is then sent to the
        // Upstream via `sender_outgoing`
        let (sender_share, receiver_share) = unbounded();

        let (send_stop_submitting, mut recv_stop_submitting) = tokio::sync::watch::channel(false);
        // Instantiates a new `Miner` (a mock of an actual Mining Device) with a job id of 0.
        let miner = Arc::new(Mutex::new(Miner::new(0)));

        // Sets an initial target for the `Miner`.
        // TODO: This is hard coded for the purposes of a demo, should be set by the SV1
        // `mining.set_difficulty` message received from the Upstream role
        let target_vec: [u8; 32] = custom_target.unwrap_or([
            0, 0, 0, 0, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0, 0, 0, 0, 0, 0,
        ]);
        let default_target = U256::from_big_endian(target_vec.as_ref());
        miner.safe_lock(|m| m.new_target(default_target)).unwrap();

        let miner_cloned = miner.clone();

        // Reads messages sent by the Upstream from the socket to be passed to the
        // `receiver_incoming`
        task::spawn(async move {
            tokio::select!(
                _ = tokio::signal::ctrl_c() => { },
                _ = async {
                    let mut messages = BufReader::new(reader).lines();
                    while let Ok(message) = messages.next_line().await {
                        match message {
                            Some(msg) => {
                                if let Err(e) = sender_incoming.send(msg).await {
                                    error!("Failed to send message to receiver_incoming: {:?}", e);
                                    break; // Exit the loop if sending fails
                                }
                            }
                            None => {
                                error!("Error reading from socket");
                                break; // Exit the loop on read failure
                            }
                        }
                    }
                    error!("Reader task terminated.");
                } => {}
            )
        });

        // Waits to receive a message from `sender_outgoing` and writes it to the socket for the
        // Upstream to receive
        task::spawn(async move {
            tokio::select!(
              _ = tokio::signal::ctrl_c() => { },
              _ = async {
                  loop {
                      let message: String = receiver_outgoing.recv().await.expect("SV1 Miner: Failed to receive message");
                      (writer).write_all(message.as_bytes()).await.expect("SV1 Miner: Failed to write message to socket");
                      if message.contains("mining.submit") && single_submit {
                          send_stop_submitting.send(true).expect("SV1 Miner: Failed to send stop submitting");
                      }
                  }
              } => {}
            )
        });

        // Clone the sender to the Upstream node to use it in another task below as
        // `sender_outgoing` is consumed by the initialization of `Client`
        let sender_outgoing_clone = sender_outgoing.clone();

        // Initialize Client
        let client = Arc::new(Mutex::new(Client {
            client_id,
            extranonce1: None,
            extranonce2_size: None,
            version_rolling_mask: None,
            version_rolling_min_bit: None,
            status: ClientStatus::Init,
            sented_authorize_request: vec![],
            authorized: vec![],
            receiver_incoming,
            sender_outgoing,
            miner,
        }));

        // configure subscribe and authorize
        Self::send_configure(client.clone()).await;

        // Gets the latest candidate block header hash from the `Miner` by calling the `next_share`
        // method. Mocks the act of the `Miner` incrementing the nonce. Performs this in a loop,
        // incrementing the nonce each time, to mimic a Mining Device generating continuous hashes.
        // For each generated block header, sends to the `receiver_share` the relevant values that
        // generated the candidate block header needed to then format and send as a "mining.submit"
        // message to the Upstream node.
        // Is a separate thread as it can be CPU intensive and we do not want to block the reading
        // and writing of messages to the socket.
        std::thread::spawn(move || loop {
            if miner_cloned.safe_lock(|m| m.next_share()).unwrap().is_ok() {
                let nonce = miner_cloned.safe_lock(|m| m.header.unwrap().nonce).unwrap();
                let time = miner_cloned.safe_lock(|m| m.header.unwrap().time).unwrap();
                let job_id = miner_cloned.safe_lock(|m| m.job_id).unwrap();
                let version = miner_cloned.safe_lock(|m| m.version).unwrap();
                // Sends relevant candidate block header values needed to construct a
                // `mining.submit` message to the `receiver_share` in the task that is responsible
                // for sending messages to the Upstream node.
                if sender_share
                    .try_send((nonce, job_id.unwrap(), version.unwrap(), time))
                    .is_err()
                {
                    warn!("Share channel is not available");
                    break;
                }
                // Introduce a delay of 0.2 seconds after sending a share
                std::thread::sleep(Duration::from_millis(200));
            }
            miner_cloned
                .safe_lock(|m| m.header.as_mut().map(|h| h.nonce += 1))
                .unwrap();
        });
        // Task to receive relevant candidate block header values needed to construct a
        // `mining.submit` message. This message is contructed as a `client_to_server::Submit` and
        // then serialized into json to be sent to the Upstream via the `sender_outgoing` sender.
        let cloned = client.clone();
        task::spawn(async move {
            tokio::select!(
              _ = recv_stop_submitting.changed() => {
                warn!("Stopping miner")
              },
              _ = tokio::signal::ctrl_c() => {
                  info!("Stopping miner");
              },
              _ = async {
              let recv = receiver_share.clone();
              loop {
                  let (nonce, job_id, _version, ntime) = recv.recv().await.unwrap();
                  if cloned.clone().safe_lock(|c| c.status).unwrap() != ClientStatus::Subscribed {
                      continue;
                  }
                  let extra_nonce2: Extranonce =
                      vec![0; cloned.safe_lock(|c| c.extranonce2_size.unwrap()).unwrap()]
                          .try_into()
                          .unwrap();
                  let submit = client_to_server::Submit {
                      id: 0,
                      user_name: "user".into(), // TODO: user name should NOT be hardcoded
                      job_id: job_id.to_string(),
                      extra_nonce2,
                      time: HexU32Be(ntime),
                      nonce: HexU32Be(nonce),
                      version_bits: None,
                  };
                  let message: json_rpc::Message = submit.into();
                  let message = format!("{}\n", serde_json::to_string(&message).unwrap());
                  sender_outgoing_clone.send(message).await.unwrap();
              }
              } => {}
            )
        });
        let recv_incoming = client.safe_lock(|c| c.receiver_incoming.clone()).unwrap();

        loop {
            match client.clone().safe_lock(|c| c.status).unwrap() {
                ClientStatus::Init => panic!("impossible state"),
                ClientStatus::Configured => {
                    let incoming = recv_incoming.clone().recv().await.unwrap();
                    Self::parse_message(client.clone(), Ok(incoming)).await;
                }
                ClientStatus::Subscribed => {
                    Self::send_authorize(client.clone()).await;
                    break;
                }
            }
        }
        // Waits for the `sender_incoming` to get message line from socket to be parsed by the
        // `Client`
        tokio::select!(
            _ = tokio::signal::ctrl_c() => {
                warn!("Stopping sv1 miner");
            },
            _ = async {
                loop {
                    if let Ok(incoming) = recv_incoming.clone().recv().await {
                        Self::parse_message(client.clone(), Ok(incoming)).await;
                    } else {
                        warn!("Error reading from socket via `recv_incoming` channel");
                        break;
                    }
                }
            } => {}
        );
    }

    /// Parse SV1 messages received from the Upstream node.
    async fn parse_message(
        self_: Arc<Mutex<Self>>,
        incoming_message: Result<String, async_channel::TryRecvError>,
    ) {
        // If we have a line (1 line represents 1 sv1 incoming message), then handle that message
        if let Ok(line) = incoming_message {
            info!(
                "CLIENT {} - Received: {}",
                self_.safe_lock(|s| s.client_id).unwrap(),
                line
            );
            let message: json_rpc::Message = serde_json::from_str(&line).unwrap();
            // If has a message, it sends it back
            if let Some(m) = self_
                .safe_lock(|s| s.handle_message(message).unwrap())
                .unwrap()
            {
                let sender = self_.safe_lock(|s| s.sender_outgoing.clone()).unwrap();
                Self::send_message(sender, m).await;
            }
        };
    }

    /// Send SV1 messages to the receiver_outgoing which writes to the socket (aka Upstream node)
    async fn send_message(sender: Sender<String>, msg: json_rpc::Message) {
        let msg = format!("{}\n", serde_json::to_string(&msg).unwrap());
        info!(" - Send: {}", &msg);
        sender.send(msg).await.unwrap();
    }

    pub(crate) async fn send_configure(self_: Arc<Mutex<Self>>) {
        // This loop is probably unnecessary as the first state is `Init`
        loop {
            if let ClientStatus::Init = self_.safe_lock(|s| s.status).unwrap() {
                break;
            }
        }
        let id = time::SystemTime::now()
            .duration_since(time::SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let configure = self_.safe_lock(|s| s.configure(id)).unwrap();
        let sender = self_.safe_lock(|s| s.sender_outgoing.clone()).unwrap();
        Self::send_message(sender, configure).await;
        // Update status as configured
        self_
            .safe_lock(|s| s.status = ClientStatus::Configured)
            .unwrap();
    }

    pub async fn send_authorize(self_: Arc<Mutex<Self>>) {
        let id = time::SystemTime::now()
            .duration_since(time::SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let authorize = self_
            .safe_lock(|s| {
                s.authorize(id, "user".to_string(), "password".to_string())
                    .unwrap()
            })
            .unwrap();
        self_
            .safe_lock(|s| s.sented_authorize_request.push((id, "user".to_string())))
            .unwrap();
        let sender = self_.safe_lock(|s| s.sender_outgoing.clone()).unwrap();

        Self::send_message(sender, authorize).await;
    }
}

impl IsClient<'static> for Client {
    /// Updates miner with new job
    fn handle_notify(
        &mut self,
        notify: server_to_client::Notify<'static>,
    ) -> Result<(), Error<'static>> {
        let mut extranonce: Vec<u8> = self.extranonce1.clone().unwrap().into();
        for _ in 0..self.extranonce2_size.unwrap() {
            extranonce.push(0)
        }

        let new_job = Job::from_notify(notify, extranonce);
        self.miner.safe_lock(|m| m.new_header(new_job)).unwrap();
        Ok(())
    }

    fn handle_configure(
        &mut self,
        _conf: &mut server_to_client::Configure,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn handle_subscribe(
        &mut self,
        _subscribe: &server_to_client::Subscribe,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn set_extranonce1(&mut self, extranonce1: Extranonce<'static>) {
        self.extranonce1 = Some(extranonce1);
    }

    fn extranonce1(&self) -> Extranonce<'static> {
        self.extranonce1.clone().unwrap()
    }

    fn set_extranonce2_size(&mut self, extra_nonce2_size: usize) {
        self.extranonce2_size = Some(extra_nonce2_size);
    }

    fn extranonce2_size(&self) -> usize {
        self.extranonce2_size.unwrap()
    }

    fn version_rolling_mask(&self) -> Option<HexU32Be> {
        self.version_rolling_mask.clone()
    }

    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_mask = mask;
    }

    fn set_version_rolling_min_bit(&mut self, min: Option<HexU32Be>) {
        self.version_rolling_min_bit = min;
    }

    fn set_status(&mut self, status: ClientStatus) {
        self.status = status;
    }

    fn signature(&self) -> String {
        format!("{}", self.client_id)
    }

    fn status(&self) -> ClientStatus {
        self.status
    }

    fn version_rolling_min_bit(&mut self) -> Option<HexU32Be> {
        self.version_rolling_min_bit.clone()
    }

    fn id_is_authorize(&mut self, id: &u64) -> Option<String> {
        let req: Vec<&(u64, String)> = self
            .sented_authorize_request
            .iter()
            .filter(|x| x.0 == *id)
            .collect();
        match req.len() {
            0 => None,
            _ => Some(req[0].1.clone()),
        }
    }

    fn id_is_submit(&mut self, _: &u64) -> bool {
        false
    }

    fn authorize_user_name(&mut self, name: String) {
        self.authorized.push(name)
    }

    fn is_authorized(&self, name: &String) -> bool {
        self.authorized.contains(name)
    }

    fn authorize(
        &mut self,
        id: u64,
        name: String,
        password: String,
    ) -> Result<json_rpc::Message, Error> {
        match self.status() {
            ClientStatus::Init => Err(Error::IncorrectClientStatus("mining.authorize".to_string())),
            _ => {
                self.sented_authorize_request.push((id, "user".to_string()));
                Ok(client_to_server::Authorize { id, name, password }.into())
            }
        }
    }

    fn last_notify(&self) -> Option<server_to_client::Notify> {
        None
    }

    fn handle_error_message(
        &mut self,
        _message: v1::Message,
    ) -> Result<Option<json_rpc::Message>, Error<'static>> {
        Ok(None)
    }

    fn handle_set_difficulty(
        &mut self,
        conf: &mut server_to_client::SetDifficulty,
    ) -> Result<(), Error<'static>> {
        let dif = conf.value;
        let target =
            target_from_difficulty(dif).unwrap_or_else(|| panic!("Invalid difficulty: {}", dif));
        self.miner.safe_lock(|m| m.target = Some(target)).unwrap();
        Ok(())
    }

    fn handle_set_extranonce(
        &mut self,
        _conf: &mut server_to_client::SetExtranonce,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }

    fn handle_set_version_mask(
        &mut self,
        _conf: &mut server_to_client::SetVersionMask,
    ) -> Result<(), Error<'static>> {
        Ok(())
    }
}

fn target_from_difficulty(diff: f64) -> Option<U256> {
    let pdiff = 26959946667150639794667015087019630673637144422540572481103610249215.0;
    if diff == 0.0 {
        Some(U256::from_big_endian(&[0; 32]))
    } else {
        let t = pdiff.div(diff);
        let as_big_int: BigUint = match t > 0.0 {
            true => BigUint::from_f64(t)?,
            false => BigUint::from_f64(1.0 / t)?,
        };
        let mut bytes = as_big_int.to_bytes_be();
        if bytes.len() > 32 {
            None
        } else {
            let mut front_padding = vec![0; 32 - bytes.len()];
            front_padding.append(&mut bytes);
            let as_u256: [u8; 32] = front_padding.try_into().unwrap();
            Some(U256::from_big_endian(as_u256.as_ref()))
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/src/job.rs">
use std::convert::TryInto;
use stratum_common::roles_logic_sv2;
use v1::server_to_client;

/// Represents a new Job built from an incoming `mining.notify` message from the Upstream server.
pub(crate) struct Job {
    /// ID of the job used while submitting share generated from this job.
    /// TODO: Currently is `u32` and is hardcoded, but should be String and set by the incoming
    /// `mining.notify` message.
    pub(crate) job_id: u32,
    /// Hash of previous block
    pub(crate) prev_hash: [u8; 32],
    /// Merkle root
    /// TODO: Currently is hardcoded. This field should be replaced with three fields: 1)
    /// `coinbase_1` - the first half of the coinbase transaction before the `extranonce` which is
    /// inserted by the miner, 2) `coinbase_2` - the second half of the coinbase transaction after
    /// the `extranonce` which is inserted by the miner, and 3) `merkle_branches` - the merkle
    /// branches to build the merkle root sans the coinbase transaction
    // coinbase_1: Vec<u32>,
    // coinbase_2: Vec<u32>,
    // merkle_brances: Vec<[u8; 32]>,
    pub(crate) merkle_root: [u8; 32],
    pub(crate) version: u32,
    pub(crate) nbits: u32,
}

impl Job {
    pub fn from_notify(notify_msg: server_to_client::Notify<'_>, extranonce: Vec<u8>) -> Self {
        let job_id = notify_msg
            .job_id
            .parse::<u32>()
            .expect("expect valid job_id on String");

        // Convert prev hash from Vec<u8> into expected [u32; 8]
        let prev_hash_vec: Vec<u8> = notify_msg.prev_hash.into();
        let prev_hash_slice: &[u8] = prev_hash_vec.as_slice();
        let prev_hash: &[u8; 32] = prev_hash_slice.try_into().expect("Expected len 32");
        let prev_hash = *prev_hash;

        let coinbase_tx_prefix: Vec<u8> = notify_msg.coin_base1.into();
        let coinbase_tx_suffix: Vec<u8> = notify_msg.coin_base2.into();
        let path: Vec<Vec<u8>> = notify_msg
            .merkle_branch
            .into_iter()
            .map(|node| node.into())
            .collect();

        let merkle_root = roles_logic_sv2::utils::merkle_root_from_path(
            &coinbase_tx_prefix,
            &coinbase_tx_suffix,
            &extranonce,
            &path,
        )
        .unwrap();
        let merkle_root: [u8; 32] = merkle_root.try_into().unwrap();

        Job {
            job_id,
            prev_hash,
            nbits: notify_msg.bits.0,
            version: notify_msg.version.0,
            merkle_root,
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/src/lib.rs">
pub mod client;
pub mod job;
pub mod miner;
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/src/main.rs">
pub(crate) mod client;
pub(crate) mod job;
pub(crate) mod miner;
use std::{net::SocketAddr, str::FromStr};

pub(crate) use client::Client;

#[tokio::main]
async fn main() {
    tracing_subscriber::fmt().init();

    const ADDR: &str = "127.0.0.1:34255";
    Client::connect(
        80,
        SocketAddr::from_str(ADDR).expect("Invalid upstream address"),
        false,
        None,
    )
    .await
}
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device-sv1/src/miner.rs">
use crate::job::Job;
use primitive_types::U256;
use std::convert::TryInto;
use stratum_common::roles_logic_sv2::bitcoin::{
    blockdata::block::{Header, Version},
    hash_types::{BlockHash, TxMerkleNode},
    hashes::{sha256d::Hash as DHash, Hash},
    CompactTarget,
};
use tracing::info;

/// A mock representation of a Mining Device that produces block header hashes to be submitted by
/// the `Client` to the Upstream node (either a SV1 Pool server or a SV1 <-> SV2 Translator Proxy
/// server).
#[derive(Debug)]
pub(crate) struct Miner {
    /// Mock of mined candidate block header.
    pub(crate) header: Option<Header>,
    /// Current mining target.
    pub(crate) target: Option<U256>,
    /// ID of the job used while submitting share generated from this job.
    pub(crate) job_id: Option<u32>,
    /// Block header version
    pub(crate) version: Option<u32>,
    /// TODO: RRQ: Remove?
    pub(crate) _handicap: u32,
}

impl Miner {
    /// Instantiates a new Miner instance.
    pub(crate) fn new(handicap: u32) -> Self {
        Self {
            target: None,
            header: None,
            job_id: None,
            version: None,
            _handicap: handicap,
        }
    }

    /// Updates target when a new target is received by the SV1 `Client`.
    pub(crate) fn new_target(&mut self, target: U256) {
        self.target = Some(target);
    }

    /// Mocks out the mining of a new candidate block header.
    /// `Client` calls `new_header` when it receives a new `mining.notify` message from the
    /// Upstream node indicating the `Miner` should start mining on a new job.
    pub(crate) fn new_header(&mut self, new_job: Job) {
        self.job_id = Some(new_job.job_id);
        self.version = Some(new_job.version);
        let prev_hash: [u8; 32] = new_job.prev_hash;
        let prev_hash = DHash::from_byte_array(prev_hash);
        let merkle_root: [u8; 32] = new_job.merkle_root.to_vec().try_into().unwrap();
        let merkle_root = DHash::from_byte_array(merkle_root);
        let header = Header {
            version: Version::from_consensus(new_job.version as i32),
            prev_blockhash: BlockHash::from_raw_hash(prev_hash),
            merkle_root: TxMerkleNode::from_raw_hash(merkle_root),
            time: std::time::SystemTime::now()
                .duration_since(
                    std::time::SystemTime::UNIX_EPOCH - std::time::Duration::from_secs(60),
                )
                .unwrap()
                .as_secs() as u32,
            bits: CompactTarget::from_consensus(new_job.nbits),
            nonce: 0,
        };
        self.header = Some(header);
    }

    /// Called by the `Client` to retrieve the latest candidate block header hash. The actual
    /// incrementing of the nonce is mocked out in a thread in `Client::new()`.
    pub(crate) fn next_share(&mut self) -> Result<(), ()> {
        let header = self.header.as_ref().ok_or(())?;
        let hash_ = header.block_hash();
        let mut hash: [u8; 32] = *hash_.to_raw_hash().as_ref();
        hash.reverse();
        let hash = U256::from_big_endian(hash.as_ref());
        if hash < *self.target.as_ref().ok_or(())? {
            info!(
                "Found share with nonce: {}, for target: {:?}, hash: {:?}",
                header.nonce, self.target, hash
            );
            Ok(())
        } else {
            Err(())
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device/Cargo.toml">
[package]
name = "mining_device"
version = "0.1.3"
authors = ["The Stratum V2 Developers"]
edition = "2018"
publish = false
documentation = "https://github.com/stratum-mining/stratum"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
name = "mining_device"
path = "src/lib/mod.rs"


[dependencies]
stratum-common = { path = "../../../common", features = ["with_network_helpers"] }
async-channel = "1.5.1"
buffer_sv2 = { path = "../../../utils/buffer" }
async-recursion = "0.3.2"
rand = "0.8.4"
futures = "0.3.5"
key-utils = { path = "../../../utils/key-utils" }
clap = { version = "^4.5.4", features = ["derive"] }
tracing = { version = "0.1" }
tracing-subscriber = "0.3"
sha2 = "0.10.6"
tokio = "1.44.1"
primitive-types = "0.13.1"
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device/README.md">
# CPU Sv2 mining device

Header only sv2 cpu miner.

```
Usage: mining_device [OPTIONS] --address-pool <ADDRESS_POOL>

Options:
  -p, --pubkey-pool <PUBKEY_POOL>
          Pool pub key, when left empty the pool certificate is not checked
  -i, --id-device <ID_DEVICE>
          Sometimes used by the pool to identify the device
  -a, --address-pool <ADDRESS_POOL>
          Address of the pool in this format ip:port or domain:port
      --handicap <HANDICAP>
          This value is used to slow down the cpu miner, it represents the number of micro-seconds that are awaited between hashes [default: 0]
      --id-user <ID_USER>
          User id, used when a new channel is opened, it can be used by the pool to identify the miner
      --nominal-hashrate-multiplier <NOMINAL_HASHRATE_MULTIPLIER>
          This floating point number is used to modify the advertised nominal hashrate when opening a channel with the upstream.
          If 0.0 < nominal_hashrate_multiplier < 1.0, the CPU miner will advertise a nominal hashrate that is smaller than its real capacity.
          If nominal_hashrate_multiplier > 1.0, the CPU miner will advertise a nominal hashrate that is bigger than its real capacity.
          If empty, the CPU miner will simply advertise its real capacity.
  -h, --help
          Print help
  -V, --version
          Print version
```

Usage example:
```
cargo run --release -- --address-pool 127.0.0.1:20000 --id-device device_id::SOLO::bc1qxy2kgdygjrsqtzq2n0yrf2493p83kkfjhx0wlh
```

## handicap

CPU mining could damage the system due to excessive heat.

The `--handicap` parameter should be used as a safety mechanism to slow down the hashrate in order to preserve hardware.

## nominal hashrate multiplier

Let's imagine that:
- the upstream wants to receive shares every ~100s (on average)
- the CPU miner nominal hashrate is 1k H/s

Maybe we want to do a test where we don't want to wait ~100s before a share is submitted by the CPU miner.

In that case, we need the CPU miner to advertise a smaller hashrate, which will force the upstream to set a lower
difficulty target.

The `--nominal-hashrate-multiplier` can be used to advertise a custom nominal hashrate.

In the scenario described above, we could launch the CPU miner with `--nominal-hashrate-multiplier 0.01`. 

The CPU miner would advertise 0.01k H/s, which would cause the upstream to set the difficulty target such that the CPU miner would find a share within ~1s.

This feature can also be used to advertise a bigger nominal hashrate by using values above `1.0`.

That can also be useful for testing difficulty adjustment algorithms on Sv2 upstreams.
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device/src/lib/mod.rs">
#![allow(clippy::option_map_unit_fn)]
use async_channel::{Receiver, Sender};
use key_utils::Secp256k1PublicKey;
use primitive_types::U256;
use rand::{thread_rng, Rng};
use std::{
    net::{SocketAddr, ToSocketAddrs},
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
    thread::available_parallelism,
    time::{Duration, Instant},
};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self,
        bitcoin::{blockdata::block::Header, hash_types::BlockHash, hashes::Hash, CompactTarget},
        codec_sv2,
        codec_sv2::{Initiator, StandardEitherFrame, StandardSv2Frame},
        common_messages_sv2::{Protocol, SetupConnection, SetupConnectionSuccess},
        errors::Error,
        handlers::{
            common::ParseCommonMessagesFromUpstream,
            mining::{ParseMiningMessagesFromUpstream, SendTo, SupportedChannelTypes},
        },
        mining_sv2::*,
        parsers::{Mining, MiningDeviceMessages},
        utils::{Id, Mutex},
    },
};
use tokio::net::TcpStream;
use tracing::{debug, error, info};

pub async fn connect(
    address: String,
    pub_key: Option<Secp256k1PublicKey>,
    device_id: Option<String>,
    user_id: Option<String>,
    handicap: u32,
    nominal_hashrate_multiplier: Option<f32>,
    single_submit: bool,
) {
    let address = address
        .clone()
        .to_socket_addrs()
        .expect("Invalid pool address, use one of this formats: ip:port, domain:port")
        .next()
        .expect("Invalid pool address, use one of this formats: ip:port, domain:port");
    info!("Connecting to pool at {}", address);
    let socket = loop {
        let pool = tokio::time::timeout(Duration::from_secs(5), TcpStream::connect(address)).await;
        match pool {
            Ok(result) => match result {
                Ok(socket) => break socket,
                Err(e) => {
                    error!(
                        "Failed to connect to Upstream role at {}, retrying in 5s: {}",
                        address, e
                    );
                    tokio::time::sleep(Duration::from_secs(5)).await;
                }
            },
            Err(_) => {
                error!("Pool is unresponsive, terminating");
                std::process::exit(1);
            }
        }
    };
    info!("Pool tcp connection established at {}", address);
    let address = socket.peer_addr().unwrap();
    let initiator = Initiator::new(pub_key.map(|e| e.0));
    let (receiver, sender) =
        Connection::new(socket, codec_sv2::HandshakeRole::Initiator(initiator))
            .await
            .unwrap();
    info!("Pool noise connection established at {}", address);
    Device::start(
        receiver,
        sender,
        address,
        device_id,
        user_id,
        handicap,
        nominal_hashrate_multiplier,
        single_submit,
    )
    .await
}

pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

struct SetupConnectionHandler {}
use std::convert::TryInto;
use stratum_common::roles_logic_sv2::{bitcoin::block::Version, common_messages_sv2::Reconnect};

impl SetupConnectionHandler {
    pub fn new() -> Self {
        SetupConnectionHandler {}
    }
    fn get_setup_connection_message(
        address: SocketAddr,
        device_id: Option<String>,
    ) -> SetupConnection<'static> {
        let endpoint_host = address.ip().to_string().into_bytes().try_into().unwrap();
        let vendor = String::new().try_into().unwrap();
        let hardware_version = String::new().try_into().unwrap();
        let firmware = String::new().try_into().unwrap();
        let device_id = device_id.unwrap_or_default();
        info!(
            "Creating SetupConnection message with device id: {:?}",
            device_id
        );
        SetupConnection {
            protocol: Protocol::MiningProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0b0000_0000_0000_0000_0000_0000_0000_0001,
            endpoint_host,
            endpoint_port: address.port(),
            vendor,
            hardware_version,
            firmware,
            device_id: device_id.try_into().unwrap(),
        }
    }
    pub async fn setup(
        self_: Arc<Mutex<Self>>,
        receiver: &mut Receiver<EitherFrame>,
        sender: &mut Sender<EitherFrame>,
        device_id: Option<String>,
        address: SocketAddr,
    ) {
        let setup_connection = Self::get_setup_connection_message(address, device_id);

        let sv2_frame: StdFrame = MiningDeviceMessages::Common(setup_connection.into())
            .try_into()
            .unwrap();
        let sv2_frame = sv2_frame.into();
        sender.send(sv2_frame).await.unwrap();
        info!("Setup connection sent to {}", address);

        let mut incoming: StdFrame = receiver.recv().await.unwrap().try_into().unwrap();
        let message_type = incoming.get_header().unwrap().msg_type();
        let payload = incoming.payload();
        ParseCommonMessagesFromUpstream::handle_message_common(self_, message_type, payload)
            .unwrap();
    }
}

impl ParseCommonMessagesFromUpstream for SetupConnectionHandler {
    fn handle_setup_connection_success(
        &mut self,
        m: SetupConnectionSuccess,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        use roles_logic_sv2::handlers::common::SendTo;
        info!(
            "Received `SetupConnectionSuccess`: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendTo::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        error!("Setup connection error");
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, roles_logic_sv2::errors::Error> {
        todo!()
    }

    fn handle_reconnect(
        &mut self,
        _m: Reconnect,
    ) -> Result<roles_logic_sv2::handlers::common::SendTo, Error> {
        todo!()
    }
}

#[derive(Debug, Clone)]
struct NewWorkNotifier {
    should_send: bool,
    sender: Sender<()>,
}

#[derive(Debug)]
pub struct Device {
    #[allow(dead_code)]
    receiver: Receiver<EitherFrame>,
    sender: Sender<EitherFrame>,
    #[allow(dead_code)]
    channel_opened: bool,
    channel_id: Option<u32>,
    miner: Arc<Mutex<Miner>>,
    jobs: Vec<NewMiningJob<'static>>,
    prev_hash: Option<SetNewPrevHash<'static>>,
    sequence_numbers: Id,
    notify_changes_to_mining_thread: NewWorkNotifier,
}

fn open_channel(
    device_id: Option<String>,
    nominal_hashrate_multiplier: Option<f32>,
    handicap: u32,
) -> OpenStandardMiningChannel<'static> {
    let user_identity = device_id.unwrap_or_default().try_into().unwrap();
    let id: u32 = 10;
    info!("Measuring CPU hashrate");
    let measured_hashrate = measure_hashrate(5, handicap) as f32;
    info!("Measured CPU hashrate is {}", measured_hashrate);
    let nominal_hash_rate = match nominal_hashrate_multiplier {
        Some(m) => measured_hashrate * m,
        None => measured_hashrate,
    };

    info!("MINING DEVICE: send open channel with request id {}", id);

    OpenStandardMiningChannel {
        request_id: id.into(),
        user_identity,
        nominal_hash_rate,
        max_target: vec![0xFF_u8; 32].try_into().unwrap(),
    }
}

impl Device {
    #[allow(clippy::too_many_arguments)]
    async fn start(
        mut receiver: Receiver<EitherFrame>,
        mut sender: Sender<EitherFrame>,
        addr: SocketAddr,
        device_id: Option<String>,
        user_id: Option<String>,
        handicap: u32,
        nominal_hashrate_multiplier: Option<f32>,
        single_submit: bool,
    ) {
        let setup_connection_handler = Arc::new(Mutex::new(SetupConnectionHandler::new()));
        SetupConnectionHandler::setup(
            setup_connection_handler,
            &mut receiver,
            &mut sender,
            device_id,
            addr,
        )
        .await;
        info!("Pool sv2 connection established at {}", addr);
        let miner = Arc::new(Mutex::new(Miner::new(handicap)));
        let (notify_changes_to_mining_thread, update_miners) = async_channel::unbounded();
        let self_ = Self {
            channel_opened: false,
            receiver: receiver.clone(),
            sender: sender.clone(),
            miner: miner.clone(),
            jobs: Vec::new(),
            prev_hash: None,
            channel_id: None,
            sequence_numbers: Id::new(),
            notify_changes_to_mining_thread: NewWorkNotifier {
                should_send: true,
                sender: notify_changes_to_mining_thread,
            },
        };
        let open_channel = MiningDeviceMessages::Mining(Mining::OpenStandardMiningChannel(
            open_channel(user_id, nominal_hashrate_multiplier, handicap),
        ));
        let frame: StdFrame = open_channel.try_into().unwrap();
        self_.sender.send(frame.into()).await.unwrap();
        let self_mutex = std::sync::Arc::new(Mutex::new(self_));
        let cloned = self_mutex.clone();

        let (share_send, share_recv) = async_channel::unbounded();

        start_mining_threads(update_miners, miner, share_send);
        tokio::task::spawn(async move {
            let recv = share_recv.clone();
            loop {
                let (nonce, job_id, version, ntime) = recv.recv().await.unwrap();
                Self::send_share(cloned.clone(), nonce, job_id, version, ntime).await;
                if single_submit {
                    break;
                }
            }
        });

        loop {
            let mut incoming: StdFrame = receiver.recv().await.unwrap().try_into().unwrap();
            let message_type = incoming.get_header().unwrap().msg_type();
            let payload = incoming.payload();
            let next =
                Device::handle_message_mining(self_mutex.clone(), message_type, payload).unwrap();
            let mut notify_changes_to_mining_thread = self_mutex
                .safe_lock(|s| s.notify_changes_to_mining_thread.clone())
                .unwrap();
            if notify_changes_to_mining_thread.should_send
                && (message_type == roles_logic_sv2::mining_sv2::MESSAGE_TYPE_NEW_MINING_JOB
                    || message_type
                        == roles_logic_sv2::mining_sv2::MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH
                    || message_type == roles_logic_sv2::mining_sv2::MESSAGE_TYPE_SET_TARGET)
            {
                notify_changes_to_mining_thread
                    .sender
                    .send(())
                    .await
                    .unwrap();
                notify_changes_to_mining_thread.should_send = false;
            };
            match next {
                SendTo::RelayNewMessageToRemote(_, m) => {
                    let sv2_frame: StdFrame = MiningDeviceMessages::Mining(m).try_into().unwrap();
                    let either_frame: EitherFrame = sv2_frame.into();
                    sender.send(either_frame).await.unwrap();
                }
                SendTo::None(_) => (),
                _ => panic!(),
            }
        }
    }

    async fn send_share(
        self_mutex: Arc<Mutex<Self>>,
        nonce: u32,
        job_id: u32,
        version: u32,
        ntime: u32,
    ) {
        let share =
            MiningDeviceMessages::Mining(Mining::SubmitSharesStandard(SubmitSharesStandard {
                channel_id: self_mutex.safe_lock(|s| s.channel_id.unwrap()).unwrap(),
                sequence_number: self_mutex.safe_lock(|s| s.sequence_numbers.next()).unwrap(),
                job_id,
                nonce,
                ntime,
                version,
            }));
        let frame: StdFrame = share.try_into().unwrap();
        let sender = self_mutex.safe_lock(|s| s.sender.clone()).unwrap();
        sender.send(frame.into()).await.unwrap();
    }
}

impl ParseMiningMessagesFromUpstream<()> for Device {
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::Standard
    }

    fn is_work_selection_enabled(&self) -> bool {
        false
    }

    fn handle_open_standard_mining_channel_success(
        &mut self,
        m: OpenStandardMiningChannelSuccess,
    ) -> Result<SendTo<()>, Error> {
        self.channel_opened = true;
        self.channel_id = Some(m.channel_id);
        let req_id = m.get_request_id_as_u32();
        info!(
            "MINING DEVICE: channel opened with: group id {}, channel id {}, request id {}",
            m.group_channel_id, m.channel_id, req_id
        );
        self.miner
            .safe_lock(|miner| miner.new_target(m.target.to_vec()))
            .unwrap();
        self.notify_changes_to_mining_thread.should_send = true;
        Ok(SendTo::None(None))
    }

    fn handle_open_extended_mining_channel_success(
        &mut self,
        _: OpenExtendedMiningChannelSuccess,
    ) -> Result<SendTo<()>, Error> {
        unreachable!()
    }

    fn handle_open_mining_channel_error(
        &mut self,
        _: OpenMiningChannelError,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_update_channel_error(&mut self, _: UpdateChannelError) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_close_channel(&mut self, _: CloseChannel) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_extranonce_prefix(
        &mut self,
        _: SetExtranoncePrefix,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_submit_shares_success(
        &mut self,
        m: SubmitSharesSuccess,
    ) -> Result<SendTo<()>, Error> {
        info!("Received SubmitSharesSuccess");
        debug!("SubmitSharesSuccess: {}", m);
        Ok(SendTo::None(None))
    }

    fn handle_submit_shares_error(&mut self, m: SubmitSharesError) -> Result<SendTo<()>, Error> {
        error!(
            "Received SubmitSharesError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::None(None))
    }

    fn handle_new_mining_job(&mut self, m: NewMiningJob) -> Result<SendTo<()>, Error> {
        info!(
            "Received new mining job for channel id: {} with job id: {} is future: {}",
            m.channel_id,
            m.job_id,
            m.is_future()
        );
        debug!("NewMiningJob: {}", m);
        match (m.is_future(), self.prev_hash.as_ref()) {
            (false, Some(p_h)) => {
                self.miner
                    .safe_lock(|miner| miner.new_header(p_h, &m))
                    .unwrap();
                self.jobs = vec![m.as_static()];
                self.notify_changes_to_mining_thread.should_send = true;
            }
            (true, _) => self.jobs.push(m.as_static()),
            (false, None) => {
                panic!()
            }
        }
        Ok(SendTo::None(None))
    }

    fn handle_new_extended_mining_job(
        &mut self,
        _: NewExtendedMiningJob,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_new_prev_hash(&mut self, m: SetNewPrevHash) -> Result<SendTo<()>, Error> {
        info!(
            "Received SetNewPrevHash channel id: {}, job id: {}",
            m.channel_id, m.job_id
        );
        debug!("SetNewPrevHash: {}", m);
        let jobs: Vec<&NewMiningJob<'static>> = self
            .jobs
            .iter()
            .filter(|j| j.job_id == m.job_id && j.is_future())
            .collect();
        match jobs.len() {
            0 => {
                self.prev_hash = Some(m.as_static());
            }
            1 => {
                self.miner
                    .safe_lock(|miner| miner.new_header(&m, jobs[0]))
                    .unwrap();
                self.jobs = vec![jobs[0].clone()];
                self.prev_hash = Some(m.as_static());
                self.notify_changes_to_mining_thread.should_send = true;
            }
            _ => panic!(),
        }
        Ok(SendTo::None(None))
    }

    fn handle_set_custom_mining_job_success(
        &mut self,
        _: SetCustomMiningJobSuccess,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_custom_mining_job_error(
        &mut self,
        _: SetCustomMiningJobError,
    ) -> Result<SendTo<()>, Error> {
        todo!()
    }

    fn handle_set_target(&mut self, m: SetTarget) -> Result<SendTo<()>, Error> {
        info!("Received SetTarget for channel id: {}", m.channel_id);
        debug!("SetTarget: {}", m);
        self.miner
            .safe_lock(|miner| miner.new_target(m.maximum_target.to_vec()))
            .unwrap();
        self.notify_changes_to_mining_thread.should_send = true;
        Ok(SendTo::None(None))
    }

    fn handle_set_group_channel(&mut self, _m: SetGroupChannel) -> Result<SendTo<()>, Error> {
        todo!()
    }
}

#[derive(Debug, Clone)]
struct Miner {
    header: Option<Header>,
    target: Option<U256>,
    job_id: Option<u32>,
    version: Option<u32>,
    handicap: u32,
}

impl Miner {
    fn new(handicap: u32) -> Self {
        Self {
            target: None,
            header: None,
            job_id: None,
            version: None,
            handicap,
        }
    }

    fn new_target(&mut self, target: Vec<u8>) {
        // target is sent in LE format, we'll keep it that way
        let hex_string = target
            .iter()
            .fold("".to_string(), |acc, b| acc + format!("{b:02x}").as_str());
        info!("Set target to {}", hex_string);
        // Store the target as U256 in little-endian format
        self.target = Some(U256::from_little_endian(target.as_slice()));
    }

    fn new_header(&mut self, set_new_prev_hash: &SetNewPrevHash, new_job: &NewMiningJob) {
        self.job_id = Some(new_job.job_id);
        self.version = Some(new_job.version);
        let prev_hash: [u8; 32] = set_new_prev_hash.prev_hash.to_vec().try_into().unwrap();
        let prev_hash = Hash::from_byte_array(prev_hash);
        let merkle_root: [u8; 32] = new_job.merkle_root.to_vec().try_into().unwrap();
        let merkle_root = Hash::from_byte_array(merkle_root);
        // fields need to be added as BE and the are converted to LE in the background before
        // hashing
        let header = Header {
            version: Version::from_consensus(new_job.version as i32),
            prev_blockhash: BlockHash::from_raw_hash(prev_hash),
            merkle_root,
            time: std::time::SystemTime::now()
                .duration_since(
                    std::time::SystemTime::UNIX_EPOCH - std::time::Duration::from_secs(60),
                )
                .unwrap()
                .as_secs() as u32,
            bits: CompactTarget::from_consensus(set_new_prev_hash.nbits),
            nonce: 0,
        };
        self.header = Some(header);
    }
    pub fn next_share(&mut self) -> NextShareOutcome {
        if let Some(header) = self.header.as_ref() {
            let hash_ = header.block_hash();
            let hash: [u8; 32] = *hash_.to_raw_hash().as_ref();

            // Convert both hash and target to Target type for comparison
            let hash_target: Target = hash.into();

            // Convert U256 target to [u8; 32] array and then to Target
            if let Some(target) = self.target {
                let target_bytes = target.to_little_endian();
                let mut target_array = [0u8; 32];
                target_array.copy_from_slice(&target_bytes);
                let target: Target = target_array.into();
                if hash_target <= target {
                    info!(
                        "Found share with nonce: {}, for target: {:?}, with hash: {:?}",
                        header.nonce, self.target, hash,
                    );
                    NextShareOutcome::ValidShare
                } else {
                    NextShareOutcome::InvalidShare
                }
            } else {
                std::thread::yield_now();
                NextShareOutcome::NoTarget
            }
        } else {
            std::thread::yield_now();
            NextShareOutcome::NoHeader
        }
    }
}

enum NextShareOutcome {
    ValidShare,
    InvalidShare,
    NoTarget,
    NoHeader,
}

impl NextShareOutcome {
    pub fn is_valid(&self) -> bool {
        matches!(self, NextShareOutcome::ValidShare)
    }
}

// returns hashrate based on how fast the device hashes over the given duration
fn measure_hashrate(duration_secs: u64, handicap: u32) -> f64 {
    let mut rng = thread_rng();
    let prev_hash: [u8; 32] = generate_random_32_byte_array().to_vec().try_into().unwrap();
    let prev_hash = Hash::from_byte_array(prev_hash);
    // We create a random block that we can hash, we are only interested in knowing how many hashes
    // per unit of time we can do
    let merkle_root: [u8; 32] = generate_random_32_byte_array().to_vec().try_into().unwrap();
    let merkle_root = Hash::from_byte_array(merkle_root);
    let header = Header {
        version: Version::from_consensus(rng.gen()),
        prev_blockhash: BlockHash::from_raw_hash(prev_hash),
        merkle_root,
        time: std::time::SystemTime::now()
            .duration_since(std::time::SystemTime::UNIX_EPOCH - std::time::Duration::from_secs(60))
            .unwrap()
            .as_secs() as u32,
        bits: CompactTarget::from_consensus(rng.gen()),
        nonce: 0,
    };
    let start_time = Instant::now();
    let mut hashes: u64 = 0;
    let duration = Duration::from_secs(duration_secs);
    let mut miner = Miner::new(handicap);
    // We put the target to 0 we are only interested in how many hashes per unit of time we can do
    // and do not want to be botherd by messages about valid shares found.
    miner.new_target(vec![0_u8; 32]);
    miner.header = Some(header);

    while start_time.elapsed() < duration {
        miner.next_share();
        hashes += 1;
    }

    let elapsed_secs = start_time.elapsed().as_secs_f64();
    let hashrate_single_thread = hashes as f64 / elapsed_secs;

    // we just measured for a single thread, need to multiply by the available parallelism
    let p = available_parallelism().unwrap().get();

    hashrate_single_thread * p as f64
}
fn generate_random_32_byte_array() -> [u8; 32] {
    let mut rng = thread_rng();
    let mut arr = [0u8; 32];
    rng.fill(&mut arr[..]);
    arr
}

fn start_mining_threads(
    have_new_job: Receiver<()>,
    miner: Arc<Mutex<Miner>>,
    share_send: Sender<(u32, u32, u32, u32)>,
) {
    tokio::task::spawn(async move {
        let mut killers: Vec<Arc<AtomicBool>> = vec![];
        loop {
            let p = available_parallelism().unwrap().get() as u32;
            let unit = u32::MAX / p;
            while have_new_job.recv().await.is_ok() {
                while let Some(killer) = killers.pop() {
                    killer.store(true, Ordering::Relaxed);
                }
                let miner = miner.safe_lock(|m| m.clone()).unwrap();
                for i in 0..p {
                    let mut miner = miner.clone();
                    let share_send = share_send.clone();
                    let killer = Arc::new(AtomicBool::new(false));
                    miner.header.as_mut().map(|h| h.nonce = i * unit);
                    killers.push(killer.clone());
                    std::thread::spawn(move || {
                        mine(miner, share_send, killer);
                    });
                }
            }
        }
    });
}

fn mine(mut miner: Miner, share_send: Sender<(u32, u32, u32, u32)>, kill: Arc<AtomicBool>) {
    if miner.handicap != 0 {
        loop {
            if kill.load(Ordering::Relaxed) {
                break;
            }
            std::thread::sleep(std::time::Duration::from_micros(miner.handicap.into()));
            if miner.next_share().is_valid() {
                let nonce = miner.header.unwrap().nonce;
                let time = miner.header.unwrap().time;
                let job_id = miner.job_id.unwrap();
                let version = miner.version;
                share_send
                    .try_send((nonce, job_id, version.unwrap(), time))
                    .unwrap();
            }
            miner
                .header
                .as_mut()
                .map(|h| h.nonce = h.nonce.wrapping_add(1));
        }
    } else {
        loop {
            if miner.next_share().is_valid() {
                if kill.load(Ordering::Relaxed) {
                    break;
                }
                let nonce = miner.header.unwrap().nonce;
                let time = miner.header.unwrap().time;
                let job_id = miner.job_id.unwrap();
                let version = miner.version;
                share_send
                    .try_send((nonce, job_id, version.unwrap(), time))
                    .unwrap();
            }
            miner
                .header
                .as_mut()
                .map(|h| h.nonce = h.nonce.wrapping_add(1));
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/test-utils/mining-device/src/main.rs">
#![allow(special_module_name)]
#![allow(clippy::option_map_unit_fn)]
use key_utils::Secp256k1PublicKey;

use clap::Parser;
use tracing::info;

#[derive(Parser, Debug)]
#[command(version, about, long_about = None)]
struct Args {
    #[arg(
        short,
        long,
        help = "Pool pub key, when left empty the pool certificate is not checked"
    )]
    pubkey_pool: Option<Secp256k1PublicKey>,
    #[arg(
        short,
        long,
        help = "Sometimes used by the pool to identify the device"
    )]
    id_device: Option<String>,
    #[arg(
        short,
        long,
        help = "Address of the pool in this format ip:port or domain:port"
    )]
    address_pool: String,
    #[arg(
        long,
        help = "This value is used to slow down the cpu miner, it represents the number of micro-seconds that are awaited between hashes",
        default_value = "0"
    )]
    handicap: u32,
    #[arg(
        long,
        help = "User id, used when a new channel is opened, it can be used by the pool to identify the miner"
    )]
    id_user: Option<String>,
    #[arg(
        long,
        help = "This floating point number is used to modify the advertised nominal hashrate when opening a channel with the upstream.\
         \nIf 0.0 < nominal_hashrate_multiplier < 1.0, the CPU miner will advertise a nominal hashrate that is smaller than its real capacity.\
         \nIf nominal_hashrate_multiplier > 1.0, the CPU miner will advertise a nominal hashrate that is bigger than its real capacity.\
         \nIf empty, the CPU miner will simply advertise its real capacity."
    )]
    nominal_hashrate_multiplier: Option<f32>,
}

#[tokio::main(flavor = "current_thread")]
async fn main() {
    let args = Args::parse();
    tracing_subscriber::fmt::init();
    info!("start");
    let _ = mining_device::connect(
        args.address_pool,
        args.pubkey_pool,
        args.id_device,
        args.id_user,
        args.handicap,
        args.nominal_hashrate_multiplier,
        false,
    )
    .await;
}
</file>

<file path="stratum-1.4.0/roles/translator/Cargo.toml">
[package]
name = "translator_sv2"
version = "1.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "Server used to bridge SV1 miners to SV2 pools"
documentation = "https://docs.rs/translator_sv2"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[lib]
name = "translator_sv2"
path = "src/lib/mod.rs"

[[bin]]
name = "translator_sv2"
path = "src/main.rs"

[dependencies]
stratum-common = { path = "../../common", features = ["with_network_helpers"] }
async-channel = "1.5.1"
async-recursion = "0.3.2"
buffer_sv2 = { path = "../../utils/buffer" }
once_cell = "1.12.0"
serde = { version = "1.0.89", default-features = false, features = ["derive", "alloc"] }
serde_json = { version = "1.0.64", default-features = false, features = ["alloc"] }
futures = "0.3.25"
tokio = { version = "1.44.1", features = ["full"] }
ext-config = { version = "0.14.0", features = ["toml"], package = "config" }
tracing = { version = "0.1" }
v1 = { path = "../../protocols/v1", package="sv1_api" }
error_handling = { path = "../../utils/error-handling" }
key-utils = { path = "../../utils/key-utils" }
tokio-util = { version = "0.7.10", features = ["codec"] }
rand = "0.8.4"
primitive-types = "0.13.1"
clap = { version = "4.5.39", features = ["derive"] }
config-helpers = { path = "../roles-utils/config-helpers" }


[dev-dependencies]
sha2 = "0.10.6"
</file>

<file path="stratum-1.4.0/roles/translator/config-examples/tproxy-config-hosted-pool-example.toml">
# Braiins Pool Upstream Connection
# upstream_authority_pubkey = "u95GEReVMjK6k5YqiSFNqqTnKU4ypU2Wm8awa6tmbmDmk1bWt"
# upstream_address = "18.196.32.109"
# upstream_port = 3336

# Hosted SRI Pool Upstream Connection
upstream_address = "75.119.150.111"
upstream_port = 34254
upstream_authority_pubkey = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"

# Local Mining Device Downstream Connection
downstream_address = "0.0.0.0"
downstream_port = 34255

# Version support
max_supported_version = 2
min_supported_version = 2

# Minimum extranonce2 size for downstream
# Max value: 16 (leaves 0 bytes for search space splitting of downstreams)
# Max value for CGminer: 8
# Min value: 2
min_extranonce2_size = 4

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./tproxy.log"

# Difficulty params
[downstream_difficulty_config]
# hashes/s of the weakest miner that will be connecting (e.g.: 10 Th/s = 10_000_000_000_000.0)
min_individual_miner_hashrate=10_000_000_000_000.0
# target number of shares per minute the miner should be sending
shares_per_minute = 6.0

[upstream_difficulty_config]
# interval in seconds to elapse before updating channel hashrate with the pool
channel_diff_update_interval = 60
# estimated accumulated hashrate of all downstream miners (e.g.: 10 Th/s = 10_000_000_000_000.0)
channel_nominal_hashrate = 10_000_000_000_000.0
</file>

<file path="stratum-1.4.0/roles/translator/config-examples/tproxy-config-local-jdc-example.toml">
# Braiins Pool Upstream Connection
# upstream_authority_pubkey = "u95GEReVMjK6k5YqiSFNqqTnKU4ypU2Wm8awa6tmbmDmk1bWt"
# upstream_address = "18.196.32.109"
# upstream_port = 3336

# Local SRI JDC Upstream Connection
upstream_address = "127.0.0.1"
upstream_port = 34265
upstream_authority_pubkey = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"

# Local Mining Device Downstream Connection
downstream_address = "0.0.0.0"
downstream_port = 34255

# Version support
max_supported_version = 2
min_supported_version = 2

# Minimum extranonce2 size for downstream
# Max value: 16 (leaves 0 bytes for search space splitting of downstreams)
# Max value for CGminer: 8
# Min value: 2
min_extranonce2_size = 4

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./tproxy.log"

# Difficulty params
[downstream_difficulty_config]
# hashes/s of the weakest miner that will be connecting (e.g.: 10 Th/s = 10_000_000_000_000.0)
min_individual_miner_hashrate=10_000_000_000_000.0
# target number of shares per minute the miner should be sending
shares_per_minute = 6.0

[upstream_difficulty_config]
# interval in seconds to elapse before updating channel hashrate with the pool
channel_diff_update_interval = 60
# estimated accumulated hashrate of all downstream miners (e.g.: 10 Th/s = 10_000_000_000_000.0)
channel_nominal_hashrate = 10_000_000_000_000.0
</file>

<file path="stratum-1.4.0/roles/translator/config-examples/tproxy-config-local-pool-example.toml">
# Braiins Pool Upstream Connection
# upstream_authority_pubkey = "u95GEReVMjK6k5YqiSFNqqTnKU4ypU2Wm8awa6tmbmDmk1bWt"
# upstream_address = "18.196.32.109"
# upstream_port = 3336

# Local SRI Pool Upstream Connection
upstream_address = "127.0.0.1"
upstream_port = 34254
upstream_authority_pubkey = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"

# Local Mining Device Downstream Connection
downstream_address = "0.0.0.0"
downstream_port = 34255

# Version support
max_supported_version = 2
min_supported_version = 2

# Minimum extranonce2 size for downstream
# Max value: 16 (leaves 0 bytes for search space splitting of downstreams)
# Max value for CGminer: 8
# Min value: 2
min_extranonce2_size = 4

# Enable this option to set a predefined log file path.
# When enabled, logs will always be written to this file.
# The CLI option --log-file (or -f) will override this setting if provided.
# log_file = "./tproxy.log"

# Difficulty params
[downstream_difficulty_config]
# hashes/s of the weakest miner that will be connecting (e.g.: 10 Th/s = 10_000_000_000_000.0)
min_individual_miner_hashrate=10_000_000_000_000.0
# target number of shares per minute the miner should be sending
shares_per_minute = 6.0

[upstream_difficulty_config]
# interval in seconds to elapse before updating channel hashrate with the pool
channel_diff_update_interval = 60
# estimated accumulated hashrate of all downstream miners (e.g.: 10 Th/s = 10_000_000_000_000.0)
channel_nominal_hashrate = 10_000_000_000_000.0
</file>

<file path="stratum-1.4.0/roles/translator/README.md">
# SV1 to SV2 Translator Proxy

This proxy is designed to sit in between a SV1 Downstream role (most typically Mining Device(s) 
running SV1 firmware) and a SV2 Upstream role (most typically a SV2 Pool Server with Extended
Channel support).

The most typical high level configuration is:

```
<--- Most Downstream ----------------------------------------- Most Upstream --->

+---------------------------------------------------+  +------------------------+
|                     Mining Farm                   |  |      Remote Pool       |
|                                                   |  |                        |
|  +-------------------+     +------------------+   |  |   +-----------------+  |
|  | SV1 Mining Device | <-> | Translator Proxy | <------> | SV2 Pool Server |  |
|  +-------------------+     +------------------+   |  |   +-----------------+  |
|                                                   |  |                        |
+---------------------------------------------------+  +------------------------+

```

## Setup

### Configuration File

`tproxy-config-local-jdc-example.toml` and `tproxy-config-local-pool-example.toml` are examples of configuration files for the Translator Proxy.

The configuration file contains the following information:

1. The SV2 Upstream connection information which includes the SV2 Pool authority public key 
   (`upstream_authority_pubkey`) and the SV2 Pool connection address (`upstream_address`) and port
   (`upstream_port`).
2. The SV1 Downstream socket information which includes the listening IP address
   (`downstream_address`) and port (`downstream_port`).
3. The maximum and minimum SRI versions (`max_supported_version` and `min_supported_version`) that
   the Translator Proxy implementer wants to support. Currently the only available version is `2`.
4. The desired minimum `extranonce2` size that the Translator Proxy implementer wants to use
   (`min_extranonce2_size`). The `extranonce2` size is ultimately decided by the SV2 Upstream role,
   but if the specified size meets the SV2 Upstream role's requirements, the size specified in this
   configuration file should be favored.
5. The downstream difficulty params such as:
- the hashrate (hashes/s) of the weakest Mining Device that will be connecting to the Translator Proxy (`min_individual_miner_hashrate`)
- the number of shares per minute that Mining Devices should be sending to the Translator Proxy (`shares_per_minute`). 
6. The upstream difficulty params such as:
- the interval in seconds to elapse before updating channel hashrate with the pool (`channel_diff_update_interval`)
- the estimated aggregate hashrate of all SV1 Downstream roles (`channel_nominal_hashrate`)

### Run

There are two files in `roles/translator/config-examples`:
- `tproxy-config-local-jdc-example.toml` which assumes the Job Declaration protocol is used and a JD Client is deployed locally
- `tproxy-config-local-pool-example.toml` which assumes Job Declaration protocol is NOT used, and a Pool is deployed locally

```bash
cd roles/translator/config-examples/
cargo run -- -c tproxy-config-local-jdc-example.toml

### Limitations

The current implementation always replies to Sv1 `mining.submit` with `"result": true`, regardless of whether the share was rejected on Sv2 upstream.
</file>

<file path="stratum-1.4.0/roles/translator/src/args.rs">
//! Defines the structure and parsing logic for command-line arguments.
//!
//! It provides the `Args` struct to hold parsed arguments,
//! and the `from_args` function to parse them from the command line.
use clap::Parser;
use ext_config::{Config, File, FileFormat};
use std::path::PathBuf;
use tracing::error;
use translator_sv2::{
    config::TranslatorConfig,
    error::{Error, ProxyResult},
};

/// Holds the parsed CLI arguments.
#[derive(Parser, Debug)]
#[command(author, version, about = "Translator Proxy", long_about = None)]
pub struct Args {
    #[arg(
        short = 'c',
        long = "config",
        help = "Path to the TOML configuration file",
        default_value = "proxy-config.toml"
    )]
    pub config_path: PathBuf,
    #[arg(
        short = 'f',
        long = "log-file",
        help = "Path to the log file. If not set, logs will only be written to stdout."
    )]
    pub log_file: Option<PathBuf>,
}

/// Process CLI args, if any.
#[allow(clippy::result_large_err)]
pub fn process_cli_args<'a>() -> ProxyResult<'a, TranslatorConfig> {
    // Parse CLI arguments
    let args = Args::parse();

    // Build configuration from the provided file path
    let config_path = args.config_path.to_str().ok_or_else(|| {
        error!("Invalid configuration path.");
        Error::BadCliArgs
    })?;

    let settings = Config::builder()
        .add_source(File::new(config_path, FileFormat::Toml))
        .build()?;

    // Deserialize settings into TranslatorConfig
    let mut config = settings.try_deserialize::<TranslatorConfig>()?;

    config.set_log_dir(args.log_file);

    Ok(config)
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/config.rs">
//! ## Translator Configuration Module
//!
//! Defines [`TranslatorConfig`], the primary configuration structure for the Translator.
//!
//! This module provides the necessary structures to configure the Translator,
//! managing connections and settings for both upstream and downstream interfaces.
//!
//! This module handles:
//! - Upstream server address, port, and authentication key ([`UpstreamConfig`])
//! - Downstream interface address and port ([`DownstreamConfig`])
//! - Supported protocol versions
//! - Downstream difficulty adjustment parameters ([`DownstreamDifficultyConfig`])
//! - Upstream difficulty adjustment parameters ([`UpstreamDifficultyConfig`])
use std::path::{Path, PathBuf};

use key_utils::Secp256k1PublicKey;
use serde::Deserialize;

/// Configuration for the Translator.
#[derive(Debug, Deserialize, Clone)]
pub struct TranslatorConfig {
    /// The address of the upstream server.
    pub upstream_address: String,
    /// The port of the upstream server.
    pub upstream_port: u16,
    /// The Secp256k1 public key used to authenticate the upstream authority.
    pub upstream_authority_pubkey: Secp256k1PublicKey,
    /// The address for the downstream interface.
    pub downstream_address: String,
    /// The port for the downstream interface.
    pub downstream_port: u16,
    /// The maximum supported protocol version for communication.
    pub max_supported_version: u16,
    /// The minimum supported protocol version for communication.
    pub min_supported_version: u16,
    /// The minimum size required for the extranonce2 field in mining submissions.
    pub min_extranonce2_size: u16,
    /// Configuration settings for managing difficulty on the downstream connection.
    pub downstream_difficulty_config: DownstreamDifficultyConfig,
    /// Configuration settings for managing difficulty on the upstream connection.
    pub upstream_difficulty_config: UpstreamDifficultyConfig,
    /// The path to the log file for the Translator.
    log_file: Option<PathBuf>,
}

impl TranslatorConfig {
    pub fn set_log_dir(&mut self, log_dir: Option<PathBuf>) {
        if let Some(dir) = log_dir {
            self.log_file = Some(dir);
        }
    }
    pub fn log_dir(&self) -> Option<&Path> {
        self.log_file.as_deref()
    }
}

/// Configuration settings specific to the upstream connection.
pub struct UpstreamConfig {
    /// The address of the upstream server.
    address: String,
    /// The port of the upstream server.
    port: u16,
    /// The Secp256k1 public key used to authenticate the upstream authority.
    authority_pubkey: Secp256k1PublicKey,
    /// Configuration settings for managing difficulty on the upstream connection.
    difficulty_config: UpstreamDifficultyConfig,
}

impl UpstreamConfig {
    /// Creates a new `UpstreamConfig` instance.
    pub fn new(
        address: String,
        port: u16,
        authority_pubkey: Secp256k1PublicKey,
        difficulty_config: UpstreamDifficultyConfig,
    ) -> Self {
        Self {
            address,
            port,
            authority_pubkey,
            difficulty_config,
        }
    }
}

/// Configuration settings specific to the downstream connection.
pub struct DownstreamConfig {
    /// The address for the downstream interface.
    address: String,
    /// The port for the downstream interface.
    port: u16,
    /// Configuration settings for managing difficulty on the downstream connection.
    difficulty_config: DownstreamDifficultyConfig,
}

impl DownstreamConfig {
    /// Creates a new `DownstreamConfig` instance.
    pub fn new(address: String, port: u16, difficulty_config: DownstreamDifficultyConfig) -> Self {
        Self {
            address,
            port,
            difficulty_config,
        }
    }
}

impl TranslatorConfig {
    /// Creates a new `TranslatorConfig` instance by combining upstream and downstream
    /// configurations and specifying version and extranonce constraints.
    pub fn new(
        upstream: UpstreamConfig,
        downstream: DownstreamConfig,
        max_supported_version: u16,
        min_supported_version: u16,
        min_extranonce2_size: u16,
    ) -> Self {
        Self {
            upstream_address: upstream.address,
            upstream_port: upstream.port,
            upstream_authority_pubkey: upstream.authority_pubkey,
            downstream_address: downstream.address,
            downstream_port: downstream.port,
            max_supported_version,
            min_supported_version,
            min_extranonce2_size,
            downstream_difficulty_config: downstream.difficulty_config,
            upstream_difficulty_config: upstream.difficulty_config,
            log_file: None,
        }
    }
}

/// Configuration settings for managing difficulty adjustments on the downstream connection.
#[derive(Debug, Deserialize, Clone)]
pub struct DownstreamDifficultyConfig {
    /// The minimum hashrate expected from an individual miner on the downstream connection.
    pub min_individual_miner_hashrate: f32,
    /// The target number of shares per minute for difficulty adjustment.
    pub shares_per_minute: f32,
    /// The number of shares submitted since the last difficulty update.
    #[serde(default = "u32::default")]
    pub submits_since_last_update: u32,
    /// The timestamp of the last difficulty update.
    #[serde(default = "u64::default")]
    pub timestamp_of_last_update: u64,
}

impl DownstreamDifficultyConfig {
    /// Creates a new `DownstreamDifficultyConfig` instance.
    pub fn new(
        min_individual_miner_hashrate: f32,
        shares_per_minute: f32,
        submits_since_last_update: u32,
        timestamp_of_last_update: u64,
    ) -> Self {
        Self {
            min_individual_miner_hashrate,
            shares_per_minute,
            submits_since_last_update,
            timestamp_of_last_update,
        }
    }
}
impl PartialEq for DownstreamDifficultyConfig {
    fn eq(&self, other: &Self) -> bool {
        other.min_individual_miner_hashrate.round() as u32
            == self.min_individual_miner_hashrate.round() as u32
    }
}

/// Configuration settings for difficulty adjustments on the upstream connection.
#[derive(Debug, Deserialize, Clone)]
pub struct UpstreamDifficultyConfig {
    /// The interval in seconds at which the channel difficulty should be updated.
    pub channel_diff_update_interval: u32,
    /// The nominal hashrate for the channel, used in difficulty calculations.
    pub channel_nominal_hashrate: f32,
    /// The timestamp of the last difficulty update for the channel.
    #[serde(default = "u64::default")]
    pub timestamp_of_last_update: u64,
    /// Indicates whether shares from downstream should be aggregated before submitting upstream.
    #[serde(default = "bool::default")]
    pub should_aggregate: bool,
}

impl UpstreamDifficultyConfig {
    /// Creates a new `UpstreamDifficultyConfig` instance.
    pub fn new(
        channel_diff_update_interval: u32,
        channel_nominal_hashrate: f32,
        timestamp_of_last_update: u64,
        should_aggregate: bool,
    ) -> Self {
        Self {
            channel_diff_update_interval,
            channel_nominal_hashrate,
            timestamp_of_last_update,
            should_aggregate,
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/downstream_sv1/diff_management.rs">
//! ## Downstream SV1 Difficulty Management Module
//!
//! This module contains the logic and helper functions
//! for managing difficulty and hashrate adjustments for downstream mining clients
//! communicating via the SV1 protocol.
//!
//! It handles tasks such as:
//! - Converting SV2 targets received from upstream into SV1 difficulty values.
//! - Calculating and updating individual miner hashrates based on submitted shares.
//! - Preparing SV1 `mining.set_difficulty` messages.
//! - Potentially managing difficulty thresholds and adjustment logic for downstream miners.

use super::{Downstream, DownstreamMessages, SetDownstreamTarget};

use super::super::error::{Error, ProxyResult};
use primitive_types::U256;
use std::{ops::Div, sync::Arc};
use stratum_common::roles_logic_sv2::{
    codec_sv2::binary_sv2,
    mining_sv2::Target,
    utils::{hash_rate_to_target, Mutex},
};
use tracing::debug;
use v1::json_rpc;

impl Downstream {
    /// Initializes the difficulty management parameters for a downstream connection.
    ///
    /// This function sets the initial timestamp for the last difficulty update and
    /// resets the count of submitted shares. It also adds the miner's configured
    /// minimum hashrate to the aggregated channel nominal hashrate stored in the
    /// upstream difficulty configuration.Finally, it sends a `SetDownstreamTarget` message upstream
    /// to the Bridge to inform it of the initial target for this new connection, derived from
    /// the provided `init_target`.This should typically be called once when a downstream connection
    /// is established.
    pub async fn init_difficulty_management(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        let (connection_id, upstream_difficulty_config, miner_hashrate, init_target) = self_
            .safe_lock(|d| {
                _ = d.difficulty_mgmt.reset_counter();
                (
                    d.connection_id,
                    d.upstream_difficulty_config.clone(),
                    d.hashrate,
                    d.target.clone(),
                )
            })?;
        // add new connection hashrate to channel hashrate
        upstream_difficulty_config.safe_lock(|u| {
            u.channel_nominal_hashrate += miner_hashrate;
        })?;
        // update downstream target with bridge
        let init_target = binary_sv2::U256::from(init_target);
        Self::send_message_upstream(
            self_,
            DownstreamMessages::SetDownstreamTarget(SetDownstreamTarget {
                channel_id: connection_id,
                new_target: init_target.into(),
            }),
        )
        .await?;

        Ok(())
    }

    /// Removes the disconnecting miner's hashrate from the aggregated channel nominal hashrate.
    ///
    /// This function is called when a downstream miner disconnects to ensure that their
    /// individual hashrate is subtracted from the total nominal hashrate reported for
    /// the channel to the upstream server.
    #[allow(clippy::result_large_err)]
    pub fn remove_miner_hashrate_from_channel(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        self_.safe_lock(|d| {
            d.upstream_difficulty_config
                .safe_lock(|u| {
                    let hashrate_to_subtract = d.hashrate;
                    if u.channel_nominal_hashrate >= hashrate_to_subtract {
                        u.channel_nominal_hashrate -= hashrate_to_subtract;
                    } else {
                        u.channel_nominal_hashrate = 0.0;
                    }
                })
                .map_err(|_e| Error::PoisonLock)
        })??;
        Ok(())
    }

    /// Attempts to update the difficulty settings for a downstream miner based on their
    /// performance.
    ///
    /// This function is triggered periodically or based on share submissions. It calculates
    /// the miner's estimated hashrate based on the number of shares submitted and the elapsed
    /// time since the last update. If the estimated hashrate has changed significantly according to
    /// predefined thresholds, a new target is calculated, a `mining.set_difficulty` message is
    /// sent to the miner, and a `SetDownstreamTarget` message is sent upstream to the Bridge to
    /// notify it of the target change for this channel. The difficulty management parameters
    /// (timestamp and share count) are then reset.
    pub async fn try_update_difficulty_settings(
        self_: Arc<Mutex<Self>>,
    ) -> ProxyResult<'static, ()> {
        let (timestamp_of_last_update, shares_since_last_update, channel_id, shares_per_minute) =
            self_.clone().safe_lock(|d| {
                (
                    d.difficulty_mgmt.last_update_timestamp(),
                    d.difficulty_mgmt.shares_since_last_update(),
                    d.connection_id,
                    d.shares_per_minute,
                )
            })?;
        debug!("Time of last diff update: {:?}", timestamp_of_last_update);
        debug!("Number of shares submitted: {:?}", shares_since_last_update);

        if let Some(new_hashrate) = Self::update_miner_hashrate(self_.clone())? {
            let new_target: Target =
                hash_rate_to_target(new_hashrate.into(), shares_per_minute.into())?.into();
            debug!("New target from hashrate: {:?}", new_target);
            let message = Self::get_set_difficulty(new_target.clone())?;
            let target = binary_sv2::U256::from(new_target);
            Downstream::send_message_downstream(self_.clone(), message).await?;
            let update_target_msg = SetDownstreamTarget {
                channel_id,
                new_target: target.into(),
            };
            // notify bridge of target update
            Downstream::send_message_upstream(
                self_.clone(),
                DownstreamMessages::SetDownstreamTarget(update_target_msg),
            )
            .await?;
        }
        Ok(())
    }

    /// Increments the counter for shares submitted by this downstream miner.
    ///
    /// This function is called each time a valid share is received from the miner.
    /// The count is used in the difficulty adjustment logic to estimate the miner's
    /// performance over a period.
    #[allow(clippy::result_large_err)]
    pub(super) fn save_share(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        self_.safe_lock(|d| {
            d.difficulty_mgmt.increment_shares_since_last_update();
        })?;
        Ok(())
    }

    /// Converts an SV2 target received from upstream into an SV1 difficulty value
    /// and formats it as a `mining.set_difficulty` JSON-RPC message.
    #[allow(clippy::result_large_err)]
    pub(super) fn get_set_difficulty(target: Target) -> ProxyResult<'static, json_rpc::Message> {
        let value = Downstream::difficulty_from_target(target)?;
        debug!("Difficulty from target: {:?}", value);
        let set_target = v1::methods::server_to_client::SetDifficulty { value };
        let message: json_rpc::Message = set_target.into();
        Ok(message)
    }

    /// Converts target received by the `SetTarget` SV2 message from the Upstream role into the
    /// difficulty for the Downstream role sent via the SV1 `mining.set_difficulty` message.
    #[allow(clippy::result_large_err)]
    pub(super) fn difficulty_from_target(target: Target) -> ProxyResult<'static, f64> {
        // reverse because target is LE and this function relies on BE
        let mut target = binary_sv2::U256::from(target).to_vec();

        target.reverse();

        let target = target.as_slice();
        debug!("Target: {:?}", target);

        // If received target is 0, return 0
        if Downstream::is_zero(target) {
            return Ok(0.0);
        }
        let target = U256::from_big_endian(target);
        let pdiff: [u8; 32] = [
            0, 0, 0, 0, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
            255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
        ];
        let pdiff = U256::from_big_endian(pdiff.as_ref());

        if pdiff > target {
            let diff = pdiff.div(target);
            Ok(diff.low_u64() as f64)
        } else {
            let diff = target.div(pdiff);
            let diff = diff.low_u64() as f64;
            // TODO still results in a difficulty that is too low
            Ok(1.0 / diff)
        }
    }

    /// Updates the miner's estimated hashrate and adjusts the aggregated channel nominal hashrate.
    ///
    /// This function calculates the miner's realized shares per minute over the period
    /// since the last update and uses it, along with the current target, to estimate
    /// their hashrate. It then compares this new estimate to the previous one and
    /// updates the miner's stored hashrate and the channel's aggregated hashrate
    /// if the change is significant based on time-dependent thresholds.
    #[allow(clippy::result_large_err)]
    pub fn update_miner_hashrate(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, Option<f32>> {
        let update = self_.super_safe_lock(|d| {
            let previous_hashrate = d.hashrate;
            let previous_target = d.target.clone();
            let update = d.difficulty_mgmt.try_vardiff(
                previous_hashrate,
                &previous_target,
                d.shares_per_minute,
            );
            if let Ok(Some(new_hashrate)) = update {
                // update channel hashrate and target
                let new_target: Target =
                    hash_rate_to_target(new_hashrate.into(), d.shares_per_minute.into())
                        .expect("Something went wrong while target calculation")
                        .into();
                d.hashrate = new_hashrate;
                d.target = new_target.clone();
                let hashrate_delta = new_hashrate - previous_hashrate;
                d.upstream_difficulty_config.super_safe_lock(|c| {
                    if c.channel_nominal_hashrate + hashrate_delta > 0.0 {
                        c.channel_nominal_hashrate += hashrate_delta;
                    } else {
                        c.channel_nominal_hashrate = 0.0;
                    }
                });
            }
            update
        })?;
        Ok(update)
    }

    /// Helper function to check if target is set to zero for some reason (typically happens when
    /// Downstream role first connects).
    /// https://stackoverflow.com/questions/65367552/checking-a-vecu8-to-see-if-its-all-zero
    fn is_zero(buf: &[u8]) -> bool {
        let (prefix, aligned, suffix) = unsafe { buf.align_to::<u128>() };

        prefix.iter().all(|&x| x == 0)
            && suffix.iter().all(|&x| x == 0)
            && aligned.iter().all(|&x| x == 0)
    }
}

#[cfg(test)]
mod test {

    use crate::config::{DownstreamDifficultyConfig, UpstreamDifficultyConfig};
    use async_channel::unbounded;
    use rand::{thread_rng, Rng};
    use sha2::{Digest, Sha256};
    use std::{
        sync::Arc,
        time::{Duration, Instant},
    };
    use stratum_common::roles_logic_sv2::{
        self,
        codec_sv2::binary_sv2::{self, U256},
        mining_sv2::Target,
        utils::Mutex,
    };

    use crate::downstream_sv1::Downstream;

    #[ignore] // as described in issue #988
    #[test]
    fn test_diff_management() {
        let expected_shares_per_minute = 1000.0;
        let total_run_time = std::time::Duration::from_secs(60);
        let initial_nominal_hashrate = measure_hashrate(5);
        let target = match roles_logic_sv2::utils::hash_rate_to_target(
            initial_nominal_hashrate,
            expected_shares_per_minute,
        ) {
            Ok(target) => target,
            Err(_) => panic!(),
        };

        let mut share = generate_random_80_byte_array();
        let timer = std::time::Instant::now();
        let mut elapsed = std::time::Duration::from_secs(0);
        let mut count = 0;
        while elapsed <= total_run_time {
            // start hashing util a target is met and submit to
            mock_mine(target.clone().into(), &mut share);
            elapsed = timer.elapsed();
            count += 1;
        }

        let calculated_share_per_min = count as f32 / (elapsed.as_secs_f32() / 60.0);
        // This is the error margin for a confidence of 99.99...% given the expect number of shares
        // per minute TODO the review the math under it
        let error_margin = get_error(expected_shares_per_minute);
        let error = (calculated_share_per_min - expected_shares_per_minute as f32).abs();
        assert!(
            error <= error_margin as f32,
            "Calculated shares per minute are outside the 99.99...% confidence interval. Error: {error:?}, Error margin: {error_margin:?}, {calculated_share_per_min:?}"
        );
    }

    fn get_error(lambda: f64) -> f64 {
        let z_score_99 = 6.0;
        z_score_99 * lambda.sqrt()
    }

    fn mock_mine(target: Target, share: &mut [u8; 80]) {
        let mut hashed: Target = [255_u8; 32].into();
        while hashed > target {
            hashed = hash(share);
        }
    }

    // returns hashrate based on how fast the device hashes over the given duration
    fn measure_hashrate(duration_secs: u64) -> f64 {
        let mut share = generate_random_80_byte_array();
        let start_time = Instant::now();
        let mut hashes: u64 = 0;
        let duration = Duration::from_secs(duration_secs);

        while start_time.elapsed() < duration {
            for _ in 0..10000 {
                hash(&mut share);
                hashes += 1;
            }
        }

        let elapsed_secs = start_time.elapsed().as_secs_f64();

        hashes as f64 / elapsed_secs
    }

    fn hash(share: &mut [u8; 80]) -> Target {
        let nonce: [u8; 8] = share[0..8].try_into().unwrap();
        let mut nonce = u64::from_le_bytes(nonce);
        nonce += 1;
        share[0..8].copy_from_slice(&nonce.to_le_bytes());
        let hash = Sha256::digest(&share).to_vec();
        let hash: U256<'static> = hash.try_into().unwrap();
        hash.into()
    }

    fn generate_random_80_byte_array() -> [u8; 80] {
        let mut rng = thread_rng();
        let mut arr = [0u8; 80];
        rng.fill(&mut arr[..]);
        arr
    }

    #[tokio::test]
    async fn test_converge_to_spm_from_low() {
        test_converge_to_spm(1.0).await
    }
    //TODO
    //#[tokio::test]
    //async fn test_converge_to_spm_from_high() {
    //    test_converge_to_spm(1_000_000_000_000).await
    //}

    async fn test_converge_to_spm(start_hashrate: f64) {
        let downstream_conf = DownstreamDifficultyConfig {
            min_individual_miner_hashrate: start_hashrate as f32, // updated below
            shares_per_minute: 1000.0,                            // 1000 shares per minute
            submits_since_last_update: 0,
            timestamp_of_last_update: 0, // updated below
        };
        let upstream_config = UpstreamDifficultyConfig {
            channel_diff_update_interval: 60,
            channel_nominal_hashrate: 0.0,
            timestamp_of_last_update: 0,
            should_aggregate: false,
        };
        let (tx_sv1_submit, _rx_sv1_submit) = unbounded();
        let (tx_outgoing, _rx_outgoing) = unbounded();
        let downstream = Downstream::new(
            1,
            vec![],
            vec![],
            None,
            None,
            tx_sv1_submit,
            tx_outgoing,
            false,
            0,
            downstream_conf.clone(),
            Arc::new(Mutex::new(upstream_config)),
        );

        let total_run_time = std::time::Duration::from_secs(75);
        let config_shares_per_minute = downstream_conf.shares_per_minute;
        let timer = std::time::Instant::now();
        let mut elapsed = std::time::Duration::from_secs(0);

        let expected_nominal_hashrate = measure_hashrate(5);
        let expected_target = match roles_logic_sv2::utils::hash_rate_to_target(
            expected_nominal_hashrate,
            config_shares_per_minute.into(),
        ) {
            Ok(target) => target,
            Err(_) => panic!(),
        };

        let mut initial_target = downstream.target.clone();
        let downstream = Arc::new(Mutex::new(downstream));
        Downstream::init_difficulty_management(downstream.clone())
            .await
            .unwrap();
        let mut share = generate_random_80_byte_array();
        while elapsed <= total_run_time {
            mock_mine(initial_target.clone(), &mut share);
            Downstream::save_share(downstream.clone()).unwrap();
            Downstream::try_update_difficulty_settings(downstream.clone())
                .await
                .unwrap();
            initial_target = downstream.safe_lock(|d| d.target.clone()).unwrap();
            elapsed = timer.elapsed();
        }
        let expected_0s = trailing_0s(expected_target.inner_as_ref().to_vec());
        let actual_0s = trailing_0s(binary_sv2::U256::from(initial_target.clone()).to_vec());
        assert!(expected_0s.abs_diff(actual_0s) <= 1);
    }

    fn trailing_0s(mut v: Vec<u8>) -> usize {
        let mut ret = 0;
        while v.pop() == Some(0) {
            ret += 1;
        }
        ret
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/downstream_sv1/downstream.rs">
//! ## Downstream SV1 Module: Downstream Connection Logic
//!
//! Defines the [`Downstream`] structure, which represents and manages an
//! individual connection from a downstream SV1 mining client.
//!
//! This module is responsible for:
//! - Accepting incoming TCP connections from SV1 miners.
//! - Handling the SV1 protocol handshake (`mining.subscribe`, `mining.authorize`,
//!   `mining.configure`).
//! - Receiving SV1 `mining.submit` messages from miners.
//! - Translating SV1 `mining.submit` messages into internal [`DownstreamMessages`] (specifically
//!   [`SubmitShareWithChannelId`]) and sending them to the Bridge.
//! - Receiving translated SV1 `mining.notify` messages from the Bridge and sending them to the
//!   connected miner.
//! - Managing the miner's extranonce1, extranonce2 size, and version rolling parameters.
//! - Implementing downstream-specific difficulty management logic, including tracking submitted
//!   shares and updating the miner's difficulty target.
//! - Implementing the necessary SV1 server traits ([`IsServer`]) and SV2 roles logic traits
//!   ([`IsMiningDownstream`], [`IsDownstream`]).

use crate::{
    config::{DownstreamDifficultyConfig, UpstreamDifficultyConfig},
    downstream_sv1,
    error::ProxyResult,
    status,
};
use async_channel::{bounded, Receiver, Sender};
use error_handling::handle_result;
use futures::{FutureExt, StreamExt};
use tokio::{
    io::{AsyncWriteExt, BufReader},
    net::{TcpListener, TcpStream},
    sync::broadcast,
    task::AbortHandle,
};

use super::{kill, DownstreamMessages, SubmitShareWithChannelId, SUBSCRIBE_TIMEOUT_SECS};

use stratum_common::roles_logic_sv2::{
    mining_sv2::Target,
    utils::{hash_rate_to_target, Mutex},
    vardiff::Vardiff,
    VardiffState,
};

use crate::error::Error;
use futures::select;
use tokio_util::codec::{FramedRead, LinesCodec};

use std::{net::SocketAddr, sync::Arc};
use tracing::{debug, info, warn};
use v1::{
    client_to_server::{self, Submit},
    json_rpc, server_to_client,
    utils::{Extranonce, HexU32Be},
    IsServer,
};

/// The maximum allowed length for a single line (JSON-RPC message) received from an SV1 client.
const MAX_LINE_LENGTH: usize = 2_usize.pow(16);

/// Handles the sending and receiving of messages to and from an SV2 Upstream role (most typically
/// a SV2 Pool server).
#[derive(Debug)]
pub struct Downstream {
    /// The unique identifier assigned to this downstream connection/channel.
    pub(super) connection_id: u32,
    /// List of authorized Downstream Mining Devices.
    authorized_names: Vec<String>,
    /// The extranonce1 value assigned to this downstream miner.
    extranonce1: Vec<u8>,
    /// `extranonce1` to be sent to the Downstream in the SV1 `mining.subscribe` message response.
    //extranonce1: Vec<u8>,
    //extranonce2_size: usize,
    /// Version rolling mask bits
    version_rolling_mask: Option<HexU32Be>,
    /// Minimum version rolling mask bits size
    version_rolling_min_bit: Option<HexU32Be>,
    /// Sends a SV1 `mining.submit` message received from the Downstream role to the `Bridge` for
    /// translation into a SV2 `SubmitSharesExtended`.
    tx_sv1_bridge: Sender<DownstreamMessages>,
    /// Sends message to the SV1 Downstream role.
    tx_outgoing: Sender<json_rpc::Message>,
    /// True if this is the first job received from `Upstream`.
    first_job_received: bool,
    /// The expected size of the extranonce2 field provided by the miner.
    extranonce2_len: usize,
    // Current Channel target
    pub target: Target,
    // Current channel hashrate
    pub hashrate: f32,
    // Shares_per_minute
    pub shares_per_minute: f32,
    /// Configuration and state for managing difficulty adjustments specific
    /// to this individual downstream miner.
    pub(super) difficulty_mgmt: Box<dyn Vardiff>,
    /// Configuration settings for the upstream channel's difficulty management.
    pub(super) upstream_difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
}

impl Downstream {
    // not huge fan of test specific code in codebase.
    #[cfg(test)]
    pub fn new(
        connection_id: u32,
        authorized_names: Vec<String>,
        extranonce1: Vec<u8>,
        version_rolling_mask: Option<HexU32Be>,
        version_rolling_min_bit: Option<HexU32Be>,
        tx_sv1_bridge: Sender<DownstreamMessages>,
        tx_outgoing: Sender<json_rpc::Message>,
        first_job_received: bool,
        extranonce2_len: usize,
        difficulty_mgmt: DownstreamDifficultyConfig,
        upstream_difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
    ) -> Self {
        let hashrate = difficulty_mgmt.min_individual_miner_hashrate;
        let target = hash_rate_to_target(hashrate.into(), difficulty_mgmt.shares_per_minute.into())
            .unwrap()
            .into();
        let downstream_difficulty_state = VardiffState::new().unwrap();
        Downstream {
            connection_id,
            authorized_names,
            extranonce1,
            version_rolling_mask,
            version_rolling_min_bit,
            tx_sv1_bridge,
            tx_outgoing,
            first_job_received,
            extranonce2_len,
            hashrate,
            target,
            shares_per_minute: difficulty_mgmt.shares_per_minute,
            difficulty_mgmt: Box::new(downstream_difficulty_state),
            upstream_difficulty_config,
        }
    }
    /// Instantiates and manages a new handler for a single downstream SV1 client connection.
    ///
    /// This is the primary function called for each new incoming TCP stream from a miner.
    /// It sets up the communication channels, initializes the `Downstream` struct state,
    /// and spawns the necessary tasks to handle:
    /// 1. Reading incoming messages from the miner's socket.
    /// 2. Writing outgoing messages to the miner's socket.
    /// 3. Sending job notifications to the miner (handling initial job and subsequent updates).
    ///
    /// It uses shutdown channels to coordinate graceful termination of the spawned tasks.
    #[allow(clippy::too_many_arguments)]
    pub async fn new_downstream(
        stream: TcpStream,
        connection_id: u32,
        tx_sv1_bridge: Sender<DownstreamMessages>,
        mut rx_sv1_notify: broadcast::Receiver<server_to_client::Notify<'static>>,
        tx_status: status::Sender,
        extranonce1: Vec<u8>,
        last_notify: Option<server_to_client::Notify<'static>>,
        extranonce2_len: usize,
        host: String,
        difficulty_config: DownstreamDifficultyConfig,
        upstream_difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
        task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
    ) {
        let hashrate = difficulty_config.min_individual_miner_hashrate;
        let target =
            hash_rate_to_target(hashrate.into(), difficulty_config.shares_per_minute.into())
                .expect("Couldn't convert hashrate to target")
                .into();

        let downstream_difficulty_state =
            VardiffState::new().expect("Couldn't initialize vardiff module");
        // Reads and writes from Downstream SV1 Mining Device Client
        let (socket_reader, mut socket_writer) = stream.into_split();
        let (tx_outgoing, receiver_outgoing) = bounded(10);

        let downstream = Arc::new(Mutex::new(Downstream {
            connection_id,
            authorized_names: vec![],
            extranonce1,
            //extranonce1: extranonce1.to_vec(),
            version_rolling_mask: None,
            version_rolling_min_bit: None,
            tx_sv1_bridge,
            tx_outgoing,
            first_job_received: false,
            extranonce2_len,
            hashrate,
            target,
            shares_per_minute: difficulty_config.shares_per_minute,
            difficulty_mgmt: Box::new(downstream_difficulty_state),
            upstream_difficulty_config,
        }));
        let self_ = downstream.clone();

        let host_ = host.clone();
        // The shutdown channel is used local to the `Downstream::new_downstream()` function.
        // Each task is set broadcast a shutdown message at the end of their lifecycle with
        // `kill()`, and each task has a receiver to listen for the shutdown message. When a
        // shutdown message is received the task should `break` its loop. For any errors that should
        // shut a task down, we should `break` out of the loop, so that the `kill` function
        // can send the shutdown broadcast. EXTRA: The since all downstream tasks rely on
        // receiving messages with a future (either TCP recv or Receiver<_>) we use the
        // futures::select! macro to merge the receiving end of a task channels into a single loop
        // within the task
        let (tx_shutdown, rx_shutdown): (Sender<bool>, Receiver<bool>) = async_channel::bounded(3);

        let rx_shutdown_clone = rx_shutdown.clone();
        let tx_shutdown_clone = tx_shutdown.clone();
        let tx_status_reader = tx_status.clone();
        let task_collector_mining_device = task_collector.clone();
        // Task to read from SV1 Mining Device Client socket via `socket_reader`. Depending on the
        // SV1 message received, a message response is sent directly back to the SV1 Downstream
        // role, or the message is sent upwards to the Bridge for translation into a SV2 message
        // and then sent to the SV2 Upstream role.
        let socket_reader_task = tokio::task::spawn(async move {
            let reader = BufReader::new(socket_reader);
            let mut messages =
                FramedRead::new(reader, LinesCodec::new_with_max_length(MAX_LINE_LENGTH));
            loop {
                // Read message from SV1 Mining Device Client socket
                // On message receive, parse to `json_rpc:Message` and send to Upstream
                // `Translator.receive_downstream` via `sender_upstream` done in
                // `send_message_upstream`.
                select! {
                    res = messages.next().fuse() => {
                        match res {
                            Some(Ok(incoming)) => {
                                debug!("Receiving from Mining Device {}: {:?}", &host_, &incoming);
                                let incoming: json_rpc::Message = handle_result!(tx_status_reader, serde_json::from_str(&incoming));
                                // Handle what to do with message
                                // if let json_rpc::Message

                                // if message is Submit Shares update difficulty management
                                if let v1::Message::StandardRequest(standard_req) = incoming.clone() {
                                    if let Ok(Submit{..}) = standard_req.try_into() {
                                        handle_result!(tx_status_reader, Self::save_share(self_.clone()));
                                    }
                                }

                                let res = Self::handle_incoming_sv1(self_.clone(), incoming).await;
                                handle_result!(tx_status_reader, res);
                            }
                            Some(Err(_)) => {
                                handle_result!(tx_status_reader, Err(Error::Sv1MessageTooLong));
                            }
                            None => {
                                handle_result!(tx_status_reader, Err(
                                    std::io::Error::new(
                                        std::io::ErrorKind::ConnectionAborted,
                                        "Connection closed by client"
                                    )
                                ));
                            }
                        }
                    },
                    _ = rx_shutdown_clone.recv().fuse() => {
                        break;
                    }
                };
            }
            kill(&tx_shutdown_clone).await;
            warn!("Downstream: Shutting down sv1 downstream reader");
        });
        let _ = task_collector_mining_device.safe_lock(|a| {
            a.push((
                socket_reader_task.abort_handle(),
                "socket_reader_task".to_string(),
            ))
        });

        let rx_shutdown_clone = rx_shutdown.clone();
        let tx_shutdown_clone = tx_shutdown.clone();
        let tx_status_writer = tx_status.clone();
        let host_ = host.clone();

        let task_collector_new_sv1_message_no_transl = task_collector.clone();
        // Task to receive SV1 message responses to SV1 messages that do NOT need translation.
        // These response messages are sent directly to the SV1 Downstream role.
        let socket_writer_task = tokio::task::spawn(async move {
            loop {
                select! {
                    res = receiver_outgoing.recv().fuse() => {
                        let to_send = handle_result!(tx_status_writer, res);
                        let to_send = match serde_json::to_string(&to_send) {
                            Ok(string) => format!("{string}\n"),
                            Err(_e) => {
                                debug!("\nDownstream: Bad SV1 server message\n");
                                break;
                            }
                        };
                        debug!("Sending to Mining Device: {} - {:?}", &host_, &to_send);
                        let res = socket_writer
                                    .write_all(to_send.as_bytes())
                                    .await;
                        handle_result!(tx_status_writer, res);
                    },
                    _ = rx_shutdown_clone.recv().fuse() => {
                            break;
                        }
                };
            }
            kill(&tx_shutdown_clone).await;
            warn!(
                "Downstream: Shutting down sv1 downstream writer: {}",
                &host_
            );
        });
        let _ = task_collector_new_sv1_message_no_transl.safe_lock(|a| {
            a.push((
                socket_writer_task.abort_handle(),
                "socket_writer_task".to_string(),
            ))
        });

        let tx_status_notify = tx_status;
        let self_ = downstream.clone();

        let task_collector_notify_task = task_collector.clone();
        let notify_task = tokio::task::spawn(async move {
            let timeout_timer = std::time::Instant::now();
            let mut first_sent = false;
            loop {
                let is_a = match downstream.safe_lock(|d| !d.authorized_names.is_empty()) {
                    Ok(is_a) => is_a,
                    Err(_e) => {
                        debug!("\nDownstream: Poison Lock - authorized_names\n");
                        break;
                    }
                };
                if is_a && !first_sent && last_notify.is_some() {
                    let target = downstream
                        .safe_lock(|d| d.target.clone())
                        .expect("downstream target couldn't be computed");
                    // make sure the mining start time is initialized and reset number of shares
                    // submitted
                    handle_result!(
                        tx_status_notify,
                        Self::init_difficulty_management(downstream.clone()).await
                    );
                    let message =
                        handle_result!(tx_status_notify, Self::get_set_difficulty(target));
                    handle_result!(
                        tx_status_notify,
                        Downstream::send_message_downstream(downstream.clone(), message).await
                    );

                    let sv1_mining_notify_msg = last_notify.clone().unwrap();

                    let message: json_rpc::Message = sv1_mining_notify_msg.into();
                    handle_result!(
                        tx_status_notify,
                        Downstream::send_message_downstream(downstream.clone(), message).await
                    );
                    if let Err(_e) = downstream.clone().safe_lock(|s| {
                        s.first_job_received = true;
                    }) {
                        debug!("\nDownstream: Poison Lock - first_job_received\n");
                        break;
                    }
                    first_sent = true;
                } else if is_a {
                    // if hashrate has changed, update difficulty management, and send new
                    // mining.set_difficulty
                    select! {
                        res = rx_sv1_notify.recv().fuse() => {
                            // if hashrate has changed, update difficulty management, and send new mining.set_difficulty
                            handle_result!(tx_status_notify, Self::try_update_difficulty_settings(downstream.clone()).await);

                            let sv1_mining_notify_msg = handle_result!(tx_status_notify, res);
                            let message: json_rpc::Message = sv1_mining_notify_msg.clone().into();

                            handle_result!(tx_status_notify, Downstream::send_message_downstream(downstream.clone(), message).await);
                        },
                        _ = rx_shutdown.recv().fuse() => {
                                break;
                            }
                    };
                } else {
                    // timeout connection if miner does not send the authorize message after sending
                    // a subscribe
                    if timeout_timer.elapsed().as_secs() > SUBSCRIBE_TIMEOUT_SECS {
                        debug!(
                            "Downstream: miner.subscribe/miner.authorize TIMOUT for {}",
                            &host
                        );
                        break;
                    }
                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                }
            }
            let _ = Self::remove_miner_hashrate_from_channel(self_);
            kill(&tx_shutdown).await;
            warn!(
                "Downstream: Shutting down sv1 downstream job notifier for {}",
                &host
            );
        });

        let _ = task_collector_notify_task
            .safe_lock(|a| a.push((notify_task.abort_handle(), "notify_task".to_string())));
    }

    /// Accepts incoming TCP connections from SV1 mining clients on the configured address.
    ///
    /// For each new connection, it attempts to open a new SV1 downstream channel
    /// via the Bridge (`bridge.on_new_sv1_connection`). If successful, it spawns
    /// a new task using `Downstream::new_downstream` to handle
    /// the communication and logic for that specific miner connection.
    /// This method runs indefinitely, listening for and accepting new connections.
    #[allow(clippy::too_many_arguments)]
    pub fn accept_connections(
        downstream_addr: SocketAddr,
        tx_sv1_submit: Sender<DownstreamMessages>,
        tx_mining_notify: broadcast::Sender<server_to_client::Notify<'static>>,
        tx_status: status::Sender,
        bridge: Arc<Mutex<crate::proxy::Bridge>>,
        downstream_difficulty_config: DownstreamDifficultyConfig,
        upstream_difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
        task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
    ) {
        let accept_connections = tokio::task::spawn({
            let task_collector = task_collector.clone();
            async move {
                let listener = TcpListener::bind(downstream_addr).await.unwrap();

                while let Ok((stream, _)) = listener.accept().await {
                    let expected_hash_rate =
                        downstream_difficulty_config.min_individual_miner_hashrate;
                    let open_sv1_downstream = bridge
                        .safe_lock(|s| s.on_new_sv1_connection(expected_hash_rate))
                        .unwrap();

                    let host = stream.peer_addr().unwrap().to_string();

                    match open_sv1_downstream {
                        Ok(opened) => {
                            info!("PROXY SERVER - ACCEPTING FROM DOWNSTREAM: {}", host);
                            Downstream::new_downstream(
                                stream,
                                opened.channel_id,
                                tx_sv1_submit.clone(),
                                tx_mining_notify.subscribe(),
                                tx_status.listener_to_connection(),
                                opened.extranonce,
                                opened.last_notify,
                                opened.extranonce2_len as usize,
                                host,
                                downstream_difficulty_config.clone(),
                                upstream_difficulty_config.clone(),
                                task_collector.clone(),
                            )
                            .await;
                        }
                        Err(e) => {
                            tracing::error!(
                                "Failed to create a new downstream connection: {:?}",
                                e
                            );
                        }
                    }
                }
            }
        });
        let _ = task_collector.safe_lock(|a| {
            a.push((
                accept_connections.abort_handle(),
                "accept_connections".to_string(),
            ))
        });
    }

    /// Handles incoming SV1 JSON-RPC messages from a downstream miner.
    ///
    /// This function acts as the entry point for processing messages received
    /// from a miner after framing. It uses the `IsServer` trait implementation
    /// to parse and handle standard SV1 requests (`mining.subscribe`, `mining.authorize`,
    /// `mining.submit`, `mining.configure`). Depending on the message type, it may generate a
    /// direct SV1 response to be sent back to the miner or indicate that the message needs to
    /// be translated and sent upstream (handled elsewhere, typically by the Bridge).
    async fn handle_incoming_sv1(
        self_: Arc<Mutex<Self>>,
        message_sv1: json_rpc::Message,
    ) -> Result<(), super::super::error::Error<'static>> {
        // `handle_message` in `IsServer` trait + calls `handle_request`
        // TODO: Map err from V1Error to Error::V1Error
        let response = self_.safe_lock(|s| s.handle_message(message_sv1)).unwrap();
        match response {
            Ok(res) => {
                if let Some(r) = res {
                    // If some response is received, indicates no messages translation is needed
                    // and response should be sent directly to the SV1 Downstream. Otherwise,
                    // message will be sent to the upstream Translator to be translated to SV2 and
                    // forwarded to the `Upstream`
                    // let sender = self_.safe_lock(|s| s.connection.sender_upstream)
                    if let Err(e) = Self::send_message_downstream(self_, r.into()).await {
                        return Err(e.into());
                    }
                    Ok(())
                } else {
                    // If None response is received, indicates this SV1 message received from the
                    // Downstream MD is passed to the `Translator` for translation into SV2
                    Ok(())
                }
            }
            Err(e) => Err(e.into()),
        }
    }

    /// Sends a SV1 JSON-RPC message to the downstream miner's socket writer task.
    ///
    /// This method is used to send response messages or notifications (like
    /// `mining.notify` or `mining.set_difficulty`) to the connected miner.
    /// The message is sent over the internal `tx_outgoing` channel, which is
    /// read by the socket writer task responsible for serializing and writing
    /// the message to the TCP stream.
    pub(super) async fn send_message_downstream(
        self_: Arc<Mutex<Self>>,
        response: json_rpc::Message,
    ) -> Result<(), async_channel::SendError<v1::Message>> {
        let sender = self_.safe_lock(|s| s.tx_outgoing.clone()).unwrap();
        debug!("To DOWN: {:?}", response);
        sender.send(response).await
    }

    /// Sends a message originating from the downstream handler to the Bridge.
    ///
    /// This function is used to forward messages that require translation or
    /// central processing by the Bridge, such as `SubmitShares` or `SetDownstreamTarget`.
    /// The message is sent over the internal `tx_sv1_bridge` channel.
    pub(super) async fn send_message_upstream(
        self_: Arc<Mutex<Self>>,
        msg: DownstreamMessages,
    ) -> ProxyResult<'static, ()> {
        let sender = self_.safe_lock(|s| s.tx_sv1_bridge.clone()).unwrap();
        debug!("To Bridge: {:?}", msg);
        let _ = sender.send(msg).await;
        Ok(())
    }
}

/// Implements `IsServer` for `Downstream` to handle the SV1 messages.
impl IsServer<'static> for Downstream {
    /// Handles the incoming SV1 `mining.configure` message.
    ///
    /// This message is received after `mining.subscribe` and `mining.authorize`.
    /// It allows the miner to negotiate capabilities, particularly regarding
    /// version rolling. This method processes the version rolling mask and
    /// minimum bit count provided by the client.
    ///
    /// Returns a tuple containing:
    /// 1. `Option<server_to_client::VersionRollingParams>`: The version rolling parameters
    ///    negotiated by the server (proxy).
    /// 2. `Option<bool>`: A boolean indicating whether the server (proxy) supports version rolling
    ///    (always `Some(false)` for TProxy according to the SV1 spec when not supporting work
    ///    selection).
    fn handle_configure(
        &mut self,
        request: &client_to_server::Configure,
    ) -> (Option<server_to_client::VersionRollingParams>, Option<bool>) {
        info!("Down: Configuring");
        debug!("Down: Handling mining.configure: {:?}", &request);

        // TODO 0x1FFFE000 should be configured
        // = 11111111111111110000000000000
        // this is a reasonable default as it allows all 16 version bits to be used
        // If the tproxy/pool needs to use some version bits this needs to be configurable
        // so upstreams can negotiate with downstreams. When that happens this should consider
        // the min_bit_count in the mining.configure message
        self.version_rolling_mask = request
            .version_rolling_mask()
            .map(|mask| HexU32Be(mask & 0x1FFFE000));
        self.version_rolling_min_bit = request.version_rolling_min_bit_count();

        debug!(
            "Negotiated version_rolling_mask is {:?}",
            self.version_rolling_mask
        );
        (
            Some(server_to_client::VersionRollingParams::new(
                self.version_rolling_mask.clone().unwrap_or(HexU32Be(0)),
                self.version_rolling_min_bit.clone().unwrap_or(HexU32Be(0)),
            ).expect("Version mask invalid, automatic version mask selection not supported, please change it in carte::downstream_sv1::mod.rs")),
            Some(false),
        )
    }

    /// Handles the incoming SV1 `mining.subscribe` message.
    ///
    /// This is typically the first message received from a new client. In the SV1
    /// protocol, it's used to subscribe to job notifications and receive session
    /// details like extranonce1 and extranonce2 size. This method acknowledges the subscription and
    /// provides the necessary details derived from the upstream SV2 connection (extranonce1 and
    /// extranonce2 size). It also provides subscription IDs for the
    /// `mining.set_difficulty` and `mining.notify` methods.
    fn handle_subscribe(&self, request: &client_to_server::Subscribe) -> Vec<(String, String)> {
        info!("Down: Subscribing");
        debug!("Down: Handling mining.subscribe: {:?}", &request);

        let set_difficulty_sub = (
            "mining.set_difficulty".to_string(),
            downstream_sv1::new_subscription_id(),
        );
        let notify_sub = (
            "mining.notify".to_string(),
            "ae6812eb4cd7735a302a8a9dd95cf71f".to_string(),
        );

        vec![set_difficulty_sub, notify_sub]
    }

    /// Any numbers of workers may be authorized at any time during the session. In this way, a
    /// large number of independent Mining Devices can be handled with a single SV1 connection.
    /// https://bitcoin.stackexchange.com/questions/29416/how-do-pool-servers-handle-multiple-workers-sharing-one-connection-with-stratum
    fn handle_authorize(&self, request: &client_to_server::Authorize) -> bool {
        info!("Down: Authorizing");
        debug!("Down: Handling mining.authorize: {:?}", &request);
        true
    }

    /// Handles the incoming SV1 `mining.submit` message.
    ///
    /// This message is sent by the miner when they find a share that meets
    /// their current difficulty target. It contains the job ID, ntime, nonce,
    /// and extranonce2.
    ///
    /// This method processes the submitted share, potentially validates it
    /// against the downstream target (although this might happen in the Bridge
    /// or difficulty management logic), translates it into a
    /// [`SubmitShareWithChannelId`], and sends it to the Bridge for
    /// translation to SV2 and forwarding upstream if it meets the upstream target.
    fn handle_submit(&self, request: &client_to_server::Submit<'static>) -> bool {
        info!("Down: Submitting Share {:?}", request);
        debug!("Down: Handling mining.submit: {:?}", &request);

        // TODO: Check if receiving valid shares by adding diff field to Downstream

        let to_send = SubmitShareWithChannelId {
            channel_id: self.connection_id,
            share: request.clone(),
            extranonce: self.extranonce1.clone(),
            extranonce2_len: self.extranonce2_len,
            version_rolling_mask: self.version_rolling_mask.clone(),
        };

        self.tx_sv1_bridge
            .try_send(DownstreamMessages::SubmitShares(to_send))
            .unwrap();

        true
    }

    /// Indicates to the server that the client supports the mining.set_extranonce method.
    fn handle_extranonce_subscribe(&self) {}

    /// Checks if a Downstream role is authorized.
    fn is_authorized(&self, name: &str) -> bool {
        self.authorized_names.contains(&name.to_string())
    }

    /// Authorizes a Downstream role.
    fn authorize(&mut self, name: &str) {
        self.authorized_names.push(name.to_string());
    }

    /// Sets the `extranonce1` field sent in the SV1 `mining.notify` message to the value specified
    /// by the SV2 `OpenExtendedMiningChannelSuccess` message sent from the Upstream role.
    fn set_extranonce1(
        &mut self,
        _extranonce1: Option<Extranonce<'static>>,
    ) -> Extranonce<'static> {
        self.extranonce1.clone().try_into().unwrap()
    }

    /// Returns the `Downstream`'s `extranonce1` value.
    fn extranonce1(&self) -> Extranonce<'static> {
        self.extranonce1.clone().try_into().unwrap()
    }

    /// Sets the `extranonce2_size` field sent in the SV1 `mining.notify` message to the value
    /// specified by the SV2 `OpenExtendedMiningChannelSuccess` message sent from the Upstream role.
    fn set_extranonce2_size(&mut self, _extra_nonce2_size: Option<usize>) -> usize {
        self.extranonce2_len
    }

    /// Returns the `Downstream`'s `extranonce2_size` value.
    fn extranonce2_size(&self) -> usize {
        self.extranonce2_len
    }

    /// Returns the version rolling mask.
    fn version_rolling_mask(&self) -> Option<HexU32Be> {
        self.version_rolling_mask.clone()
    }

    /// Sets the version rolling mask.
    fn set_version_rolling_mask(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_mask = mask;
    }

    /// Sets the minimum version rolling bit.
    fn set_version_rolling_min_bit(&mut self, mask: Option<HexU32Be>) {
        self.version_rolling_min_bit = mask
    }

    fn notify(&mut self) -> Result<json_rpc::Message, v1::error::Error> {
        unreachable!()
    }
}

#[cfg(test)]
mod tests {
    use stratum_common::roles_logic_sv2::{codec_sv2::binary_sv2::U256, mining_sv2::Target};

    use super::*;

    #[test]
    fn gets_difficulty_from_target() {
        let target = vec![
            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 128, 255, 127,
            0, 0, 0, 0, 0,
        ];
        let target_u256 = U256::Owned(target);
        let target = Target::from(target_u256);
        let actual = Downstream::difficulty_from_target(target).unwrap();
        let expect = 512.0;
        assert_eq!(actual, expect);
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/downstream_sv1/mod.rs">
//! ## Downstream SV1 Module
//!
//! This module defines the structures, messages, and utility functions
//! used for handling the downstream connection with SV1 mining clients.
//!
//! It includes definitions for messages exchanged with a Bridge component,
//! structures for submitting shares and updating targets, and constants
//! and functions for managing client interactions.
//!
//! The module is organized into the following sub-modules:
//! - [`diff_management`]: (Declared here, likely contains downstream difficulty logic)
//! - [`downstream`]: Defines the core [`Downstream`] struct and its functionalities.

use stratum_common::roles_logic_sv2::mining_sv2::Target;
use v1::{client_to_server::Submit, utils::HexU32Be};
pub mod diff_management;
pub mod downstream;
pub use downstream::Downstream;

/// This constant defines a timeout duration. It is used to enforce
/// that clients sending a `mining.subscribe` message must follow up
/// with a `mining.authorize` within this period. This prevents
/// resource exhaustion attacks where clients open connections
/// with only `mining.subscribe` without intending to mine.
const SUBSCRIBE_TIMEOUT_SECS: u64 = 10;

/// The messages that are sent from the downstream handling logic
/// to a central "Bridge" component for further processing.
#[derive(Debug)]
pub enum DownstreamMessages {
    /// Represents a submitted share from a downstream miner,
    /// wrapped with the relevant channel ID.
    SubmitShares(SubmitShareWithChannelId),
    /// Represents an update to the downstream target for a specific channel.
    SetDownstreamTarget(SetDownstreamTarget),
}

/// wrapper around a `mining.submit` with extra channel informationfor the Bridge to
/// process
#[derive(Debug)]
pub struct SubmitShareWithChannelId {
    pub channel_id: u32,
    pub share: Submit<'static>,
    pub extranonce: Vec<u8>,
    pub extranonce2_len: usize,
    pub version_rolling_mask: Option<HexU32Be>,
}

/// message for notifying the bridge that a downstream target has updated
/// so the Bridge can process the update
#[derive(Debug)]
pub struct SetDownstreamTarget {
    pub channel_id: u32,
    pub new_target: Target,
}

/// This is just a wrapper function to send a message on the Downstream task shutdown channel
/// it does not matter what message is sent because the receiving ends should shutdown on any
/// message
pub async fn kill(sender: &async_channel::Sender<bool>) {
    // safe to unwrap since the only way this can fail is if all receiving channels are dropped
    // meaning all tasks have already dropped
    sender.send(true).await.unwrap();
}

/// Generates a new, hardcoded string intended to be used as a subscription ID.
///
/// FIXME
pub fn new_subscription_id() -> String {
    "ae6812eb4cd7735a302a8a9dd95cf71f".into()
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/error.rs">
//! ## Translator Error Module
//!
//! Defines the custom error types used throughout the translator proxy.
//!
//! This module centralizes error handling by providing:
//! - A primary `Error` enum encompassing various error kinds from different sources (I/O, parsing,
//!   protocol logic, channels, configuration, etc.).
//! - A specific `ChannelSendError` enum for errors occurring during message sending over
//!   asynchronous channels.

use ext_config::ConfigError;
use std::{fmt, sync::PoisonError};
use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::{self, binary_sv2, framing_sv2, Frame},
    mining_sv2::{ExtendedExtranonce, NewExtendedMiningJob, SetCustomMiningJob},
    parsers::{AnyMessage, Mining},
    vardiff::error::VardiffError,
};
use v1::server_to_client::{Notify, SetDifficulty};

pub type ProxyResult<'a, T> = core::result::Result<T, Error<'a>>;

/// Represents specific errors that can occur when sending messages over various
/// channels used within the translator.
///
/// Each variant corresponds to a failure in sending a particular type of message
/// on its designated channel.
#[derive(Debug)]
pub enum ChannelSendError<'a> {
    /// Failure sending an SV2 `SubmitSharesExtended` message.
    SubmitSharesExtended(
        async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>,
    ),
    /// Failure sending an SV2 `SetNewPrevHash` message.
    SetNewPrevHash(async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>),
    /// Failure sending an SV2 `NewExtendedMiningJob` message.
    NewExtendedMiningJob(async_channel::SendError<NewExtendedMiningJob<'a>>),
    /// Failure broadcasting an SV1 `Notify` message
    Notify(tokio::sync::broadcast::error::SendError<Notify<'a>>),
    /// Failure sending a generic SV1 message.
    V1Message(async_channel::SendError<v1::Message>),
    /// Represents a generic channel send failure, described by a string.
    General(String),
    /// Failure sending extranonce information.
    Extranonce(async_channel::SendError<(ExtendedExtranonce, u32)>),
    /// Failure sending an SV2 `SetCustomMiningJob` message.
    SetCustomMiningJob(
        async_channel::SendError<roles_logic_sv2::mining_sv2::SetCustomMiningJob<'a>>,
    ),
    /// Failure sending new template information (prevhash and coinbase).
    NewTemplate(
        async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    ),
}

#[derive(Debug)]
pub enum Error<'a> {
    VecToSlice32(Vec<u8>),
    /// Errors on bad CLI argument input.
    BadCliArgs,
    /// Errors on bad `serde_json` serialize/deserialize.
    BadSerdeJson(serde_json::Error),
    /// Errors on bad `config` TOML deserialize.
    BadConfigDeserialize(ConfigError),
    /// Errors from `binary_sv2` crate.
    BinarySv2(binary_sv2::Error),
    /// Errors on bad noise handshake.
    CodecNoise(codec_sv2::noise_sv2::Error),
    /// Errors from `framing_sv2` crate.
    FramingSv2(framing_sv2::Error),
    /// Errors on bad `TcpStream` connection.
    Io(std::io::Error),
    /// Errors due to invalid extranonce from upstream
    InvalidExtranonce(String),
    /// Errors on bad `String` to `int` conversion.
    ParseInt(std::num::ParseIntError),
    /// Errors from `roles_logic_sv2` crate.
    RolesSv2Logic(roles_logic_sv2::errors::Error),
    UpstreamIncoming(roles_logic_sv2::errors::Error),
    /// SV1 protocol library error
    V1Protocol(v1::error::Error<'a>),
    #[allow(dead_code)]
    SubprotocolMining(String),
    // Locking Errors
    PoisonLock,
    // Channel Receiver Error
    ChannelErrorReceiver(async_channel::RecvError),
    TokioChannelErrorRecv(tokio::sync::broadcast::error::RecvError),
    // Channel Sender Errors
    ChannelErrorSender(ChannelSendError<'a>),
    SetDifficultyToMessage(SetDifficulty),
    Infallible(std::convert::Infallible),
    // used to handle SV2 protocol error messages from pool
    #[allow(clippy::enum_variant_names)]
    Sv2ProtocolError(Mining<'a>),
    #[allow(clippy::enum_variant_names)]
    TargetError(roles_logic_sv2::errors::Error),
    Sv1MessageTooLong,
}

impl fmt::Display for Error<'_> {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        use Error::*;
        match self {
            BadCliArgs => write!(f, "Bad CLI arg input"),
            BadSerdeJson(ref e) => write!(f, "Bad serde json: `{e:?}`"),
            BadConfigDeserialize(ref e) => write!(f, "Bad `config` TOML deserialize: `{e:?}`"),
            BinarySv2(ref e) => write!(f, "Binary SV2 error: `{e:?}`"),
            CodecNoise(ref e) => write!(f, "Noise error: `{e:?}"),
            FramingSv2(ref e) => write!(f, "Framing SV2 error: `{e:?}`"),
            InvalidExtranonce(ref e) => write!(f, "Invalid Extranonce error: `{e:?}"),
            Io(ref e) => write!(f, "I/O error: `{e:?}"),
            ParseInt(ref e) => write!(f, "Bad convert from `String` to `int`: `{e:?}`"),
            RolesSv2Logic(ref e) => write!(f, "Roles SV2 Logic Error: `{e:?}`"),
            V1Protocol(ref e) => write!(f, "V1 Protocol Error: `{e:?}`"),
            SubprotocolMining(ref e) => write!(f, "Subprotocol Mining Error: `{e:?}`"),
            UpstreamIncoming(ref e) => write!(f, "Upstream parse incoming error: `{e:?}`"),
            PoisonLock => write!(f, "Poison Lock error"),
            ChannelErrorReceiver(ref e) => write!(f, "Channel receive error: `{e:?}`"),
            TokioChannelErrorRecv(ref e) => write!(f, "Channel receive error: `{e:?}`"),
            ChannelErrorSender(ref e) => write!(f, "Channel send error: `{e:?}`"),
            SetDifficultyToMessage(ref e) => {
                write!(f, "Error converting SetDifficulty to Message: `{e:?}`")
            }
            VecToSlice32(ref e) => write!(f, "Standard Error: `{e:?}`"),
            Infallible(ref e) => write!(f, "Infallible Error:`{e:?}`"),
            Sv2ProtocolError(ref e) => {
                write!(f, "Received Sv2 Protocol Error from upstream: `{e:?}`")
            }
            TargetError(ref e) => {
                write!(f, "Impossible to get target from hashrate: `{e:?}`")
            }
            Sv1MessageTooLong => {
                write!(f, "Received an sv1 message that is longer than max len")
            }
        }
    }
}

impl From<binary_sv2::Error> for Error<'_> {
    fn from(e: binary_sv2::Error) -> Self {
        Error::BinarySv2(e)
    }
}

impl From<codec_sv2::noise_sv2::Error> for Error<'_> {
    fn from(e: codec_sv2::noise_sv2::Error) -> Self {
        Error::CodecNoise(e)
    }
}

impl From<framing_sv2::Error> for Error<'_> {
    fn from(e: framing_sv2::Error) -> Self {
        Error::FramingSv2(e)
    }
}

impl From<std::io::Error> for Error<'_> {
    fn from(e: std::io::Error) -> Self {
        Error::Io(e)
    }
}

impl From<std::num::ParseIntError> for Error<'_> {
    fn from(e: std::num::ParseIntError) -> Self {
        Error::ParseInt(e)
    }
}

impl From<roles_logic_sv2::errors::Error> for Error<'_> {
    fn from(e: roles_logic_sv2::errors::Error) -> Self {
        Error::RolesSv2Logic(e)
    }
}

impl From<serde_json::Error> for Error<'_> {
    fn from(e: serde_json::Error) -> Self {
        Error::BadSerdeJson(e)
    }
}

impl From<ConfigError> for Error<'_> {
    fn from(e: ConfigError) -> Self {
        Error::BadConfigDeserialize(e)
    }
}

impl<'a> From<v1::error::Error<'a>> for Error<'a> {
    fn from(e: v1::error::Error<'a>) -> Self {
        Error::V1Protocol(e)
    }
}

impl From<async_channel::RecvError> for Error<'_> {
    fn from(e: async_channel::RecvError) -> Self {
        Error::ChannelErrorReceiver(e)
    }
}

impl From<tokio::sync::broadcast::error::RecvError> for Error<'_> {
    fn from(e: tokio::sync::broadcast::error::RecvError) -> Self {
        Error::TokioChannelErrorRecv(e)
    }
}

//*** LOCK ERRORS ***
impl<T> From<PoisonError<T>> for Error<'_> {
    fn from(_e: PoisonError<T>) -> Self {
        Error::PoisonLock
    }
}

// *** CHANNEL SENDER ERRORS ***
impl<'a> From<async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>>
    for Error<'a>
{
    fn from(
        e: async_channel::SendError<roles_logic_sv2::mining_sv2::SubmitSharesExtended<'a>>,
    ) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SubmitSharesExtended(e))
    }
}

impl<'a> From<async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>>
    for Error<'a>
{
    fn from(e: async_channel::SendError<roles_logic_sv2::mining_sv2::SetNewPrevHash<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SetNewPrevHash(e))
    }
}

impl<'a> From<tokio::sync::broadcast::error::SendError<Notify<'a>>> for Error<'a> {
    fn from(e: tokio::sync::broadcast::error::SendError<Notify<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::Notify(e))
    }
}

impl From<async_channel::SendError<v1::Message>> for Error<'_> {
    fn from(e: async_channel::SendError<v1::Message>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::V1Message(e))
    }
}

impl From<async_channel::SendError<(ExtendedExtranonce, u32)>> for Error<'_> {
    fn from(e: async_channel::SendError<(ExtendedExtranonce, u32)>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::Extranonce(e))
    }
}

impl<'a> From<async_channel::SendError<NewExtendedMiningJob<'a>>> for Error<'a> {
    fn from(e: async_channel::SendError<NewExtendedMiningJob<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::NewExtendedMiningJob(e))
    }
}

impl<'a> From<async_channel::SendError<SetCustomMiningJob<'a>>> for Error<'a> {
    fn from(e: async_channel::SendError<SetCustomMiningJob<'a>>) -> Self {
        Error::ChannelErrorSender(ChannelSendError::SetCustomMiningJob(e))
    }
}

impl<'a>
    From<
        async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    > for Error<'a>
{
    fn from(
        e: async_channel::SendError<(
            roles_logic_sv2::template_distribution_sv2::SetNewPrevHash<'a>,
            Vec<u8>,
        )>,
    ) -> Self {
        Error::ChannelErrorSender(ChannelSendError::NewTemplate(e))
    }
}

impl From<Vec<u8>> for Error<'_> {
    fn from(e: Vec<u8>) -> Self {
        Error::VecToSlice32(e)
    }
}

impl From<SetDifficulty> for Error<'_> {
    fn from(e: SetDifficulty) -> Self {
        Error::SetDifficultyToMessage(e)
    }
}

impl From<std::convert::Infallible> for Error<'_> {
    fn from(e: std::convert::Infallible) -> Self {
        Error::Infallible(e)
    }
}

impl<'a> From<Mining<'a>> for Error<'a> {
    fn from(e: Mining<'a>) -> Self {
        Error::Sv2ProtocolError(e)
    }
}

impl From<async_channel::SendError<Frame<AnyMessage<'_>, codec_sv2::buffer_sv2::Slice>>>
    for Error<'_>
{
    fn from(
        value: async_channel::SendError<Frame<AnyMessage<'_>, codec_sv2::buffer_sv2::Slice>>,
    ) -> Self {
        Error::ChannelErrorSender(ChannelSendError::General(value.to_string()))
    }
}

impl From<VardiffError> for Error<'_> {
    fn from(value: VardiffError) -> Self {
        Self::RolesSv2Logic(value.into())
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/mod.rs">
//! ## Translator Sv2
//!
//! Provides the core logic and main struct (`TranslatorSv2`) for running a
//! Stratum V1 to Stratum V2 translation proxy.
//!
//! This module orchestrates the interaction between downstream SV1 miners and upstream SV2
//! applications (proxies or pool servers).
//!
//! The central component is the `TranslatorSv2` struct, which encapsulates the state and
//! provides the `start` method as the main entry point for running the translator service.
//! It relies on several sub-modules (`config`, `downstream_sv1`, `upstream_sv2`, `proxy`, `status`,
//! etc.) for specialized functionalities.
use async_channel::{bounded, unbounded};
use futures::FutureExt;
use rand::Rng;
use status::Status;
use std::{
    net::{IpAddr, SocketAddr},
    str::FromStr,
    sync::Arc,
};
pub use stratum_common::roles_logic_sv2::utils::Mutex;

use tokio::{
    select,
    sync::{broadcast, Notify},
    task::{self, AbortHandle},
};
use tracing::{debug, error, info, warn};
pub use v1::server_to_client;

use config::TranslatorConfig;

use crate::status::State;

pub mod config;
pub mod downstream_sv1;
pub mod error;
pub mod proxy;
pub mod status;
pub mod upstream_sv2;
pub mod utils;

/// The main struct that manages the SV1/SV2 translator.
#[derive(Clone, Debug)]
pub struct TranslatorSv2 {
    config: TranslatorConfig,
    reconnect_wait_time: u64,
    shutdown: Arc<Notify>,
}

impl TranslatorSv2 {
    /// Creates a new `TranslatorSv2`.
    ///
    /// Initializes the translator with the given configuration and sets up
    /// the reconnect wait time.
    pub fn new(config: TranslatorConfig) -> Self {
        let mut rng = rand::thread_rng();
        let wait_time = rng.gen_range(0..=3000);
        Self {
            config,
            reconnect_wait_time: wait_time,
            shutdown: Arc::new(Notify::new()),
        }
    }

    /// Starts the translator.
    ///
    /// This method starts the main event loop, which handles connections,
    /// protocol translation, job management, and status reporting.
    pub async fn start(self) {
        // Status channel for components to signal errors or state changes.
        let (tx_status, rx_status) = unbounded();

        // Shared mutable state for the current mining target.
        let target = Arc::new(Mutex::new(vec![0; 32]));

        // Broadcast channel to send SV1 `mining.notify` messages from the Bridge
        // to all connected Downstream (SV1) clients.
        let (tx_sv1_notify, _rx_sv1_notify): (
            broadcast::Sender<server_to_client::Notify>,
            broadcast::Receiver<server_to_client::Notify>,
        ) = broadcast::channel(10);

        // FIXME: Remove this task collector mechanism.
        // Collector for holding handles to spawned tasks for potential abortion.
        let task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>> =
            Arc::new(Mutex::new(Vec::new()));

        // Delegate initial setup and connection logic to internal_start.
        Self::internal_start(
            self.config.clone(),
            tx_sv1_notify.clone(),
            target.clone(),
            tx_status.clone(),
            task_collector.clone(),
        )
        .await;

        debug!("Starting up signal listener");
        let task_collector_ = task_collector.clone();

        debug!("Starting up status listener");
        let wait_time = self.reconnect_wait_time;
        // Check all tasks if is_finished() is true, if so exit
        // Spawn a task to listen for Ctrl+C signal.
        tokio::spawn({
            let shutdown_signal = self.shutdown.clone();
            async move {
                if tokio::signal::ctrl_c().await.is_ok() {
                    info!("Interrupt received");
                    // Notify the main loop to begin shutdown.
                    shutdown_signal.notify_one();
                }
            }
        });

        // Main status loop.
        loop {
            select! {
                // Listen for status updates from components.
                task_status = rx_status.recv().fuse() => {
                    if let Ok(task_status_) = task_status {
                        match task_status_.state {
                            // If any critical component shuts down due to error, shut down the whole translator.
                            // Logic needs to be improved, maybe respawn rather than a total shutdown.
                            State::DownstreamShutdown(err) | State::BridgeShutdown(err) | State::UpstreamShutdown(err) => {
                                error!("SHUTDOWN from: {}", err);
                                self.shutdown();
                            }
                            // If the upstream signals a need to reconnect.
                            State::UpstreamTryReconnect(err) => {
                                error!("Trying to reconnect the Upstream because of: {}", err);
                                let task_collector1 = task_collector_.clone();
                                let tx_sv1_notify1 = tx_sv1_notify.clone();
                                let target = target.clone();
                                let tx_status = tx_status.clone();
                                let proxy_config = self.config.clone();
                                // Spawn a new task to handle the reconnection process.
                                tokio::spawn (async move {
                                    // Wait for the randomized delay to avoid thundering herd issues.
                                    tokio::time::sleep(std::time::Duration::from_millis(wait_time)).await;

                                    // Abort all existing tasks before restarting.
                                    let task_collector_aborting = task_collector1.clone();
                                    kill_tasks(task_collector_aborting.clone());

                                    warn!("Trying reconnecting to upstream");
                                    // Restart the internal components.
                                    Self::internal_start(
                                        proxy_config,
                                        tx_sv1_notify1,
                                        target.clone(),
                                        tx_status.clone(),
                                        task_collector1,
                                    )
                                    .await;
                                });
                            }
                            // Log healthy status messages.
                            State::Healthy(msg) => {
                                info!("HEALTHY message: {}", msg);
                            }
                        }
                    } else {
                        info!("Channel closed");
                        kill_tasks(task_collector.clone());
                        break; // Channel closed
                    }
                }
                // Listen for the shutdown signal (from Ctrl+C or explicit call).
                _ = self.shutdown.notified() => {
                    info!("Shutting down gracefully...");
                    kill_tasks(task_collector.clone());
                    break;
                }
            }
        }
    }

    /// Internal helper function to initialize and start the core components.
    ///
    /// Sets up communication channels between the Bridge, Upstream, and Downstream.
    /// Creates, connects, and starts the Upstream (SV2) handler.
    /// Waits for initial data (extranonce, target) from the Upstream.
    /// Creates and starts the Bridge (protocol translation logic).
    /// Starts the Downstream (SV1) listener to accept miner connections.
    /// Collects task handles for graceful shutdown management.
    async fn internal_start(
        proxy_config: TranslatorConfig,
        tx_sv1_notify: broadcast::Sender<server_to_client::Notify<'static>>,
        target: Arc<Mutex<Vec<u8>>>,
        tx_status: async_channel::Sender<Status<'static>>,
        task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
    ) {
        // Channel: Bridge -> Upstream (SV2 SubmitSharesExtended)
        let (tx_sv2_submit_shares_ext, rx_sv2_submit_shares_ext) = bounded(10);

        // Channel: Downstream -> Bridge (SV1 Messages)
        let (tx_sv1_bridge, rx_sv1_downstream) = unbounded();

        // Channel: Upstream -> Bridge (SV2 NewExtendedMiningJob)
        let (tx_sv2_new_ext_mining_job, rx_sv2_new_ext_mining_job) = bounded(10);

        // Channel: Upstream -> internal_start -> Bridge (Initial Extranonce)
        let (tx_sv2_extranonce, rx_sv2_extranonce) = bounded(1);

        // Channel: Upstream -> Bridge (SV2 SetNewPrevHash)
        let (tx_sv2_set_new_prev_hash, rx_sv2_set_new_prev_hash) = bounded(10);

        // Prepare upstream connection address.
        let upstream_addr = SocketAddr::new(
            IpAddr::from_str(&proxy_config.upstream_address)
                .expect("Failed to parse upstream address!"),
            proxy_config.upstream_port,
        );

        // Shared difficulty configuration
        let diff_config = Arc::new(Mutex::new(proxy_config.upstream_difficulty_config.clone()));
        let task_collector_upstream = task_collector.clone();
        // Instantiate the Upstream (SV2) component.
        let upstream = match upstream_sv2::Upstream::new(
            upstream_addr,
            proxy_config.upstream_authority_pubkey,
            rx_sv2_submit_shares_ext,  // Receives shares from Bridge
            tx_sv2_set_new_prev_hash,  // Sends prev hash updates to Bridge
            tx_sv2_new_ext_mining_job, // Sends new jobs to Bridge
            proxy_config.min_extranonce2_size,
            tx_sv2_extranonce,                           // Sends initial extranonce
            status::Sender::Upstream(tx_status.clone()), // Sends status updates
            target.clone(),                              // Shares target state
            diff_config.clone(),                         // Shares difficulty config
            task_collector_upstream,
        )
        .await
        {
            Ok(upstream) => upstream,
            Err(e) => {
                // FIXME: Send error to status main loop, and then exit.
                error!("Failed to create upstream: {}", e);
                return;
            }
        };
        let task_collector_init_task = task_collector.clone();

        // Spawn the core initialization logic in a separate task.
        // This allows the main `start` loop to remain responsive to shutdown signals
        // even during potentially long-running connection attempts.
        let task = task::spawn(async move {
            // Connect to the SV2 Upstream role
            match upstream_sv2::Upstream::connect(
                upstream.clone(),
                proxy_config.min_supported_version,
                proxy_config.max_supported_version,
            )
            .await
            {
                Ok(_) => info!("Connected to Upstream!"),
                Err(e) => {
                    // FIXME: Send error to status main loop, and then exit.
                    error!("Failed to connect to Upstream EXITING! : {}", e);
                    return;
                }
            }

            // Start the task to parse incoming messages from the Upstream.
            if let Err(e) = upstream_sv2::Upstream::parse_incoming(upstream.clone()) {
                error!("failed to create sv2 parser: {}", e);
                return;
            }

            debug!("Finished starting upstream listener");
            // Start the task handler to process share submissions received from the Bridge.
            if let Err(e) = upstream_sv2::Upstream::handle_submit(upstream.clone()) {
                error!("Failed to create submit handler: {}", e);
                return;
            }

            // Wait to receive the initial extranonce information from the Upstream.
            // This is needed before the Bridge can be fully initialized.
            let (extended_extranonce, up_id) = rx_sv2_extranonce.recv().await.unwrap();
            loop {
                let target: [u8; 32] = target.safe_lock(|t| t.clone()).unwrap().try_into().unwrap();
                if target != [0; 32] {
                    break;
                };
                tokio::time::sleep(std::time::Duration::from_millis(100)).await;
            }

            let task_collector_bridge = task_collector_init_task.clone();
            // Instantiate the Bridge component.
            let b = proxy::Bridge::new(
                rx_sv1_downstream,
                tx_sv2_submit_shares_ext,
                rx_sv2_set_new_prev_hash,
                rx_sv2_new_ext_mining_job,
                tx_sv1_notify.clone(),
                status::Sender::Bridge(tx_status.clone()),
                extended_extranonce,
                target,
                up_id,
                task_collector_bridge,
            );
            // Start the Bridge's main processing loop.
            proxy::Bridge::start(b.clone());

            // Prepare downstream listening address.
            let downstream_addr = SocketAddr::new(
                IpAddr::from_str(&proxy_config.downstream_address).unwrap(),
                proxy_config.downstream_port,
            );

            let task_collector_downstream = task_collector_init_task.clone();
            // Start accepting connections from Downstream (SV1) miners.
            downstream_sv1::Downstream::accept_connections(
                downstream_addr,
                tx_sv1_bridge,
                tx_sv1_notify,
                status::Sender::DownstreamListener(tx_status.clone()),
                b,
                proxy_config.downstream_difficulty_config,
                diff_config,
                task_collector_downstream,
            );
        }); // End of init task
        let _ =
            task_collector.safe_lock(|t| t.push((task.abort_handle(), "init task".to_string())));
    }

    /// Closes Translator role and any open connection associated with it.
    ///
    /// Note that this method will result in a full exit of the  running
    /// Translator and any open connection most be re-initiated upon new
    /// start.
    pub fn shutdown(&self) {
        self.shutdown.notify_one();
    }
}

// Helper function to iterate through the collected task handles and abort them
fn kill_tasks(task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>) {
    let _ = task_collector.safe_lock(|t| {
        while let Some(handle) = t.pop() {
            handle.0.abort();
            warn!("Killed task: {:?}", handle.1);
        }
    });
}

#[cfg(test)]
mod tests {
    use super::TranslatorSv2;
    use ext_config::{Config, File, FileFormat};

    use crate::*;

    #[tokio::test]
    async fn test_shutdown() {
        let config_path = "config-examples/tproxy-config-hosted-pool-example.toml";
        let config: TranslatorConfig = match Config::builder()
            .add_source(File::new(config_path, FileFormat::Toml))
            .build()
        {
            Ok(settings) => match settings.try_deserialize::<TranslatorConfig>() {
                Ok(c) => c,
                Err(e) => {
                    dbg!(&e);
                    return;
                }
            },
            Err(e) => {
                dbg!(&e);
                return;
            }
        };
        let translator = TranslatorSv2::new(config.clone());
        let cloned = translator.clone();
        tokio::spawn(async move {
            cloned.start().await;
        });
        translator.shutdown();
        let ip = config.downstream_address.clone();
        let port = config.downstream_port;
        let translator_addr = format!("{ip}:{port}");
        assert!(std::net::TcpListener::bind(translator_addr).is_ok());
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/proxy/bridge.rs">
//!  ## Proxy Bridge Module
//!
//! This module defines the [`Bridge`] structure, which acts as the central component
//! responsible for translating messages and coordinating communication between
//! the upstream SV2 role and the downstream SV1 mining clients.
//!
//! The Bridge manages message queues, maintains the state required for translation
//! (such as job IDs, previous hashes, and mining jobs), handles share submissions
//! from downstream, and forwards translated jobs received from upstream to downstream miners.
//!
//! This module handles:
//! - Receiving SV1 `mining.submit` messages from [`Downstream`] connections.
//! - Translating SV1 submits into SV2 `SubmitSharesExtended`.
//! - Receiving SV2 `SetNewPrevHash` and `NewExtendedMiningJob` from the upstream.
//! - Translating SV2 job messages into SV1 `mining.notify` messages.
//! - Sending translated SV2 submits to the upstream.
//! - Broadcasting translated SV1 notifications to connected downstream miners.
//! - Managing channel state and difficulty related to job translation.
//! - Handling new downstream SV1 connections.
use super::super::{
    downstream_sv1::{DownstreamMessages, SetDownstreamTarget, SubmitShareWithChannelId},
    error::{
        Error::{self, PoisonLock},
        ProxyResult,
    },
    status,
};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use std::sync::Arc;
use stratum_common::roles_logic_sv2::{
    channel_logic::channel_factory::{
        ExtendedChannelKind, OnNewShare, ProxyExtendedChannelFactory, Share,
    },
    mining_sv2::{
        ExtendedExtranonce, NewExtendedMiningJob, SetNewPrevHash, SubmitSharesExtended, Target,
    },
    parsers::Mining,
    utils::{GroupId, Mutex},
    Error as RolesLogicError,
};
use tokio::{sync::broadcast, task::AbortHandle};
use tracing::{debug, error, info, warn};
use v1::{client_to_server::Submit, server_to_client, utils::HexU32Be};

/// Bridge between the SV2 `Upstream` and SV1 `Downstream` responsible for the following messaging
/// translation:
/// 1. SV1 `mining.submit` -> SV2 `SubmitSharesExtended`
/// 2. SV2 `SetNewPrevHash` + `NewExtendedMiningJob` -> SV1 `mining.notify`
#[derive(Debug)]
pub struct Bridge {
    /// Receives a SV1 `mining.submit` message from the Downstream role.
    rx_sv1_downstream: Receiver<DownstreamMessages>,
    /// Sends SV2 `SubmitSharesExtended` messages translated from SV1 `mining.submit` messages to
    /// the `Upstream`.
    tx_sv2_submit_shares_ext: Sender<SubmitSharesExtended<'static>>,
    /// Receives a SV2 `SetNewPrevHash` message from the `Upstream` to be translated (along with a
    /// SV2 `NewExtendedMiningJob` message) to a SV1 `mining.submit` for the `Downstream`.
    rx_sv2_set_new_prev_hash: Receiver<SetNewPrevHash<'static>>,
    /// Receives a SV2 `NewExtendedMiningJob` message from the `Upstream` to be translated (along
    /// with a SV2 `SetNewPrevHash` message) to a SV1 `mining.submit` to be sent to the
    /// `Downstream`.
    rx_sv2_new_ext_mining_job: Receiver<NewExtendedMiningJob<'static>>,
    /// Sends SV1 `mining.notify` message (translated from the SV2 `SetNewPrevHash` and
    /// `NewExtendedMiningJob` messages stored in the `NextMiningNotify`) to the `Downstream`.
    tx_sv1_notify: broadcast::Sender<server_to_client::Notify<'static>>,
    /// Allows the bridge the ability to communicate back to the main thread any status updates
    /// that would interest the main thread for error handling
    tx_status: status::Sender,
    /// Stores the most recent SV1 `mining.notify` values to be sent to the `Downstream` upon
    /// receiving a new SV2 `SetNewPrevHash` and `NewExtendedMiningJob` messages **before** any
    /// Downstream role connects to the proxy.
    ///
    /// Once the proxy establishes a connection with the SV2 Upstream role, it immediately receives
    /// a SV2 `SetNewPrevHash` and `NewExtendedMiningJob` message. This happens before the
    /// connection to the Downstream role(s) occur. The `last_notify` member fields allows these
    /// first notify values to be relayed to the `Downstream` once a Downstream role connects. Once
    /// a Downstream role connects and receives the first notify values, this member field is no
    /// longer used.
    last_notify: Option<server_to_client::Notify<'static>>,
    pub(self) channel_factory: ProxyExtendedChannelFactory,
    /// Stores `NewExtendedMiningJob` messages received from the upstream with the `is_future` flag
    /// set. These jobs are buffered until a corresponding `SetNewPrevHash` message is
    /// received.
    future_jobs: Vec<NewExtendedMiningJob<'static>>,
    /// Stores the last received SV2 `SetNewPrevHash` message. Used in conjunction with
    /// `future_jobs` to construct `mining.notify` messages.
    last_p_hash: Option<SetNewPrevHash<'static>>,
    /// The mining target currently in use by the downstream miners connected to this bridge.
    /// This target is derived from the upstream's requirements but may be adjusted locally.
    target: Arc<Mutex<Vec<u8>>>,
    /// The job ID of the last sent `mining.notify` message.
    last_job_id: u32,
    task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
}

impl Bridge {
    #[allow(clippy::too_many_arguments)]
    /// Instantiates a new `Bridge` with the provided communication channels and initial
    /// configurations.
    ///
    /// Sets up the core communication pathways between upstream and downstream handlers
    /// and initializes the internal state, including the channel factory.
    pub fn new(
        rx_sv1_downstream: Receiver<DownstreamMessages>,
        tx_sv2_submit_shares_ext: Sender<SubmitSharesExtended<'static>>,
        rx_sv2_set_new_prev_hash: Receiver<SetNewPrevHash<'static>>,
        rx_sv2_new_ext_mining_job: Receiver<NewExtendedMiningJob<'static>>,
        tx_sv1_notify: broadcast::Sender<server_to_client::Notify<'static>>,
        tx_status: status::Sender,
        extranonces: ExtendedExtranonce,
        target: Arc<Mutex<Vec<u8>>>,
        up_id: u32,
        task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
    ) -> Arc<Mutex<Self>> {
        let ids = Arc::new(Mutex::new(GroupId::new()));
        let share_per_min = 1.0;
        let upstream_target: [u8; 32] =
            target.safe_lock(|t| t.clone()).unwrap().try_into().unwrap();
        let upstream_target: Target = upstream_target.into();
        Arc::new(Mutex::new(Self {
            rx_sv1_downstream,
            tx_sv2_submit_shares_ext,
            rx_sv2_set_new_prev_hash,
            rx_sv2_new_ext_mining_job,
            tx_sv1_notify,
            tx_status,
            last_notify: None,
            channel_factory: ProxyExtendedChannelFactory::new(
                ids,
                extranonces,
                None,
                share_per_min,
                ExtendedChannelKind::Proxy { upstream_target },
                None,
                up_id,
            ),
            future_jobs: vec![],
            last_p_hash: None,
            target,
            last_job_id: 0,
            task_collector,
        }))
    }

    /// Handles the event of a new SV1 downstream client connecting.
    ///
    /// Creates a new extended channel using the internal `channel_factory` for the
    /// new connection. It assigns a unique channel ID, determines the initial
    /// extranonce and target for the miner, and provides the last known
    /// `mining.notify` message to immediately send to the new client.
    #[allow(clippy::result_large_err)]
    pub fn on_new_sv1_connection(
        &mut self,
        hash_rate: f32,
    ) -> ProxyResult<'static, OpenSv1Downstream> {
        match self.channel_factory.new_extended_channel(0, hash_rate, 0) {
            Ok(messages) => {
                for message in messages {
                    match message {
                        Mining::OpenExtendedMiningChannelSuccess(success) => {
                            let extranonce = success.extranonce_prefix.to_vec();
                            let extranonce2_len = success.extranonce_size;
                            self.target.safe_lock(|t| *t = success.target.to_vec())?;
                            return Ok(OpenSv1Downstream {
                                channel_id: success.channel_id,
                                last_notify: self.last_notify.clone(),
                                extranonce,
                                target: self.target.clone(),
                                extranonce2_len,
                            });
                        }
                        Mining::OpenMiningChannelError(_) => todo!(),
                        Mining::SetNewPrevHash(_) => (),
                        Mining::NewExtendedMiningJob(_) => (),
                        _ => unreachable!(),
                    }
                }
            }
            Err(_) => {
                return Err(Error::SubprotocolMining(
                    "Bridge: failed to open new extended channel".to_string(),
                ))
            }
        };
        Err(Error::SubprotocolMining(
            "Bridge: Invalid mining message when opening downstream connection".to_string(),
        ))
    }

    /// Starts the tasks responsible for receiving and processing
    /// messages from both upstream SV2 and downstream SV1 connections.
    ///
    /// This function spawns three main tasks:
    /// 1. `handle_new_prev_hash`: Listens for SV2 `SetNewPrevHash` messages.
    /// 2. `handle_new_extended_mining_job`: Listens for SV2 `NewExtendedMiningJob` messages.
    /// 3. `handle_downstream_messages`: Listens for `DownstreamMessages` (e.g., submit shares) from
    ///    downstream clients.
    pub fn start(self_: Arc<Mutex<Self>>) {
        Self::handle_new_prev_hash(self_.clone());
        Self::handle_new_extended_mining_job(self_.clone());
        Self::handle_downstream_messages(self_);
    }

    /// Task handler that receives `DownstreamMessages` and dispatches them.
    ///
    /// This loop continuously receives messages from the `rx_sv1_downstream` channel.
    /// It matches on the `DownstreamMessages` variant and calls the appropriate
    /// handler function (`handle_submit_shares` or `handle_update_downstream_target`).
    fn handle_downstream_messages(self_: Arc<Mutex<Self>>) {
        let task_collector_handle_downstream =
            self_.safe_lock(|b| b.task_collector.clone()).unwrap();
        let (rx_sv1_downstream, tx_status) = self_
            .safe_lock(|s| (s.rx_sv1_downstream.clone(), s.tx_status.clone()))
            .unwrap();
        let handle_downstream = tokio::task::spawn(async move {
            loop {
                let msg = handle_result!(tx_status, rx_sv1_downstream.clone().recv().await);

                match msg {
                    DownstreamMessages::SubmitShares(share) => {
                        handle_result!(
                            tx_status,
                            Self::handle_submit_shares(self_.clone(), share).await
                        );
                    }
                    DownstreamMessages::SetDownstreamTarget(new_target) => {
                        handle_result!(
                            tx_status,
                            Self::handle_update_downstream_target(self_.clone(), new_target)
                        );
                    }
                };
            }
        });
        let _ = task_collector_handle_downstream.safe_lock(|a| {
            a.push((
                handle_downstream.abort_handle(),
                "handle_downstream_message".to_string(),
            ))
        });
    }

    /// Receives a `SetDownstreamTarget` message and updates the downstream target for a specific
    /// channel.
    ///
    /// This function is called when the downstream logic determines that a miner's
    /// target needs to be updated (e.g., due to difficulty adjustment). It updates
    /// the target within the internal `channel_factory` for the specified channel ID.
    #[allow(clippy::result_large_err)]
    fn handle_update_downstream_target(
        self_: Arc<Mutex<Self>>,
        new_target: SetDownstreamTarget,
    ) -> ProxyResult<'static, ()> {
        self_.safe_lock(|b| {
            b.channel_factory
                .update_target_for_channel(new_target.channel_id, new_target.new_target);
        })?;
        Ok(())
    }
    /// Receives a `SubmitShareWithChannelId` message from a downstream miner,
    /// validates the share, and sends it upstream if it meets the upstream target.
    async fn handle_submit_shares(
        self_: Arc<Mutex<Self>>,
        share: SubmitShareWithChannelId,
    ) -> ProxyResult<'static, ()> {
        let (tx_sv2_submit_shares_ext, target_mutex, tx_status) = self_.safe_lock(|s| {
            (
                s.tx_sv2_submit_shares_ext.clone(),
                s.target.clone(),
                s.tx_status.clone(),
            )
        })?;
        let upstream_target: [u8; 32] = target_mutex.safe_lock(|t| t.clone())?.try_into()?;
        let mut upstream_target: Target = upstream_target.into();
        self_.safe_lock(|s| s.channel_factory.set_target(&mut upstream_target))?;

        let sv2_submit = self_.safe_lock(|s| {
            s.translate_submit(share.channel_id, share.share, share.version_rolling_mask)
        })??;
        let res = self_
            .safe_lock(|s| s.channel_factory.on_submit_shares_extended(sv2_submit))
            .map_err(|_| PoisonLock);

        match res {
            Ok(Ok(OnNewShare::SendErrorDownstream(e))) => {
                warn!(
                    "Submit share error {:?}",
                    std::str::from_utf8(&e.error_code.to_vec()[..])
                );
            }
            Ok(Ok(OnNewShare::SendSubmitShareUpstream((share, _)))) => {
                info!("SHARE MEETS UPSTREAM TARGET");
                match share {
                    Share::Extended(share) => {
                        tx_sv2_submit_shares_ext.send(share).await?;
                    }
                    // We are in an extended channel shares are extended
                    Share::Standard(_) => unreachable!(),
                }
            }
            // We are in an extended channel this variant is group channle only
            Ok(Ok(OnNewShare::RelaySubmitShareUpstream)) => unreachable!(),
            Ok(Ok(OnNewShare::ShareMeetDownstreamTarget)) => {
                debug!("SHARE MEETS DOWNSTREAM TARGET");
            }
            // Proxy do not have JD capabilities
            Ok(Ok(OnNewShare::ShareMeetBitcoinTarget(..))) => unreachable!(),
            Ok(Err(e)) => error!("Error: {:?}", e),
            Err(e) => {
                let _ = tx_status
                    .send(status::Status {
                        state: status::State::BridgeShutdown(e),
                    })
                    .await;
            }
        }
        Ok(())
    }

    /// Translates a SV1 `mining.submit` message into an SV2 `SubmitSharesExtended` message.
    ///
    /// This function performs the necessary transformations to convert the data
    /// format used by SV1 submissions (`job_id`, `nonce`, `time`, `extra_nonce2`,
    /// `version_bits`) into the SV2 `SubmitSharesExtended` structure,
    /// taking into account version rolling if a mask is provided.
    #[allow(clippy::result_large_err)]
    fn translate_submit(
        &self,
        channel_id: u32,
        sv1_submit: Submit,
        version_rolling_mask: Option<HexU32Be>,
    ) -> ProxyResult<'static, SubmitSharesExtended<'static>> {
        let last_version = self
            .channel_factory
            .last_valid_job_version()
            .ok_or(Error::RolesSv2Logic(RolesLogicError::NoValidJob))?;
        let version = match (sv1_submit.version_bits, version_rolling_mask) {
            // regarding version masking see https://github.com/slushpool/stratumprotocol/blob/master/stratum-extensions.mediawiki#changes-in-request-miningsubmit
            (Some(vb), Some(mask)) => (last_version & !mask.0) | (vb.0 & mask.0),
            (None, None) => last_version,
            _ => return Err(Error::V1Protocol(v1::error::Error::InvalidSubmission)),
        };
        let mining_device_extranonce: Vec<u8> = sv1_submit.extra_nonce2.into();
        let extranonce2 = mining_device_extranonce;
        Ok(SubmitSharesExtended {
            channel_id,
            // I put 0 below cause sequence_number is not what should be TODO
            sequence_number: 0,
            job_id: sv1_submit.job_id.parse::<u32>()?,
            nonce: sv1_submit.nonce.0,
            ntime: sv1_submit.time.0,
            version,
            extranonce: extranonce2.try_into()?,
        })
    }

    /// Internal helper function to handle a received SV2 `SetNewPrevHash` message.
    ///
    /// This function processes a `SetNewPrevHash` message received from the upstream.
    /// It updates the Bridge's stored last previous hash, informs the `channel_factory`
    /// about the new previous hash, and then checks the `future_jobs` buffer for
    /// a corresponding `NewExtendedMiningJob`. If a matching future job is found, it constructs a
    /// SV1 `mining.notify` message and broadcasts it to all downstream clients. It also updates
    /// the `last_notify` state for new connections.
    async fn handle_new_prev_hash_(
        self_: Arc<Mutex<Self>>,
        sv2_set_new_prev_hash: SetNewPrevHash<'static>,
        tx_sv1_notify: broadcast::Sender<server_to_client::Notify<'static>>,
    ) -> Result<(), Error<'static>> {
        while !crate::upstream_sv2::upstream::IS_NEW_JOB_HANDLED
            .load(std::sync::atomic::Ordering::SeqCst)
        {
            tokio::task::yield_now().await;
        }
        self_.safe_lock(|s| s.last_p_hash = Some(sv2_set_new_prev_hash.clone()))?;

        let on_new_prev_hash_res = self_.safe_lock(|s| {
            s.channel_factory
                .on_new_prev_hash(sv2_set_new_prev_hash.clone())
        })?;
        on_new_prev_hash_res?;

        let mut future_jobs = self_.safe_lock(|s| {
            let future_jobs = s.future_jobs.clone();
            s.future_jobs = vec![];
            future_jobs
        })?;

        let mut match_a_future_job = false;
        while let Some(job) = future_jobs.pop() {
            if job.job_id == sv2_set_new_prev_hash.job_id {
                let j_id = job.job_id;
                // Create the mining.notify to be sent to the Downstream.
                let notify = crate::proxy::next_mining_notify::create_notify(
                    sv2_set_new_prev_hash.clone(),
                    job,
                    true,
                );

                // Get the sender to send the mining.notify to the Downstream
                tx_sv1_notify.send(notify.clone())?;
                match_a_future_job = true;
                self_.safe_lock(|s| {
                    s.last_notify = Some(notify);
                    s.last_job_id = j_id;
                })?;
                break;
            }
        }
        if !match_a_future_job {
            debug!("No future jobs for {:?}", sv2_set_new_prev_hash);
        }
        Ok(())
    }

    /// Task handler that receives SV2 `SetNewPrevHash` messages from the upstream.
    ///
    /// This loop continuously receives `SetNewPrevHash` messages. It calls the
    /// internal `handle_new_prev_hash_` helper function to process each message.
    fn handle_new_prev_hash(self_: Arc<Mutex<Self>>) {
        let task_collector_handle_new_prev_hash =
            self_.safe_lock(|b| b.task_collector.clone()).unwrap();
        let (tx_sv1_notify, rx_sv2_set_new_prev_hash, tx_status) = self_
            .safe_lock(|s| {
                (
                    s.tx_sv1_notify.clone(),
                    s.rx_sv2_set_new_prev_hash.clone(),
                    s.tx_status.clone(),
                )
            })
            .unwrap();
        debug!("Starting handle_new_prev_hash task");
        let handle_new_prev_hash = tokio::task::spawn(async move {
            loop {
                // Receive `SetNewPrevHash` from `Upstream`
                let sv2_set_new_prev_hash: SetNewPrevHash =
                    handle_result!(tx_status, rx_sv2_set_new_prev_hash.clone().recv().await);
                debug!(
                    "handle_new_prev_hash job_id: {:?}",
                    &sv2_set_new_prev_hash.job_id
                );
                handle_result!(
                    tx_status.clone(),
                    Self::handle_new_prev_hash_(
                        self_.clone(),
                        sv2_set_new_prev_hash,
                        tx_sv1_notify.clone(),
                    )
                    .await
                )
            }
        });
        let _ = task_collector_handle_new_prev_hash.safe_lock(|a| {
            a.push((
                handle_new_prev_hash.abort_handle(),
                "handle_new_prev_hash".to_string(),
            ))
        });
    }

    /// Internal helper function to handle a received SV2 `NewExtendedMiningJob` message.
    ///
    /// This function processes a `NewExtendedMiningJob` message received from the upstream.
    /// It first informs the `channel_factory` about the new job. If the job's `is_future` is true,
    /// the job is buffered in `future_jobs`. If `is_future` is false, it expects a
    /// corresponding `SetNewPrevHash` (which should have been received prior according to the
    /// protocol) and immediately constructs and broadcasts a SV1 `mining.notify` message to
    /// downstream clients, updating the `last_notify` state.
    async fn handle_new_extended_mining_job_(
        self_: Arc<Mutex<Self>>,
        sv2_new_extended_mining_job: NewExtendedMiningJob<'static>,
        tx_sv1_notify: broadcast::Sender<server_to_client::Notify<'static>>,
    ) -> Result<(), Error<'static>> {
        // convert to non segwit jobs so we dont have to depend if miner's support segwit or not
        self_.safe_lock(|s| {
            s.channel_factory
                .on_new_extended_mining_job(sv2_new_extended_mining_job.as_static().clone())
        })??;

        // If future_job=true, this job is meant for a future SetNewPrevHash that the proxy
        // has yet to receive. Insert this new job into the job_mapper .
        if sv2_new_extended_mining_job.is_future() {
            self_.safe_lock(|s| s.future_jobs.push(sv2_new_extended_mining_job.clone()))?;
            Ok(())

        // If future_job=false, this job is meant for the current SetNewPrevHash.
        } else {
            let last_p_hash_option = self_.safe_lock(|s| s.last_p_hash.clone())?;

            // last_p_hash is an Option<SetNewPrevHash> so we need to map to the correct error type
            // to be handled
            let last_p_hash = last_p_hash_option.ok_or(Error::RolesSv2Logic(
                RolesLogicError::JobIsNotFutureButPrevHashNotPresent,
            ))?;

            let j_id = sv2_new_extended_mining_job.job_id;
            // Create the mining.notify to be sent to the Downstream.
            // clean_jobs must be false because it's not a NewPrevHash template
            let notify = crate::proxy::next_mining_notify::create_notify(
                last_p_hash,
                sv2_new_extended_mining_job.clone(),
                false,
            );
            // Get the sender to send the mining.notify to the Downstream
            tx_sv1_notify.send(notify.clone())?;
            self_.safe_lock(|s| {
                s.last_notify = Some(notify);
                s.last_job_id = j_id;
            })?;
            Ok(())
        }
    }

    /// Task handler that receives SV2 `NewExtendedMiningJob` messages from the upstream.
    ///
    /// This loop continuously receives `NewExtendedMiningJob` messages. It calls the
    /// internal `handle_new_extended_mining_job_` helper function to process each message.
    /// After processing, it signals that a new job has been handled (used for synchronization
    /// with the `handle_new_prev_hash` task).
    fn handle_new_extended_mining_job(self_: Arc<Mutex<Self>>) {
        let task_collector_new_extended_mining_job =
            self_.safe_lock(|b| b.task_collector.clone()).unwrap();
        let (tx_sv1_notify, rx_sv2_new_ext_mining_job, tx_status) = self_
            .safe_lock(|s| {
                (
                    s.tx_sv1_notify.clone(),
                    s.rx_sv2_new_ext_mining_job.clone(),
                    s.tx_status.clone(),
                )
            })
            .unwrap();
        debug!("Starting handle_new_extended_mining_job task");
        let handle_new_extended_mining_job = tokio::task::spawn(async move {
            loop {
                // Receive `NewExtendedMiningJob` from `Upstream`
                let sv2_new_extended_mining_job: NewExtendedMiningJob = handle_result!(
                    tx_status.clone(),
                    rx_sv2_new_ext_mining_job.clone().recv().await
                );
                debug!(
                    "handle_new_extended_mining_job job_id: {:?}",
                    &sv2_new_extended_mining_job.job_id
                );
                handle_result!(
                    tx_status,
                    Self::handle_new_extended_mining_job_(
                        self_.clone(),
                        sv2_new_extended_mining_job,
                        tx_sv1_notify.clone(),
                    )
                    .await
                );
                crate::upstream_sv2::upstream::IS_NEW_JOB_HANDLED
                    .store(true, std::sync::atomic::Ordering::SeqCst);
            }
        });
        let _ = task_collector_new_extended_mining_job.safe_lock(|a| {
            a.push((
                handle_new_extended_mining_job.abort_handle(),
                "handle_new_extended_mining_job".to_string(),
            ))
        });
    }
}

/// Represents the necessary information to initialize a new SV1 downstream connection
/// after it has been registered with the Bridge's channel factory.
///
/// This structure is returned by `Bridge::on_new_sv1_connection` and contains the
/// channel ID assigned to the connection, the initial job notification to send,
/// and the extranonce and target specific to this channel.
pub struct OpenSv1Downstream {
    /// The unique ID assigned to this downstream channel by the channel factory.
    pub channel_id: u32,
    /// The most recent `mining.notify` message to send to the new client immediately
    /// upon connection to provide them with a job.
    pub last_notify: Option<server_to_client::Notify<'static>>,
    /// The extranonce prefix assigned to this channel.
    pub extranonce: Vec<u8>,
    /// The mining target assigned to this channel
    pub target: Arc<Mutex<Vec<u8>>>,
    /// The size of the extranonce2 field expected from the miner for this channel.
    pub extranonce2_len: u16,
}

#[cfg(test)]
mod test {
    use super::*;
    use async_channel::bounded;

    pub mod test_utils {
        use super::*;

        #[allow(dead_code)]
        pub struct BridgeInterface {
            pub tx_sv1_submit: Sender<DownstreamMessages>,
            pub rx_sv2_submit_shares_ext: Receiver<SubmitSharesExtended<'static>>,
            pub tx_sv2_set_new_prev_hash: Sender<SetNewPrevHash<'static>>,
            pub tx_sv2_new_ext_mining_job: Sender<NewExtendedMiningJob<'static>>,
            pub rx_sv1_notify: broadcast::Receiver<server_to_client::Notify<'static>>,
        }

        pub fn create_bridge(
            extranonces: ExtendedExtranonce,
        ) -> (Arc<Mutex<Bridge>>, BridgeInterface) {
            let (tx_sv1_submit, rx_sv1_submit) = bounded(1);
            let (tx_sv2_submit_shares_ext, rx_sv2_submit_shares_ext) = bounded(1);
            let (tx_sv2_set_new_prev_hash, rx_sv2_set_new_prev_hash) = bounded(1);
            let (tx_sv2_new_ext_mining_job, rx_sv2_new_ext_mining_job) = bounded(1);
            let (tx_sv1_notify, rx_sv1_notify) = broadcast::channel(1);
            let (tx_status, _rx_status) = bounded(1);
            let upstream_target = vec![
                0, 0, 0, 0, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                0, 0, 0, 0, 0, 0, 0,
            ];
            let interface = BridgeInterface {
                tx_sv1_submit,
                rx_sv2_submit_shares_ext,
                tx_sv2_set_new_prev_hash,
                tx_sv2_new_ext_mining_job,
                rx_sv1_notify,
            };

            let task_collector = Arc::new(Mutex::new(vec![]));
            let b = Bridge::new(
                rx_sv1_submit,
                tx_sv2_submit_shares_ext,
                rx_sv2_set_new_prev_hash,
                rx_sv2_new_ext_mining_job,
                tx_sv1_notify,
                status::Sender::Bridge(tx_status),
                extranonces,
                Arc::new(Mutex::new(upstream_target)),
                1,
                task_collector,
            );
            (b, interface)
        }

        pub fn create_sv1_submit(job_id: u32) -> Submit<'static> {
            Submit {
                user_name: "test_user".to_string(),
                job_id: job_id.to_string(),
                extra_nonce2: v1::utils::Extranonce::try_from([0; 32].to_vec()).unwrap(),
                time: v1::utils::HexU32Be(1),
                nonce: v1::utils::HexU32Be(1),
                version_bits: None,
                id: 0,
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/proxy/mod.rs">
pub mod bridge;
pub mod next_mining_notify;
pub use bridge::Bridge;
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/proxy/next_mining_notify.rs">
//! Provides functionality to convert Stratum V2 job into a
//! Stratum V1 `mining.notify` message.
use stratum_common::roles_logic_sv2::{
    job_creator::extended_job_to_non_segwit,
    mining_sv2::{NewExtendedMiningJob, SetNewPrevHash},
};
use tracing::debug;
use v1::{
    server_to_client,
    utils::{HexU32Be, MerkleNode, PrevHash},
};

/// Creates a new SV1 `mining.notify` message if both SV2 `SetNewPrevHash` and
/// `NewExtendedMiningJob` messages have been received. If one of these messages is still being
/// waited on, the function returns `None`.
/// If clean_jobs = false, it means a new job is created, with the same PrevHash
pub fn create_notify(
    new_prev_hash: SetNewPrevHash<'static>,
    new_job: NewExtendedMiningJob<'static>,
    clean_jobs: bool,
) -> server_to_client::Notify<'static> {
    // TODO 32 must be changed!
    let new_job = extended_job_to_non_segwit(new_job, 32)
        .expect("failed to convert extended job to non segwit");
    // Make sure that SetNewPrevHash + NewExtendedMiningJob is matching (not future)
    let job_id = new_job.job_id.to_string();

    // U256<'static> -> MerkleLeaf
    let prev_hash = PrevHash(new_prev_hash.prev_hash.clone());

    // B064K<'static'> -> HexBytes
    let coin_base1 = new_job.coinbase_tx_prefix.to_vec().into();
    let coin_base2 = new_job.coinbase_tx_suffix.to_vec().into();

    // Seq0255<'static, U56<'static>> -> Vec<Vec<u8>>
    let merkle_path = new_job.merkle_path.clone().into_static().0;
    let merkle_branch: Vec<MerkleNode> = merkle_path.into_iter().map(MerkleNode).collect();

    // u32 -> HexBytes
    let version = HexU32Be(new_job.version);
    let bits = HexU32Be(new_prev_hash.nbits);
    let time = HexU32Be(match new_job.is_future() {
        true => new_prev_hash.min_ntime,
        false => new_job.min_ntime.clone().into_inner().unwrap(),
    });

    let notify_response = server_to_client::Notify {
        job_id,
        prev_hash,
        coin_base1,
        coin_base2,
        merkle_branch,
        version,
        bits,
        time,
        clean_jobs,
    };
    debug!("\nNextMiningNotify: {:?}\n", notify_response);
    notify_response
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/status.rs">
//! ## Status Reporting System for Translator
//!
//! This module defines how internal components of the Translator report
//! health, errors, and shutdown conditions back to the main runtime loop in `lib/mod.rs`.
//!
//! At the core, tasks send a [`Status`] (wrapping a [`State`]) through a channel,
//! which is tagged with a [`Sender`] enum to indicate the origin of the message.
//!
//! This allows for centralized, consistent error handling across the application.

use stratum_common::roles_logic_sv2;

use crate::error::{self, Error};

/// Identifies the component that originated a [`Status`] update.
///
/// Each sender is associated with a dedicated side of the status channel.
/// This lets the central loop distinguish between errors from different parts of the system.
#[derive(Debug)]
pub enum Sender {
    /// Sender for downstream connections.
    Downstream(async_channel::Sender<Status<'static>>),
    /// Sender for downstream listener.
    DownstreamListener(async_channel::Sender<Status<'static>>),
    /// Sender for bridge connections.
    Bridge(async_channel::Sender<Status<'static>>),
    /// Sender for upstream connections.
    Upstream(async_channel::Sender<Status<'static>>),
    /// Sender for template receiver.
    TemplateReceiver(async_channel::Sender<Status<'static>>),
}

impl Sender {
    /// Converts a `DownstreamListener` sender to a `Downstream` sender.
    /// FIXME: Use `From` trait and remove this
    pub fn listener_to_connection(&self) -> Self {
        match self {
            Self::DownstreamListener(inner) => Self::Downstream(inner.clone()),
            _ => unreachable!(),
        }
    }

    /// Sends a status update.
    pub async fn send(
        &self,
        status: Status<'static>,
    ) -> Result<(), async_channel::SendError<Status<'_>>> {
        match self {
            Self::Downstream(inner) => inner.send(status).await,
            Self::DownstreamListener(inner) => inner.send(status).await,
            Self::Bridge(inner) => inner.send(status).await,
            Self::Upstream(inner) => inner.send(status).await,
            Self::TemplateReceiver(inner) => inner.send(status).await,
        }
    }
}

impl Clone for Sender {
    fn clone(&self) -> Self {
        match self {
            Self::Downstream(inner) => Self::Downstream(inner.clone()),
            Self::DownstreamListener(inner) => Self::DownstreamListener(inner.clone()),
            Self::Bridge(inner) => Self::Bridge(inner.clone()),
            Self::Upstream(inner) => Self::Upstream(inner.clone()),
            Self::TemplateReceiver(inner) => Self::TemplateReceiver(inner.clone()),
        }
    }
}

/// The kind of event or status being reported by a task.
#[derive(Debug)]
pub enum State<'a> {
    /// Downstream connection shutdown.
    DownstreamShutdown(Error<'a>),
    /// Bridge connection shutdown.
    BridgeShutdown(Error<'a>),
    /// Upstream connection shutdown.
    UpstreamShutdown(Error<'a>),
    /// Upstream connection trying to reconnect.
    UpstreamTryReconnect(Error<'a>),
    /// Component is healthy.
    Healthy(String),
}

/// Wraps a status update, to be passed through a status channel.
#[derive(Debug)]
pub struct Status<'a> {
    pub state: State<'a>,
}

/// Sends a [`Status`] message tagged with its [`Sender`] to the central loop.
///
/// This is the core logic used to determine which status variant should be sent
/// based on the error type and sender context.
async fn send_status(
    sender: &Sender,
    e: error::Error<'static>,
    outcome: error_handling::ErrorBranch,
) -> error_handling::ErrorBranch {
    match sender {
        Sender::Downstream(tx) => {
            tx.send(Status {
                state: State::Healthy(e.to_string()),
            })
            .await
            .unwrap_or(());
        }
        Sender::DownstreamListener(tx) => {
            tx.send(Status {
                state: State::DownstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
        Sender::Bridge(tx) => {
            tx.send(Status {
                state: State::BridgeShutdown(e),
            })
            .await
            .unwrap_or(());
        }
        Sender::Upstream(tx) => match e {
            Error::ChannelErrorReceiver(_) => {
                tx.send(Status {
                    state: State::UpstreamTryReconnect(e),
                })
                .await
                .unwrap_or(());
            }
            _ => {
                tx.send(Status {
                    state: State::UpstreamShutdown(e),
                })
                .await
                .unwrap_or(());
            }
        },
        Sender::TemplateReceiver(tx) => {
            tx.send(Status {
                state: State::UpstreamShutdown(e),
            })
            .await
            .unwrap_or(());
        }
    }
    outcome
}

/// Centralized error dispatcher for the Translator.
///
/// Used by the `handle_result!` macro across the codebase.
/// Decides whether the task should `Continue` or `Break` based on the error type and source.
pub async fn handle_error(
    sender: &Sender,
    e: error::Error<'static>,
) -> error_handling::ErrorBranch {
    tracing::error!("Error: {:?}", &e);
    match e {
        Error::VecToSlice32(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad CLI argument input.
        Error::BadCliArgs => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `serde_json` serialize/deserialize.
        Error::BadSerdeJson(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `config` TOML deserialize.
        Error::BadConfigDeserialize(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Errors from `binary_sv2` crate.
        Error::BinarySv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad noise handshake.
        Error::CodecNoise(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors from `framing_sv2` crate.
        Error::FramingSv2(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        //If the pool sends the tproxy an invalid extranonce
        Error::InvalidExtranonce(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Errors on bad `TcpStream` connection.
        Error::Io(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors on bad `String` to `int` conversion.
        Error::ParseInt(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Errors from `roles_logic_sv2` crate.
        Error::RolesSv2Logic(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        Error::UpstreamIncoming(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // SV1 protocol library error
        Error::V1Protocol(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        Error::SubprotocolMining(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Locking Errors
        Error::PoisonLock => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        // Channel Receiver Error
        Error::ChannelErrorReceiver(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::TokioChannelErrorRecv(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        // Channel Sender Errors
        Error::ChannelErrorSender(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::SetDifficultyToMessage(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
        Error::Infallible(_) => send_status(sender, e, error_handling::ErrorBranch::Break).await,
        Error::Sv2ProtocolError(ref inner) => {
            match inner {
                // dont notify main thread just continue
                roles_logic_sv2::parsers::Mining::SubmitSharesError(_) => {
                    error_handling::ErrorBranch::Continue
                }
                _ => send_status(sender, e, error_handling::ErrorBranch::Break).await,
            }
        }
        Error::TargetError(_) => {
            send_status(sender, e, error_handling::ErrorBranch::Continue).await
        }
        Error::Sv1MessageTooLong => {
            send_status(sender, e, error_handling::ErrorBranch::Break).await
        }
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/upstream_sv2/diff_management.rs">
//! ## Upstream SV2 Difficulty Management
//!
//! This module contains logic for managing difficulty and hashrate updates
//! specifically for the upstream SV2 connection.
//!
//! It defines method for the [`Upstream`] struct
//! related to checking configuration intervals and sending
//! `UpdateChannel` messages to the upstream server
//! based on configured nominal hashrate changes.

use super::Upstream;

use super::super::{
    error::ProxyResult,
    upstream_sv2::{EitherFrame, Message, StdFrame},
};
use std::{sync::Arc, time::Duration};
use stratum_common::roles_logic_sv2::{
    codec_sv2::binary_sv2::U256, mining_sv2::UpdateChannel, parsers::Mining, utils::Mutex,
    Error as RolesLogicError,
};

impl Upstream {
    /// Attempts to update the upstream channel's nominal hashrate if the configured
    /// update interval has elapsed or if the nominal hashrate has changed
    pub(super) async fn try_update_hashrate(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        let (channel_id_option, diff_mgmt, tx_frame, last_sent_hashrate) =
            self_.safe_lock(|u| {
                (
                    u.channel_id,
                    u.difficulty_config.clone(),
                    u.connection.sender.clone(),
                    u.last_sent_hashrate,
                )
            })?;

        let channel_id = channel_id_option.ok_or(super::super::error::Error::RolesSv2Logic(
            RolesLogicError::NotFoundChannelId,
        ))?;

        let (timeout, new_hashrate) = diff_mgmt
            .safe_lock(|d| (d.channel_diff_update_interval, d.channel_nominal_hashrate))?;

        let has_changed = Some(new_hashrate) != last_sent_hashrate;

        if has_changed {
            // Send UpdateChannel only if hashrate actually changed
            let update_channel = UpdateChannel {
                channel_id,
                nominal_hash_rate: new_hashrate,
                maximum_target: U256::from([0xff; 32]),
            };
            let message = Message::Mining(Mining::UpdateChannel(update_channel));
            let either_frame: StdFrame = message.try_into()?;
            let frame: EitherFrame = either_frame.into();

            tx_frame.send(frame).await?;

            self_.safe_lock(|u| u.last_sent_hashrate = Some(new_hashrate))?;
        }

        // Always sleep, regardless of update
        tokio::time::sleep(Duration::from_secs(timeout as u64)).await;
        Ok(())
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/upstream_sv2/mod.rs">
//! ## Upstream SV2 Module
//!
//! This module encapsulates the logic for handling the upstream connection using the SV2 protocol.
//!
//! The module is organized into the following sub-modules:
//! - [`diff_management`]: Contains logic related to managing difficulty and hashrate updates.
//! - [`upstream`]: Defines the main [`Upstream`] struct and its core functionalities.
//! - [`upstream_connection`]: Handles the underlying connection details and frame
//!   sending/receiving.

use stratum_common::roles_logic_sv2::{
    codec_sv2::{StandardEitherFrame, StandardSv2Frame},
    parsers::AnyMessage,
};

pub mod diff_management;
pub mod upstream;
pub mod upstream_connection;
pub use upstream::Upstream;
pub use upstream_connection::UpstreamConnection;

pub type Message = AnyMessage<'static>;
pub type StdFrame = StandardSv2Frame<Message>;
pub type EitherFrame = StandardEitherFrame<Message>;

/// Represents the state or parameters negotiated during an SV2 Setup Connection message.
#[derive(Clone, Copy, Debug)]
pub struct Sv2MiningConnection {
    _version: u16,
    _setup_connection_flags: u32,
    #[allow(dead_code)]
    setup_connection_success_flags: u32,
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/upstream_sv2/upstream_connection.rs">
//! ## Upstream SV2 Connection Module
//!
//! Defines [`UpstreamConnection`], the structure responsible for managing the
//! communication channels with an upstream.

use super::{super::error::ProxyResult, EitherFrame, StdFrame};
use async_channel::{Receiver, Sender};

/// Handles the sending and receiving of messages to and from an SV2 Upstream role (most typically
/// a SV2 Pool server).
/// On upstream, we have a sv2connection, so we use the connection from network helpers
/// use network_helpers::Connection;
/// this does the dirty work of reading byte by byte in the socket and puts them in a complete
/// Sv2Messages frame and when the message is ready then sends to our Upstream
/// sender_incoming + receiver_outgoing are in network_helpers::Connection
#[derive(Debug, Clone)]
pub struct UpstreamConnection {
    /// Receives messages from the SV2 Upstream role
    pub receiver: Receiver<EitherFrame>,
    /// Sends messages to the SV2 Upstream role
    pub sender: Sender<EitherFrame>,
}

impl UpstreamConnection {
    /// Send a SV2 message to the Upstream role
    pub async fn send(&mut self, sv2_frame: StdFrame) -> ProxyResult<'static, ()> {
        let either_frame = sv2_frame.into();
        self.sender.send(either_frame).await?;
        Ok(())
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/upstream_sv2/upstream.rs">
//! ## Upstream SV2 Module: Upstream Connection Logic
//!
//! Defines the [`Upstream`] structure, which represents and manages the connection
//! to a single upstream role.
//!
//! This module is responsible for:
//! - Establishing and maintaining the network connection to the upstream role.
//! - Performing the SV2 handshake and opening mining channels.
//! - Sending translated SV2 `SubmitSharesExtended` messages received from the Bridge to the
//!   upstream pool.
//! - Receiving SV2 job messages (`SetNewPrevHash`, `NewExtendedMiningJob`, etc.) from the upstream
//!   pool and forwarding them to the Bridge for translation.
//! - Handling various SV2 messages related to connection setup, channel management, and mining
//!   operations.
//! - Managing difficulty updates for the upstream channel based on aggregated hashrate from
//!   downstream miners.
//! - Implementing the necessary SV2 roles logic traits (`IsUpstream`, `IsMiningUpstream`,
//!   `ParseCommonMessagesFromUpstream`, `ParseMiningMessagesFromUpstream`).

use crate::{
    config::UpstreamDifficultyConfig,
    downstream_sv1::Downstream,
    error::{
        Error::{CodecNoise, InvalidExtranonce, PoisonLock, UpstreamIncoming},
        ProxyResult,
    },
    status,
    upstream_sv2::{EitherFrame, Message, StdFrame, UpstreamConnection},
};
use async_channel::{Receiver, Sender};
use error_handling::handle_result;
use key_utils::Secp256k1PublicKey;
use std::{
    net::SocketAddr,
    sync::{atomic::AtomicBool, Arc},
};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        self,
        codec_sv2::{self, binary_sv2::u256_from_int, framing_sv2, HandshakeRole, Initiator},
        common_messages_sv2::{Protocol, SetupConnection},
        handlers::{
            common::{ParseCommonMessagesFromUpstream, SendTo as SendToCommon},
            mining::{ParseMiningMessagesFromUpstream, SendTo},
        },
        mining_sv2::{
            ExtendedExtranonce, Extranonce, NewExtendedMiningJob, OpenExtendedMiningChannel,
            SetNewPrevHash, SubmitSharesExtended,
        },
        parsers::Mining,
        utils::Mutex,
        Error as RolesLogicError,
        Error::NoUpstreamsConnected,
    },
};
use tokio::{
    net::TcpStream,
    task::AbortHandle,
    time::{sleep, Duration},
};
use tracing::{debug, error, info, warn};

use stratum_common::roles_logic_sv2::{
    bitcoin::BlockHash, common_messages_sv2::Reconnect, handlers::mining::SupportedChannelTypes,
    mining_sv2::SetGroupChannel,
};

/// Atomic boolean flag used for synchronization between receiving a new job
/// and handling a new previous hash. Indicates whether a `NewExtendedMiningJob`
/// has been fully processed.
pub static IS_NEW_JOB_HANDLED: AtomicBool = AtomicBool::new(true);
/// Represents the currently active `prevhash` of the mining job being worked on OR being submitted
/// from the Downstream role.
#[derive(Debug, Clone)]
#[allow(dead_code)]
struct PrevHash {
    /// `prevhash` of mining job.
    prev_hash: BlockHash,
    /// `nBits` encoded difficulty target.
    nbits: u32,
}

/// Represents a connection to a single SV2 Upstream role.
///
/// This struct holds the state and communication channels necessary to interact
/// with the upstream server, including sending share submissions, receiving job
/// templates, and managing the SV2 protocol handshake and channel lifecycle.
#[derive(Debug, Clone)]
pub struct Upstream {
    /// Newly assigned identifier of the channel, stable for the whole lifetime of the connection,
    /// e.g. it is used for broadcasting new jobs by the `NewExtendedMiningJob` message.
    pub(super) channel_id: Option<u32>,
    /// Identifier of the job as provided by the `NewExtendedMiningJob` message.
    job_id: Option<u32>,
    /// Identifier of the job as provided by the ` SetCustomMiningJobSucces` message
    last_job_id: Option<u32>,
    /// Bytes used as implicit first part of `extranonce`.
    extranonce_prefix: Option<Vec<u8>>,
    /// Represents a connection to a SV2 Upstream role.
    pub(super) connection: UpstreamConnection,
    /// Receives SV2 `SubmitSharesExtended` messages translated from SV1 `mining.submit` messages.
    /// Translated by and sent from the `Bridge`.
    rx_sv2_submit_shares_ext: Receiver<SubmitSharesExtended<'static>>,
    /// Sends SV2 `SetNewPrevHash` messages to be translated (along with SV2 `NewExtendedMiningJob`
    /// messages) into SV1 `mining.notify` messages. Received and translated by the `Bridge`.
    tx_sv2_set_new_prev_hash: Sender<SetNewPrevHash<'static>>,
    /// Sends SV2 `NewExtendedMiningJob` messages to be translated (along with SV2 `SetNewPrevHash`
    /// messages) into SV1 `mining.notify` messages. Received and translated by the `Bridge`.
    tx_sv2_new_ext_mining_job: Sender<NewExtendedMiningJob<'static>>,
    /// Sends the extranonce1 and the channel id received in the SV2
    /// `OpenExtendedMiningChannelSuccess` message to be used by the `Downstream` and sent to
    /// the Downstream role in a SV2 `mining.subscribe` response message. Passed to the
    /// `Downstream` on connection creation.
    tx_sv2_extranonce: Sender<(ExtendedExtranonce, u32)>,
    /// This allows the upstream threads to be able to communicate back to the main thread its
    /// current status.
    tx_status: status::Sender,
    /// The first `target` is received by the Upstream role in the SV2
    /// `OpenExtendedMiningChannelSuccess` message, then updated periodically via SV2 `SetTarget`
    /// messages. Passed to the `Downstream` on connection creation and sent to the Downstream role
    /// via the SV1 `mining.set_difficulty` message.
    target: Arc<Mutex<Vec<u8>>>,
    /// Tracks the most recently sent nominal hashrate to prevent unnecessary updates.
    pub last_sent_hashrate: Option<f32>,
    /// Minimum `extranonce2` size. Initially requested in the `proxy-config.toml`, and ultimately
    /// set by the SV2 Upstream via the SV2 `OpenExtendedMiningChannelSuccess` message.
    pub min_extranonce_size: u16,
    /// The size of the extranonce1 provided by the upstream role.
    pub upstream_extranonce1_size: usize,
    // values used to update the channel with the correct nominal hashrate.
    // each Downstream instance will add and subtract their hashrates as needed
    // and the upstream just needs to occasionally check if it has changed more than
    // than the configured percentage
    pub(super) difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
    task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
}

impl PartialEq for Upstream {
    fn eq(&self, other: &Self) -> bool {
        self.channel_id == other.channel_id
    }
}

impl Upstream {
    /// Instantiate a new `Upstream`.
    /// Connect to the SV2 Upstream role (most typically a SV2 Pool). Initializes the
    /// `UpstreamConnection` with a channel to send and receive messages from the SV2 Upstream
    /// role and uses channels provided in the function arguments to send and receive messages
    /// from the `Downstream`.
    #[allow(clippy::too_many_arguments)]
    pub async fn new(
        address: SocketAddr,
        authority_public_key: Secp256k1PublicKey,
        rx_sv2_submit_shares_ext: Receiver<SubmitSharesExtended<'static>>,
        tx_sv2_set_new_prev_hash: Sender<SetNewPrevHash<'static>>,
        tx_sv2_new_ext_mining_job: Sender<NewExtendedMiningJob<'static>>,
        min_extranonce_size: u16,
        tx_sv2_extranonce: Sender<(ExtendedExtranonce, u32)>,
        tx_status: status::Sender,
        target: Arc<Mutex<Vec<u8>>>,
        difficulty_config: Arc<Mutex<UpstreamDifficultyConfig>>,
        task_collector: Arc<Mutex<Vec<(AbortHandle, String)>>>,
    ) -> ProxyResult<'static, Arc<Mutex<Self>>> {
        // Connect to the SV2 Upstream role retry connection every 5 seconds.
        let socket = loop {
            match TcpStream::connect(address).await {
                Ok(socket) => break socket,
                Err(e) => {
                    error!(
                        "Failed to connect to Upstream role at {}, retrying in 5s: {}",
                        address, e
                    );

                    sleep(Duration::from_secs(5)).await;
                }
            }
        };

        let pub_key: Secp256k1PublicKey = authority_public_key;
        let initiator = Initiator::from_raw_k(pub_key.into_bytes())?;

        info!(
            "PROXY SERVER - ACCEPTING FROM UPSTREAM: {}",
            socket.peer_addr()?
        );

        // Channel to send and receive messages to the SV2 Upstream role
        let (receiver, sender) = Connection::new(socket, HandshakeRole::Initiator(initiator))
            .await
            .unwrap();
        // Initialize `UpstreamConnection` with channel for SV2 Upstream role communication and
        // channel for downstream Translator Proxy communication
        let connection = UpstreamConnection { receiver, sender };

        Ok(Arc::new(Mutex::new(Self {
            connection,
            rx_sv2_submit_shares_ext,
            extranonce_prefix: None,
            tx_sv2_set_new_prev_hash,
            tx_sv2_new_ext_mining_job,
            channel_id: None,
            job_id: None,
            last_job_id: None,
            min_extranonce_size,
            upstream_extranonce1_size: 16, /* 16 is the default since that is the only value the
                                            * pool supports currently */
            tx_sv2_extranonce,
            tx_status,
            target,
            last_sent_hashrate: None,
            difficulty_config,
            task_collector,
        })))
    }

    /// Performs the SV2 connection setup handshake with the Upstream role.
    ///
    /// Sends a `SetupConnection` message specifying supported protocol versions
    /// and flags. Waits for the upstream to respond with either `SetupConnectionSuccess`
    /// or `SetupConnectionError`.Upon successful setup, it then sends an
    /// `OpenExtendedMiningChannel` request to establish a mining channel, including the
    /// negotiated minimum extranonce size and initial nominal hashrate.
    pub async fn connect(
        self_: Arc<Mutex<Self>>,
        min_version: u16,
        max_version: u16,
    ) -> ProxyResult<'static, ()> {
        // Get the `SetupConnection` message with Mining Device information (currently hard coded)
        let setup_connection = Self::get_setup_connection_message(min_version, max_version, false)?;
        let mut connection = self_.safe_lock(|s| s.connection.clone())?;

        // Put the `SetupConnection` message in a `StdFrame` to be sent over the wire
        let sv2_frame: StdFrame = Message::Common(setup_connection.into()).try_into()?;
        // Send the `SetupConnection` frame to the SV2 Upstream role
        // Only one Upstream role is supported, panics if multiple connections are encountered
        connection.send(sv2_frame).await?;

        // Wait for the SV2 Upstream to respond with either a `SetupConnectionSuccess` or a
        // `SetupConnectionError` inside a SV2 binary message frame
        let mut incoming: StdFrame = match connection.receiver.recv().await {
            Ok(frame) => frame.try_into()?,
            Err(e) => {
                error!("Upstream connection closed: {}", e);
                return Err(CodecNoise(
                    codec_sv2::noise_sv2::Error::ExpectedIncomingHandshakeMessage,
                ));
            }
        };

        // Gets the binary frame message type from the message header
        let message_type = if let Some(header) = incoming.get_header() {
            header.msg_type()
        } else {
            return Err(framing_sv2::Error::ExpectedHandshakeFrame.into());
        };
        // Gets the message payload
        let payload = incoming.payload();

        // Handle the incoming message (should be either `SetupConnectionSuccess` or
        // `SetupConnectionError`)
        ParseCommonMessagesFromUpstream::handle_message_common(
            self_.clone(),
            message_type,
            payload,
        )?;

        // Send open channel request before returning
        let nominal_hash_rate = self_.safe_lock(|u| {
            u.difficulty_config
                .safe_lock(|c| c.channel_nominal_hashrate)
                .map_err(|_e| PoisonLock)
        })??;
        let user_identity = "ABC".to_string().try_into()?;

        // Get the min_extranonce_size from the instance
        let min_extranonce_size = self_.safe_lock(|u| u.min_extranonce_size)?;

        let open_channel = Mining::OpenExtendedMiningChannel(OpenExtendedMiningChannel {
            request_id: 0, // TODO
            user_identity, // TODO
            nominal_hash_rate,
            max_target: u256_from_int(u64::MAX), // TODO
            min_extranonce_size,
        });

        // reset channel hashrate so downstreams can manage from now on out
        self_.safe_lock(|u| {
            u.difficulty_config
                .safe_lock(|d| d.channel_nominal_hashrate = 0.0)
                .map_err(|_e| PoisonLock)
        })??;

        let sv2_frame: StdFrame = Message::Mining(open_channel).try_into()?;
        connection.send(sv2_frame).await?;

        Ok(())
    }

    /// Spawns tasks to handle incoming SV2 messages from the Upstream role.
    ///
    /// This method creates two main asynchronous tasks:
    /// 1. A task to handle incoming SV2 frames, parsing them, routing them to the appropriate
    ///    message handlers (`handle_message_mining`), and forwarding translated messages to the
    ///    Bridge or responding directly to the upstream if necessary.
    /// 2. A task to periodically check and update the nominal hashrate sent to the upstream based
    ///    on th
    #[allow(clippy::result_large_err)]
    pub fn parse_incoming(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        let clone = self_.clone();
        let task_collector = self_.safe_lock(|s| s.task_collector.clone()).unwrap();
        let collector1 = task_collector.clone();
        let collector2 = task_collector.clone();
        let (
            tx_frame,
            tx_sv2_extranonce,
            tx_sv2_new_ext_mining_job,
            tx_sv2_set_new_prev_hash,
            recv,
            tx_status,
        ) = clone.safe_lock(|s| {
            (
                s.connection.sender.clone(),
                s.tx_sv2_extranonce.clone(),
                s.tx_sv2_new_ext_mining_job.clone(),
                s.tx_sv2_set_new_prev_hash.clone(),
                s.connection.receiver.clone(),
                s.tx_status.clone(),
            )
        })?;
        {
            let self_ = self_.clone();
            let tx_status = tx_status.clone();
            let start_diff_management = tokio::task::spawn(async move {
                // No need to start diff management immediatly
                sleep(Duration::from_secs(10)).await;
                loop {
                    handle_result!(tx_status, Self::try_update_hashrate(self_.clone()).await);
                }
            });
            let _ = collector1.safe_lock(|a| {
                a.push((
                    start_diff_management.abort_handle(),
                    "start_diff_management".to_string(),
                ))
            });
        }

        let parse_incoming = tokio::task::spawn(async move {
            loop {
                // Waiting to receive a message from the SV2 Upstream role
                let incoming = handle_result!(tx_status, recv.recv().await);
                let mut incoming: StdFrame = handle_result!(tx_status, incoming.try_into());
                // On message receive, get the message type from the message header and get the
                // message payload
                let message_type =
                    incoming
                        .get_header()
                        .ok_or(super::super::error::Error::FramingSv2(
                            framing_sv2::Error::ExpectedSv2Frame,
                        ));

                let message_type = handle_result!(tx_status, message_type).msg_type();

                let payload = incoming.payload();

                // Gets the response message for the received SV2 Upstream role message
                // `handle_message_mining` takes care of the SetupConnection +
                // SetupConnection.Success
                let next_message_to_send =
                    Upstream::handle_message_mining(self_.clone(), message_type, payload);

                // Routes the incoming messages accordingly
                match next_message_to_send {
                    // No translation required, simply respond to SV2 pool w a SV2 message
                    Ok(SendTo::Respond(message_for_upstream)) => {
                        let message = Message::Mining(message_for_upstream);

                        let frame: StdFrame = handle_result!(tx_status, message.try_into());
                        let frame: EitherFrame = frame.into();

                        // Relay the response message to the Upstream role
                        handle_result!(tx_status, tx_frame.send(frame).await);
                    }
                    // Does not send the messages anywhere, but instead handle them internally
                    Ok(SendTo::None(Some(m))) => {
                        match m {
                            Mining::OpenExtendedMiningChannelSuccess(m) => {
                                let prefix_len = m.extranonce_prefix.len();
                                // update upstream_extranonce1_size for tracking
                                let miner_extranonce2_size = self_
                                    .safe_lock(|u| {
                                        u.upstream_extranonce1_size = prefix_len;
                                        u.min_extranonce_size as usize
                                    })
                                    .map_err(|_e| PoisonLock);
                                let miner_extranonce2_size =
                                    handle_result!(tx_status, miner_extranonce2_size);
                                let extranonce_prefix: Extranonce = m.extranonce_prefix.into();
                                // Create the extended extranonce that will be saved in bridge and
                                // it will be used to open downstream (sv1) channels
                                // range 0 is the extranonce1 from upstream
                                // range 1 is the extranonce1 added by the tproxy
                                // range 2 is the extranonce2 used by the miner for rolling
                                // range 0 + range 1 is the extranonce1 sent to the miner
                                let tproxy_e1_len = super::super::utils::proxy_extranonce1_len(
                                    m.extranonce_size as usize,
                                    miner_extranonce2_size,
                                );
                                let range_0 = 0..prefix_len; // upstream extranonce1
                                let range_1 = prefix_len..prefix_len + tproxy_e1_len; // downstream extranonce1
                                let range_2 = prefix_len + tproxy_e1_len
                                    ..prefix_len + m.extranonce_size as usize; // extranonce2
                                let extended = handle_result!(tx_status, ExtendedExtranonce::from_upstream_extranonce(
                                    extranonce_prefix.clone(), range_0.clone(), range_1.clone(), range_2.clone(),
                                ).map_err(|err| InvalidExtranonce(format!("Impossible to create a valid extended extranonce from {extranonce_prefix:?} {range_0:?} {range_1:?} {range_2:?}: {err:?}"))));
                                handle_result!(
                                    tx_status,
                                    tx_sv2_extranonce.send((extended, m.channel_id)).await
                                );
                            }
                            Mining::NewExtendedMiningJob(m) => {
                                let job_id = m.job_id;
                                let res = self_
                                    .safe_lock(|s| {
                                        let _ = s.job_id.insert(job_id);
                                    })
                                    .map_err(|_e| PoisonLock);
                                handle_result!(tx_status, res);
                                handle_result!(tx_status, tx_sv2_new_ext_mining_job.send(m).await);
                            }
                            Mining::SetNewPrevHash(m) => {
                                handle_result!(tx_status, tx_sv2_set_new_prev_hash.send(m).await);
                            }
                            Mining::CloseChannel(_m) => {
                                error!("Received Mining::CloseChannel msg from upstream!");
                                handle_result!(tx_status, Err(NoUpstreamsConnected));
                            }
                            Mining::OpenMiningChannelError(_)
                            | Mining::UpdateChannelError(_)
                            | Mining::SubmitSharesError(_)
                            | Mining::SetCustomMiningJobError(_) => {
                                error!("parse_incoming SV2 protocol error Message");
                                handle_result!(tx_status, Err(m));
                            }
                            // impossible state: handle_message_mining only returns
                            // the above 3 messages in the Ok(SendTo::None(Some(m))) case to be sent
                            // to the bridge for translation.
                            _ => panic!(),
                        }
                    }
                    Ok(SendTo::None(None)) => (),
                    // No need to handle impossible state just panic cause are impossible and we
                    // will never panic ;-) Verified: handle_message_mining only either panics,
                    // returns Ok(SendTo::None(None)) or Ok(SendTo::None(Some(m))), or returns Err
                    Ok(_) => panic!(),
                    Err(e) => {
                        let status = status::Status {
                            state: status::State::UpstreamShutdown(UpstreamIncoming(e)),
                        };
                        error!(
                            "TERMINATING: Error handling pool role message: {:?}",
                            status
                        );
                        if let Err(e) = tx_status.send(status).await {
                            error!("Status channel down: {:?}", e);
                        }

                        break;
                    }
                }
            }
        });
        let _ = collector2
            .safe_lock(|a| a.push((parse_incoming.abort_handle(), "parse_incoming".to_string())));

        Ok(())
    }

    // Retrieves the current job ID.
    //
    // If work selection is enabled (which it is not for a Translator Proxy),
    // it would return the last `SetCustomMiningJobSuccess` job ID. If
    // work selection is disabled, it returns the job ID from the last
    // `NewExtendedMiningJob`
    #[allow(clippy::result_large_err)]
    fn get_job_id(
        self_: &Arc<Mutex<Self>>,
    ) -> Result<Result<u32, super::super::error::Error<'static>>, super::super::error::Error<'static>>
    {
        self_
            .safe_lock(|s| {
                if s.is_work_selection_enabled() {
                    s.last_job_id
                        .ok_or(super::super::error::Error::RolesSv2Logic(
                            RolesLogicError::NoValidTranslatorJob,
                        ))
                } else {
                    s.job_id.ok_or(super::super::error::Error::RolesSv2Logic(
                        RolesLogicError::NoValidJob,
                    ))
                }
            })
            .map_err(|_e| PoisonLock)
    }

    /// Spawns a task to handle outgoing `SubmitSharesExtended` messages.
    ///
    /// This task continuously receives `SubmitSharesExtended` messages from the
    /// `rx_sv2_submit_shares_ext` channel (populated by the Bridge). It updates
    /// the channel ID and job ID in the submit message (ensuring they match
    /// the current upstream channel details), encodes the message into an SV2 frame,
    /// and sends it to the upstream server.
    #[allow(clippy::result_large_err)]
    pub fn handle_submit(self_: Arc<Mutex<Self>>) -> ProxyResult<'static, ()> {
        let task_collector = self_.safe_lock(|s| s.task_collector.clone()).unwrap();
        let clone = self_.clone();
        let (tx_frame, receiver, tx_status) = clone.safe_lock(|s| {
            (
                s.connection.sender.clone(),
                s.rx_sv2_submit_shares_ext.clone(),
                s.tx_status.clone(),
            )
        })?;

        let handle_submit = tokio::task::spawn(async move {
            loop {
                let mut sv2_submit: SubmitSharesExtended =
                    handle_result!(tx_status, receiver.recv().await);

                let channel_id = self_
                    .safe_lock(|s| {
                        s.channel_id
                            .ok_or(super::super::error::Error::RolesSv2Logic(
                                RolesLogicError::NotFoundChannelId,
                            ))
                    })
                    .map_err(|_e| PoisonLock);
                sv2_submit.channel_id =
                    handle_result!(tx_status, handle_result!(tx_status, channel_id));
                let job_id = Self::get_job_id(&self_);
                sv2_submit.job_id = handle_result!(tx_status, handle_result!(tx_status, job_id));

                let message = Message::Mining(
                    roles_logic_sv2::parsers::Mining::SubmitSharesExtended(sv2_submit),
                );

                let frame: StdFrame = handle_result!(tx_status, message.try_into());
                // Doesnt actually send because of Braiins Pool issue that needs to be fixed

                let frame: EitherFrame = frame.into();
                handle_result!(tx_status, tx_frame.send(frame).await);
            }
        });
        let _ = task_collector
            .safe_lock(|a| a.push((handle_submit.abort_handle(), "handle_submit".to_string())));

        Ok(())
    }

    // Unimplemented method to check if a submitted share is contained within the upstream target.
    //
    // This method is currently unimplemented (`todo!()`). Its purpose would be
    // to validate a share against the target set by the upstream pool.
    fn _is_contained_in_upstream_target(&self, _share: SubmitSharesExtended) -> bool {
        todo!()
    }

    // Creates the initial `SetupConnection` message for the SV2 handshake.
    //
    // This message contains information about the proxy acting as a mining device,
    // including supported protocol versions, flags, and hardcoded endpoint details.
    //
    // TODO: The Mining Device information is currently hardcoded. It should ideally
    // be configurable or derived from the downstream connections.
    #[allow(clippy::result_large_err)]
    fn get_setup_connection_message(
        min_version: u16,
        max_version: u16,
        is_work_selection_enabled: bool,
    ) -> ProxyResult<'static, SetupConnection<'static>> {
        let endpoint_host = "0.0.0.0".to_string().into_bytes().try_into()?;
        let vendor = String::new().try_into()?;
        let hardware_version = String::new().try_into()?;
        let firmware = String::new().try_into()?;
        let device_id = String::new().try_into()?;
        let flags = match is_work_selection_enabled {
            false => 0b0000_0000_0000_0000_0000_0000_0000_0100,
            true => 0b0000_0000_0000_0000_0000_0000_0000_0110,
        };
        Ok(SetupConnection {
            protocol: Protocol::MiningProtocol,
            min_version,
            max_version,
            flags,
            endpoint_host,
            endpoint_port: 50,
            vendor,
            hardware_version,
            firmware,
            device_id,
        })
    }
}

impl ParseCommonMessagesFromUpstream for Upstream {
    // Handles the SV2 `SetupConnectionSuccess` message received from the upstream.
    //
    // Returns `Ok(SendToCommon::None(None))` as this message is handled internally
    // and does not require a direct response or forwarding.
    fn handle_setup_connection_success(
        &mut self,
        m: roles_logic_sv2::common_messages_sv2::SetupConnectionSuccess,
    ) -> Result<SendToCommon, RolesLogicError> {
        info!(
            "Received `SetupConnectionSuccess`: version={}, flags={:b}",
            m.used_version, m.flags
        );
        Ok(SendToCommon::None(None))
    }

    fn handle_setup_connection_error(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::SetupConnectionError,
    ) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }

    fn handle_channel_endpoint_changed(
        &mut self,
        _: roles_logic_sv2::common_messages_sv2::ChannelEndpointChanged,
    ) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }

    fn handle_reconnect(&mut self, _m: Reconnect) -> Result<SendToCommon, RolesLogicError> {
        todo!()
    }
}

/// Connection-wide SV2 Upstream role messages parser implemented by a downstream ("downstream"
/// here is relative to the SV2 Upstream role and is represented by this `Upstream` struct).
impl ParseMiningMessagesFromUpstream<Downstream> for Upstream {
    /// Returns the type of channel used between this proxy and the SV2 Upstream.
    /// For a Translator Proxy, this is always `Extended`.
    fn get_channel_type(&self) -> SupportedChannelTypes {
        SupportedChannelTypes::Extended
    }

    /// Indicates whether work selection is enabled for this upstream connection.
    /// For a Translator Proxy, work selection is handled by the upstream pool,
    /// so this method always returns `false`.
    fn is_work_selection_enabled(&self) -> bool {
        false
    }

    /// The SV2 `OpenStandardMiningChannelSuccess` message is NOT handled because it is NOT used
    /// for the Translator Proxy as only `Extended` channels are used between the SV1/SV2 Translator
    /// Proxy and the SV2 Upstream role.
    fn handle_open_standard_mining_channel_success(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::OpenStandardMiningChannelSuccess,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        panic!("Standard Mining Channels are not used in Translator Proxy")
    }

    /// Handles the SV2 `OpenExtendedMiningChannelSuccess` message.
    ///
    /// This message is received after requesting to open an extended mining channel.
    /// It provides the assigned `channel_id`, the extranonce prefix, the initial
    /// mining `target`, and the expected `extranonce_size`. It stores the `channel_id` and
    /// `extranonce_prefix`, updates the shared `target`, and prepares the extranonce
    /// information (including calculating the size for the TProxy's added extranonce1) to be
    /// sent to the Downstream handler for use with SV1 clients.
    ///
    /// Returns `Ok(SendTo<Downstream>::None(Some(Mining::OpenExtendedMiningChannelSuccess)))`
    /// to indicate that the message has been handled internally and should be
    /// forwarded to the Bridge.
    fn handle_open_extended_mining_channel_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::OpenExtendedMiningChannelSuccess,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received OpenExtendedMiningChannelSuccess with request id: {} and channel id: {}",
            m.request_id, m.channel_id
        );
        debug!("OpenStandardMiningChannelSuccess: {}", m);
        let tproxy_e1_len = super::super::utils::proxy_extranonce1_len(
            m.extranonce_size as usize,
            self.min_extranonce_size.into(),
        ) as u16;
        if self.min_extranonce_size + tproxy_e1_len < m.extranonce_size {
            return Err(RolesLogicError::InvalidExtranonceSize(
                self.min_extranonce_size,
                m.extranonce_size,
            ));
        }
        self.target.safe_lock(|t| *t = m.target.to_vec())?;

        info!("Up: Successfully Opened Extended Mining Channel");
        self.channel_id = Some(m.channel_id);
        self.extranonce_prefix = Some(m.extranonce_prefix.to_vec());
        let m = Mining::OpenExtendedMiningChannelSuccess(m.into_static());
        Ok(SendTo::None(Some(m)))
    }

    /// Handles the SV2 `OpenExtendedMiningChannelError` message (TODO).
    fn handle_open_mining_channel_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::OpenMiningChannelError,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received OpenExtendedMiningChannelError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::None(Some(Mining::OpenMiningChannelError(
            m.as_static(),
        ))))
    }

    /// Handles the SV2 `UpdateChannelError` message (TODO).
    fn handle_update_channel_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::UpdateChannelError,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received UpdateChannelError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::None(Some(Mining::UpdateChannelError(
            m.as_static(),
        ))))
    }

    /// Handles the SV2 `CloseChannel` message (TODO).
    fn handle_close_channel(
        &mut self,
        m: roles_logic_sv2::mining_sv2::CloseChannel,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!("Received CloseChannel for channel id: {}", m.channel_id);
        Ok(SendTo::None(Some(Mining::CloseChannel(m.as_static()))))
    }

    /// Handles the SV2 `SetExtranoncePrefix` message (TODO).
    fn handle_set_extranonce_prefix(
        &mut self,
        _: roles_logic_sv2::mining_sv2::SetExtranoncePrefix,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        todo!()
    }

    /// Handles the SV2 `SubmitSharesSuccess` message.
    fn handle_submit_shares_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SubmitSharesSuccess,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!("Received SubmitSharesSuccess");
        debug!("SubmitSharesSuccess: {}", m);
        Ok(SendTo::None(None))
    }

    /// Handles the SV2 `SubmitSharesError` message.
    fn handle_submit_shares_error(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SubmitSharesError,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        error!(
            "Received SubmitSharesError with error code {}",
            std::str::from_utf8(m.error_code.as_ref()).unwrap_or("unknown error code")
        );
        Ok(SendTo::None(None))
    }

    /// The SV2 `NewMiningJob` message is NOT handled because it is NOT used for the Translator
    /// Proxy as only `Extended` channels are used between the SV1/SV2 Translator Proxy and the SV2
    /// Upstream role.
    fn handle_new_mining_job(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::NewMiningJob,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        panic!("Standard Mining Channels are not used in Translator Proxy")
    }

    /// Handles the SV2 `NewExtendedMiningJob` message which is used (along with the SV2
    /// `SetNewPrevHash` message) to later create a SV1 `mining.notify` for the Downstream
    /// role.
    fn handle_new_extended_mining_job(
        &mut self,
        m: NewExtendedMiningJob,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received new extended mining job for channel id: {} with job id: {} is_future: {}",
            m.channel_id,
            m.job_id,
            m.is_future()
        );
        debug!("NewExtendedMiningJob: {}", m);
        if self.is_work_selection_enabled() {
            Ok(SendTo::None(None))
        } else {
            IS_NEW_JOB_HANDLED.store(false, std::sync::atomic::Ordering::SeqCst);
            if !m.version_rolling_allowed {
                warn!("VERSION ROLLING NOT ALLOWED IS A TODO");
                // todo!()
            }

            let message = Mining::NewExtendedMiningJob(m.into_static());

            Ok(SendTo::None(Some(message)))
        }
    }

    /// Handles the SV2 `SetNewPrevHash` message which is used (along with the SV2
    /// `NewExtendedMiningJob` message) to later create a SV1 `mining.notify` for the Downstream
    /// role.
    fn handle_set_new_prev_hash(
        &mut self,
        m: SetNewPrevHash,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received SetNewPrevHash channel id: {}, job id: {}",
            m.channel_id, m.job_id
        );
        debug!("SetNewPrevHash: {}", m);
        if self.is_work_selection_enabled() {
            Ok(SendTo::None(None))
        } else {
            let message = Mining::SetNewPrevHash(m.into_static());
            Ok(SendTo::None(Some(message)))
        }
    }

    /// Handles the SV2 `SetCustomMiningJobSuccess` message (TODO).
    fn handle_set_custom_mining_job_success(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SetCustomMiningJobSuccess,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!(
            "Received SetCustomMiningJobSuccess for channel id: {} for job id: {}",
            m.channel_id, m.job_id
        );
        debug!("SetCustomMiningJobSuccess: {}", m);
        self.last_job_id = Some(m.job_id);
        Ok(SendTo::None(None))
    }

    /// Handles the SV2 `SetCustomMiningJobError` message (TODO).
    fn handle_set_custom_mining_job_error(
        &mut self,
        _m: roles_logic_sv2::mining_sv2::SetCustomMiningJobError,
    ) -> Result<roles_logic_sv2::handlers::mining::SendTo<Downstream>, RolesLogicError> {
        unimplemented!()
    }

    /// Handles the SV2 `SetTarget` message which updates the Downstream role(s) target
    /// difficulty via the SV1 `mining.set_difficulty` message.
    fn handle_set_target(
        &mut self,
        m: roles_logic_sv2::mining_sv2::SetTarget,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        info!("Received SetTarget for channel id: {}", m.channel_id);
        debug!("SetTarget: {}", m);
        let m = m.into_static();
        self.target.safe_lock(|t| *t = m.maximum_target.to_vec())?;
        Ok(SendTo::None(None))
    }

    fn handle_set_group_channel(
        &mut self,
        _m: SetGroupChannel,
    ) -> Result<SendTo<Downstream>, RolesLogicError> {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/roles/translator/src/lib/utils.rs">
/// Calculates the required length of the proxy's extranonce1.
///
/// The proxy needs to calculate an extranonce1 value to send to the
/// upstream server.  This function determines the length of that
/// extranonce1 value
/// FIXME: The pool only supported 16 bytes exactly for its
/// `extranonce1` field is no longer the case and the
/// code needs to be changed to support variable `extranonce1` lengths.
pub fn proxy_extranonce1_len(
    channel_extranonce2_size: usize,
    downstream_extranonce2_len: usize,
) -> usize {
    // full_extranonce_len - pool_extranonce1_len - miner_extranonce2 = tproxy_extranonce1_len
    channel_extranonce2_size - downstream_extranonce2_len
}
</file>

<file path="stratum-1.4.0/roles/translator/src/main.rs">
mod args;

pub use translator_sv2::{
    config, downstream_sv1, error, proxy, status, upstream_sv2, TranslatorSv2,
};

use tracing::info;

use crate::args::process_cli_args;
use config_helpers::logging::init_logging;
/// Entrypoint for the Translator binary.
///
/// Loads the configuration from TOML and initializes the main runtime
/// defined in `translator_sv2::TranslatorSv2`. Errors during startup are logged.
#[tokio::main]
async fn main() {
    let proxy_config = match process_cli_args() {
        Ok(p) => p,
        Err(e) => panic!("failed to load config: {e}"),
    };
    init_logging(proxy_config.log_dir());
    info!("Proxy Config: {:?}", &proxy_config);

    TranslatorSv2::new(proxy_config).start().await;
}
</file>

<file path="stratum-1.4.0/rustfmt.toml">
edition = "2018"
imports_indent = "Block"
imports_layout = "Mixed"
imports_granularity = "Crate"
wrap_comments = true
format_code_in_doc_comments = true
comment_width = 100             # Default 80
normalize_comments = false
normalize_doc_attributes = false
format_strings = false
</file>

<file path="stratum-1.4.0/scripts/build_header.sh">
#! /bin/sh
cargo install --version 0.21.0 cbindgen --force

rm -f ./sv2.h
touch ./sv2.h

dir=${1:-../protocols}

cd "$dir"
  cbindgen --crate const_sv2 >> ../scripts/sv2.h
  cbindgen --crate binary_codec_sv2 >> ../scripts/sv2.h
  cbindgen --crate common_messages_sv2 >> ../scripts/sv2.h
  cbindgen --crate template_distribution_sv2 >> ../scripts/sv2.h
  cbindgen --crate codec_sv2 >> ../scripts/sv2.h
  cbindgen --crate sv2_ffi >> ../scripts/sv2.h
cd ..
</file>

<file path="stratum-1.4.0/scripts/build-on-all-workspaces.sh">
#!/bin/sh

WORKSPACES="benches common protocols roles utils"

for workspace in $WORKSPACES; do
    echo "Executing build on: $workspace"
    cargo +1.75.0 build --manifest-path="$workspace/Cargo.toml" -- 
    if [ $? -ne 0 ]; then
        echo "Build found some errors in: $workspace"
        exit 1
    fi

    echo "Running fmt on: $workspace"
    (cd $workspace && cargo +nightly fmt)
    if [ $? -ne 0 ]; then
        echo "Fmt failed in: $workspace"
        exit 1
    fi
done

echo "build success!"
</file>

<file path="stratum-1.4.0/scripts/clippy-fmt-and-test.sh">
#!/bin/sh

WORKSPACES="benches common protocols roles utils"

for workspace in $WORKSPACES; do
    echo "Executing clippy on: $workspace"
    cargo +1.75.0 clippy --manifest-path="$workspace/Cargo.toml" -- -D warnings -A dead-code
    if [ $? -ne 0 ]; then
        echo "Clippy found some errors in: $workspace"
        exit 1
    fi

    echo "Running tests on: $workspace"
    cargo +1.75 test --manifest-path="$workspace/Cargo.toml"
    if [ $? -ne 0 ]; then
        echo "Tests failed in: $workspace"
        exit 1
    fi

    echo "Running fmt on: $workspace"
    (cd $workspace && cargo +nightly fmt)
    if [ $? -ne 0 ]; then
        echo "Fmt failed in: $workspace"
        exit 1
    fi
done

echo "Clippy success, all tests passed!"
</file>

<file path="stratum-1.4.0/scripts/coverage-protocols.sh">
#!/bin/bash
tarpaulin() {
  crate_name=$1
  output_dir="target/tarpaulin-reports/$crate_name"
  mkdir -p "$output_dir"
  cargo +nightly tarpaulin --verbose --out Xml --output-dir "$output_dir"
}

cd protocols
tarpaulin

crates=(
  "v1"
  "v2/binary-sv2/codec"
  "v2/binary-sv2/derive_codec"
  "v2/binary-sv2"
  "v2/noise-sv2"
  "v2/framing-sv2"
  "v2/codec-sv2"
  "v2/subprotocols/common-messages"
  "v2/subprotocols/template-distribution"
  "v2/subprotocols/mining"
  "v2/subprotocols/job-declaration"
  "v2/sv2-ffi"
  "v2/roles-logic-sv2"
)

for crate in "${crates[@]}"; do
  echo "Running Tarpaulin for $crate..."
  crate_name=$(basename "$crate") 
  cd "$crate" || exit 1            
  tarpaulin "$crate_name-coverage"
  cd - || exit 1
done
</file>

<file path="stratum-1.4.0/scripts/coverage-roles.sh">
#!/bin/bash
tarpaulin() {
  crate_name=$1
  output_dir="target/tarpaulin-reports/$crate_name"
  mkdir -p "$output_dir"
  cargo +nightly tarpaulin --verbose --out Xml --output-dir "$output_dir" --all-features
}

cd roles
tarpaulin

crates=(
  "pool"
  "test-utils/mining-device"
  "test-utils/mining-device-sv1"
  "translator"
  "jd-client"
  "jd-server"
)

for crate in "${crates[@]}"; do
  echo "Running Tarpaulin for $crate..."
  crate_name=$(basename "$crate") 
  cd "$crate" || exit 1            
  tarpaulin "$crate_name-coverage"
  cd - || exit 1
done
</file>

<file path="stratum-1.4.0/scripts/coverage-utils.sh">
#!/bin/bash

tarpaulin() {
  crate_name=$1
  output_dir="target/tarpaulin-reports/$crate_name"
  mkdir -p "$output_dir"
  cargo +nightly tarpaulin --verbose --out Xml --output-dir "$output_dir" --all-features
}

cd utils
tarpaulin

crates=(
  "buffer"
  "error-handling"
  "key-utils"
  "bip32-key-derivation"
)

for crate in "${crates[@]}"; do
  echo "Running Tarpaulin for $crate..."
  crate_name=$(basename "$crate") 
  cd "$crate" || exit 1            
  tarpaulin "$crate_name-coverage"
  cd - || exit 1
done
</file>

<file path="stratum-1.4.0/scripts/release-libs.sh">
#!/bin/sh

# USAGE:
#   ./scripts/cargo-publish.sh <crate-dir>

# the script returns 0 on success of cargo publish, and 1 on failure
# the only exception is when cargo publish fails because the crate is already published
# in that case, the script also returns 0

CRATE_DIR="$1"

echo "Publishing crate in directory: $CRATE_DIR"

cd "$CRATE_DIR"

CARGO_COMMAND="cargo publish"

OUTPUT="$($CARGO_COMMAND 2>&1)"
EXIT_CODE=$?
echo "Ran cargo command, exit code was $EXIT_CODE"

if [ "$EXIT_CODE" -eq 0 ] ; then
  echo "Publish command succeeded: $CRATE_DIR"
  exit 0
fi

# If cargo failed, check whether it was 'already uploaded'
if echo "$OUTPUT" | grep -q "already uploaded"; then
  echo "Crate is already published: $CRATE_DIR"
  exit 0
fi

echo "Publish command failed for $CRATE_DIR"
echo "$OUTPUT"
exit 1
</file>

<file path="stratum-1.4.0/scripts/rust/clippy.sh">
#!/bin/bash

workspaces=("benches" "common" "roles" "protocols" "utils" "test/integration-tests")

# print current rust version
echo "Rust version: $(rustc --version)"

for workspace in "${workspaces[@]}"; do
  echo "Running clippy for workspace: $workspace"
  cargo clippy --manifest-path="$workspace/Cargo.toml" -- -D warnings
  if [[ $? -ne 0 ]]; then
    echo "Clippy failed for workspace: $workspace"
    exit 1
  else
    echo "Clippy passed for workspace: $workspace"
  fi
done

echo "Clippy success!"
exit 0
</file>

<file path="stratum-1.4.0/scripts/sv2-publish.sh">
#! /usr/bin/sh

# This program utilizes cargo-smart-release to auto-version and publish the sv2 crates
# See https://crates.io/crates/cargo-smart-release for more information about the crate

# *** For publishing the following flags should be passed as arguments:
# `-e -u --no-push`

# *** For updating versions without publishing the following flags should be passed as arguments:
# `-e -u --no-publish --no-push --no-changelog`

output=$(cargo smart-release \
    sv1_api \
    binary_sv2 \
    binary_codec_sv2 \
    derive_codec_sv2 \
    codec_sv2 \
    framing_sv2 \
    noise_sv2 \
    roles_logic_sv2 \
    common_messages_sv2 \
    job_declaration_sv2 \
    mining_sv2 \
    template_distribution_sv2 \
    sv2_ffi \
    buffer_sv2 \
    error_handling \
    network_helpers \
    translator_sv2 \
    pool_sv2 \
    mining_proxy_sv2 \
    $@)

if echo "$output" | grep -q "Error: There is no crate eligible for publishing"; then
    echo "No crate eligible for publishing. Exiting with success code."
    exit 0
fi
echo "$output"
</file>

<file path="stratum-1.4.0/test/integration-tests/.config/nextest.toml">
[profile.default]

# SRI has flaky integration tests, which we are ok to live with for now
# but if a test fails more than 3 times, it's safe to assume it's failing deterministically
# and that's a reliable indication that we shouldn't merge this PR
retries = { backoff = "fixed", count = 3, delay = "2s" }

# only run one test at a time, which allows a human-friendly experience for inspecting logs
test-threads = 1

# label as slow if a test runs for more than 60s
# kill it after 120s
slow-timeout = { period = "60s", terminate-after = 2 }

# display status for all levels (pass, fail, flaky, slow, etc)
status-level = "all"
final-status-level = "all"
</file>

<file path="stratum-1.4.0/test/integration-tests/Cargo.toml">
[package]
name = "integration_tests_sv2"
version = "0.1.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
documentation = "https://github.com/stratum-mining/stratum"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[dependencies]
async-channel = { version = "1.5.1", default-features = false }
corepc-node = { version = "0.7.0", default-features = false, features = ["28_0"] }
flate2 = { version = "1.1.0", default-features = false, features = ["rust_backend"] }
minreq = { version = "2.12.0", default-features = false, features = ["https"] }
once_cell = { version = "1.19.0", default-features = false }
rand = { version = "0.9.0", default-features = false, features = ["thread_rng"] }
tar = { version = "0.4.41", default-features = false }
tokio = { version="1.44.1", default-features = false,  features = ["tracing"] }
tracing = { version = "0.1.41", default-features = false }
tracing-subscriber = { version = "0.3.19", default-features = false }

jd_client = { path = "../../roles/jd-client" }
jd_server = { path = "../../roles/jd-server" }
key-utils = { path = "../../utils/key-utils" }
mining_device = { path = "../../roles/test-utils/mining-device" }
mining_device_sv1 = { path = "../../roles/test-utils/mining-device-sv1" }
pool_sv2 = { path = "../../roles/pool" }
config-helpers = { path = "../../roles/roles-utils/config-helpers" }
stratum-common = { path = "../../common" , features = ["with_network_helpers", "sv1"]}
translator_sv2 = { path = "../../roles/translator" }
sv1_api = { path = "../../protocols/v1", optional = true }

[lib]
path = "lib/mod.rs"

[features]
default = []
sv1 = ["sv1_api", "stratum-common/sv1"]
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/interceptor.rs">
use crate::types::MsgType;
use stratum_common::roles_logic_sv2::parsers::AnyMessage;

#[derive(Debug, Clone, PartialEq, Eq)]
pub enum MessageDirection {
    ToDownstream,
    ToUpstream,
}

/// Represents an action that [`Sniffer`] can take on intercepted messages.
#[derive(Debug, Clone)]
pub enum InterceptAction {
    /// Prevents a message from being forwarded and stored into the message aggregator.
    IgnoreMessage(IgnoreMessage),
    /// Intercepts and modifies a message before forwarding it.
    ReplaceMessage(Box<ReplaceMessage>),
}

impl InterceptAction {
    /// Returns the action if it is `IgnoreMessage` or `ReplaceMessage`
    /// with the specified message type.
    pub fn find_matching_action(
        &self,
        msg_type: MsgType,
        direction: MessageDirection,
    ) -> Option<&Self> {
        match self {
            InterceptAction::IgnoreMessage(bm)
                if bm.direction == direction && bm.expected_message_type == msg_type =>
            {
                Some(self)
            }

            InterceptAction::ReplaceMessage(im)
                if im.direction == direction && im.expected_message_type == msg_type =>
            {
                Some(self)
            }

            _ => None,
        }
    }
}
/// Defines an action that prevents a message from being forwarded.
///
/// When a message matching the specified type and direction is intercepted,
/// it will not be added to the message aggregator for inspection and will not be
/// forwarded to the destination. All other messages will continue to be forwarded normally.
#[derive(Debug, Clone)]
pub struct IgnoreMessage {
    direction: MessageDirection,
    expected_message_type: MsgType,
}

impl IgnoreMessage {
    /// Creates a new [`IgnoreMessage`] action.
    ///
    /// - `direction`: The direction of the message to be ignored.
    /// - `expected_message_type`: The type of message to be ignored.
    pub fn new(direction: MessageDirection, expected_message_type: MsgType) -> Self {
        IgnoreMessage {
            direction,
            expected_message_type,
        }
    }
}

impl From<IgnoreMessage> for InterceptAction {
    fn from(value: IgnoreMessage) -> Self {
        InterceptAction::IgnoreMessage(value)
    }
}

/// Allows [`Sniffer`] to replace some intercepted message before forwarding it.
#[derive(Debug, Clone)]
pub struct ReplaceMessage {
    direction: MessageDirection,
    expected_message_type: MsgType,
    pub(crate) replacement_message: AnyMessage<'static>,
}

impl ReplaceMessage {
    /// Constructor of `ReplaceMessage`
    /// - `direction`: direction of message to be intercepted and replaced
    /// - `expected_message_type`: type of message to be intercepted and replaced
    /// - `replacement_message`: message to replace the intercepted one
    /// - `replacement_message_type`: type of message to replace the intercepted one
    pub fn new(
        direction: MessageDirection,
        expected_message_type: MsgType,
        replacement_message: AnyMessage<'static>,
    ) -> Self {
        Self {
            direction,
            expected_message_type,
            replacement_message,
        }
    }
}

impl From<ReplaceMessage> for InterceptAction {
    fn from(value: ReplaceMessage) -> Self {
        InterceptAction::ReplaceMessage(Box::new(value))
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/message_aggregator.rs">
use std::{collections::VecDeque, sync::Arc};
use stratum_common::roles_logic_sv2::{parsers::AnyMessage, utils::Mutex};

use crate::types::MsgType;

#[derive(Debug, Clone)]
pub struct MessagesAggregator {
    messages: Arc<Mutex<VecDeque<(MsgType, AnyMessage<'static>)>>>,
}

impl Default for MessagesAggregator {
    fn default() -> Self {
        Self::new()
    }
}

impl MessagesAggregator {
    /// Creates a new [`MessagesAggregator`].
    pub fn new() -> Self {
        Self {
            messages: Arc::new(Mutex::new(VecDeque::new())),
        }
    }

    /// Adds a message to the end of the queue.
    pub fn add_message(&self, msg_type: MsgType, message: AnyMessage<'static>) {
        self.messages
            .safe_lock(|messages| messages.push_back((msg_type, message)))
            .unwrap();
    }

    /// Returns false if the queue is empty, true otherwise.
    pub fn is_empty(&self) -> bool {
        self.messages
            .safe_lock(|messages| messages.is_empty())
            .unwrap()
    }

    /// returns true if contains message_type
    pub fn has_message_type(&self, message_type: u8) -> bool {
        let has_message: bool = self
            .messages
            .safe_lock(|messages| {
                for (t, _) in messages.iter() {
                    if *t == message_type {
                        return true; // Exit early with `true`
                    }
                }
                false // Default value if no match is found
            })
            .unwrap();
        has_message
    }

    /// returns true if contains message_type and removes messages from the queue
    /// until the first message of type message_type.
    pub fn has_message_type_with_remove(&self, message_type: u8) -> bool {
        self.messages
            .safe_lock(|messages| {
                let mut cloned_messages = messages.clone();
                for (pos, (t, _)) in cloned_messages.iter().enumerate() {
                    if *t == message_type {
                        let drained = cloned_messages.drain(pos + 1..).collect();
                        *messages = drained;
                        return true;
                    }
                }
                false
            })
            .unwrap()
    }

    /// The aggregator queues messages in FIFO order, so this function returns the oldest message in
    /// the queue.
    ///
    /// The returned message is removed from the queue.
    pub fn next_message(&self) -> Option<(MsgType, AnyMessage<'static>)> {
        let is_state = self
            .messages
            .safe_lock(|messages| {
                let mut cloned = messages.clone();
                if let Some((msg_type, msg)) = cloned.pop_front() {
                    *messages = cloned;
                    Some((msg_type, msg))
                } else {
                    None
                }
            })
            .unwrap();
        is_state
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/mock_roles.rs">
use crate::{
    message_aggregator::MessagesAggregator,
    types::{MessageFrame, MsgType},
    utils::{create_downstream, create_upstream, message_from_frame, wait_for_client},
};
use async_channel::Sender;
use std::net::SocketAddr;
use stratum_common::roles_logic_sv2::{
    codec_sv2::{StandardEitherFrame, Sv2Frame},
    parsers::AnyMessage,
};
use tokio::net::TcpStream;

pub struct MockDownstream {
    upstream_address: SocketAddr,
    messages_from_upstream: MessagesAggregator,
}

impl MockDownstream {
    pub fn new(upstream_address: SocketAddr) -> Self {
        Self {
            upstream_address,
            messages_from_upstream: MessagesAggregator::new(),
        }
    }

    pub async fn start(&self) -> Sender<MessageFrame> {
        let upstream_address = self.upstream_address;
        let (upstream_receiver, upstream_sender) = create_upstream(loop {
            match TcpStream::connect(upstream_address).await {
                Ok(stream) => break stream,
                Err(_) => {
                    println!("MockDownstream: unable to connect to upstream, retrying");
                }
            }
        })
        .await
        .expect("Failed to create upstream");
        let messages_from_upstream = self.messages_from_upstream.clone();
        tokio::spawn(async move {
            while let Ok(mut frame) = upstream_receiver.recv().await {
                let (msg_type, msg) = message_from_frame(&mut frame);
                messages_from_upstream.add_message(msg_type, msg);
            }
        });
        upstream_sender
    }

    pub fn next_message_from_upstream(&self) -> Option<(MsgType, AnyMessage<'static>)> {
        self.messages_from_upstream.next_message()
    }
}

pub struct MockUpstream {
    listening_address: SocketAddr,
    messages_from_dowsntream: MessagesAggregator,
    // First item in tuple refer to the message(MsgType) received and second to what
    // response(AnyMessage) should the upstream send back.
    response_messages: Vec<(MsgType, AnyMessage<'static>)>,
}

impl MockUpstream {
    pub fn new(
        listening_address: SocketAddr,
        response_messages: Vec<(MsgType, AnyMessage<'static>)>,
    ) -> Self {
        Self {
            listening_address,
            messages_from_dowsntream: MessagesAggregator::new(),
            response_messages,
        }
    }

    pub async fn start(&self) {
        let listening_address = self.listening_address;
        let messages_from_dowsntream = self.messages_from_dowsntream.clone();
        let response_messages = self.response_messages.clone();
        tokio::spawn(async move {
            let (downstream_receiver, downstream_sender) =
                create_downstream(wait_for_client(listening_address).await)
                    .await
                    .expect("Failed to connect to downstream");
            while let Ok(mut frame) = downstream_receiver.recv().await {
                let (msg_type, msg) = message_from_frame(&mut frame);
                // save messages received from downstream
                messages_from_dowsntream.add_message(msg_type, msg);
                // find a response if the user provided one
                let response = response_messages
                    .iter()
                    .find(|(m_type, _)| m_type == &msg_type);
                // send response back to the downstream if found
                if let Some((_, response_msg)) = response {
                    let message = StandardEitherFrame::<AnyMessage<'_>>::Sv2(
                        Sv2Frame::from_message(response_msg.clone(), msg_type, 0, false)
                            .expect("Failed to create the frame"),
                    );
                    downstream_sender.send(message).await.unwrap();
                }
            }
        });
    }

    pub fn next_message_from_downstream(&self) -> Option<(MsgType, AnyMessage<'static>)> {
        self.messages_from_dowsntream.next_message()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::start_template_provider;
    use std::{convert::TryInto, net::TcpListener};
    use stratum_common::roles_logic_sv2::{
        codec_sv2::{StandardEitherFrame, Sv2Frame},
        common_messages_sv2::{Protocol, SetupConnection, SetupConnectionSuccess, *},
        parsers::CommonMessages,
    };

    #[tokio::test]
    async fn test_mock_downstream() {
        let (_tp, socket) = start_template_provider(None);
        let mock_downstream = MockDownstream::new(socket);
        let send_to_upstream = mock_downstream.start().await;
        let setup_connection =
            AnyMessage::Common(CommonMessages::SetupConnection(SetupConnection {
                protocol: Protocol::TemplateDistributionProtocol,
                min_version: 2,
                max_version: 2,
                flags: 0,
                endpoint_host: b"0.0.0.0".to_vec().try_into().unwrap(),
                endpoint_port: 8081,
                vendor: b"Bitmain".to_vec().try_into().unwrap(),
                hardware_version: b"901".to_vec().try_into().unwrap(),
                firmware: b"abcX".to_vec().try_into().unwrap(),
                device_id: b"89567".to_vec().try_into().unwrap(),
            }));
        let message = StandardEitherFrame::<AnyMessage<'_>>::Sv2(
            Sv2Frame::from_message(setup_connection, MESSAGE_TYPE_SETUP_CONNECTION, 0, false)
                .expect("Failed to create the frame"),
        );
        send_to_upstream.send(message).await.unwrap();
        mock_downstream
            .messages_from_upstream
            .has_message_type(MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS);
    }

    #[tokio::test]
    async fn test_mock_upstream() {
        let port = TcpListener::bind("127.0.0.1:0")
            .unwrap()
            .local_addr()
            .unwrap()
            .port();
        let upstream_socket_addr = SocketAddr::from(([127, 0, 0, 1], port));
        let mock_downstream = MockDownstream::new(upstream_socket_addr);
        let upon_receiving_setup_connection = MESSAGE_TYPE_SETUP_CONNECTION;
        let respond_with_success = AnyMessage::Common(CommonMessages::SetupConnectionSuccess(
            SetupConnectionSuccess {
                used_version: 2,
                flags: 0,
            },
        ));
        let mock_upstream = MockUpstream::new(
            upstream_socket_addr,
            vec![(upon_receiving_setup_connection, respond_with_success)],
        );
        mock_upstream.start().await;
        let send_to_upstream = mock_downstream.start().await;
        let setup_connection =
            AnyMessage::Common(CommonMessages::SetupConnection(SetupConnection {
                protocol: Protocol::TemplateDistributionProtocol,
                min_version: 2,
                max_version: 2,
                flags: 0,
                endpoint_host: b"0.0.0.0".to_vec().try_into().unwrap(),
                endpoint_port: 8081,
                vendor: b"Bitmain".to_vec().try_into().unwrap(),
                hardware_version: b"901".to_vec().try_into().unwrap(),
                firmware: b"abcX".to_vec().try_into().unwrap(),
                device_id: b"89567".to_vec().try_into().unwrap(),
            }));
        let message = StandardEitherFrame::<AnyMessage<'_>>::Sv2(
            Sv2Frame::from_message(setup_connection, MESSAGE_TYPE_SETUP_CONNECTION, 0, false)
                .expect("Failed to create the frame"),
        );
        send_to_upstream.send(message).await.unwrap();
        mock_downstream
            .messages_from_upstream
            .has_message_type(MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS);
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/mod.rs">
use crate::{sniffer::*, template_provider::*};
use config_helpers::CoinbaseOutput;
use corepc_node::{ConnectParams, CookieValues};
use interceptor::InterceptAction;
use jd_client::JobDeclaratorClient;
use jd_server::JobDeclaratorServer;
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use pool_sv2::PoolSv2;
use rand::{rng, Rng};
use std::{
    convert::{TryFrom, TryInto},
    net::SocketAddr,
    sync::Once,
};
use translator_sv2::TranslatorSv2;
use utils::get_available_address;

pub mod interceptor;
pub mod message_aggregator;
pub mod mock_roles;
pub mod sniffer;
pub mod sniffer_error;
#[cfg(feature = "sv1")]
pub mod sv1_sniffer;
pub mod template_provider;
pub mod types;
pub(crate) mod utils;

const SHARES_PER_MINUTE: f32 = 120.0;

static LOGGER: Once = Once::new();

/// Each test function should call `start_tracing()` to enable logging.
pub fn start_tracing() {
    LOGGER.call_once(|| {
        tracing_subscriber::fmt::init();
    });
}

pub fn start_sniffer(
    identifier: &str,
    upstream: SocketAddr,
    check_on_drop: bool,
    action: Vec<InterceptAction>,
) -> (Sniffer, SocketAddr) {
    let listening_address = get_available_address();
    let sniffer = Sniffer::new(
        identifier,
        listening_address,
        upstream,
        check_on_drop,
        action,
    );
    sniffer.start();
    (sniffer, listening_address)
}

pub async fn start_pool(template_provider_address: Option<SocketAddr>) -> (PoolSv2, SocketAddr) {
    use pool_sv2::config::PoolConfig;
    let listening_address = get_available_address();
    let authority_public_key = Secp256k1PublicKey::try_from(
        "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72".to_string(),
    )
    .expect("failed");
    let authority_secret_key = Secp256k1SecretKey::try_from(
        "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n".to_string(),
    )
    .expect("failed");
    let cert_validity_sec = 3600;
    let coinbase_outputs = vec![CoinbaseOutput::from_descriptor(
        "wpkh(036adc3bdf21e6f9a0f0fb0066bf517e5b7909ed1563d6958a10993849a7554075)",
    )
    .unwrap()];
    let pool_signature = "Stratum V2 SRI Pool".to_string();
    let tp_address = if let Some(tp_add) = template_provider_address {
        tp_add.to_string()
    } else {
        "127.0.0.1:8442".to_string()
    };
    let connection_config = pool_sv2::config::ConnectionConfig::new(
        listening_address.to_string(),
        cert_validity_sec,
        pool_signature,
    );
    let template_provider_config = pool_sv2::config::TemplateProviderConfig::new(tp_address, None);
    let authority_config =
        pool_sv2::config::AuthorityConfig::new(authority_public_key, authority_secret_key);
    let share_batch_size = 1;
    let config = PoolConfig::new(
        connection_config,
        template_provider_config,
        authority_config,
        coinbase_outputs,
        SHARES_PER_MINUTE,
        share_batch_size,
    );
    let pool = PoolSv2::new(config);
    assert!(pool.start().await.is_ok());
    (pool, listening_address)
}

pub fn start_template_provider(sv2_interval: Option<u32>) -> (TemplateProvider, SocketAddr) {
    let address = get_available_address();
    let sv2_interval = sv2_interval.unwrap_or(20);
    let template_provider = TemplateProvider::start(address.port(), sv2_interval);
    template_provider.generate_blocks(1);
    (template_provider, address)
}

pub fn start_jdc(
    pool: &[(SocketAddr, SocketAddr)], // (pool_address, jds_address)
    tp_address: SocketAddr,
) -> (JobDeclaratorClient, SocketAddr) {
    use jd_client::config::{
        JobDeclaratorClientConfig, PoolConfig, ProtocolConfig, TPConfig, Upstream,
    };
    let jdc_address = get_available_address();
    let max_supported_version = 2;
    let min_supported_version = 2;
    let withhold = false;
    let authority_public_key = Secp256k1PublicKey::try_from(
        "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72".to_string(),
    )
    .unwrap();
    let authority_secret_key = Secp256k1SecretKey::try_from(
        "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n".to_string(),
    )
    .unwrap();
    let coinbase_outputs = vec![CoinbaseOutput::from_descriptor(
        "wpkh(036adc3bdf21e6f9a0f0fb0066bf517e5b7909ed1563d6958a10993849a7554075)",
    )
    .unwrap()];
    let authority_pubkey = Secp256k1PublicKey::try_from(
        "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72".to_string(),
    )
    .unwrap();
    let upstreams = pool
        .iter()
        .map(|(pool_addr, jds_addr)| {
            Upstream::new(
                authority_pubkey,
                pool_addr.to_string(),
                jds_addr.to_string(),
            )
        })
        .collect();
    let pool_config = PoolConfig::new(authority_public_key, authority_secret_key);
    let tp_config = TPConfig::new(1000, tp_address.to_string(), None);
    let protocol_config = ProtocolConfig::new(
        max_supported_version,
        min_supported_version,
        coinbase_outputs,
    );
    let jdc_signature = "JDC".to_string();
    let jd_client_proxy = JobDeclaratorClientConfig::new(
        jdc_address,
        protocol_config,
        withhold,
        pool_config,
        tp_config,
        upstreams,
        std::time::Duration::from_secs(1),
        jdc_signature,
    );
    let ret = jd_client::JobDeclaratorClient::new(jd_client_proxy);
    let ret_clone = ret.clone();
    tokio::spawn(async move { ret_clone.start().await });
    (ret, jdc_address)
}

pub fn start_jds(tp_rpc_connection: &ConnectParams) -> (JobDeclaratorServer, SocketAddr) {
    use jd_server::config::{CoreRpc, JobDeclaratorServerConfig};
    let authority_public_key = Secp256k1PublicKey::try_from(
        "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72".to_string(),
    )
    .unwrap();
    let authority_secret_key = Secp256k1SecretKey::try_from(
        "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n".to_string(),
    )
    .unwrap();
    let listen_jd_address = get_available_address();
    let cert_validity_sec = 3600;
    let coinbase_outputs = vec![CoinbaseOutput::from_descriptor(
        "wpkh(036adc3bdf21e6f9a0f0fb0066bf517e5b7909ed1563d6958a10993849a7554075)",
    )
    .unwrap()];
    if let Ok(Some(CookieValues { user, password })) = tp_rpc_connection.get_cookie_values() {
        let ip = tp_rpc_connection.rpc_socket.ip().to_string();
        let url = jd_server::Uri::builder()
            .scheme("http")
            .authority(ip)
            .path_and_query("")
            .build()
            .unwrap();
        let core_rpc = CoreRpc::new(
            url.to_string(),
            tp_rpc_connection.rpc_socket.port(),
            user,
            password,
        );
        let config = JobDeclaratorServerConfig::new(
            listen_jd_address.to_string(),
            authority_public_key,
            authority_secret_key,
            cert_validity_sec,
            coinbase_outputs,
            core_rpc,
            std::time::Duration::from_secs(1),
        );
        let job_declarator_server = JobDeclaratorServer::new(config);
        let job_declarator_server_clone = job_declarator_server.clone();
        tokio::spawn(async move {
            job_declarator_server_clone.start().await.unwrap();
        });
        (job_declarator_server, listen_jd_address)
    } else {
        panic!("Failed to get TP cookie values");
    }
}

pub fn start_sv2_translator(upstream: SocketAddr) -> (TranslatorSv2, SocketAddr) {
    let upstream_address = upstream.ip().to_string();
    let upstream_port = upstream.port();
    let upstream_authority_pubkey = Secp256k1PublicKey::try_from(
        "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72".to_string(),
    )
    .expect("failed");
    let listening_address = get_available_address();
    let listening_port = listening_address.port();
    let min_individual_miner_hashrate = measure_hashrate(1) as f32;
    let channel_diff_update_interval = 60;
    let channel_nominal_hashrate = min_individual_miner_hashrate;
    let downstream_difficulty_config = translator_sv2::config::DownstreamDifficultyConfig::new(
        min_individual_miner_hashrate,
        SHARES_PER_MINUTE,
        0,
        0,
    );
    let upstream_difficulty_config = translator_sv2::config::UpstreamDifficultyConfig::new(
        channel_diff_update_interval,
        channel_nominal_hashrate,
        0,
        false,
    );
    let upstream_conf = translator_sv2::config::UpstreamConfig::new(
        upstream_address,
        upstream_port,
        upstream_authority_pubkey,
        upstream_difficulty_config,
    );
    let downstream_conf = translator_sv2::config::DownstreamConfig::new(
        listening_address.ip().to_string(),
        listening_port,
        downstream_difficulty_config,
    );

    let min_extranonce2_size = 4;

    let config = translator_sv2::config::TranslatorConfig::new(
        upstream_conf,
        downstream_conf,
        2,
        2,
        min_extranonce2_size,
    );
    let translator_v2 = translator_sv2::TranslatorSv2::new(config);
    let clone_translator_v2 = translator_v2.clone();
    tokio::spawn(async move {
        clone_translator_v2.start().await;
    });
    (translator_v2, listening_address)
}

pub fn measure_hashrate(duration_secs: u64) -> f64 {
    use stratum_common::roles_logic_sv2::bitcoin::hashes::{sha256d, Hash, HashEngine};

    let mut share = {
        let mut rng = rng();
        let mut arr = [0u8; 80];
        rng.fill(&mut arr[..]);
        arr
    };
    let start_time = std::time::Instant::now();
    let mut hashes: u64 = 0;
    let duration = std::time::Duration::from_secs(duration_secs);

    let hash = |share: &mut [u8; 80]| {
        let nonce: [u8; 8] = share[0..8].try_into().unwrap();
        let mut nonce = u64::from_le_bytes(nonce);
        nonce += 1;
        share[0..8].copy_from_slice(&nonce.to_le_bytes());
        let mut engine = sha256d::Hash::engine();
        engine.input(share);
        sha256d::Hash::from_engine(engine);
    };

    loop {
        if start_time.elapsed() >= duration {
            break;
        }
        hash(&mut share);
        hashes += 1;
    }

    let elapsed_secs = start_time.elapsed().as_secs_f64();

    hashes as f64 / elapsed_secs
}

pub fn start_mining_device_sv1(
    upstream_addr: SocketAddr,
    single_submit: bool,
    custom_target: Option<[u8; 32]>,
) {
    tokio::spawn(async move {
        mining_device_sv1::client::Client::connect(80, upstream_addr, single_submit, custom_target)
            .await;
    });
}

pub fn start_mining_device_sv2(
    upstream: SocketAddr,
    pub_key: Option<Secp256k1PublicKey>,
    device_id: Option<String>,
    user_id: Option<String>,
    handicap: u32,
    nominal_hashrate_multiplier: Option<f32>,
    single_submit: bool,
) {
    tokio::spawn(async move {
        mining_device::connect(
            upstream.to_string(),
            pub_key,
            device_id,
            user_id,
            handicap,
            nominal_hashrate_multiplier,
            single_submit,
        )
        .await;
    });
}

#[cfg(feature = "sv1")]
pub fn start_sv1_sniffer(upstream_address: SocketAddr) -> (sv1_sniffer::SnifferSV1, SocketAddr) {
    let listening_address = get_available_address();
    let sniffer_sv1 = sv1_sniffer::SnifferSV1::new(listening_address, upstream_address);
    sniffer_sv1.start();
    (sniffer_sv1, listening_address)
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/sniffer_error.rs">
#[derive(Debug, PartialEq)]
pub enum SnifferError {
    DownstreamClosed,
    UpstreamClosed,
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/sniffer.rs">
use crate::{
    interceptor::{InterceptAction, MessageDirection},
    message_aggregator::MessagesAggregator,
    types::MsgType,
    utils::{
        create_downstream, create_upstream, recv_from_down_send_to_up, recv_from_up_send_to_down,
        wait_for_client,
    },
};
use std::net::SocketAddr;
use stratum_common::roles_logic_sv2::parsers::AnyMessage;
use tokio::{net::TcpStream, select};

/// Allows to intercept messages sent between two roles.
///
/// Can be useful for testing purposes, as it allows to assert that the roles have sent specific
/// messages in a specific order and to inspect the messages details.
///
/// The downstream (or client) role connects to the [`Sniffer`] `listening_address` and the
/// [`Sniffer`] connects to the `upstream` server. This way, the Sniffer can intercept messages sent
/// between the downstream and upstream roles.
///
/// Messages received from downstream are stored in the `messages_from_downstream` aggregator and
/// forwarded to the upstream role. Alternatively, messages received from upstream are stored in
/// the `messages_from_upstream` and forwarded to the downstream role. Both
/// `messages_from_downstream` and `messages_from_upstream` aggregators can be accessed as FIFO
/// queues via [`Sniffer::next_message_from_downstream`] and
/// [`Sniffer::next_message_from_upstream`], respectively.
///
/// In order to replace or ignore the messages sent between the roles, [`InterceptAction`] can be
/// used in [`Sniffer::new`].
#[derive(Debug, Clone)]
pub struct Sniffer<'a> {
    identifier: &'a str,
    listening_address: SocketAddr,
    upstream_address: SocketAddr,
    messages_from_downstream: MessagesAggregator,
    messages_from_upstream: MessagesAggregator,
    check_on_drop: bool,
    action: Vec<InterceptAction>,
}

impl<'a> Sniffer<'a> {
    /// Creates a new sniffer that listens on the given listening address and connects to the given
    /// upstream address.
    pub fn new(
        identifier: &'a str,
        listening_address: SocketAddr,
        upstream_address: SocketAddr,
        check_on_drop: bool,
        action: Vec<InterceptAction>,
    ) -> Self {
        Self {
            identifier,
            listening_address,
            upstream_address,
            messages_from_downstream: MessagesAggregator::new(),
            messages_from_upstream: MessagesAggregator::new(),
            check_on_drop,
            action,
        }
    }

    /// Starts the sniffer.
    ///
    /// The sniffer should be started after the upstream role have been initialized and is ready to
    /// accept messages and before the downstream role starts sending messages.
    pub fn start(&self) {
        let listening_address = self.listening_address;
        let upstream_address = self.upstream_address;
        let messages_from_downstream = self.messages_from_downstream.clone();
        let messages_from_upstream = self.messages_from_upstream.clone();
        let action = self.action.clone();
        let identifier = self.identifier.to_string();
        tokio::spawn(async move {
            let (downstream_receiver, downstream_sender) =
                create_downstream(wait_for_client(listening_address).await)
                    .await
                    .expect("Failed to create downstream");
            let (upstream_receiver, upstream_sender) = create_upstream(
                TcpStream::connect(upstream_address)
                    .await
                    .expect("Failed to connect to upstream"),
            )
            .await
            .expect("Failed to create upstream");
            select! {
                _ = tokio::signal::ctrl_c() => { },
                _ = recv_from_down_send_to_up(downstream_receiver, upstream_sender, messages_from_downstream, action.clone(), &identifier) => { },
                _ = recv_from_up_send_to_down(upstream_receiver, downstream_sender, messages_from_upstream, action, &identifier) => { },
            };
        });
    }

    /// Returns the oldest message sent by downstream.
    ///
    /// The queue is FIFO and once a message is returned it is removed from the queue.
    ///
    /// This can be used to assert that the downstream sent:
    /// - specific message types
    /// - specific message fields
    pub fn next_message_from_downstream(&self) -> Option<(MsgType, AnyMessage<'static>)> {
        self.messages_from_downstream.next_message()
    }

    /// Returns the oldest message sent by upstream.
    ///
    /// The queue is FIFO and once a message is returned it is removed from the queue.
    ///
    /// This can be used to assert that the upstream sent:
    /// - specific message types
    /// - specific message fields
    pub fn next_message_from_upstream(&self) -> Option<(MsgType, AnyMessage<'static>)> {
        self.messages_from_upstream.next_message()
    }

    /// Waits until a message of the specified type is received into the `message_direction`
    /// corresponding queue.
    pub async fn wait_for_message_type(
        &self,
        message_direction: MessageDirection,
        message_type: u8,
    ) {
        let now = std::time::Instant::now();
        loop {
            let has_message_type = match message_direction {
                MessageDirection::ToDownstream => {
                    self.messages_from_upstream.has_message_type(message_type)
                }
                MessageDirection::ToUpstream => {
                    self.messages_from_downstream.has_message_type(message_type)
                }
            };

            // ready to unblock test runtime
            if has_message_type {
                return;
            }

            // 1 min timeout
            // only for worst case, ideally should never be triggered
            if now.elapsed().as_secs() > 60 {
                panic!("Timeout waiting for message type");
            }

            // sleep to reduce async lock contention
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
        }
    }

    /// Assert message is not present in the queue
    ///
    /// Will return true if the message is not present in the queue, false otherwise.
    pub async fn assert_message_not_present(
        &self,
        message_direction: MessageDirection,
        message_type: u8,
    ) -> bool {
        let has_message_type = match message_direction {
            MessageDirection::ToDownstream => {
                self.messages_from_upstream.has_message_type(message_type)
            }
            MessageDirection::ToUpstream => {
                self.messages_from_downstream.has_message_type(message_type)
            }
        };
        !has_message_type
    }

    /// Similar to `[Sniffer::wait_for_message_type]` but also removes the messages from the queue
    /// including the specified message type.
    pub async fn wait_for_message_type_and_clean_queue(
        &self,
        message_direction: MessageDirection,
        message_type: u8,
    ) -> bool {
        let now = std::time::Instant::now();
        loop {
            let has_message_type = match message_direction {
                MessageDirection::ToDownstream => self
                    .messages_from_upstream
                    .has_message_type_with_remove(message_type),
                MessageDirection::ToUpstream => self
                    .messages_from_downstream
                    .has_message_type_with_remove(message_type),
            };

            // ready to unblock test runtime
            if has_message_type {
                return true;
            }

            // 1 min timeout
            // only for worst case, ideally should never be triggered
            if now.elapsed().as_secs() > 60 {
                panic!("Timeout waiting for message type");
            }

            // sleep to reduce async lock contention
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
        }
    }

    /// Checks whether the sniffer has received a message of the specified type.
    pub fn includes_message_type(
        &self,
        message_direction: MessageDirection,
        message_type: u8,
    ) -> bool {
        match message_direction {
            MessageDirection::ToDownstream => {
                self.messages_from_upstream.has_message_type(message_type)
            }
            MessageDirection::ToUpstream => {
                self.messages_from_downstream.has_message_type(message_type)
            }
        }
    }
}

// Utility macro to assert that the downstream and upstream roles have sent specific messages.
//
// This macro can be called in two ways:
// 1. If you want to assert the message without any of its properties, you can invoke the macro
//   with the message group, the nested message group, the message, and the expected message:
//   `assert_message!(TemplateDistribution, TemplateDistribution, $msg,
// $expected_message_variant);`.
//
// 2. If you want to assert the message with its properties, you can invoke the macro with the
//  message group, the nested message group, the message, the expected message, and the expected
//  properties and values:
//  `assert_message!(TemplateDistribution, TemplateDistribution, $msg, $expected_message_variant,
//  $expected_property, $expected_property_value, ...);`.
//  Note that you can provide any number of properties and values.
//
//  In both cases, the `$message_group` could be any variant of `AnyMessage::$message_group` and
//  the `$nested_message_group` could be any variant of
//  `AnyMessage::$message_group($nested_message_group)`.
//
//  If you dont want to provide the `$message_group` and `$nested_message_group` arguments, you can
//  utilize `assert_common_message!`, `assert_tp_message!`, `assert_mining_message!`, and
//  `assert_jd_message!` macros. All those macros are just wrappers around `assert_message!` macro
//  with predefined `$message_group` and `$nested_message_group` arguments. They also can be called
//  in two ways, with or without properties validation.
#[macro_export]
macro_rules! assert_message {
  ($message_group:ident, $nested_message_group:ident, $msg:expr, $expected_message_variant:ident,
   $($expected_property:ident, $expected_property_value:expr),*) => { match $msg {
	  Some((_, message)) => {
		match message {
		  AnyMessage::$message_group($nested_message_group::$expected_message_variant(
			  $expected_message_variant {
				$($expected_property,)*
				  ..
			  },
		  )) => {
			$(
			  assert_eq!($expected_property.clone(), $expected_property_value);
			)*
		  }
		  _ => {
			panic!(
			  "Sent wrong message: {:?}",
			  message
			);
		  }
		}
	  }
	  _ => panic!("No message received"),
		}
  };
  ($message_group:ident, $nested_message_group:ident, $msg:expr, $expected_message_variant:ident) => {
	match $msg {
	  Some((_, message)) => {
		match message {
		  AnyMessage::$message_group($nested_message_group::$expected_message_variant(_)) => {}
		  _ => {
			panic!(
			  "Sent wrong message: {:?}",
			  message
			);
		  }
		}
	  }
	  _ => panic!("No message received"),
		}
  };
}

// Assert that the message is a common message and that it has the expected properties and values.
#[macro_export]
macro_rules! assert_common_message {
  ($msg:expr, $expected_message_variant:ident, $($expected_property:ident, $expected_property_value:expr),*) => {
	assert_message!(Common, CommonMessages, $msg, $expected_message_variant, $($expected_property, $expected_property_value),*);
  };
  ($msg:expr, $expected_message_variant:ident) => {
	assert_message!(Common, CommonMessages, $msg, $expected_message_variant);
  };
}

// Assert that the message is a template distribution message and that it has the expected
// properties and values.
#[macro_export]
macro_rules! assert_tp_message {
  ($msg:expr, $expected_message_variant:ident, $($expected_property:ident, $expected_property_value:expr),*) => {
	assert_message!(TemplateDistribution, TemplateDistribution, $msg, $expected_message_variant, $($expected_property, $expected_property_value),*);
  };
  ($msg:expr, $expected_message_variant:ident) => {
	assert_message!(TemplateDistribution, TemplateDistribution, $msg, $expected_message_variant);
  };
}

// Assert that the message is a mining message and that it has the expected properties and values.
#[macro_export]
macro_rules! assert_mining_message {
  ($msg:expr, $expected_message_variant:ident, $($expected_property:ident, $expected_property_value:expr),*) => {
	assert_message!(Mining, Mining, $msg, $expected_message_variant, $($expected_property, $expected_property_value),*);
  };
  ($msg:expr, $expected_message_variant:ident) => {
	assert_message!(Mining, Mining, $msg, $expected_message_variant);
  };
}

// Assert that the message is a job declaration message and that it has the expected properties and
// values.
#[macro_export]
macro_rules! assert_jd_message {
  ($msg:expr, $expected_message_variant:ident, $($expected_property:ident, $expected_property_value:expr),*) => {
	assert_message!(JobDeclaration, JobDeclaration, $msg, $expected_message_variant, $($expected_property, $expected_property_value),*);
  };
  ($msg:expr, $expected_message_variant:ident) => {
	assert_message!(JobDeclaration, JobDeclaration, $msg, $expected_message_variant);
  };
}

// This implementation is used in order to check if a test has handled all messages sent by the
// downstream and upstream roles. If not, the test will panic.
//
// This is useful to ensure that the test has checked all exchanged messages between the roles.
impl Drop for Sniffer<'_> {
    fn drop(&mut self) {
        if self.check_on_drop {
            match (
                self.messages_from_downstream.is_empty(),
                self.messages_from_upstream.is_empty(),
            ) {
                (true, true) => {}
                (true, false) => {
                    println!(
                        "Sniffer {}: You didn't handle all upstream messages: {:?}",
                        self.identifier, self.messages_from_upstream
                    );
                    panic!();
                }
                (false, true) => {
                    println!(
                        "Sniffer {}: You didn't handle all downstream messages: {:?}",
                        self.identifier, self.messages_from_downstream
                    );
                    panic!();
                }
                (false, false) => {
                    println!(
                        "Sniffer {}: You didn't handle all downstream messages: {:?}",
                        self.identifier, self.messages_from_downstream
                    );
                    println!(
                        "Sniffer {}: You didn't handle all upstream messages: {:?}",
                        self.identifier, self.messages_from_upstream
                    );
                    panic!();
                }
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/sv1_sniffer.rs">
#![cfg(feature = "sv1")]
use crate::interceptor::MessageDirection;
use async_channel::{Receiver, Sender};
use std::{collections::VecDeque, net::SocketAddr, sync::Arc};
use stratum_common::network_helpers_sv2::sv1_connection::ConnectionSV1;
use tokio::{
    net::{TcpListener, TcpStream},
    select,
    sync::Mutex,
};

#[derive(Debug, PartialEq)]
enum SnifferError {
    DownstreamClosed,
    UpstreamClosed,
}

/// Represents an SV1 sniffer.
///
/// This struct acts as a middleman between two SV1 roles. It forwards messages from one role to
/// the other and vice versa. It also provides methods to wait for specific messages to be received
/// from the downstream or upstream role.
#[derive(Debug, Clone)]
pub struct SnifferSV1 {
    listening_address: SocketAddr,
    upstream_address: SocketAddr,
    messages_from_downstream: MessagesAggregatorSV1,
    messages_from_upstream: MessagesAggregatorSV1,
}

impl SnifferSV1 {
    /// Create a new [`SnifferSV1`] instance.
    ///
    /// The listening address is the address the sniffer will listen on for incoming connections
    /// from the downstream role. The upstream address is the address the sniffer will connect to
    /// in order to forward messages to the upstream role.
    pub fn new(listening_address: SocketAddr, upstream_address: SocketAddr) -> Self {
        Self {
            listening_address,
            upstream_address,
            messages_from_downstream: MessagesAggregatorSV1::new(),
            messages_from_upstream: MessagesAggregatorSV1::new(),
        }
    }

    /// Start the sniffer.
    pub fn start(&self) {
        let upstream_address = self.upstream_address.clone();
        let listening_address = self.listening_address.clone();
        let messages_from_downstream = self.messages_from_downstream.clone();
        let messages_from_upstream = self.messages_from_upstream.clone();
        tokio::spawn(async move {
            let listener = TcpListener::bind(listening_address)
                .await
                .expect("Failed to listen on given address");
            let sniffer_to_upstream_stream = loop {
                match TcpStream::connect(upstream_address).await {
                    Ok(s) => break s,
                    Err(_) => {
                        continue;
                    }
                }
            };
            let (downstream_stream, _) = listener
                .accept()
                .await
                .expect("Failed to accept downstream connection");
            let sniffer_to_upstream_connection =
                ConnectionSV1::new(sniffer_to_upstream_stream).await;
            let downstream_to_sniffer_connection = ConnectionSV1::new(downstream_stream).await;
            select! {
                _ = tokio::signal::ctrl_c() => { },
                _ = Self::recv_from_down_send_to_up_sv1(
                    downstream_to_sniffer_connection.receiver(),
                    sniffer_to_upstream_connection.sender(),
                    messages_from_downstream
                ) => { },
                _ = Self::recv_from_up_send_to_down_sv1(
                    sniffer_to_upstream_connection.receiver(),
                    downstream_to_sniffer_connection.sender(),
                    messages_from_upstream
                ) => { },
            };
        });
    }

    /// Wait for a specific message to be received from the downstream role.
    pub async fn wait_for_message(&self, message: &[&str], direction: MessageDirection) {
        if message.len() == 0 {
            panic!("Message cannot be empty");
        }
        let now = std::time::Instant::now();
        tokio::select!(
            _ = tokio::signal::ctrl_c() => { },
            _ = async {
                loop {
                    match direction {
                        MessageDirection::ToUpstream => {
                            if self.messages_from_downstream.has_message(message).await {
                                break;
                            }
                        }
                        MessageDirection::ToDownstream => {
                            if self.messages_from_upstream.has_message(message).await {
                                break;
                            }
                        }
                    }
                    if now.elapsed().as_secs() > 60 {
                        panic!( "Timeout: SV1 message {} not found", message.get(0).unwrap());
                    } else {
                        tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                        continue;
                    }
                }
            } => {}
        );
    }

    async fn recv_from_up_send_to_down_sv1(
        recv: Receiver<sv1_api::Message>,
        send: Sender<sv1_api::Message>,
        upstream_messages: MessagesAggregatorSV1,
    ) -> Result<(), SnifferError> {
        while let Ok(msg) = recv.recv().await {
            send.send(msg.clone())
                .await
                .map_err(|_| SnifferError::DownstreamClosed)?;
            upstream_messages.add_message(msg.clone()).await;
            tracing::info!(" Sv1 Sniffer | Forwarded: {} | Direction: ", msg);
        }
        Err(SnifferError::UpstreamClosed)
    }

    async fn recv_from_down_send_to_up_sv1(
        recv: Receiver<sv1_api::Message>,
        send: Sender<sv1_api::Message>,
        downstream_messages: MessagesAggregatorSV1,
    ) -> Result<(), SnifferError> {
        while let Ok(msg) = recv.recv().await {
            send.send(msg.clone())
                .await
                .map_err(|_| SnifferError::UpstreamClosed)?;
            downstream_messages.add_message(msg.clone()).await;
            tracing::info!(" Sv1 Sniffer | Forwarded: {} | Direction: ", msg);
        }
        Err(SnifferError::DownstreamClosed)
    }
}

/// Represents a SV1 message manager.
///
/// This struct can be used in order to aggregate and manage SV1 messages.
#[derive(Debug, Clone)]
pub(crate) struct MessagesAggregatorSV1 {
    messages: Arc<Mutex<VecDeque<sv1_api::Message>>>,
}

impl MessagesAggregatorSV1 {
    fn new() -> Self {
        Self {
            messages: Arc::new(Mutex::new(VecDeque::new())),
        }
    }

    async fn add_message(&self, message: sv1_api::Message) {
        let mut messages = self.messages.lock().await;
        messages.push_back(message);
    }

    async fn has_message(&self, expected_msg: &[&str]) -> bool {
        let messages = self.messages.lock().await;
        let ret = messages.iter().any(|msg| match msg {
            sv1_api::Message::StandardRequest(req) => req.method == *expected_msg.get(0).unwrap(),
            sv1_api::Message::Notification(notif) => notif.method == *expected_msg.get(0).unwrap(),
            sv1_api::Message::OkResponse(res) => {
                if let Ok(res) = corepc_node::serde_json::to_string(&res) {
                    for m in expected_msg {
                        if !res.contains(m) {
                            return false;
                        }
                    }
                    return true;
                } else {
                    false
                }
            }
            sv1_api::Message::ErrorResponse(res) => {
                res.error.clone().unwrap().message == *expected_msg.get(0).unwrap()
            }
        });
        ret
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/template_provider.rs">
use corepc_node::{Conf, ConnectParams, Node};
use std::{env, fs::create_dir_all, path::PathBuf};
use stratum_common::roles_logic_sv2::bitcoin::{Address, Amount, Txid};

use crate::utils::{http, tarball};

const VERSION_TP: &str = "0.1.15";

fn get_bitcoind_filename(os: &str, arch: &str) -> String {
    match (os, arch) {
        ("macos", "aarch64") => {
            format!("bitcoin-sv2-tp-{VERSION_TP}-arm64-apple-darwin-unsigned.tar.gz")
        }
        ("macos", "x86_64") => {
            format!("bitcoin-sv2-tp-{VERSION_TP}-x86_64-apple-darwin-unsigned.tar.gz")
        }
        ("linux", "x86_64") => format!("bitcoin-sv2-tp-{VERSION_TP}-x86_64-linux-gnu.tar.gz"),
        ("linux", "aarch64") => format!("bitcoin-sv2-tp-{VERSION_TP}-aarch64-linux-gnu.tar.gz"),
        _ => format!("bitcoin-sv2-tp-{VERSION_TP}-x86_64-apple-darwin-unsigned.zip"),
    }
}

/// Represents a template provider node.
///
/// The template provider is a bitcoin node that implements the Stratum V2 protocol.
#[derive(Debug)]
pub struct TemplateProvider {
    bitcoind: Node,
}

impl TemplateProvider {
    /// Start a new [`TemplateProvider`] instance.
    pub fn start(port: u16, sv2_interval: u32) -> Self {
        let current_dir: PathBuf = std::env::current_dir().expect("failed to read current dir");
        let tp_dir = current_dir.join("template-provider");
        let mut conf = Conf::default();
        conf.wallet = Some(port.to_string());
        let staticdir = format!(".bitcoin-{port}");
        conf.staticdir = Some(tp_dir.join(staticdir));
        let port_arg = format!("-sv2port={port}");
        let sv2_interval_arg = format!("-sv2interval={sv2_interval}");
        conf.args.extend(vec![
            "-txindex=1",
            "-sv2",
            &port_arg,
            "-debug=rpc",
            "-debug=sv2",
            &sv2_interval_arg,
            "-sv2feedelta=0",
            "-loglevel=sv2:trace",
            "-logtimemicros=1",
        ]);
        let os = env::consts::OS;
        let arch = env::consts::ARCH;
        let download_filename = get_bitcoind_filename(os, arch);
        let bitcoin_exe_home = tp_dir
            .join(format!("bitcoin-sv2-tp-{VERSION_TP}"))
            .join("bin");

        if !bitcoin_exe_home.exists() {
            let tarball_bytes = match env::var("BITCOIND_TARBALL_FILE") {
                Ok(path) => tarball::read_from_file(&path),
                Err(_) => {
                    let download_endpoint =
                        env::var("BITCOIND_DOWNLOAD_ENDPOINT").unwrap_or_else(|_| {
                            "https://github.com/Sjors/bitcoin/releases/download".to_owned()
                        });
                    let url =
                        format!("{download_endpoint}/sv2-tp-{VERSION_TP}/{download_filename}");
                    http::make_get_request(&url, 5)
                }
            };

            if let Some(parent) = bitcoin_exe_home.parent() {
                create_dir_all(parent).unwrap();
            }

            tarball::unpack(&tarball_bytes, &tp_dir);

            if os == "macos" {
                let bitcoind_binary = bitcoin_exe_home.join("bitcoind");
                std::process::Command::new("codesign")
                    .arg("--sign")
                    .arg("-")
                    .arg(&bitcoind_binary)
                    .output()
                    .expect("Failed to sign bitcoind binary");
            }
        }

        env::set_var("BITCOIND_EXE", bitcoin_exe_home.join("bitcoind"));
        let exe_path = corepc_node::exe_path().expect("Failed to get bitcoind path");

        // this timeout is used to avoid potential racing conditions
        // on the bitcoind executable while executing Integration Tests in parallel
        // for more context, see https://github.com/stratum-mining/stratum/issues/1278#issuecomment-2692316174
        let timeout = std::time::Duration::from_secs(10);
        let current_time = std::time::Instant::now();
        loop {
            match Node::with_conf(&exe_path, &conf) {
                Ok(bitcoind) => {
                    break TemplateProvider { bitcoind };
                }
                Err(e) => {
                    if current_time.elapsed() > timeout {
                        panic!("Failed to start bitcoind: {}", e);
                    }
                    println!("Failed to start bitcoind due to {e}");
                }
            }
        }
    }

    /// Mine `n` blocks.
    pub fn generate_blocks(&self, n: u64) {
        let mining_address = self
            .bitcoind
            .client
            .new_address()
            .expect("Failed to get mining address");
        self.bitcoind
            .client
            .generate_to_address(n as usize, &mining_address)
            .expect("Failed to generate blocks");
    }

    /// Retrun the node's RPC info.
    pub fn rpc_info(&self) -> &ConnectParams {
        &self.bitcoind.params
    }

    /// Create and broadcast a transaction to the mempool.
    ///
    /// It is recommended to use [`TemplateProvider::fund_wallet`] before calling this method to
    /// ensure the wallet has enough funds.
    pub fn create_mempool_transaction(&self) -> Result<(Address, Txid), corepc_node::Error> {
        let client = &self.bitcoind.client;
        const MILLION_SATS: Amount = Amount::from_sat(1_000_000);
        let address = client.new_address()?;
        let txid = client
            .send_to_address(&address, MILLION_SATS)?
            .txid()
            .expect("Unexpected behavior: txid is None");
        Ok((address, txid))
    }

    /// Fund the node's wallet.
    ///
    /// This can be useful before using [`TemplateProvider::create_mempool_transaction`].
    pub fn fund_wallet(&self) -> Result<(), corepc_node::Error> {
        let client = &self.bitcoind.client;
        let address = client.new_address()?;
        client.generate_to_address(101, &address)?;
        Ok(())
    }

    /// Return the hash of the most recent block.
    pub fn get_best_block_hash(&self) -> Result<String, corepc_node::Error> {
        let client = &self.bitcoind.client;
        let block_hash = client.get_best_block_hash()?.0;
        Ok(block_hash)
    }
}

#[cfg(test)]
mod tests {
    use super::TemplateProvider;
    use crate::utils::get_available_address;

    #[tokio::test]
    async fn test_create_mempool_transaction() {
        let address = get_available_address();
        let port = address.port();
        let tp = TemplateProvider::start(port, 1);
        assert!(tp.fund_wallet().is_ok());
        assert!(tp.create_mempool_transaction().is_ok());
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/types.rs">
use stratum_common::roles_logic_sv2::{codec_sv2::StandardEitherFrame, parsers::AnyMessage};

pub type MessageFrame = StandardEitherFrame<AnyMessage<'static>>;
pub type MsgType = u8;
</file>

<file path="stratum-1.4.0/test/integration-tests/lib/utils.rs">
use crate::{
    interceptor::{InterceptAction, MessageDirection},
    message_aggregator::MessagesAggregator,
    sniffer_error::SnifferError,
    types::{MessageFrame, MsgType},
};
use async_channel::{Receiver, Sender};
use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use once_cell::sync::Lazy;
use std::{
    collections::HashSet,
    convert::TryInto,
    net::{SocketAddr, TcpListener},
    sync::Mutex,
};
use stratum_common::{
    network_helpers_sv2::noise_connection::Connection,
    roles_logic_sv2::{
        codec_sv2::{
            framing_sv2::framing::Frame, HandshakeRole, Initiator, Responder, StandardEitherFrame,
            Sv2Frame,
        },
        parsers::{
            message_type_to_name, AnyMessage, CommonMessages, IsSv2Message,
            JobDeclaration::{
                AllocateMiningJobToken, AllocateMiningJobTokenSuccess, DeclareMiningJob,
                DeclareMiningJobError, DeclareMiningJobSuccess, ProvideMissingTransactions,
                ProvideMissingTransactionsSuccess, PushSolution,
            },
            TemplateDistribution,
            TemplateDistribution::CoinbaseOutputConstraints,
        },
    },
};

// prevents get_available_port from ever returning the same port twice
static UNIQUE_PORTS: Lazy<Mutex<HashSet<u16>>> = Lazy::new(|| Mutex::new(HashSet::new()));

pub fn get_available_address() -> SocketAddr {
    let port = get_available_port();
    SocketAddr::from(([127, 0, 0, 1], port))
}

fn get_available_port() -> u16 {
    let mut unique_ports = UNIQUE_PORTS.lock().unwrap();

    loop {
        let port = TcpListener::bind("127.0.0.1:0")
            .unwrap()
            .local_addr()
            .unwrap()
            .port();
        if !unique_ports.contains(&port) {
            unique_ports.insert(port);
            return port;
        }
    }
}
pub async fn wait_for_client(listen_socket: SocketAddr) -> tokio::net::TcpStream {
    let listener = tokio::net::TcpListener::bind(listen_socket)
        .await
        .expect("Impossible to listen on given address");
    if let Ok((stream, _)) = listener.accept().await {
        stream
    } else {
        panic!("Impossible to accept dowsntream connection")
    }
}

pub async fn create_downstream(
    stream: tokio::net::TcpStream,
) -> Option<(Receiver<MessageFrame>, Sender<MessageFrame>)> {
    let pub_key = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72"
        .to_string()
        .parse::<Secp256k1PublicKey>()
        .unwrap()
        .into_bytes();
    let prv_key = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n"
        .to_string()
        .parse::<Secp256k1SecretKey>()
        .unwrap()
        .into_bytes();
    let responder =
        Responder::from_authority_kp(&pub_key, &prv_key, std::time::Duration::from_secs(10000))
            .unwrap();
    if let Ok((receiver_from_client, sender_to_client)) =
        Connection::new::<'static, AnyMessage<'static>>(stream, HandshakeRole::Responder(responder))
            .await
    {
        Some((receiver_from_client, sender_to_client))
    } else {
        None
    }
}

pub async fn create_upstream(
    stream: tokio::net::TcpStream,
) -> Option<(Receiver<MessageFrame>, Sender<MessageFrame>)> {
    let initiator = Initiator::without_pk().expect("This fn call can not fail");
    if let Ok((receiver_from_server, sender_to_server)) =
        Connection::new::<'static, AnyMessage<'static>>(stream, HandshakeRole::Initiator(initiator))
            .await
    {
        Some((receiver_from_server, sender_to_server))
    } else {
        None
    }
}

pub async fn recv_from_down_send_to_up(
    recv: Receiver<MessageFrame>,
    send: Sender<MessageFrame>,
    downstream_messages: MessagesAggregator,
    action: Vec<InterceptAction>,
    identifier: &str,
) -> Result<(), SnifferError> {
    while let Ok(mut frame) = recv.recv().await {
        let (msg_type, msg) = message_from_frame(&mut frame);
        let action = action.iter().find(|action| {
            action
                .find_matching_action(msg_type, MessageDirection::ToUpstream)
                .is_some()
        });
        if let Some(action) = action {
            match action {
                InterceptAction::IgnoreMessage(_) => {
                    tracing::info!(
                        " Sv2 Sniffer {} | Ignored: {} | Direction: ",
                        identifier,
                        message_type_to_name(msg_type)
                    );
                    continue;
                }
                InterceptAction::ReplaceMessage(intercept_message) => {
                    let intercept_frame = StandardEitherFrame::<AnyMessage<'_>>::Sv2(
                        Sv2Frame::from_message(
                            intercept_message.replacement_message.clone(),
                            intercept_message.replacement_message.message_type(),
                            0,
                            false,
                        )
                        .expect("Failed to create the frame"),
                    );
                    downstream_messages.add_message(
                        intercept_message.replacement_message.message_type(),
                        intercept_message.replacement_message.clone(),
                    );
                    send.send(intercept_frame)
                        .await
                        .map_err(|_| SnifferError::UpstreamClosed)?;
                    tracing::info!(
                        " Sv2 Sniffer {} | Replaced: {} with {} | Direction: ",
                        identifier,
                        message_type_to_name(msg_type),
                        message_type_to_name(intercept_message.replacement_message.message_type())
                    );
                }
            }
        } else {
            downstream_messages.add_message(msg_type, msg);
            send.send(frame)
                .await
                .map_err(|_| SnifferError::UpstreamClosed)?;
            tracing::info!(
                " Sv2 Sniffer {} | Forwarded: {} | Direction: ",
                identifier,
                message_type_to_name(msg_type)
            );
        }
    }
    Err(SnifferError::DownstreamClosed)
}

pub async fn recv_from_up_send_to_down(
    recv: Receiver<MessageFrame>,
    send: Sender<MessageFrame>,
    upstream_messages: MessagesAggregator,
    action: Vec<InterceptAction>,
    identifier: &str,
) -> Result<(), SnifferError> {
    while let Ok(mut frame) = recv.recv().await {
        let (msg_type, msg) = message_from_frame(&mut frame);
        let action = action.iter().find(|action| {
            action
                .find_matching_action(msg_type, MessageDirection::ToDownstream)
                .is_some()
        });

        if let Some(action) = action {
            match action {
                InterceptAction::IgnoreMessage(_) => {
                    tracing::info!(
                        " Sv2 Sniffer {} | Ignored: {} | Direction: ",
                        identifier,
                        message_type_to_name(msg_type)
                    );
                    continue;
                }
                InterceptAction::ReplaceMessage(intercept_message) => {
                    let intercept_frame = StandardEitherFrame::<AnyMessage<'_>>::Sv2(
                        Sv2Frame::from_message(
                            intercept_message.replacement_message.clone(),
                            intercept_message.replacement_message.message_type(),
                            0,
                            false,
                        )
                        .expect("Failed to create the frame"),
                    );
                    upstream_messages.add_message(
                        intercept_message.replacement_message.message_type(),
                        intercept_message.replacement_message.clone(),
                    );
                    send.send(intercept_frame)
                        .await
                        .map_err(|_| SnifferError::DownstreamClosed)?;
                    tracing::info!(
                        " Sv2 Sniffer {} | Replaced: {} with {} | Direction: ",
                        identifier,
                        message_type_to_name(msg_type),
                        message_type_to_name(intercept_message.replacement_message.message_type())
                    );
                }
            }
        } else {
            upstream_messages.add_message(msg_type, msg);
            send.send(frame)
                .await
                .map_err(|_| SnifferError::DownstreamClosed)?;
            tracing::info!(
                " Sv2 Sniffer {} | Forwarded: {} | Direction: ",
                identifier,
                message_type_to_name(msg_type)
            );
        }
    }
    Err(SnifferError::UpstreamClosed)
}

pub fn message_from_frame(frame: &mut MessageFrame) -> (MsgType, AnyMessage<'static>) {
    match frame {
        Frame::Sv2(frame) => {
            if let Some(header) = frame.get_header() {
                let message_type = header.msg_type();
                let mut payload = frame.payload().to_vec();
                let message: Result<AnyMessage<'_>, _> =
                    (message_type, payload.as_mut_slice()).try_into();
                match message {
                    Ok(message) => {
                        let message = into_static(message);
                        (message_type, message)
                    }
                    _ => {
                        println!("Received frame with invalid payload or message type: {frame:?}");
                        panic!();
                    }
                }
            } else {
                println!("Received frame with invalid header: {frame:?}");
                panic!();
            }
        }
        Frame::HandShake(f) => {
            println!("Received unexpected handshake frame: {f:?}");
            panic!();
        }
    }
}

pub fn into_static(m: AnyMessage<'_>) -> AnyMessage<'static> {
    match m {
        AnyMessage::Mining(m) => AnyMessage::Mining(m.into_static()),
        AnyMessage::Common(m) => match m {
            CommonMessages::ChannelEndpointChanged(m) => {
                AnyMessage::Common(CommonMessages::ChannelEndpointChanged(m.into_static()))
            }
            CommonMessages::SetupConnection(m) => {
                AnyMessage::Common(CommonMessages::SetupConnection(m.into_static()))
            }
            CommonMessages::SetupConnectionError(m) => {
                AnyMessage::Common(CommonMessages::SetupConnectionError(m.into_static()))
            }
            CommonMessages::SetupConnectionSuccess(m) => {
                AnyMessage::Common(CommonMessages::SetupConnectionSuccess(m.into_static()))
            }
            CommonMessages::Reconnect(m) => {
                AnyMessage::Common(CommonMessages::Reconnect(m.into_static()))
            }
        },
        AnyMessage::JobDeclaration(m) => match m {
            AllocateMiningJobToken(m) => {
                AnyMessage::JobDeclaration(AllocateMiningJobToken(m.into_static()))
            }
            AllocateMiningJobTokenSuccess(m) => {
                AnyMessage::JobDeclaration(AllocateMiningJobTokenSuccess(m.into_static()))
            }
            DeclareMiningJob(m) => AnyMessage::JobDeclaration(DeclareMiningJob(m.into_static())),
            DeclareMiningJobError(m) => {
                AnyMessage::JobDeclaration(DeclareMiningJobError(m.into_static()))
            }
            DeclareMiningJobSuccess(m) => {
                AnyMessage::JobDeclaration(DeclareMiningJobSuccess(m.into_static()))
            }
            ProvideMissingTransactions(m) => {
                AnyMessage::JobDeclaration(ProvideMissingTransactions(m.into_static()))
            }
            ProvideMissingTransactionsSuccess(m) => {
                AnyMessage::JobDeclaration(ProvideMissingTransactionsSuccess(m.into_static()))
            }
            PushSolution(m) => AnyMessage::JobDeclaration(PushSolution(m.into_static())),
        },
        AnyMessage::TemplateDistribution(m) => match m {
            CoinbaseOutputConstraints(m) => {
                AnyMessage::TemplateDistribution(CoinbaseOutputConstraints(m.into_static()))
            }
            TemplateDistribution::NewTemplate(m) => {
                AnyMessage::TemplateDistribution(TemplateDistribution::NewTemplate(m.into_static()))
            }
            TemplateDistribution::RequestTransactionData(m) => AnyMessage::TemplateDistribution(
                TemplateDistribution::RequestTransactionData(m.into_static()),
            ),
            TemplateDistribution::RequestTransactionDataError(m) => {
                AnyMessage::TemplateDistribution(TemplateDistribution::RequestTransactionDataError(
                    m.into_static(),
                ))
            }
            TemplateDistribution::RequestTransactionDataSuccess(m) => {
                AnyMessage::TemplateDistribution(
                    TemplateDistribution::RequestTransactionDataSuccess(m.into_static()),
                )
            }
            TemplateDistribution::SetNewPrevHash(m) => AnyMessage::TemplateDistribution(
                TemplateDistribution::SetNewPrevHash(m.into_static()),
            ),
            TemplateDistribution::SubmitSolution(m) => AnyMessage::TemplateDistribution(
                TemplateDistribution::SubmitSolution(m.into_static()),
            ),
        },
    }
}

pub mod http {
    pub fn make_get_request(download_url: &str, retries: usize) -> Vec<u8> {
        for attempt in 1..=retries {
            let response = minreq::get(download_url).send();
            match response {
                Ok(res) => {
                    let status_code = res.status_code;
                    if (200..300).contains(&status_code) {
                        return res.as_bytes().to_vec();
                    } else if (500..600).contains(&status_code) {
                        eprintln!(
                            "Attempt {attempt}: URL {download_url} returned a server error code {status_code}"
                        );
                    } else {
                        panic!(
                            "URL {} returned unexpected status code {}. Aborting.",
                            download_url, status_code
                        );
                    }
                }
                Err(err) => {
                    eprintln!(
                        "Attempt {}: Failed to fetch URL {}: {:?}",
                        attempt + 1,
                        download_url,
                        err
                    );
                }
            }

            if attempt < retries {
                let delay = 1u64 << (attempt - 1);
                eprintln!("Retrying in {delay} seconds (exponential backoff)...");
                std::thread::sleep(std::time::Duration::from_secs(delay));
            }
        }
        // If all retries fail, panic with an error message
        panic!(
            "Cannot reach URL {} after {} attempts",
            download_url, retries
        );
    }
}

pub mod tarball {
    use flate2::read::GzDecoder;
    use std::{
        fs::File,
        io::{BufReader, Read},
        path::Path,
    };
    use tar::Archive;

    pub fn read_from_file(path: &str) -> Vec<u8> {
        let file = File::open(path).unwrap_or_else(|_| {
            panic!(
                "Cannot find {:?} specified with env var BITCOIND_TARBALL_FILE",
                path
            )
        });
        let mut reader = BufReader::new(file);
        let mut buffer = Vec::new();
        reader.read_to_end(&mut buffer).unwrap();
        buffer
    }

    pub fn unpack(tarball_bytes: &[u8], destination: &Path) {
        let decoder = GzDecoder::new(tarball_bytes);
        let mut archive = Archive::new(decoder);
        for mut entry in archive.entries().unwrap().flatten() {
            if let Ok(file) = entry.path() {
                if file.ends_with("bitcoind") {
                    entry.unpack_in(destination).unwrap();
                }
            }
        }
    }
}
</file>

<file path="stratum-1.4.0/test/integration-tests/README.md">
# SV2 Integration Tests

This is a test crate and it can be used in order to test the behavior of different roles when
working together. Each role should have a `start_[role_name]` function under `common` folder that
can be called in order to run the role. In order to assert the behavior of the role or the messages
it exchanges with other roles, you can use the `Sniffer` helper in order to listen to the messages
exchanged between the roles, and assert those messages using the `assert_message_[message_type]`
function. For examples on how to use the `Sniffer` helper, you can check the
`sniffer_integration.rs` module or other tests in the `tests` folder.

All of our tests run in regtest network. We download the Template Provider node from
https://github.com/Sjors/bitcoin/releases/download. This is a pre-built binary that we use to run an
Stratum V2 compatible bitcoin node. Note that this is the only external dependency(and Role) that we
have in our tests.

## Running Instructions

In order to run the integration tests, you can use the following command:

```bash
$ git clone git@github.com:stratum-mining/stratum.git
$ cargo test --manifest-path=test/integration-tests/Cargo.toml --verbose --test '*' -- --nocapture
```

Note: during the execution of the tests, a new directory called `template-provider` is created.
This directory holds the executable for Template Provider node, as well as the different data 
directories created for each execution.

## License
MIT OR Apache-2.0
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jd_integration.rs">
// This file contains integration tests for the `JDC/S` module.
use integration_tests_sv2::{
    interceptor::{IgnoreMessage, MessageDirection, ReplaceMessage},
    *,
};
use stratum_common::roles_logic_sv2::{
    self,
    codec_sv2::binary_sv2::{Seq064K, B032, U256},
    common_messages_sv2::*,
    job_declaration_sv2::{ProvideMissingTransactionsSuccess, PushSolution, *},
    parsers::AnyMessage,
};

// This test verifies that jd-server does not exit when a connected jd-client shuts down.
//
// It is performing the verification by shutding down a jd-client connected to a jd-server and then
// starting a new jd-client that connects to the same jd-server successfully.
#[tokio::test]
async fn jds_should_not_panic_if_jdc_shutsdown() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let (sniffer_a, sniffer_addr_a) = start_sniffer("0", jds_addr, false, vec![]);
    let (jdc, jdc_addr) = start_jdc(&[(pool_addr, sniffer_addr_a)], tp_addr);
    sniffer_a
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    jdc.shutdown();
    tokio::time::sleep(tokio::time::Duration::from_millis(500)).await;
    assert!(tokio::net::TcpListener::bind(jdc_addr).await.is_ok());
    let (sniffer, sniffer_addr) = start_sniffer("0", jds_addr, false, vec![]);
    let (_jdc_1, _jdc_addr_1) = start_jdc(&[(pool_addr, sniffer_addr)], tp_addr);
    sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
}

// This test verifies that jd-client exchange SetupConnection messages with a Template Provider.
//
// Note that jd-client starts to exchange messages with the Template Provider after it has accepted
// a downstream connection.
#[tokio::test]
async fn jdc_tp_success_setup() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let (tp_jdc_sniffer, tp_jdc_sniffer_addr) = start_sniffer("0", tp_addr, false, vec![]);
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, jds_addr)], tp_jdc_sniffer_addr);
    // This is needed because jd-client waits for a downstream connection before it starts
    // exchanging messages with the Template Provider.
    start_sv2_translator(jdc_addr);
    tp_jdc_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    tp_jdc_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
}

// This test ensures that `jd-client` does not panic even if `jd-server` leaves the connection open
// after receiving the request for token.
#[tokio::test]
async fn jdc_does_not_stackoverflow_when_no_token() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let block_from_message = IgnoreMessage::new(
        MessageDirection::ToDownstream,
        MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
    );
    let (jds_jdc_sniffer, jds_jdc_sniffer_addr) = start_sniffer(
        "JDS-JDC-sniffer",
        jds_addr,
        false,
        vec![block_from_message.into()],
    );
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, jds_jdc_sniffer_addr)], tp_addr);
    let _ = start_sv2_translator(jdc_addr);
    jds_jdc_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    jds_jdc_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    jds_jdc_sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
        )
        .await;

    // The 3-second delay simulates a scenario where JDC does not receive an
    // `AllocateMiningJobTokenSuccess` response from JDS, leaving `self.allocated_tokens` empty.
    // Without the fix introduced in [PR](https://github.com/stratum-mining/stratum/pull/720),
    // JDC would recursively call `Self::get_last_token`, eventually causing a stack overflow.
    // This test verifies that JDC now blocks/yields correctly instead of infinitely recursing.
    tokio::time::sleep(tokio::time::Duration::from_secs(3)).await;
    assert!(tokio::net::TcpListener::bind(jdc_addr).await.is_err());
}

// This test verifies that JDS does not exit when it receives a `SubmitSolution`
// while still expecting a `ProvideMissingTransactionsSuccess`.
//
// It is performing the verification by connecting to JDS after the message exchange
// to check whether it remains alive.
#[tokio::test]
async fn jds_receive_solution_while_processing_declared_job_test() {
    start_tracing();
    let (tp_1, tp_addr_1) = start_template_provider(None);
    let (tp_2, tp_addr_2) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr_1)).await;
    let (_jds, jds_addr) = start_jds(tp_1.rpc_info());

    let prev_hash = U256::Owned(vec![
        184, 103, 138, 88, 153, 105, 236, 29, 123, 246, 107, 203, 1, 33, 10, 122, 188, 139, 218,
        141, 62, 177, 158, 101, 125, 92, 214, 150, 199, 220, 29, 8,
    ]);
    let extranonce = B032::Owned(vec![
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,
        0, 0,
    ]);
    let submit_solution_replace = ReplaceMessage::new(
        MessageDirection::ToUpstream,
        MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS,
        AnyMessage::JobDeclaration(roles_logic_sv2::parsers::JobDeclaration::PushSolution(
            PushSolution {
                ntime: 0,
                nbits: 0,
                nonce: 0,
                version: 0,
                prev_hash,
                extranonce,
            },
        )),
    );

    // This sniffer sits between `jds` and `jdc`, replacing `ProvideMissingTransactionSuccess`
    // with `SubmitSolution`.
    let (sniffer_a, sniffer_a_addr) =
        start_sniffer("A", jds_addr, false, vec![submit_solution_replace.into()]);
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, sniffer_a_addr)], tp_addr_2);
    start_sv2_translator(jdc_addr);
    assert!(tp_2.fund_wallet().is_ok());
    assert!(tp_2.create_mempool_transaction().is_ok());
    sniffer_a
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
        )
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
        )
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_DECLARE_MINING_JOB,
        )
        .await;
    sniffer_a
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
        )
        .await;
    sniffer_a
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_PUSH_SOLUTION)
        .await;
    assert!(tokio::net::TcpListener::bind(jds_addr).await.is_err());
}

// This test ensures that JDS does not exit upon receiving a `ProvideMissingTransactionsSuccess`
// message containing a transaction set that differs from the `tx_short_hash_list`
// in the Declare Mining Job.
//
// It is performing the verification by connecting to JDS after the message exchange
// to check whether it remains alive
#[tokio::test]
async fn jds_wont_exit_upon_receiving_unexpected_txids_in_provide_missing_transaction_success() {
    start_tracing();
    let (tp_1, tp_addr_1) = start_template_provider(None);
    let (tp_2, tp_addr_2) = start_template_provider(None);

    assert!(tp_2.fund_wallet().is_ok());
    assert!(tp_2.create_mempool_transaction().is_ok());

    let (_pool, pool_addr) = start_pool(Some(tp_addr_1)).await;
    let (_jds, jds_addr) = start_jds(tp_1.rpc_info());

    let provide_missing_transaction_success_replace = ReplaceMessage::new(
        MessageDirection::ToUpstream,
        MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS,
        AnyMessage::JobDeclaration(
            roles_logic_sv2::parsers::JobDeclaration::ProvideMissingTransactionsSuccess(
                ProvideMissingTransactionsSuccess {
                    request_id: 1,
                    transaction_list: Seq064K::new(Vec::new()).unwrap(),
                },
            ),
        ),
    );

    // This sniffer sits between `jds` and `jdc`, replacing `ProvideMissingTransactionSuccess`
    // with `ProvideMissingTransactionSuccess` with different transaction list.
    let (sniffer, sniffer_addr) = start_sniffer(
        "A",
        jds_addr,
        false,
        vec![provide_missing_transaction_success_replace.into()],
    );

    let (_, jdc_addr_1) = start_jdc(&[(pool_addr, sniffer_addr)], tp_addr_2);
    start_sv2_translator(jdc_addr_1);

    sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_ALLOCATE_MINING_JOB_TOKEN_SUCCESS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_DECLARE_MINING_JOB,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS,
        )
        .await;

    assert!(tokio::net::TcpListener::bind(jds_addr).await.is_err());
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jd_provide_missing_transaction.rs">
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::job_declaration_sv2::*;

#[tokio::test]
async fn jds_ask_for_missing_transactions() {
    start_tracing();
    let (tp_1, tp_addr_1) = start_template_provider(None);
    let (tp_2, tp_addr_2) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr_1)).await;
    let (_jds, jds_addr) = start_jds(tp_1.rpc_info());
    let (sniffer, sniffer_addr) = start_sniffer("A", jds_addr, false, vec![]);
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, sniffer_addr)], tp_addr_2);
    start_sv2_translator(jdc_addr);
    assert!(tp_2.fund_wallet().is_ok());
    assert!(tp_2.create_mempool_transaction().is_ok());
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_DECLARE_MINING_JOB,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_PROVIDE_MISSING_TRANSACTIONS_SUCCESS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_DECLARE_MINING_JOB_SUCCESS,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jd_tproxy_integration.rs">
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::{common_messages_sv2::*, mining_sv2::*};

#[tokio::test]
async fn jd_tproxy_integration() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (jdc_pool_sniffer, jdc_pool_sniffer_addr) = start_sniffer("0", pool_addr, false, vec![]);
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let (_jdc, jdc_addr) = start_jdc(&[(jdc_pool_sniffer_addr, jds_addr)], tp_addr);
    let (_translator, tproxy_addr) = start_sv2_translator(jdc_addr);
    start_mining_device_sv1(tproxy_addr, false, None);
    jdc_pool_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL,
        )
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_OPEN_EXTENDED_MINING_CHANNEL_SUCCESS,
        )
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB,
        )
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
        )
        .await;
    jdc_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jdc_block_propogation.rs">
use integration_tests_sv2::{
    interceptor::{IgnoreMessage, MessageDirection},
    *,
};
use stratum_common::roles_logic_sv2::{job_declaration_sv2::*, template_distribution_sv2::*};

// Block propogated from JDC to TP
#[tokio::test]
async fn propogated_from_jdc_to_tp() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let current_block_hash = tp.get_best_block_hash().unwrap();
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let ignore_push_solution =
        IgnoreMessage::new(MessageDirection::ToUpstream, MESSAGE_TYPE_PUSH_SOLUTION);
    let (jdc_jds_sniffer, jdc_jds_sniffer_addr) =
        start_sniffer("0", jds_addr, false, vec![ignore_push_solution.into()]);
    let (jdc_tp_sniffer, jdc_tp_sniffer_addr) = start_sniffer("1", tp_addr, false, vec![]);
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, jdc_jds_sniffer_addr)], jdc_tp_sniffer_addr);
    let (_translator, tproxy_addr) = start_sv2_translator(jdc_addr);
    start_mining_device_sv1(tproxy_addr, false, None);
    jdc_tp_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SUBMIT_SOLUTION)
        .await;
    jdc_jds_sniffer
        .assert_message_not_present(MessageDirection::ToUpstream, MESSAGE_TYPE_PUSH_SOLUTION)
        .await;
    let new_block_hash = tp.get_best_block_hash().unwrap();
    assert_ne!(current_block_hash, new_block_hash);
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jdc_fallback.rs">
use integration_tests_sv2::{
    interceptor::{MessageDirection, ReplaceMessage},
    *,
};
use std::convert::TryInto;
use stratum_common::roles_logic_sv2::{
    common_messages_sv2::*,
    mining_sv2::{SubmitSharesError, *},
    parsers::{AnyMessage, Mining},
};

// Tests whether JDC will switch to a new pool after receiving a `SubmitSharesError` message from
// the currently connected pool.
//
// This ignore directive can be removed once this issue is resolved: https://github.com/stratum-mining/stratum/issues/1574.
#[ignore]
#[tokio::test]
async fn test_jdc_pool_fallback_after_submit_rejection() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool_1, pool_addr_1) = start_pool(Some(tp_addr)).await;
    // Sniffer between JDC and first pool
    let (sniffer_1, sniffer_addr_1) = start_sniffer(
        "0",
        pool_addr_1,
        false,
        vec![
            // Should trigger Fallback process in JDC
            ReplaceMessage::new(
                MessageDirection::ToDownstream,
                MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
                AnyMessage::Mining(Mining::SubmitSharesError(SubmitSharesError {
                    channel_id: 0,
                    sequence_number: 0,
                    error_code: "invalid-nonce".to_string().into_bytes().try_into().unwrap(),
                })),
            )
            .into(),
        ],
    );
    let (_pool_2, pool_addr_2) = start_pool(Some(tp_addr)).await;
    // Sniffer between JDC and second pool
    let (sniffer_2, sniffer_addr_2) = start_sniffer("1", pool_addr_2, false, vec![]);
    let (_jds_1, jds_addr_1) = start_jds(tp.rpc_info());
    // Sniffer between JDC and first JDS
    let (sniffer_3, sniffer_addr_3) = start_sniffer("2", jds_addr_1, false, vec![]);
    let (_jds_2, jds_addr_2) = start_jds(tp.rpc_info());
    // Sniffer between JDC and second JDS
    let (sniffer_4, sniffer_addr_4) = start_sniffer("3", jds_addr_2, false, vec![]);
    let (_jdc, jdc_addr) = start_jdc(
        &[
            (sniffer_addr_1, sniffer_addr_3),
            (sniffer_addr_2, sniffer_addr_4),
        ],
        tp_addr,
    );
    // Assert that JDC has connected to the first (Pool,JDS) pair
    sniffer_1
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer_3
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    let (_translator, sv2_translator_addr) = start_sv2_translator(jdc_addr);
    start_mining_device_sv1(sv2_translator_addr, false, None);
    // Assert that JDC switched to the second (Pool,JDS) pair
    sniffer_2
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer_4
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jdc_receives_submit_shares_success.rs">
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::mining_sv2::*;

#[tokio::test]
async fn jdc_submit_shares_success() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (sniffer, sniffer_addr) = start_sniffer("0", pool_addr, false, vec![]);
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let (_jdc, jdc_addr) = start_jdc(&[(sniffer_addr, jds_addr)], tp_addr);
    let (_translator, tproxy_addr) = start_sv2_translator(jdc_addr);
    start_mining_device_sv1(tproxy_addr, false, None);

    // make sure sure JDC gets a share acknowledgement
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/jds_block_propogation.rs">
use integration_tests_sv2::{
    interceptor::{IgnoreMessage, MessageDirection},
    *,
};
use stratum_common::roles_logic_sv2::{job_declaration_sv2::*, template_distribution_sv2::*};

// Block propogated from JDS to TP
#[tokio::test]
async fn propogated_from_jds_to_tp() {
    start_tracing();
    let (tp, tp_addr) = start_template_provider(None);
    let current_block_hash = tp.get_best_block_hash().unwrap();
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_jds, jds_addr) = start_jds(tp.rpc_info());
    let (jdc_jds_sniffer, jdc_jds_sniffer_addr) = start_sniffer("0", jds_addr, false, vec![]);
    let ignore_submit_solution =
        IgnoreMessage::new(MessageDirection::ToUpstream, MESSAGE_TYPE_SUBMIT_SOLUTION);
    let (jdc_tp_sniffer, jdc_tp_sniffer_addr) =
        start_sniffer("1", tp_addr, false, vec![ignore_submit_solution.into()]);
    let (_jdc, jdc_addr) = start_jdc(&[(pool_addr, jdc_jds_sniffer_addr)], jdc_tp_sniffer_addr);
    let (_translator, tproxy_addr) = start_sv2_translator(jdc_addr);
    start_mining_device_sv1(tproxy_addr, false, None);
    jdc_jds_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_PUSH_SOLUTION)
        .await;
    jdc_tp_sniffer
        .assert_message_not_present(MessageDirection::ToUpstream, MESSAGE_TYPE_SUBMIT_SOLUTION)
        .await;
    let new_block_hash = tp.get_best_block_hash().unwrap();
    assert_ne!(current_block_hash, new_block_hash);
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/pool_integration.rs">
// This file contains integration tests for the `PoolSv2` module.
//
// `PoolSv2` is a module that implements the Pool role in the Stratum V2 protocol.
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::{
    common_messages_sv2::{Protocol, SetupConnection, *},
    mining_sv2::*,
    parsers::{AnyMessage, CommonMessages, Mining, TemplateDistribution},
    template_distribution_sv2::*,
};

// This test starts a Template Provider and a Pool, and checks if they exchange the correct
// messages upon connection.
// The Sniffer is used as a proxy between the Upstream(Template Provider) and Downstream(Pool). The
// Pool will connect to the Sniffer, and the Sniffer will connect to the Template Provider.
#[tokio::test]
async fn success_pool_template_provider_connection() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (sniffer, sniffer_addr) = start_sniffer("", tp_addr, true, vec![]);
    let _ = start_pool(Some(sniffer_addr)).await;
    // here we assert that the downstream(pool in this case) have sent `SetupConnection` message
    // with the correct parameters, protocol, flags, min_version and max_version.  Note that the
    // macro can take any number of arguments after the message argument, but the order is
    // important where a property should be followed by its value.
    sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    assert_common_message!(
        &sniffer.next_message_from_downstream(),
        SetupConnection,
        protocol,
        Protocol::TemplateDistributionProtocol,
        flags,
        0,
        min_version,
        2,
        max_version,
        2
    );
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    assert_common_message!(
        &sniffer.next_message_from_upstream(),
        SetupConnectionSuccess
    );
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_COINBASE_OUTPUT_CONSTRAINTS,
        )
        .await;
    assert_tp_message!(
        &sniffer.next_message_from_downstream(),
        CoinbaseOutputConstraints
    );
    sniffer
        .wait_for_message_type(MessageDirection::ToDownstream, MESSAGE_TYPE_NEW_TEMPLATE)
        .await;
    assert_tp_message!(&sniffer.next_message_from_upstream(), NewTemplate);
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SET_NEW_PREV_HASH,
        )
        .await;
    assert_tp_message!(sniffer.next_message_from_upstream(), SetNewPrevHash);
}

// This test starts a Template Provider, a Pool, and a Translator Proxy, and verifies the
// correctness of the exchanged messages during connection and operation.
//
// Two Sniffers are used:
// - Between the Template Provider and the Pool.
// - Between the Pool and the Translator Proxy.
//
// The test ensures that:
// - The Template Provider sends valid `SetNewPrevHash` and `NewTemplate` messages.
// - The `minntime` field in the second `NewExtendedMiningJob` message sent to the Translator Proxy
//   matches the `header_timestamp` from the `SetNewPrevHash` message, addressing a bug that
//   occurred with non-future jobs.
//
// Related issue: https://github.com/stratum-mining/stratum/issues/1324
#[tokio::test]
async fn header_timestamp_value_assertion_in_new_extended_mining_job() {
    start_tracing();
    let sv2_interval = Some(5);
    let (_tp, tp_addr) = start_template_provider(sv2_interval);
    let tp_pool_sniffer_identifier =
        "header_timestamp_value_assertion_in_new_extended_mining_job tp_pool sniffer";
    let (tp_pool_sniffer, tp_pool_sniffer_addr) =
        start_sniffer(tp_pool_sniffer_identifier, tp_addr, false, vec![]);
    let (_, pool_addr) = start_pool(Some(tp_pool_sniffer_addr)).await;
    let pool_translator_sniffer_identifier =
        "header_timestamp_value_assertion_in_new_extended_mining_job pool_translator sniffer";
    let (pool_translator_sniffer, pool_translator_sniffer_addr) =
        start_sniffer(pool_translator_sniffer_identifier, pool_addr, false, vec![]);
    let _tproxy_addr = start_sv2_translator(pool_translator_sniffer_addr);

    tp_pool_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    assert_common_message!(
        &tp_pool_sniffer.next_message_from_upstream(),
        SetupConnectionSuccess
    );
    // Wait for a NewTemplate message from the Template Provider
    tp_pool_sniffer
        .wait_for_message_type(MessageDirection::ToDownstream, MESSAGE_TYPE_NEW_TEMPLATE)
        .await;
    assert_tp_message!(&tp_pool_sniffer.next_message_from_upstream(), NewTemplate);
    // Extract header timestamp from SetNewPrevHash message
    let header_timestamp_to_check = match tp_pool_sniffer.next_message_from_upstream() {
        Some((_, AnyMessage::TemplateDistribution(TemplateDistribution::SetNewPrevHash(msg)))) => {
            msg.header_timestamp
        }
        _ => panic!("SetNewPrevHash not found!"),
    };
    pool_translator_sniffer
        .wait_for_message_type_and_clean_queue(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH,
        )
        .await;
    // Wait for a second NewExtendedMiningJob message
    pool_translator_sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_NEW_EXTENDED_MINING_JOB,
        )
        .await;
    // Extract min_ntime from the second NewExtendedMiningJob message
    let second_job_ntime = match pool_translator_sniffer.next_message_from_upstream() {
        Some((_, AnyMessage::Mining(Mining::NewExtendedMiningJob(job)))) => {
            job.min_ntime.into_inner()
        }
        _ => panic!("Second NewExtendedMiningJob not found!"),
    };
    // Assert that min_ntime matches header_timestamp
    assert_eq!(
        second_job_ntime,
        Some(header_timestamp_to_check),
        "The `minntime` field of the second NewExtendedMiningJob does not match the `header_timestamp`!"
    );
}

// This test starts a Pool, a Sniffer, and a Sv2 Mining Device.  It then checks if the Pool receives
// a share from the Sv2 Mining Device.  While also checking all the messages exchanged between the
// Pool and the Mining Device in between.
#[tokio::test]
async fn pool_standard_channel_receives_share() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (sniffer, sniffer_addr) = start_sniffer("A", pool_addr, false, vec![]);
    start_mining_device_sv2(sniffer_addr, None, None, None, 1, None, true);
    sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL,
        )
        .await;

    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_OPEN_STANDARD_MINING_CHANNEL_SUCCESS,
        )
        .await;
    sniffer
        .wait_for_message_type(MessageDirection::ToDownstream, MESSAGE_TYPE_NEW_MINING_JOB)
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_MINING_SET_NEW_PREV_HASH,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_SUBMIT_SHARES_STANDARD,
        )
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SUBMIT_SHARES_SUCCESS,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/sniffer_integration.rs">
// This file contains integration tests for the `Sniffer` module.
use integration_tests_sv2::{
    interceptor::{IgnoreMessage, MessageDirection, ReplaceMessage},
    *,
};
use std::convert::TryInto;
use stratum_common::roles_logic_sv2::{
    common_messages_sv2::{Protocol, SetupConnection, SetupConnectionSuccess, *},
    parsers::{AnyMessage, CommonMessages},
    template_distribution_sv2::*,
};

// This test aims to assert that Sniffer is able to intercept and replace/ignore messages.
// TP -> sniffer_a -> sniffer_b -> Pool
#[tokio::test]
async fn test_sniffer_interception() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let ignore_message =
        IgnoreMessage::new(MessageDirection::ToDownstream, MESSAGE_TYPE_NEW_TEMPLATE);
    let setup_connection_message =
        AnyMessage::Common(CommonMessages::SetupConnection(SetupConnection {
            protocol: Protocol::TemplateDistributionProtocol,
            min_version: 2,
            max_version: 2,
            flags: 0,
            endpoint_host: b"0.0.0.0".to_vec().try_into().unwrap(),
            endpoint_port: 8081,
            vendor: b"Bitmain".to_vec().try_into().unwrap(),
            hardware_version: b"901".to_vec().try_into().unwrap(),
            firmware: b"abcX".to_vec().try_into().unwrap(),
            device_id: b"89567".to_vec().try_into().unwrap(),
        }));
    let setup_connection_replacement = ReplaceMessage::new(
        MessageDirection::ToUpstream,
        MESSAGE_TYPE_SETUP_CONNECTION,
        setup_connection_message,
    );
    let setup_connection_error_message = AnyMessage::Common(
        CommonMessages::SetupConnectionSuccess(SetupConnectionSuccess {
            flags: 0,
            used_version: 0,
        }),
    );
    let setup_connection_success_replacement = ReplaceMessage::new(
        MessageDirection::ToDownstream,
        MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        setup_connection_error_message,
    );
    let (sniffer_a, sniffer_a_addr) = start_sniffer(
        "A",
        tp_addr,
        false,
        vec![
            setup_connection_success_replacement.into(),
            ignore_message.into(),
        ],
    );
    let (sniffer_b, sniffer_b_addr) = start_sniffer(
        "B",
        sniffer_a_addr,
        false,
        vec![setup_connection_replacement.into()],
    );
    let _ = start_pool(Some(sniffer_b_addr)).await;
    sniffer_a
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    assert_common_message!(
        &sniffer_a.next_message_from_downstream(),
        SetupConnection,
        protocol,
        Protocol::TemplateDistributionProtocol,
        flags,
        0,
        min_version,
        2,
        max_version,
        2,
        endpoint_host,
        "0.0.0.0".to_string().into_bytes().try_into().unwrap(),
        endpoint_port,
        8081,
        vendor,
        "Bitmain".to_string().into_bytes().try_into().unwrap()
    );
    sniffer_b
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
    assert_common_message!(
        &sniffer_b.next_message_from_upstream(),
        SetupConnectionSuccess,
        used_version,
        0,
        flags,
        0
    );
    assert!(
        !(sniffer_b
            .includes_message_type(MessageDirection::ToDownstream, MESSAGE_TYPE_NEW_TEMPLATE))
    );
}

#[tokio::test]
async fn test_sniffer_wait_for_message_type_with_remove() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (sniffer, sniffer_addr) = start_sniffer("", tp_addr, false, vec![]);
    let _ = start_pool(Some(sniffer_addr)).await;
    assert!(
        sniffer
            .wait_for_message_type_and_clean_queue(
                MessageDirection::ToDownstream,
                MESSAGE_TYPE_SET_NEW_PREV_HASH,
            )
            .await
    );
    assert!(
        !(sniffer.includes_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS
        ))
    );
    assert!(
        !(sniffer.includes_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SET_NEW_PREV_HASH
        ))
    );
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/sv1.rs">
#![cfg(feature = "sv1")]
use integration_tests_sv2::*;
use interceptor::MessageDirection;

#[tokio::test]
async fn test_basic_sv1() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (_, tproxy_addr) = start_sv2_translator(pool_addr);
    let (sniffer_sv1, sniffer_sv1_addr) = start_sv1_sniffer(tproxy_addr);
    let _mining_device = start_mining_device_sv1(sniffer_sv1_addr, false, None);
    sniffer_sv1
        .wait_for_message(&["mining.configure"], MessageDirection::ToUpstream)
        .await;
    sniffer_sv1
        .wait_for_message(&["mining.authorize"], MessageDirection::ToUpstream)
        .await;
    sniffer_sv1
        .wait_for_message(
            &[
                "minimum-difficulty",
                "version-rolling",
                "version-rolling.mask",
                "version-rolling.min-bit-count",
            ],
            MessageDirection::ToDownstream,
        )
        .await;
    sniffer_sv1
        .wait_for_message(&["mining.subscribe"], MessageDirection::ToUpstream)
        .await;
    sniffer_sv1
        .wait_for_message(&["mining.notify"], MessageDirection::ToDownstream)
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/sv2_mining_device.rs">
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::common_messages_sv2::*;

#[tokio::test]
async fn sv2_mining_device_and_pool_success() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (sniffer, sniffer_addr) = start_sniffer("A", pool_addr, false, vec![]);
    start_mining_device_sv2(sniffer_addr, None, None, None, 1, None, true);
    sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    sniffer
        .wait_for_message_type(
            MessageDirection::ToDownstream,
            MESSAGE_TYPE_SETUP_CONNECTION_SUCCESS,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/integration-tests/tests/translator_integration.rs">
// This file contains integration tests for the `TranslatorSv2` module.
use integration_tests_sv2::{interceptor::MessageDirection, *};
use stratum_common::roles_logic_sv2::{
    common_messages_sv2::*,
    mining_sv2::*,
    parsers::{AnyMessage, CommonMessages, Mining},
};

// This test runs an sv2 translator between an sv1 mining device and a pool. the connection between
// the translator and the pool is intercepted by a sniffer. The test checks if the translator and
// the pool exchange the correct messages upon connection. And that the miner is able to submit
// shares.
#[tokio::test]
async fn translate_sv1_to_sv2_successfully() {
    start_tracing();
    let (_tp, tp_addr) = start_template_provider(None);
    let (_pool, pool_addr) = start_pool(Some(tp_addr)).await;
    let (pool_translator_sniffer, pool_translator_sniffer_addr) =
        start_sniffer("0", pool_addr, false, vec![]);
    let (_, tproxy_addr) = start_sv2_translator(pool_translator_sniffer_addr);
    start_mining_device_sv1(tproxy_addr, false, None);
    pool_translator_sniffer
        .wait_for_message_type(MessageDirection::ToUpstream, MESSAGE_TYPE_SETUP_CONNECTION)
        .await;
    assert_common_message!(
        &pool_translator_sniffer.next_message_from_downstream(),
        SetupConnection
    );
    assert_common_message!(
        &pool_translator_sniffer.next_message_from_upstream(),
        SetupConnectionSuccess
    );
    assert_mining_message!(
        &pool_translator_sniffer.next_message_from_downstream(),
        OpenExtendedMiningChannel
    );
    assert_mining_message!(
        &pool_translator_sniffer.next_message_from_upstream(),
        OpenExtendedMiningChannelSuccess
    );
    assert_mining_message!(
        &pool_translator_sniffer.next_message_from_upstream(),
        NewExtendedMiningJob
    );
    pool_translator_sniffer
        .wait_for_message_type(
            MessageDirection::ToUpstream,
            MESSAGE_TYPE_SUBMIT_SHARES_EXTENDED,
        )
        .await;
}
</file>

<file path="stratum-1.4.0/test/scale/Cargo.toml">
[package]
name = "scale"
version = "1.0.0"
edition = "2021"

[profile.release]
lto = true

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
clap = "2.33.3"
async-channel = "1.5.1"
async-std="1.8.0"
bytes = "1.0.1"
binary_sv2 = { path = "../../protocols/v2/binary-sv2" }
codec_sv2 = { path = "../../protocols/v2/codec-sv2", features=["noise_sv2"] }
network_helpers_sv2 = { version = "0.1", path = "../../roles/roles-utils/network-helpers" }
roles_logic_sv2 = { path = "../../protocols/v2/roles-logic-sv2" }
tokio = { version = "1.44.1", features = ["full"] }
key-utils = { version = "^1.0.0", path = "../../utils/key-utils" }
</file>

<file path="stratum-1.4.0/test/scale/README.md">
# Scale Test

This test simply outputs the time spent sending 1,000,000 SubmitSharesStandard 
through the system. When you start the test you specify -h <num of hops> -e (for encryption). 
The test spawns <num of hops> "proxies" (ports 19000->19000+<num of hops>) which simply decrypt/encrypt each 
SubmitSharesStandard message coming in (if encryption is on). Then it sends 
1,000,000 share messages to the first proxy and then times the whole system to see 
how long it takes for the last proxy to receive all 1M messages. It uses the same
network_helpers that the pool, and proxies use so it should be a good approximation
of the work they do. 

The test is run with the following command:
NOTE: running without `--release` dramatically slows down the test.

```cargo run --release -- -h 4 -e```
This runs the test with 4 hops and encryption on.

```cargo run --release -- -h 4```
This runs the test with 4 hops and encryption off.
</file>

<file path="stratum-1.4.0/test/scale/src/main.rs">
use std::thread;
use tokio::{
    net::{TcpListener, TcpStream},
    task,
};

use async_channel::{bounded, Receiver, Sender};

use clap::{App, Arg};
use codec_sv2::{HandshakeRole, Initiator, Responder, StandardEitherFrame, StandardSv2Frame};
use std::time::Duration;

use network_helpers::{
    noise_connection_tokio::Connection, plain_connection_tokio::PlainConnection,
};

use key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
use roles_logic_sv2::{
    mining_sv2::*,
    parsers::{Mining, MiningDeviceMessages},
};

pub type EitherFrame = StandardEitherFrame<Message>;
pub const AUTHORITY_PUBLIC_K: &str = "9auqWEzQDVyd2oe1JVGFLMLHZtCo2FFqZwtKA5gd9xbuEu7PH72";

pub const AUTHORITY_PRIVATE_K: &str = "mkDLTBBRxdBv998612qipDYoTK3YUrqLe8uWw7gu3iXbSrn2n";

static HOST: &str = "127.0.0.1";

#[tokio::main]
async fn main() {
    let matches = App::new("ScaleTest")
        .arg(Arg::with_name("encrypt").short("e").help("Use encryption"))
        .arg(
            Arg::with_name("hops")
                .short("h")
                .takes_value(true)
                .help("Number of hops"),
        )
        .get_matches();

    let total_messages = 1_000_000;
    let encrypt = matches.is_present("encrypt");
    let hops: u16 = matches.value_of("hops").unwrap_or("0").parse().unwrap_or(0);
    let mut orig_port: u16 = 19000;

    // create channel to tell final server number of messages
    let (tx, rx) = bounded(1);

    if hops > 0 {
        orig_port = spawn_proxies(encrypt, hops, tx, total_messages).await;
    } else {
        println!("Usage: ./program -h <hops> -e");
    }
    println!("Connecting to localhost:{}", orig_port);
    setup_driver(orig_port, encrypt, rx, total_messages, hops).await;
}

async fn setup_driver(
    server_port: u16,
    encrypt: bool,
    rx: Receiver<String>,
    total_messages: i32,
    hops: u16,
) {
    let server_stream = TcpStream::connect(format!("{}:{}", HOST, server_port))
        .await
        .unwrap();
    let (_server_receiver, server_sender): (Receiver<EitherFrame>, Sender<EitherFrame>);

    if encrypt {
        let k: Secp256k1PublicKey = AUTHORITY_PUBLIC_K.to_string().try_into().unwrap();
        let initiator = Initiator::from_raw_k(k.into_bytes()).unwrap();

        (_, server_sender, _, _) =
            Connection::new(server_stream, HandshakeRole::Initiator(initiator))
                .await
                .unwrap();
    } else {
        (_server_receiver, server_sender) = PlainConnection::new(server_stream).await;
    }
    // Create timer to see how long this method takes
    let start = std::time::Instant::now();

    send_messages(server_sender, total_messages).await;

    //listen for message on rx
    let msg = rx.recv().await.unwrap();

    let end = std::time::Instant::now();

    println!(
        "client: {} - Took {}s hops: {} encryption: {}",
        msg,
        (end - start).as_secs(),
        hops,
        encrypt
    );
}

pub type Message = MiningDeviceMessages<'static>;
pub type StdFrame = StandardSv2Frame<Message>;

async fn send_messages(stream: Sender<EitherFrame>, total_messages: i32) {
    let mut number: i32 = 0;
    println!("Creating share");
    let share = MiningDeviceMessages::Mining(Mining::SubmitSharesStandard(SubmitSharesStandard {
        channel_id: 1,
        sequence_number: number as u32,
        job_id: 2,
        nonce: 3,
        ntime: 4,
        version: 5,
    }));

    while number <= total_messages {
        //println!("client: sending msg-{}", number);
        let frame: StdFrame = share.clone().try_into().unwrap();
        let binary: EitherFrame = frame.into();

        stream.send(binary).await.unwrap();
        number += 1;
    }
}

async fn handle_messages(
    _name: String,
    client: Receiver<EitherFrame>,
    server: Option<Sender<EitherFrame>>,
    total_messages: i32,
    tx: Sender<String>,
) {
    let mut messages_received = 0;

    while messages_received <= total_messages {
        let frame: StdFrame = client.recv().await.unwrap().try_into().unwrap();

        let binary: EitherFrame = frame.into();

        if server.is_some() {
            server.as_ref().unwrap().send(binary).await.unwrap();
        } else {
            messages_received += 1;
            //println!("last server: {} got msg {}", name, messages_received);
        }
    }
    tx.send("got all messages".to_string()).await.unwrap();
}

async fn create_proxy(
    name: String,
    listen_port: u16,
    server_port: u16,
    encrypt: bool,
    total_messages: i32,
    tx: Sender<String>,
) {
    println!(
        "Creating proxy listener {}: {} connecting to: {}",
        name, listen_port, server_port
    );
    let listener = TcpListener::bind(format!("0.0.0.0:{}", listen_port))
        .await
        .unwrap();
    println!("Bound - now waiting for connection...");
    let cli_stream = listener.accept().await.unwrap().0;
    let (cli_receiver, _cli_sender): (Receiver<EitherFrame>, Sender<EitherFrame>);

    if encrypt {
        let k_pub: Secp256k1PublicKey = AUTHORITY_PUBLIC_K.to_string().try_into().unwrap();
        let k_priv: Secp256k1SecretKey = AUTHORITY_PRIVATE_K.to_string().try_into().unwrap();
        let responder = Responder::from_authority_kp(
            &k_pub.into_bytes(),
            &k_priv.into_bytes(),
            Duration::from_secs(3600),
        )
        .unwrap();
        (cli_receiver, _, _, _) = Connection::new(cli_stream, HandshakeRole::Responder(responder))
            .await
            .unwrap();
    } else {
        (cli_receiver, _cli_sender) = PlainConnection::new(cli_stream).await;
    }

    let mut server = None;
    if server_port > 0 {
        println!("Proxy {} Connecting to server: {}", name, server_port);
        let server_stream = TcpStream::connect(format!("{}:{}", HOST, server_port))
            .await
            .unwrap();
        let (_server_receiver, server_sender): (Receiver<EitherFrame>, Sender<EitherFrame>);
        let k_pub: Secp256k1PublicKey = AUTHORITY_PUBLIC_K.to_string().try_into().unwrap();

        if encrypt {
            let initiator = Initiator::from_raw_k(k_pub.into_bytes()).unwrap();
            (_, server_sender, _, _) =
                Connection::new(server_stream, HandshakeRole::Initiator(initiator))
                    .await
                    .unwrap();
        } else {
            (_server_receiver, server_sender) = PlainConnection::new(server_stream).await;
        }
        server = Some(server_sender);
    }

    println!("Proxy {} has a client", name);
    handle_messages(name, cli_receiver, server, total_messages, tx).await;
}

async fn spawn_proxies(encrypt: bool, hops: u16, tx: Sender<String>, total_messages: i32) -> u16 {
    let orig_port: u16 = 19000;
    let final_server_port = orig_port + (hops - 1);
    let mut listen_port = final_server_port;
    let mut server_port: u16 = 0;

    for name in (0..hops).rev() {
        let tx_clone = tx.clone();
        let name_clone = name.to_string();

        task::spawn(async move {
            create_proxy(
                name_clone,
                listen_port,
                server_port,
                encrypt,
                total_messages,
                tx_clone,
            )
            .await;
        });

        thread::sleep(std::time::Duration::from_secs(1));
        server_port = listen_port;
        listen_port -= 1;
    }
    orig_port
}
</file>

<file path="stratum-1.4.0/utils/bip32-key-derivation/Cargo.toml">
[package]
name = "bip32_derivation"
version = "1.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "bip32_derivation"
documentation = "https://docs.rs/bip32_derivation"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[lib]
name = "bip32_derivation"
path = "src/lib.rs"

[[bin]]
name = "bip32_derivation-bin"
path = "src/main.rs"

[dependencies]
stratum-common = { path = "../../common", version = "3.0.0" }


[dev-dependencies]
toml = { version = "0.5.6", git = "https://github.com/diondokter/toml-rs", default-features = false, rev = "c4161aa" }
</file>

<file path="stratum-1.4.0/utils/bip32-key-derivation/README.md">
# Bip32 Key Derivation

## Binary
It can be used as binary to derive child public keys from a BIP32 Master Public Key, specifying the derivation path: 
`cargo run "vpub5ZMie86usV2ZSrvUSoc3sLg9YM8cmE4xHVzhXudJhezGGoXQ8L6Hash7E4ucffBKZXXi4r5wLiCeouB4sTwSDkfivsbmFAGqvAv9Vt7k7Lg" "m/0/0"`

## Library
It can be imported by other applications that need to derive child public keys from a BIP32 Master Public Key exported from a wallet.
</file>

<file path="stratum-1.4.0/utils/bip32-key-derivation/src/lib.rs">
use std::str::FromStr;
use stratum_common::roles_logic_sv2::bitcoin::{
    bip32::{DerivationPath, Error, Xpub},
    secp256k1::Secp256k1,
};

pub fn derive_child_public_key(xpub: &Xpub, path: &str) -> Result<Xpub, Error> {
    let secp = Secp256k1::new();
    let derivation_path = DerivationPath::from_str(path)?;
    let child_pub_key = xpub.derive_pub(&secp, &derivation_path)?;
    Ok(child_pub_key)
}
</file>

<file path="stratum-1.4.0/utils/bip32-key-derivation/src/main.rs">
use bip32_derivation::derive_child_public_key;
use std::{env, str::FromStr};
use stratum_common::roles_logic_sv2::bitcoin::bip32::Xpub;

fn main() {
    let args: Vec<String> = env::args().collect();

    if args.len() != 3 {
        eprintln!("Usage: cargo run <bip32 master extened public key> <derivation path (m/0/0)>");
        std::process::exit(1);
    }

    let master_pub_key = &args[1];
    let derivation_path = &args[2];
    let bip32_extended_pub_key: Xpub = Xpub::from_str(master_pub_key).unwrap();
    let child_pub_key = derive_child_public_key(&bip32_extended_pub_key, derivation_path).unwrap();
    println!(
        "\nPublic key derived from your Master Public Key -> {:?}",
        child_pub_key.to_pub().0.to_string()
    );
    println!(
        "\nCopy/paste it in your configuration file (filling the output_script_value field)!\n"
    );
}
</file>

<file path="stratum-1.4.0/utils/buffer/BENCHES.md">
Benchmark for 2000 samples with criterion:

```
sample-size-example/with pool
                        time:   [7.4963 ms 7.5006 ms 7.5051 ms]
                        change: [+6.1229% +6.3176% +6.5036%] (p = 0.00 < 0.05)
                        Performance has regressed.
Found 26 outliers among 2000 measurements (1.30%)
  3 (0.15%) low mild
  20 (1.00%) high mild
  3 (0.15%) high severe
Benchmarking sample-size-example/without pool: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 20.4s, or reduce sample count to 490.
sample-size-example/without pool
                        time:   [10.268 ms 10.274 ms 10.279 ms]
                        change: [+2.6310% +2.7545% +2.8775%] (p = 0.00 < 0.05)
                        Performance has regressed.
Found 29 outliers among 2000 measurements (1.45%)
  5 (0.25%) low mild
  18 (0.90%) high mild
  6 (0.30%) high severe
Benchmarking sample-size-example/with control struct: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 65.8s, or reduce sample count to 150.
sample-size-example/with control struct
                        time:   [32.577 ms 32.593 ms 32.609 ms]
                        change: [+3.6492% +3.7783% +3.9036%] (p = 0.00 < 0.05)
                        Performance has regressed.
Found 61 outliers among 2000 measurements (3.05%)
  4 (0.20%) low mild
  51 (2.55%) high mild
  6 (0.30%) high severe
sample-size-example/with control struct max e
                        time:   [1.2608 ms 1.2618 ms 1.2629 ms]
                        change: [-0.3202% -0.1721% -0.0231%] (p = 0.01 < 0.05)
                        Change within noise threshold.
Found 140 outliers among 2000 measurements (7.00%)
  7 (0.35%) low mild
  42 (2.10%) high mild
  91 (4.55%) high severe
Benchmarking sample-size-example/with pool thread 1: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 68.8s, or reduce sample count to 140.
sample-size-example/with pool thread 1
                        time:   [34.626 ms 34.660 ms 34.693 ms]
                        change: [-0.3871% -0.1594% +0.0680%] (p = 0.17 > 0.05)
                        No change in performance detected.
Found 42 outliers among 2000 measurements (2.10%)
  6 (0.30%) low severe
  29 (1.45%) low mild
  6 (0.30%) high mild
  1 (0.05%) high severe
Benchmarking sample-size-example/without pool threaded 1: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 285.4s, or reduce sample count to 30.
sample-size-example/without pool threaded 1
                        time:   [142.02 ms 142.23 ms 142.49 ms]
                        change: [+0.2557% +0.4149% +0.5979%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Found 66 outliers among 2000 measurements (3.30%)
  28 (1.40%) high mild
  38 (1.90%) high severe
Benchmarking sample-size-example/with control threaded 1: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 98.8s, or reduce sample count to 100.
sample-size-example/with control threaded 1
                        time:   [49.762 ms 49.790 ms 49.819 ms]
                        change: [+0.7355% +0.8427% +0.9496%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Found 15 outliers among 2000 measurements (0.75%)
  5 (0.25%) high mild
  10 (0.50%) high severe
Benchmarking sample-size-example/with control threaded max: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 36.4s, or reduce sample count to 270.
sample-size-example/with control threaded max
                        time:   [18.177 ms 18.201 ms 18.225 ms]
                        change: [+0.3845% +0.6791% +0.9724%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Found 8 outliers among 2000 measurements (0.40%)
  6 (0.30%) high mild
  2 (0.10%) high severe
Benchmarking sample-size-example/with pool thread 2: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 161.8s, or reduce sample count to 60.
sample-size-example/with pool thread 2
                        time:   [80.684 ms 80.869 ms 81.052 ms]
                        change: [+0.9153% +1.4562% +2.0137%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Benchmarking sample-size-example/without pool threaded 2: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 389.1s, or reduce sample count to 20.
sample-size-example/without pool threaded 2
                        time:   [194.18 ms 194.24 ms 194.29 ms]
                        change: [+0.6562% +0.7258% +0.7900%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Found 46 outliers among 2000 measurements (2.30%)
  43 (2.15%) high mild
  3 (0.15%) high severe
Benchmarking sample-size-example/with control threaded 2: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 204.2s, or reduce sample count to 40.
sample-size-example/with control threaded 2
                        time:   [101.71 ms 101.75 ms 101.79 ms]
                        change: [+0.1046% +0.1841% +0.2619%] (p = 0.00 < 0.05)
                        Change within noise threshold.
Found 22 outliers among 2000 measurements (1.10%)
  6 (0.30%) low mild
  14 (0.70%) high mild
  2 (0.10%) high severe
Benchmarking sample-size-example/with control threaded max 2: Warming up for 3.0000 s
Warning: Unable to complete 2000 samples in 5.0s. You may wish to increase target time to 133.9s, or reduce sample count to 70.
sample-size-example/with control threaded max 2
                        time:   [66.953 ms 66.972 ms 66.992 ms]
                        change: [-0.0483% +0.0157% +0.0792%] (p = 0.64 > 0.05)
                        No change in performance detected.
Found 2 outliers among 2000 measurements (0.10%)
  1 (0.05%) high mild
  1 (0.05%) high severe


```

Benchmark with iai:
```
with_pool
  Instructions:           131725449 (-0.570813%)
  L1 Accesses:            199120813 (-0.570775%)
  L2 Accesses:                   63 (-10.00000%)
  RAM Accesses:                 445 (No change)
  Estimated Cycles:       199136703 (-0.570748%)

without_pool
  Instructions:           149678362 (+0.605157%)
  L1 Accesses:            201824534 (+0.604935%)
  L2 Accesses:                   53 (-5.357143%)
  RAM Accesses:                 422 (No change)
  Estimated Cycles:       201839569 (+0.604882%)

with_contro_struct
  Instructions:           398958604 (-2.922689%)
  L1 Accesses:            486039573 (-2.880062%)
  L2 Accesses:               268253 (-2.837863%)
  RAM Accesses:               60270 (-0.881492%)
  Estimated Cycles:       489490288 (-2.871507%)

with_contro_struct_max_e
  Instructions:            10530711 (-0.443323%)
  L1 Accesses:             18891089 (-0.448614%)
  L2 Accesses:                   31 (-11.42857%)
  RAM Accesses:                 147 (-2.000000%)
  Estimated Cycles:        18896389 (-0.449144%)

with_pool_trreaded_1
  Instructions:           315589669 (+6.871786%)
  L1 Accesses:            476893082 (+7.340861%)
  L2 Accesses:              1013857 (+5.119001%)
  RAM Accesses:                4562 (+7.950781%)
  Estimated Cycles:       482122037 (+7.317211%)

without_pool_threaded_1
  Instructions:          1592248076 (-0.126900%)
  L1 Accesses:           2275109426 (-0.126566%)
  L2 Accesses:               690657 (-1.331895%)
  RAM Accesses:             1869740 (-0.093935%)
  Estimated Cycles:      2344003611 (-0.127453%)

with_control_threaded
  Instructions:           422849601 (+2.644515%)
  L1 Accesses:            517408546 (+2.586379%)
  L2 Accesses:               544627 (+7.458802%)
  RAM Accesses:               61171 (+0.468088%)
  Estimated Cycles:       522272666 (+2.601768%)

with_control_max_threaded
  Instructions:            25307689 (+1.314665%)
  L1 Accesses:             39575297 (+1.421190%)
  L2 Accesses:               133930 (+31.34770%)
  RAM Accesses:                1032 (-0.096805%)
  Estimated Cycles:        40281067 (+1.805416%)

with_pool_trreaded_2
  Instructions:           143394509 (-62.39048%)
  L1 Accesses:            214663837 (-59.65541%)
  L2 Accesses:               573324 (-0.901409%)
  RAM Accesses:                2404 (-3.955254%)
  Estimated Cycles:       217614597 (-59.32865%)

without_pool_threaded_2
  Instructions:          1593243729
  L1 Accesses:           2282509490
  L2 Accesses:               656238
  RAM Accesses:             1869806
  Estimated Cycles:      2351233890

with_control_threaded_2
  Instructions:           420788729 (-1.703631%)
  L1 Accesses:            515173403 (-1.684962%)
  L2 Accesses:               513862 (+7.084478%)
  RAM Accesses:               61745 (-0.221389%)
  Estimated Cycles:       519903788 (-1.639158%)

with_control_max_threaded_2
  Instructions:            25177171 (-0.562458%)
  L1 Accesses:             39298946 (-0.696887%)
  L2 Accesses:                98796 (+18.25059%)
  RAM Accesses:                 857 (-4.671858%)
  Estimated Cycles:        39822921 (-0.502252%)

```
</file>

<file path="stratum-1.4.0/utils/buffer/benches/control_struct.rs">
use buffer_sv2::{Buffer, Slice};
use std::sync::{Arc, Mutex};

use core::{sync::atomic::Ordering, time::Duration};
use rand::Rng;

const FILE_LEN: usize = 5242880;
pub const DATA: &[u8; FILE_LEN] = include_bytes!("../fuzz/random");

#[inline(always)]
pub fn add_random_bytes(message_len: usize, buffer: &mut impl Buffer, input: &[u8]) {
    let rounds = message_len / 10;

    for i in 0..rounds {
        let writable: &mut [u8] = buffer.get_writable(10).as_mut();
        writable.copy_from_slice(&input[i..i + 10]);
    }
}

pub trait Load: AsMut<[u8]> {
    fn load(&mut self) -> usize;
}

impl Load for Vec<u8> {
    #[inline(always)]
    fn load(&mut self) -> usize {
        self.len()
    }
}

impl Load for Slice {
    #[inline(always)]
    fn load(&mut self) -> usize {
        self.shared_state.load(Ordering::SeqCst) as usize
    }
}

#[inline(always)]
pub fn keep_slice(mut slice: impl Load + Send + 'static, d: Duration) {
    std::thread::spawn(move || {
        let _time = (2_usize.pow(16) - slice.as_mut().len()) / 1000;
        std::thread::sleep(d);
        let _ = slice.load();
    });
}

#[inline(always)]
pub fn bench_no_thread(mut pool: impl Buffer, data: &[u8]) {
    let mut rng = rand::thread_rng();
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);

        pool.get_data_owned();

        add_random_bytes(message_length, &mut pool, &data[i..]);

        pool.get_data_owned();
    }
}

impl Load for SSlice {
    #[inline(always)]
    fn load(&mut self) -> usize {
        78
    }
}

impl Load for MaxESlice {
    #[inline(always)]
    fn load(&mut self) -> usize {
        78
    }
}

use std::collections::BTreeMap;
pub struct PPool {
    pool: BTreeMap<u8, Vec<u8>>,
    free_slots: Vec<u8>,
    to_free: Arc<Mutex<Vec<u8>>>,
}

pub struct SSlice {
    offset: *mut u8,
    len: usize,
    index: u8,
    to_free: Arc<Mutex<Vec<u8>>>,
}

impl From<SSlice> for Slice {
    fn from(_v: SSlice) -> Self {
        todo!()
    }
}

impl AsMut<[u8]> for SSlice {
    #[inline(always)]
    fn as_mut(&mut self) -> &mut [u8] {
        unsafe { core::slice::from_raw_parts_mut(self.offset, self.len) }
    }
}

impl AsRef<[u8]> for SSlice {
    #[inline(always)]
    fn as_ref(&self) -> &[u8] {
        unsafe { core::slice::from_raw_parts(self.offset, self.len) }
    }
}

impl Drop for SSlice {
    fn drop(&mut self) {
        let mut to_free = self.to_free.lock().unwrap();
        to_free.push(self.index);
    }
}

impl PPool {
    pub fn new(capacity: usize) -> Self {
        let mut free_slots: Vec<u8> = Vec::with_capacity(255);
        let mut map: BTreeMap<u8, Vec<u8>> = BTreeMap::new();
        for k in 0..255 {
            let mut b = vec![0; capacity];
            unsafe { b.set_len(0) };
            map.insert(k, b);
            free_slots.push(k);
        }
        let to_free = Arc::new(Mutex::new(Vec::new()));
        Self {
            pool: map,
            free_slots,
            to_free,
        }
    }

    #[inline(never)]
    pub fn free(&mut self) {
        let mut slots = self.to_free.lock().unwrap();
        for _ in 0..slots.len() {
            let slot = slots.pop().unwrap();
            self.free_slots.push(slot);
            let b = self.pool.get_mut(&slot).unwrap();
            unsafe { b.set_len(0) };
        }
    }
}

impl Buffer for PPool {
    type Slice = SSlice;

    #[inline(always)]
    fn get_writable(&mut self, len: usize) -> &mut [u8] {
        if self.free_slots.len() > 0 {
            let slot = self.free_slots[self.free_slots.len() - 1];

            let b = self.pool.get_mut(&slot).unwrap();
            let offset = b.len();

            if offset + len <= b.capacity() {
                unsafe { b.set_len(offset + len) };
                &mut b[offset..offset + len]
            } else {
                panic!()
            }
        } else {
            self.free();
            self.get_writable(len)
        }
    }

    #[inline(always)]
    fn get_data_owned(&mut self) -> Self::Slice {
        let slot = self.free_slots.pop().unwrap();

        let b = self.pool.get_mut(&slot).unwrap();

        let offset = b.as_mut_ptr();

        SSlice {
            offset,
            len: b.len(),
            index: slot,
            to_free: self.to_free.clone(),
        }
    }

    fn get_data_by_ref(&mut self, _len: usize) -> &mut [u8] {
        todo!()
    }

    fn len(&self) -> usize {
        todo!()
    }
}

unsafe impl Send for SSlice {}

pub struct MaxEfficeincy {
    inner: Vec<u8>,
}

impl MaxEfficeincy {
    pub fn new(capacity: usize) -> Self {
        let inner = vec![0; capacity];
        Self { inner }
    }
}

pub struct MaxESlice {
    offset: *mut u8,
    len: usize,
}

impl From<MaxESlice> for Slice {
    fn from(_v: MaxESlice) -> Self {
        todo!()
    }
}

impl AsMut<[u8]> for MaxESlice {
    #[inline(always)]
    fn as_mut(&mut self) -> &mut [u8] {
        unsafe { core::slice::from_raw_parts_mut(self.offset, self.len) }
    }
}

impl AsRef<[u8]> for MaxESlice {
    #[inline(always)]
    fn as_ref(&self) -> &[u8] {
        unsafe { core::slice::from_raw_parts(self.offset, self.len) }
    }
}

unsafe impl Send for MaxESlice {}

impl Buffer for MaxEfficeincy {
    type Slice = MaxESlice;

    #[inline(always)]
    fn get_writable(&mut self, len: usize) -> &mut [u8] {
        &mut self.inner[0..len]
    }

    #[inline(always)]
    fn get_data_owned(&mut self) -> Self::Slice {
        let offset = self.inner.as_mut_ptr();

        MaxESlice {
            offset,
            len: self.inner.len(),
        }
    }

    fn get_data_by_ref(&mut self, _len: usize) -> &mut [u8] {
        todo!()
    }

    fn len(&self) -> usize {
        todo!()
    }
}
</file>

<file path="stratum-1.4.0/utils/buffer/benches/pool_benchmark.rs">
use core::sync::atomic::Ordering;
use criterion::{criterion_group, criterion_main, Criterion};

use buffer_sv2::{Buffer, BufferFromSystemMemory as BufferFromMemory, BufferPool as Pool, Slice};
use core::time::Duration;
use rand::Rng;

mod control_struct;
use control_struct::{
    add_random_bytes, bench_no_thread, keep_slice, Load, MaxESlice, MaxEfficeincy, PPool, SSlice,
    DATA,
};

fn with_pool(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = Pool::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn without_pool(data: &[u8]) {
    let buffer = BufferFromMemory::new(0);
    bench_no_thread(buffer, &data[..]);
}

fn with_contro_struct(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = PPool::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn with_contro_struct_max_e(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = MaxEfficeincy::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn with_pool_trreaded_1(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

// MAX two slice x time
fn with_pool_trreaded_2(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn without_pool_threaded_1(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut buffer = BufferFromMemory::new(0);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(16) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
    }
}

// MAX two slice x time
fn without_pool_threaded_2(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut buffer = BufferFromMemory::new(0);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(16) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn with_control_threaded(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = PPool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

fn with_control_threaded_2(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = PPool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn with_control_max_threaded(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = MaxEfficeincy::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

fn with_control_max_threaded_2(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = MaxEfficeincy::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

#[inline(always)]
fn keep_slice_test(mut slice: Slice, mut control: Vec<u8>) {
    std::thread::spawn(move || {
        let mut rng = rand::thread_rng();
        let ms = rng.gen_range(0..5);
        std::thread::sleep(core::time::Duration::from_millis(ms));
        let control: &mut [u8] = control.as_mut();
        if slice.as_mut() != control {
            panic!()
        }
    });
}

#[inline(always)]
fn add_random_bytes_test(
    m_len: usize,
    buffer: &mut Pool<BufferFromMemory>,
    input: &[u8],
    w_lens: Vec<u16>,
) -> Vec<u8> {
    let mut v = Vec::with_capacity(m_len);
    let mut i: usize = 0;
    let mut written = 0;
    //println!("START {}", buffer.is_alloc_mode());

    while written < m_len {
        //println!("{}", written);
        let w_len = w_lens[(w_lens.len() - 1) % (i + 1)] as usize;
        if written + w_len >= input.len() {
            break;
        };

        let writable: &mut [u8] = buffer.get_writable(w_len).as_mut();
        writable.copy_from_slice(&input[written..written + w_len]);
        v.extend_from_slice(&input[written..written + w_len]);

        assert!(
            &writable[..] == &input[written..written + w_len]
                && &writable[..] == &v[written..written + w_len]
        );

        written += w_len;
        i += 1;
    }
    //println!("END {}", buffer.is_back_mode());
    let i = buffer.get_data_by_ref(written);
    assert!(&v[..] == i);
    v
}

#[inline(always)]
fn keep_slice_test_p(mut slice: Slice, mut control: Vec<u8>, ms: u64) {
    let i = slice.index;
    std::thread::spawn(move || {
        std::thread::sleep(core::time::Duration::from_micros(ms));
        let control: &mut [u8] = control.as_mut();
        //if slice.index == 2 && slice.owned.is_none() {
        //    println!("{:#?}", slice);
        //}
        if slice.as_mut() != control {
            println!("{:?} {:?}", &slice.as_mut()[..20], &control[..20]);
            std::process::exit(9)
        }
    });
}

const MESSAGE_LENGTH: usize = 2_usize.pow(14);

fn with_pool_trreaded_test_1(data: &[u8]) {
    let w_lens = vec![10];
    let micros = vec![64511, 9471];

    let mut pool = Pool::new(130816);

    for i in 0..1000 {
        let ms = micros[(micros.len() - 1) % (i + 1)] as u64;
        let control = add_random_bytes_test(MESSAGE_LENGTH, &mut pool, &data[i..], w_lens.clone());
        let mut slice = pool.get_data_owned();
        if slice.as_mut() != control {
            println!("{:?} {:?}", &slice.as_mut()[..20], &control[..20]);
            std::process::exit(9)
        }
        keep_slice_test_p(slice, control, ms);
    }
}

fn with_pool_trreaded_test_2(data: &[u8]) {
    let capacity: usize = 2_usize.pow(16) * 10;
    let w_lens = vec![10];
    let micros = vec![1];

    let mut pool = Pool::new(capacity);

    for i in 0..1000 {
        let ms = micros[(micros.len() - 1) % (i + 1)] as u64;
        let control = add_random_bytes_test(MESSAGE_LENGTH, &mut pool, &data[i..], w_lens.clone());
        let mut slice = pool.get_data_owned();
        if slice.as_mut() != control {
            println!("{:?} {:?}", &slice.as_mut()[..20], &control[..20]);
            std::process::exit(9)
        }
        keep_slice_test_p(slice, control, ms);
    }
}

fn criterion_benchmark(c: &mut Criterion) {
    let input = DATA;

    let mut c = c.benchmark_group("sample-size-example");

    c.sample_size(500);
    //c.bench_function("with pool threaded test", |b| {
    //    b.iter(|| with_pool_trreaded_test_1(&input[..]))
    //});
    //c.bench_function("with pool threaded test 2", |b| {
    //    b.iter(|| with_pool_trreaded_test_2(&input[..]))
    //});
    c.bench_function("with pool", |b| b.iter(|| with_pool(&input[..])));
    c.bench_function("without pool", |b| b.iter(|| without_pool(&input[..])));
    c.bench_function("with control struct", |b| {
        b.iter(|| with_contro_struct(&input[..]))
    });
    c.bench_function("with control struct max e", |b| {
        b.iter(|| with_contro_struct_max_e(&input[..]))
    });
    c.bench_function("with pool thread 1", |b| {
        b.iter(|| with_pool_trreaded_1(&input[..]))
    });
    c.bench_function("without pool threaded 1", |b| {
        b.iter(|| without_pool_threaded_1(&input[..]))
    });
    c.bench_function("with control threaded 1", |b| {
        b.iter(|| with_control_threaded(&input[..]))
    });
    c.bench_function("with control threaded max", |b| {
        b.iter(|| with_control_max_threaded(&input[..]))
    });
    c.bench_function("with pool thread 2", |b| {
        b.iter(|| with_pool_trreaded_2(&input[..]))
    });
    c.bench_function("without pool threaded 2", |b| {
        b.iter(|| without_pool_threaded_2(&input[..]))
    });
    c.bench_function("with control threaded 2", |b| {
        b.iter(|| with_control_threaded_2(&input[..]))
    });
    c.bench_function("with control threaded max 2", |b| {
        b.iter(|| with_control_max_threaded_2(&input[..]))
    });
    c.finish();
}

criterion_group!(benches, criterion_benchmark);
criterion_main!(benches);
</file>

<file path="stratum-1.4.0/utils/buffer/benches/pool_iai.rs">
use buffer_sv2::{Buffer, BufferFromSystemMemory as BufferFromMemory, BufferPool as Pool};
use core::time::Duration;
use rand::Rng;

mod control_struct;
use control_struct::{add_random_bytes, bench_no_thread, keep_slice, MaxEfficeincy, PPool, DATA};

fn with_pool() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = Pool::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn without_pool() {
    let data = DATA;
    let buffer = BufferFromMemory::new(0);
    bench_no_thread(buffer, &data[..]);
}

fn with_contro_struct() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = PPool::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn with_contro_struct_max_e() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let pool = MaxEfficeincy::new(capacity);
    bench_no_thread(pool, &data[..]);
}

fn with_pool_trreaded_1() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

// MAX two slice x time
fn with_pool_trreaded_2() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn without_pool_threaded_1() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut buffer = BufferFromMemory::new(0);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(16) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
    }
}

// MAX two slice x time
fn without_pool_threaded_2() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = Pool::new(capacity);
    let mut buffer = BufferFromMemory::new(0);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(16) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(buffer.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn with_control_threaded() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = PPool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

fn with_control_threaded_2() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = PPool::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

fn with_control_max_threaded() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = MaxEfficeincy::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_micros(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
    }
}

fn with_control_max_threaded_2() {
    let data = DATA;
    let capacity: usize = 2_usize.pow(16) * 5;
    let mut pool = MaxEfficeincy::new(capacity);
    let mut rng = rand::thread_rng();
    let d = Duration::from_nanos(10);
    for i in 0..1000 {
        let message_length = 2_usize.pow(14) - rng.gen_range(0..12000);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        add_random_bytes(message_length, &mut pool, &data[i..]);
        keep_slice(pool.get_data_owned(), d);
        std::thread::sleep(d);
    }
}

iai::main!(
    with_pool,
    without_pool,
    with_contro_struct,
    with_contro_struct_max_e,
    with_pool_trreaded_1,
    without_pool_threaded_1,
    with_control_threaded,
    with_control_max_threaded,
    with_pool_trreaded_2,
    without_pool_threaded_2,
    with_control_threaded_2,
    with_control_max_threaded_2,
);
</file>

<file path="stratum-1.4.0/utils/buffer/Cargo.toml">
[package]
name = "buffer_sv2"
version = "2.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2018"
description = "buffer"
documentation = "https://docs.rs/buffer_sv2"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]

[dependencies]
criterion = {version = "0.3", optional = true}
aes-gcm = { version = "0.10.2", features = ["alloc", "aes"], default-features = false }

[dev-dependencies]
rand = "0.8.3"
iai = "0.1"


[[bench]]
name = "pool_benchmark"
harness = false

[[bench]]
name = "pool_iai"
harness = false

[features]
debug = []
fuzz = []
</file>

<file path="stratum-1.4.0/utils/buffer/examples/basic_buffer_pool.rs">
// # Simple `BufferPool` Usage
//
// This example showcases how to:
// 1. Creating a `BufferPool`.
// 2. Obtaining a writable buffer.
// 3. Writing data into the buffer.
// 4. Retrieving the data as a referenced slice.
// 5. Retrieving the data as an owned slice.
//
// # Run
//
// ```
// cargo run --example basic_buffer_pool
// ```

use buffer_sv2::{Buffer, BufferPool};

fn main() {
    // Create a new BufferPool with a capacity of 32 bytes
    let mut buffer_pool = BufferPool::new(32);

    // Get a writable buffer from the pool
    let data_to_write = b"Ciao, mundo!"; // 12 bytes
    let writable = buffer_pool.get_writable(data_to_write.len());

    // Write data (12 bytes) into the buffer.
    writable.copy_from_slice(data_to_write);
    assert_eq!(buffer_pool.len(), 12);

    // Retrieve the data as a referenced slice
    let _data_slice = buffer_pool.get_data_by_ref(12);
    assert_eq!(buffer_pool.len(), 12);

    // Retrieve the data as an owned slice
    let data_slice = buffer_pool.get_data_owned();
    assert_eq!(buffer_pool.len(), 0);

    let expect = [67, 105, 97, 111, 44, 32, 109, 117, 110, 100, 111, 33]; // "Ciao, mundo!" ASCII
    assert_eq!(data_slice.as_ref(), expect);
}
</file>

<file path="stratum-1.4.0/utils/buffer/examples/buffer_pool_exhaustion.rs">
// # Handling Buffer Pool Exhaustion and Heap Allocation
//
// This example demonstrates how a buffer pool is filled. The back slots of the buffer pool are
// exhausted first, followed by the front of the buffer pool. Once both the back and front are
// exhausted, data is allocated on the heap at a performance decrease.
//
// 1. Fills up the back slots of the buffer pool until theyre exhausted.
// 2. Releases one slot to allow the buffer pool to switch to front mode.
// 3. Fully fills the front slots of the buffer pool.
// 4. Switches to alloc mode for direct heap allocation when both the buffer pool's back and front
//    slots are at capacity.
//
// Below is a visual representation of how the buffer pool evolves as the example progresses:
//
// --------  BACK MODE
// a-------  BACK MODE (add "a" via loop)
// aa------  BACK MODE (add "a" via loop)
// aaa-----  BACK MODE (add "a" via loop)
// aaaa----  BACK MODE (add "a" via loop)
// -aaa----  BACK MODE (pop front)
// -aaab---  BACK MODE (add "b")
// -aaabc--  BACK MODE (add "c" via loop)
// -aaabcc-  BACK MODE (add "c" via loop)
// -aaabccc  BACK MODE (add "c" via loop)
// caaabccc  BACK MODE (add "c" via loop, which gets added via front mode)
// caaabccc  ALLOC MODE (add "d", allocated in a new space in the heap)
//
// # Run
//
// ```
// cargo run --example buffer_pool_exhaustion
// ```

use buffer_sv2::{Buffer, BufferPool};
use std::collections::VecDeque;

fn main() {
    // 8 byte capacity
    let mut buffer_pool = BufferPool::new(8);
    let mut slices = VecDeque::new();

    // Write data to fill back slots
    for _ in 0..4 {
        let data_bytes = b"a"; // 1 byte
        let writable = buffer_pool.get_writable(data_bytes.len()); // Mutable slice to internal
                                                                   // buffer
        writable.copy_from_slice(data_bytes);
        let data_slice = buffer_pool.get_data_owned(); // Take ownership of allocated segment
        slices.push_back(data_slice);
    }
    assert!(buffer_pool.is_back_mode());

    // Release one slice and add another in the back (one slice in back mode must be free to switch
    // to front mode)
    slices.pop_front(); // Free the slice's associated segment in the buffer pool
    let data_bytes = b"b"; // 1 byte
    let writable = buffer_pool.get_writable(data_bytes.len());
    writable.copy_from_slice(data_bytes);
    let data_slice = buffer_pool.get_data_owned();
    slices.push_back(data_slice);
    assert!(buffer_pool.is_back_mode()); // Still in back mode

    // Write data to switch to front mode
    for _ in 0..4 {
        let data_bytes = b"c"; // 1 byte
        let writable = buffer_pool.get_writable(data_bytes.len());
        writable.copy_from_slice(data_bytes);
        let data_slice = buffer_pool.get_data_owned();
        slices.push_back(data_slice);
    }
    assert!(buffer_pool.is_front_mode()); // Confirm front mode

    // Add another slice, causing a switch to alloc mode
    let data_bytes = b"d"; // 1 byte
    let writable = buffer_pool.get_writable(data_bytes.len());
    writable.copy_from_slice(data_bytes);
    let data_slice = buffer_pool.get_data_owned();
    slices.push_back(data_slice);
    assert!(buffer_pool.is_alloc_mode());
}
</file>

<file path="stratum-1.4.0/utils/buffer/examples/variable_sized_messages.rs">
// # Handling Variable-Sized Messages
//
// This example demonstrates how to the `BufferPool` handles messages of varying sizes.
//
// # Run
//
// ```
// cargo run --example variable_sized_messages
// ```

use buffer_sv2::{Buffer, BufferPool};
use std::collections::VecDeque;

fn main() {
    // Initialize a BufferPool with a capacity of 32 bytes
    let mut buffer_pool = BufferPool::new(32);
    let mut slices = VecDeque::new();

    // Function to write data to the buffer pool and store the slice
    let write_data = |pool: &mut BufferPool<_>, data: &[u8], slices: &mut VecDeque<_>| {
        let writable = pool.get_writable(data.len());
        writable.copy_from_slice(data);
        let data_slice = pool.get_data_owned();
        slices.push_back(data_slice);
        println!("{:?}", &pool);
        println!();
    };

    // Write a small message to the first slot
    let small_message = b"Hello";
    write_data(&mut buffer_pool, small_message, &mut slices);
    assert!(buffer_pool.is_back_mode());
    assert_eq!(slices.back().unwrap().as_ref(), small_message);

    // Write a medium-sized message to the second slot
    let medium_message = b"Rust programming";
    write_data(&mut buffer_pool, medium_message, &mut slices);
    assert!(buffer_pool.is_back_mode());
    assert_eq!(slices.back().unwrap().as_ref(), medium_message);

    // Write a large message that exceeds the remaining pool capacity
    let large_message = b"This message is larger than the remaining buffer pool capacity.";
    write_data(&mut buffer_pool, large_message, &mut slices);
    assert!(buffer_pool.is_alloc_mode());
    assert_eq!(slices.back().unwrap().as_ref(), large_message);

    while let Some(slice) = slices.pop_front() {
        drop(slice);
    }

    // Write another small message
    let another_small_message = b"Hi";
    write_data(&mut buffer_pool, another_small_message, &mut slices);
    assert_eq!(slices.back().unwrap().as_ref(), another_small_message);

    // Verify that the buffer pool has returned to back mode for the last write
    assert!(buffer_pool.is_back_mode());
}
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/.gitignore">
target
corpus
artifacts
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/Cargo.toml">
[package]
name = "buffer-fuzz"
version = "0.0.0"
authors = ["Automatically generated"]
publish = false
edition = "2018"

[package.metadata]
cargo-fuzz = true

[dependencies]
libfuzzer-sys = { version = "0.4.0", features = ["arbitrary-derive"] }
arbitrary = { version = "1", features = ["derive"] }
rand = "0.8.3"
buffer_sv2 = {version = "2.0.0", path = "..", features = ["fuzz"]}
affinity = "0.1.1"
threadpool = "1.8.1"
lazy_static = "1.4.0"

# Prevent this from interfering with workspaces
[workspace]
members = ["."]

[[bin]]
name = "slower"
path = "fuzz_targets/slower.rs"
test = false
doc = false

[[bin]]
name = "faster"
path = "fuzz_targets/faster.rs"
test = false
doc = false
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/fuzz_targets/faster.rs">
#![no_main]
use arbitrary::Arbitrary;
use buffer_sv2::BufferPool as Pool;
use buffer_sv2::{Buffer, Slice};
use libfuzzer_sys::fuzz_target;
use std::fs::File;
use std::io::Read;
use std::sync::{Arc, Mutex};
use std::time::Duration;
use threadpool::ThreadPool;
#[macro_use]
extern crate lazy_static;

// A global ThreadPool is used cause after the creation of ~4M threads asan report an error
lazy_static! {
    static ref T_POOL: Arc<Mutex<ThreadPool>> = Arc::new(Mutex::new(ThreadPool::new(1000)));
}

#[inline(always)]
fn add_random_bytes_test(bytes: usize, buffer: &mut impl Buffer, input: &[u8]) -> Vec<u8> {
    let rounds = bytes / 10;

    let mut v = Vec::with_capacity(bytes);

    for _ in 0..rounds {
        let writable: &mut [u8] = buffer.get_writable(input.len());
        if writable.is_empty() {
            return v;
        }
        writable.copy_from_slice(input);
        v.extend_from_slice(input);
    }
    v
}

#[inline(always)]
fn keep_slice_test(mut slice: Slice, mut control: Vec<u8>, ms: u64, t_pool: &mut ThreadPool) {
    t_pool.execute(move || {
        #[allow(deprecated)]
        std::thread::sleep(Duration::from_micros(ms));
        let control: &mut [u8] = control.as_mut();
        if slice.as_mut() != control {
            println!("{:?}", slice);
            let slice = slice.as_mut();
            println!(
                "{:?} {} {:?} {}",
                &slice[..5],
                slice.len(),
                &control[..5],
                control.len()
            );
            assert!(slice == control);
        }
    })
}

const MESSAGE_LENGTH: usize = 2_usize.pow(14);

#[derive(Arbitrary, Debug)]
struct Input {
    micros: Vec<u16>,
    capacity: u32,
}

fuzz_target!(|data: Input| {
    let mut t_pool = T_POOL.lock().unwrap();
    let mut data = data;
    if data.micros.is_empty() {
        data.micros.push(1);
    }

    let mut file = File::open("random").unwrap();
    let mut input: Vec<u8> = Vec::new();
    file.read_to_end(&mut input).unwrap();

    let mut pool = Pool::new(data.capacity as usize);

    for i in 0..1000 {
        let ms = data.micros[(data.micros.len() - 1) % (i + 1)] as u64;
        let control = add_random_bytes_test(MESSAGE_LENGTH, &mut pool, &input[i..i + 10]);
        keep_slice_test(pool.get_data_owned(), control, ms, &mut t_pool);
    }

    // Otherway asan will complain
    t_pool.join();
});
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/fuzz_targets/slower.rs">
#![no_main]
use affinity::{get_core_num, set_thread_affinity};
use arbitrary::Arbitrary;
use buffer_sv2::BufferPool as Pool;
use buffer_sv2::{Buffer, Slice};
use libfuzzer_sys::fuzz_target;
use std::fs::File;
use std::io::Read;
use std::sync::{Arc, Mutex};
use std::time::Duration;
use threadpool::ThreadPool;
#[macro_use]
extern crate lazy_static;

// A global ThreadPool is used cause after the creation of ~4M threads asan report an error
lazy_static! {
    static ref T_POOL: Arc<Mutex<ThreadPool>> = Arc::new(Mutex::new(ThreadPool::new(1000)));
}

#[inline(always)]
fn add_random_bytes_test(
    m_len: usize,
    buffer: &mut impl Buffer,
    input: &[u8],
    w_lens: Vec<u16>,
) -> Vec<u8> {
    let mut v = Vec::with_capacity(m_len);
    let mut i: usize = 0;
    let mut written = 0;
    let now = std::time::SystemTime::now();

    while written < m_len {
        let w_len = w_lens[(w_lens.len() - 1) % (i + 1)] as usize;
        if written + w_len >= input.len() {
            break;
        };

        let writable: &mut [u8] = buffer.get_writable(w_len);
        writable.copy_from_slice(&input[written..written + w_len]);
        v.extend_from_slice(&input[written..written + w_len]);

        written += w_len;
        i += 1;

        let elapsed = now.elapsed().unwrap().as_millis();
        if elapsed > 500 {
            println!();
            println!("The input is taking more than half seconds:");
            break;
        }
    }

    v
}

#[inline(always)]
fn keep_slice_test(
    mut slice: Slice,
    mut control: Vec<u8>,
    ms: u64,
    core: usize,
    t_pool: &mut ThreadPool,
) {
    t_pool.execute(move || {
        set_thread_affinity(&[core][..]).unwrap();
        #[allow(deprecated)]
        std::thread::sleep(Duration::from_micros(ms));
        let control: &mut [u8] = control.as_mut();
        if slice.as_mut() != control {
            println!("{:?} {:?}", &slice.as_mut()[..20], &control[..20]);
            assert!(slice.as_mut() == control);
        }
    });
}

const MESSAGE_LENGTH: u32 = 2_u32.pow(14); // - rng.gen_range(0..12000);
const MAX_MESSAGE_LEN: usize = 9_000_000;

#[derive(Arbitrary, Debug)]
struct Input {
    micros: Vec<u16>,
    rounds: (u8, u8, u8),
    m_len: Vec<u32>,
    w_len: Vec<u16>,
    capacity: u32,
}

fuzz_target!(|data: Input| {
    let mut t_pool = T_POOL.lock().unwrap();
    let mut file = File::open("random").unwrap();
    let mut input: Vec<u8> = Vec::new();
    file.read_to_end(&mut input).unwrap();

    let mut data = data;

    if data.micros.is_empty() {
        data.micros.push(1);
    }

    if data.m_len.is_empty() {
        data.m_len.push(MESSAGE_LENGTH)
    }

    if data.w_len.is_empty() {
        data.w_len.push(10)
    }

    let rounds = data.rounds.0 as usize + data.rounds.1 as usize + data.rounds.2 as usize;

    let mut pool = Pool::new(data.capacity as usize);

    let cores: Vec<usize> = (0..get_core_num()).collect();

    let now = std::time::SystemTime::now();

    for i in 0..rounds {
        if i >= input.len() {
            break;
        }

        let message_len = data.m_len[(data.m_len.len() - 1) % (i + 1)] as usize;

        // Too big messages take too much time to be written
        if message_len > MAX_MESSAGE_LEN {
            continue;
        }

        let ms = data.micros[(data.micros.len() - 1) % (i + 1)] as u64;

        let core = cores[(cores.len() - 1) % (i + 1)];

        let control =
            add_random_bytes_test(message_len, &mut pool, &input[i..], data.w_len.clone());
        keep_slice_test(pool.get_data_owned(), control, ms, core, &mut t_pool);

        // If an input is taking more than 1'' just print the input and pass to the next one
        let elapsed = now.elapsed().unwrap().as_secs();
        if elapsed > 1 {
            println!();
            println!("The input is taking more than 1 seconds:");
            break;
        }
    }

    // Otherway asan will complain
    t_pool.join();
});
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/run.sh">
#! /bin/sh
set -ex

rustup toolchain install nightly
cargo +nightly install cargo-fuzz
cargo +nightly --version
cargo +nightly fuzz run faster -- -rss_limit_mb=5000000000 -runs=1000
cargo +nightly fuzz run slower -- -rss_limit_mb=5000000000 -runs=1000
</file>

<file path="stratum-1.4.0/utils/buffer/fuzz/rust-toolchain.toml">
[toolchain]
channel = "nightly-2020-04-06"
components = [ "rustfmt", "rustc-dev" ]
targets = [ "x86_64-unknown-linux-gnu"]
profile = "minimal"
</file>

<file path="stratum-1.4.0/utils/buffer/README.md">
# `buffer_sv2`

[![crates.io](https://img.shields.io/crates/v/buffer_sv2.svg)](https://crates.io/crates/buffer_sv2)
[![docs.rs](https://docs.rs/buffer_sv2/badge.svg)](https://docs.rs/buffer_sv2)
[![rustc+](https://img.shields.io/badge/rustc-1.75.0%2B-lightgrey.svg)](https://blog.rust-lang.org/2023/12/28/Rust-1.75.0.html)
[![license](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](https://github.com/stratum-mining/stratum/blob/main/LICENSE.md)
[![codecov](https://codecov.io/gh/stratum-mining/stratum/branch/main/graph/badge.svg?flag=buffer_sv2-coverage)](https://codecov.io/gh/stratum-mining/stratum)

`buffer_sv2` handles memory management for Stratum V2 (Sv2) roles. It provides a memory-efficient
buffer pool that minimizes allocations and deallocations for high-throughput message frame
processing in Sv2 roles. Memory allocation overhead is minimized by reusing large buffers,
improving performance and reducing latency. The buffer pool tracks the usage of memory slices,
using shared state tracking to safely manage memory across multiple threads.

## Main Components

- **Buffer Trait**: An interface for working with memory buffers. This trait has two implementations
  (`BufferPool` and `BufferFromSystemMemory`) that includes a `Write` trait to replace
  `std::io::Write` in `no_std` environments.
- **BufferPool**: A thread-safe pool of reusable memory buffers for high-throughput applications.
- **BufferFromSystemMemory**: Manages a dynamically growing buffer in system memory for applications
  where performance is not a concern.
- **Slice**: A contiguous block of memory, either preallocated or dynamically allocated.

## Usage

To include this crate in your project, run:

```bash
cargo add buffer_sv2
```

This crate can be built with the following feature flags:

- `debug`: Provides additional tracking for debugging memory management issues.
- `fuzz`: Enables support for fuzz testing.

### Unsafe Code
There are four unsafe code blocks instances:

- `buffer_pool/mod.rs`: `fn get_writable_(&mut self, len: usize, shared_state: u8, without_check: bool) -> &mut [u8] { .. }` in the `impl<T: Buffer> BufferPool<T>`
- `slice.rs`:
  - `unsafe impl Send for Slice {}`
  - `fn as_mut(&mut self) -> &mut [u8] { .. }` in the `impl AsMut<[u8]> for Slice`
  - `fn as_ref(&mut self) -> &mut [u8] { .. }` in the `impl AsMut<[u8]> for Slice`

### Examples

This crate provides three examples demonstrating how the memory is managed:

1. **[Basic Usage Example](https://github.com/stratum-mining/stratum/blob/main/utils/buffer/examples/basic_buffer_pool.rs)**:
   Creates a buffer pool, writes to it, and retrieves the data from it.

2. **[Buffer Pool Exhaustion Example](https://github.com/stratum-mining/stratum/blob/main/utils/buffer/examples/buffer_pool_exhaustion.rs)**:
   Demonstrates how data is added to a buffer pool and dynamically allocates directly to the heap
   once the buffer pool's capacity has been exhausted.

3. **[Variable Sized Messages Example](https://github.com/stratum-mining/stratum/blob/main/utils/buffer/examples/variable_sized_messages.rs)**:
   Writes messages of variable sizes to the buffer pool.

## `Buffer` Trait

The `Buffer` trait is designed to work with the
[`codec_sv2`](https://docs.rs/codec_sv2/1.3.0/codec_sv2/index.html) decoders, which operate by:

1. Filling a buffer with the size of the protocol header being decoded.
2. Parsing the filled bytes to compute the message length.
3. Filling a buffer with the size of the message.
4. Using the header and message to construct a
   [`framing_sv2::framing::Frame`](https://docs.rs/framing_sv2/2.0.0/framing_sv2/framing/enum.Frame.html).

To fill the buffer, the `codec_sv2` decoder must pass a reference of the buffer to a filler. To
construct a `Frame`, the decoder must pass ownership of the buffer to the `Frame`.

```rust
fn get_writable(&mut self, len: usize) -> &mut [u8];
```

This `get_writable` method returns a mutable reference to the buffer, starting at the current
length and ending at `len`, and sets the buffer length to the previous length plus `len`.

```rust
get_data_owned(&mut self) -> Slice;
```

This `get_data_owned` method returns a `Slice` that implements `AsMut<[u8]>` and `Send`.

The `Buffer` trait is implemented for `BufferFromSystemMemory` and `BufferPool`. It includes a
`Write` trait to replace `std::io::Write` in `no_std` environments.

## `BufferPoolFromSystemMemory`
`BufferFromSystemMemory` is a simple implementation of the `Buffer` trait. Each time a new buffer is
needed, it creates a new `Vec<u8>`.

- `get_writable(..)` returns mutable references to the inner vector.
- `get_data_owned(..)` returns the inner vector.

## `BufferPool`
While `BufferFromSystemMemory` is sufficient for many cases, `BufferPool` offers a more efficient
solution for high-performance applications, such as proxies and pools with thousands of connections.

When created, `BufferPool` preallocates a user-defined capacity of bytes in the heap using a
`Vec<u8>`. When `get_data_owned(..)` is called, it creates a `Slice` that contains a view into the
preallocated memory. `BufferPool` guarantees that slices never overlap and maintains unique
ownership of each `Slice`.

`Slice` implements the `Drop`, allowing the view into the preallocated memory to be reused upon
dropping.

### Buffer Management and Allocation

`BufferPool` is useful for working with sequentially processed buffers, such as filling a buffer,
retrieving it, and then reusing it as needed. `BufferPool` optimizes for memory reuse by providing
pre-allocated memory that can be used in one of three modes:

1. **Back Mode**: Default mode where allocations start from the back of the buffer.
2. **Front Mode**: Used when slots at the back are full but memory can still be reused by moving to
   the front.
3. **Alloc Mode**: Falls back to system memory allocation (`BufferFromSystemMemory`) when both back
   and front sections are full, providing additional capacity but with reduced performance.

`BufferPool` can only be fragmented between the front and back and between back and end.

#### Fragmentation, Overflow, and Optimization
`BufferPool` can allocate a maximum of `8` `Slice`s (as it uses an `AtomicU8` to track used and
freed slots) and up to the defined capacity in bytes. If all `8` slots are taken or there is no more
space in the preallocated memory, `BufferPool` falls back to `BufferFromSystemMemory`.

Typically, `BufferPool` is used to process messages sequentially (decode, respond, decode). It is
optimized to check for any freed slots starting from the beginning, then reuse these before
considering further allocation. It is also optimized to drop all the slices and to drop the last
slice. It also efficiently handles scenarios where all slices are dropped or when the last slice is
released, reducing memory fragmentation.

The following cases illustrate typical memory usage patterns within `BufferPool`:
1. Slots fill from back to front, switching as each area reaches capacity.
2. Pool resets upon full usage, then reuses back slots.
3. After filling the back, front slots are used when they become available.

Below is a graphical representation of the most optimized cases. A number means that the slot is
taken, the minus symbol (`-`) means the slot is free. There are `8` slots.

Case 1: Buffer pool exhaustion
```
--------  BACK MODE
1-------  BACK MODE
12------  BACK MODE
123-----  BACK MODE
1234----  BACK MODE
12345---  BACK MODE
123456--  BACK MODE
1234567-  BACK MODE
12345678  BACK MODE (buffer is now full)
12345678  ALLOC MODE (new bytes being allocated in a new space in the heap)
12345678  ALLOC MODE (new bytes being allocated in a new space in the heap)
..... and so on
```

Case 2: Buffer pool reset to remain in back mode
```
--------  BACK MODE
1-------  BACK MODE
12------  BACK MODE
123-----  BACK MODE
1234----  BACK MODE
12345---  BACK MODE
123456--  BACK MODE
1234567-  BACK MODE
12345678  BACK MODE (buffer is now full)
--------  RESET
9-------  BACK MODE
9a------  BACK MODE
```

Case 3: Buffer pool switches from back to front to back modes
```
--------  BACK MODE
1-------  BACK MODE
12------  BACK MODE
123-----  BACK MODE
1234----  BACK MODE
12345---  BACK MODE
123456--  BACK MODE
1234567-  BACK MODE
12345678  BACK MODE (buffer is now full)
--345678  Consume first two data bytes from the buffer
-9345678  SWITCH TO FRONT MODE
a9345678  FRONT MODE (buffer is now full)
a93456--  Consume last two data bytes from the buffer
a93456b-  SWITCH TO BACK MODE
a93456bc  BACK MODE (buffer is now full)
```

## Benchmarks and Performance

To run benchmarks, execute:

```
cargo bench --features criterion
```

## Benchmarks Comparisons

`BufferPool` is benchmarked against `BufferFromSystemMemory` and two additional structure for
reference: `PPool` (a hashmap-based pool) and `MaxEfficeincy` (a highly optimized but unrealistic
control implementation written such that the benchmarks do not panic and the compiler does not
complain). `BufferPool` generally provides better performance and lower latency than `PPool` and
`BufferFromSystemMemory`.

**Note**: Both `PPool` and `MaxEfficeincy` are completely broken and are only useful as references
for the benchmarks.

### `BENCHES.md` Benchmarks
The `BufferPool` always outperforms the `PPool` (hashmap-based pool) and the solution without a
pool.

Executed for 2,000 samples:

```
* single thread with  `BufferPool`: ---------------------------------- 7.5006 ms
* single thread with  `BufferFromSystemMemory`: ---------------------- 10.274 ms
* single thread with  `PPoll`: --------------------------------------- 32.593 ms
* single thread with  `MaxEfficeincy`: ------------------------------- 1.2618 ms
* multi-thread with   `BufferPool`: ---------------------------------- 34.660 ms
* multi-thread with   `BufferFromSystemMemory`: ---------------------- 142.23 ms
* multi-thread with   `PPoll`: --------------------------------------- 49.790 ms
* multi-thread with   `MaxEfficeincy`: ------------------------------- 18.201 ms
* multi-thread 2 with `BufferPool`: ---------------------------------- 80.869 ms
* multi-thread 2 with `BufferFromSystemMemory`: ---------------------- 192.24 ms
* multi-thread 2 with `PPoll`: --------------------------------------- 101.75 ms
* multi-thread 2 with `MaxEfficeincy`: ------------------------------- 66.972 ms
```

### Single Thread Benchmarks

If the buffer is not sent to another context `BufferPool`, it is 1.4 times faster than no pool, 4.3
time faster than the `PPool`, and 5.7 times slower than max efficiency.

Average times for 1,000 operations:

- `BufferPool`: 7.5 ms
- `BufferFromSystemMemory`: 10.27 ms
- `PPool`: 32.59 ms
- `MaxEfficiency`: 1.26 ms

```
for 0..1000:
  add random bytes to the buffer
  get the buffer
  add random bytes to the buffer
  get the buffer
  drop the 2 buffer
```

### Multi-Threaded Benchmarks (most similar to actual use case)

If the buffer is sent to other contexts, `BufferPool` is 4 times faster than no pool, 0.6 times
faster than `PPool`, and 1.8 times slower than max efficiency.

- `BufferPool`: 34.66 ms
- `BufferFromSystemMemory`: 142.23 ms
- `PPool`: 49.79 ms
- `MaxEfficiency`: 18.20 ms

```
for 0..1000:
  add random bytes to the buffer
  get the buffer
  send the buffer to another thread   -> wait 1 ms and then drop it
  add random bytes to the buffer
  get the buffer
  send the buffer to another thread   -> wait 1 ms and then drop it
```

### Multi threads 2
```
for 0..1000:
  add random bytes to the buffer
  get the buffer
  send the buffer to another thread   -> wait 1 ms and then drop it
  add random bytes to the buffer
  get the buffer
  send the buffer to another thread   -> wait 1 ms and then drop it
  wait for the 2 buffer to be dropped
```

## Fuzz Testing
Install `cargo-fuzz` with:

```bash
cargo install cargo-fuzz
```

Run the fuzz tests:

```bash
cd ./fuzz
cargo fuzz run slower -- -rss_limit_mb=5000000000
cargo fuzz run faster -- -rss_limit_mb=5000000000
```
The test must be run with `-rss_limit_mb=5000000000` as this flag checks `BufferPool` with
capacities from `0` to `2^32`.

`BufferPool` is fuzz-tested to ensure memory reliability across different scenarios, including
delayed memory release and cross-thread access. The tests checks if slices created by `BufferPool`
still contain the same bytes contained at creation time after a random amount of time and after it
has been sent to other threads.

There are 2 fuzzy test, the first (faster) it map a smaller input space to
Two main fuzz tests are provided:

1. Faster: Maps a smaller input space to test the most likely inputs
2. Slower: Has a bigger input space to explore "all" the edge case. It forces the buffer to be sent
   to different cores.

Both tests have been run for several hours without crashes.
</file>

<file path="stratum-1.4.0/utils/buffer/src/buffer_pool/mod.rs">
// # Buffer Pool
//
// A memory-efficient buffer pool that minimizes allocations and deallocations for high-throughput
// message frame processing in Sv2 roles.
//
// Provides primitives for reusing memory buffers, clearing old memory chunks, and switching
// between different modes (back, front, and alloc) to manage memory effectively and reduce
// expensive heap allocations. It uses atomic operations and shared state tracking to safely manage
// memory across multiple threads.
//
// Supports different allocation modes to optimize memory usage:
//
// - **Back Mode**: Allocates from the back of the buffer pool (default).
// - **Front Mode**: Allocates from the front when the back is full the front has space.
// - **Alloc Mode**: Falls back to heap allocation when the buffer pool cannot fulfill requests
//   (with reduced performance).
//
// ## Usage
//
// When an incoming Sv2 message is received, it needs to be buffered for processing. The pool first
// checks the back part (`PoolBack`) to see if there is enough memory available. If the back is
// full, the pool attempts to clear used memory chunks. If clearing fails, it switches to the front
// (`PoolFront`) and tries again, this time allocating memory from the front of the `BufferPool`.
// If both back and front are full and no memory can be cleared, the pool falls back to allocating
// fresh memory from the heap (`PoolMode::Alloc`) at a performance reduction. After processing the
// message, the memory can be cleared, and the buffer pool resets, making the memory available
// for future messages.

// **Note**: To prevent leaks or deadlocks, ensure that memory slices are properly released after
// use by allowing them to go out of scope or explicitly dropping them. Avoid holding onto slices
// longer than necessary or cloning them. After processing, you can obtain ownership of the data
// using methods like `get_data_owned()` and then let the slice be dropped.

use alloc::{vec, vec::Vec};
use core::sync::atomic::Ordering;

#[cfg(test)]
use crate::buffer::TestBufferFromMemory;
use crate::{
    buffer::BufferFromSystemMemory,
    slice::{SharedState, Slice},
    Buffer,
};
#[cfg(feature = "debug")]
use std::time::SystemTime;

use aes_gcm::aead::Buffer as AeadBuffer;

mod pool_back;
pub use pool_back::PoolBack;

// Maximum number of memory slices the buffer pool can concurrently manage.
//
// This value limits the number of slices the `BufferPool` can track and manage at once. Once the
// pool reaches its capacity of `8` slices, it may need to free memory or switch modes (e.g., to
// system memory). The choice of `8` ensures a balance between performance and memory management.
// The use of `usize` allows for compatibility with platform-dependent memory operations.
pub const POOL_CAPACITY: usize = 8;

// Manages the "front" section of the `BufferPool`.
//
// Handles the allocation of memory slices at the front of the buffer pool. It tracks the number of
// slices in use and attempts to free unused slices when necessary to maximize available memory.
// The front of the buffer pool is used if the back of the buffer pool is filled up.
#[derive(Debug, Clone)]
pub struct PoolFront {
    // Starting index of the front section of the buffer pool.
    back_start: usize,

    // Maximum number of bytes that can be allocated in the front section of the buffer pool.
    //
    // Helps manage how much memory can be used before triggering memory clearing or switching
    // `PoolMode`.
    byte_capacity: usize,

    // Number of allocated slices in the front section of the buffer pool.
    len: usize,
}

impl PoolFront {
    // Initializes a new `PoolFront` with the specified byte capacity and back start position.
    #[inline(always)]
    fn new(byte_capacity: usize, back_start: usize) -> Self {
        Self {
            byte_capacity,
            back_start,
            len: 0,
        }
    }

    // Attempts to clear unused memory slices at the tail of the front section.
    //
    // Returns `true` if slices were successfully cleared, otherwise `false` if no slices could be
    // cleared or memory conditions prevent clearing.
    #[inline(always)]
    fn try_clear_tail(&mut self, memory: &mut InnerMemory, mut shared_state: u8) -> bool {
        #[cfg(feature = "fuzz")]
        assert!(self.len > 0);
        // 1001_1100   bs=3 len=1
        // 0000_0001   >> 7
        shared_state >>= POOL_CAPACITY - self.len;

        let element_to_drop = shared_state.trailing_zeros();

        match element_to_drop {
            0 => false,
            8 => {
                self.len = 0;

                let raw_offset = memory.raw_offset();
                memory.move_raw_at_offset_unchecked(raw_offset);

                true
            }
            _ => {
                #[cfg(feature = "fuzz")]
                assert!(
                    element_to_drop <= self.back_start as u32 && element_to_drop <= self.len as u32
                );

                self.len -= element_to_drop as usize;

                memory.len = self.len;
                let raw_offset = memory.raw_offset();
                memory.move_raw_at_offset_unchecked(raw_offset);

                true
            }
        }
    }

    // Clears the front memory slices if conditions allow and checks if the memory pool has
    // capacity to allocate `len` bytes in the buffer.
    //
    // Returns `Ok` if memory was successfully cleared and there is sufficient capacity, otherwise
    // an `Err(PoolMode::Back)` if the memory cannot be cleared or lacks capacity. This error
    // indicates the `BufferPool` should attempt a transition to use the back of the buffer pool.
    #[inline(always)]
    fn clear(
        &mut self,
        memory: &mut InnerMemory,
        shared_state: u8,
        len: usize,
    ) -> Result<(), PoolMode> {
        if self.len > 0 && self.try_clear_tail(memory, shared_state) {
            if memory.has_capacity_until_offset(len, self.byte_capacity) {
                Ok(())
            } else {
                Err(PoolMode::Back)
            }
        } else {
            Err(PoolMode::Back)
        }
    }

    // Attempts to allocate a writable memory region in the front section of the buffer pool,
    // returning a writable slice if successful, or transitioning to a new pool mode if necessary.
    //
    // Returns a pointer to the writable memory (`Ok(*mut u8)`) if successful, otherwise an
    // `Err(PoolMode::Back)` if the memory cannot be cleared or lacks capacity. This error
    // indicates the `BufferPool` should attempt a transition to use the back of the buffer pool.
    #[inline(always)]
    fn get_writable(
        &mut self,
        len: usize,
        memory: &mut InnerMemory,
        shared_state: u8,
    ) -> Result<*mut u8, PoolMode> {
        let pool_has_slice_capacity = self.len < self.back_start;
        let pool_has_head_capacity = memory.has_capacity_until_offset(len, self.byte_capacity);

        if pool_has_slice_capacity && pool_has_head_capacity {
            return Ok(memory.get_writable_raw_unchecked(len));
        };

        self.clear(memory, shared_state, len)
            .map(|_| memory.get_writable_raw_unchecked(len))
    }
}

/// Current mode of operation for the `BufferPool`.
///
/// The pool operates in three modes based on memory availability: it first allocates from the
/// back, then from the front if the back is full, and finally from system memory (with reduced
/// performance) if both sections are exhausted.
#[derive(Debug, Clone)]
pub enum PoolMode {
    /// The buffer pool is operating in "back" mode, where memory is allocated from the back of the
    // buffer pool.
    Back,

    /// The buffer pool is operating in "front" mode, where memory is allocated from the front of
    /// the buffer pool. Used when the back is full.
    Front(PoolFront),

    /// The pool has exhausted its internal memory, and it is now allocating directly from the
    /// system memory (heap).
    Alloc,
}

// Internal memory management for the `BufferPool`.
//
// Handles allocating, tracking, and managing memory slices for manipulating memory offsets,
// copying data, and managing capacity. It uses a contiguous block of memory (`Vec<u8>`), tracking
// its usage through offsets (`raw_offset`, `raw_length`), and manages slice allocations through
// `slots`. Used by `BufferPool` to optimize memory reused and minimize heap allocations.
#[derive(Debug, Clone)]
pub struct InnerMemory {
    // Underlying contiguous block of memory to be managed.
    pool: Vec<u8>,

    // Current offset into the contiguous block of memory where the next write will occur.
    pub(crate) raw_offset: usize,

    // Length of the valid data within the contiguous block of memory, starting from `raw_offset`.
    pub(crate) raw_len: usize,

    // Tracks individual chunks of memory being used in the buffer pool by tracking the start and
    // length of each allocated memory slice.
    slots: [(usize, usize); POOL_CAPACITY],

    // A pointer to the current slot. Represents how many slots are currently occupied.
    len: usize,
}

impl InnerMemory {
    // Initializes a new `InnerMemory` with a specified size of the internal memory buffer
    // (`capacity`), in bytes.
    fn new(capacity: usize) -> Self {
        let pool = vec![0; capacity];
        Self {
            pool,
            raw_offset: 0,
            raw_len: 0,
            slots: [(0_usize, 0_usize); POOL_CAPACITY],
            len: 0,
        }
    }

    // Resets the internal memory pool, clearing all used memory and resetting the slot tracking.
    #[inline(always)]
    fn reset(&mut self) {
        self.raw_len = 0;
        self.raw_offset = 0;
        self.slots = [(0_usize, 0_usize); POOL_CAPACITY];
    }

    // Resets only the raw memory state, without affecting the slot tracking.
    #[inline(always)]
    fn reset_raw(&mut self) {
        self.raw_len = 0;
        self.raw_offset = 0;
    }

    // Returns the capacity of the front portion of the buffer.
    //
    // Used to determine how much space is available in the front of the buffer pool, based on the
    // `back_start` position.
    #[inline(always)]
    fn get_front_capacity(&self, back_start: usize) -> usize {
        #[cfg(feature = "fuzz")]
        assert!(
            back_start >= 1
                && back_start < POOL_CAPACITY
                && self.slots[back_start].0 != 0_usize
                && self.slots[back_start].1 != 0_usize
                && self.slots[back_start].0 + self.slots[back_start].1 <= self.pool.len()
        );

        self.slots[back_start].0
    }

    // Calculates the offset for the next writable section of memory. Returns the offset based on
    // the current memory length and slot usage.
    #[inline(always)]
    fn raw_offset(&self) -> usize {
        match self.len {
            0 => 0,
            _ => {
                let index = self.len - 1;
                #[cfg(feature = "fuzz")]
                assert!(
                    index < POOL_CAPACITY
                        && self.slots[index].1 != 0_usize
                        && self.slots[index].0 + self.slots[index].1 <= self.pool.len()
                );

                let (index, len) = self.slots[index];

                index + len
            }
        }
    }

    // Calculates the offset for a specific length of memory. Returns the offset based on the
    // provided length and the current state of the memory slots.
    #[inline(always)]
    fn raw_offset_from_len(&self, len: usize) -> usize {
        match len {
            0 => 0,
            _ => {
                let index = len - 1;
                #[cfg(feature = "fuzz")]
                assert!(
                    index < POOL_CAPACITY
                        && self.slots[index].1 != 0_usize
                        && self.slots[index].0 + self.slots[index].1 <= self.pool.len()
                );

                let (index, len) = self.slots[index];

                index + len
            }
        }
    }

    // Moves the raw data to the front of the memory pool to avoid fragmentation, if necessary.
    //
    // Used to compact the raw data by moving all the active slices to the front of the memory
    // pool, making the pool contiguous again. This process is only performed when needed to free
    // up space for new allocations without increasing the total memory footprint.
    #[inline(always)]
    fn move_raw_at_front(&mut self) {
        match self.raw_len {
            0 => self.raw_offset = 0,
            _ => {
                self.pool
                    .copy_within(self.raw_offset..self.raw_offset + self.raw_len, 0);
                self.raw_offset = 0;
            }
        }
    }

    // Tries to update the length and offset of the memory pool and moves the raw offset if there
    // is enough capacity to accommodate new memory. Returns `true` if successful, otherwise false.
    #[inline(always)]
    fn try_change_len(&mut self, slot_len: usize, raw_len: usize) -> bool {
        let raw_offset = self.raw_offset_from_len(slot_len);

        let end = raw_offset + self.raw_len;
        if end + raw_len <= self.pool.capacity() {
            self.len = slot_len;
            self.move_raw_at_offset_unchecked(raw_offset);
            true
        } else {
            false
        }
    }

    // Moves the raw data to a specific offset within the memory pool to avoid fragmentation, if
    // necessary.
    //
    // Misuse of this function can lead to undefined behavior, such as memory corruption or
    // crashes, if it operates on out-of-bounds or misaligned memory.
    #[inline(always)]
    fn move_raw_at_offset_unchecked(&mut self, offset: usize) {
        match self.raw_len {
            0 => self.raw_offset = offset,
            _ => {
                self.pool
                    .copy_within(self.raw_offset..self.raw_offset + self.raw_len, offset);
                self.raw_offset = offset;
            }
        }
    }

    // Inserts raw data at the front of the memory pool, adjusting the raw offset and length.
    #[inline(never)]
    fn prepend_raw_data(&mut self, raw_data: &[u8]) {
        self.raw_offset = 0;
        self.raw_len = raw_data.len();

        let dest = &mut self.pool[0..self.raw_len];

        dest.copy_from_slice(raw_data);
    }

    // Copies the internal raw memory into another buffer. Used when transitioning memory between
    // different pool modes.
    #[inline(never)]
    fn copy_into_buffer(&mut self, buffer: &mut impl Buffer) {
        let writable = buffer.get_writable(self.raw_len);
        writable.copy_from_slice(&self.pool[self.raw_offset..self.raw_offset + self.raw_len]);
    }

    // Checks if there is enough capacity at the tail of the memory pool to accommodate `len`
    // bytes.
    #[inline(always)]
    fn has_tail_capacity(&self, len: usize) -> bool {
        let end = self.raw_offset + self.raw_len;
        end + len <= self.pool.capacity()
    }

    // Checks if there is enough capacity in the memory pool up to the specified offset to
    // accommodate `len` bytes.
    #[inline(always)]
    fn has_capacity_until_offset(&self, len: usize, offset: usize) -> bool {
        self.raw_offset + self.raw_len + len <= offset
    }

    // Returns a raw pointer to the writable memory region of the memory pool, marking the section
    // as used.
    #[inline(always)]
    fn get_writable_raw_unchecked(&mut self, len: usize) -> *mut u8 {
        let writable_offset = self.raw_offset + self.raw_len;
        self.raw_len += len;
        self.pool[writable_offset..writable_offset + len].as_mut_ptr()
    }

    /// Provides access to the raw memory slice containing the data written into the buffer,
    /// returning it as a [`Slice`]. This method is the primary mechanism for making processed data
    /// available from the buffer to the rest of the system.
    ///
    /// After this call, the buffer advances internally to a new slot, allowing new data to be
    /// written into an unused portion of memory. This approach avoids memory duplication and
    /// ensures efficient reuse of the buffer without transferring ownership of the memory.
    ///
    /// This method is typically used when the data in the buffer is ready for processing, sending,
    /// or further manipulation. The returned `Slice` contains the data, while the buffer itself
    /// remains ready to handle new incoming data by pointing to a fresh memory region.
    #[inline(always)]
    fn get_data_owned(
        &mut self,
        shared_state: &mut SharedState,
        #[cfg(feature = "debug")] mode: u8,
    ) -> Slice {
        let slice = &mut self.pool[self.raw_offset..self.raw_offset + self.raw_len];

        let mut index: u8 = crate::slice::INGORE_INDEX;

        if self.raw_len > 0 {
            self.slots[self.len] = (self.raw_offset, self.raw_len);

            self.len += 1;
            index = self.len as u8;

            #[cfg(feature = "debug")]
            shared_state.toogle(index, mode);

            #[cfg(not(feature = "debug"))]
            shared_state.toogle(index);
        }

        let offset = slice.as_mut_ptr();

        self.raw_offset += self.raw_len;
        self.raw_len = 0;

        Slice {
            offset,
            len: slice.len(),
            index,
            shared_state: shared_state.clone(),
            owned: None,
            #[cfg(feature = "debug")]
            mode,
            #[cfg(feature = "debug")]
            time: SystemTime::now(),
        }
    }
}

/// A pool of reusable memory buffers to optimize memory allocation for Sv2 message frames.
///
/// Manages memory slices across three pool modes: back (default), front, and system memory. It
/// reuses preallocated memory slices, minimizing the need for frequent memory allocation. The pool
/// is thread-safe, using atomic state tracking.
///
/// Type `T` implements the [`Buffer`] trait, which defines how memory is handled in the buffer
/// pool. [`BufferFromSystemMemory`] is used as the default system memory allocator.
#[derive(Debug)]
pub struct BufferPool<T: Buffer> {
    // Manages memory allocation from the back section of the buffer pool.
    pool_back: PoolBack,

    /// Tracks the current mode of memory allocation (back, front, or system).
    pub mode: PoolMode,

    // Tracks the usage state of memory slices using atomic operations, ensuring memory is not
    // prematurely reused and allowing safe concurrent access across threads.
    shared_state: SharedState,

    // Core memory area from which slices are allocated and reused. Manages the actual memory
    // buffer used by the buffer pool.
    inner_memory: InnerMemory,

    // Allocates memory directly from system memory when the buffer pool is full, acting as a
    // fallback when preallocated memory cannot satisfy buffer requests.
    system_memory: T,

    // Tracks the starting index for buffer access, determining where data begins to be read or
    // written in the buffer pool. Primarily used when `as_ref` or `as_mut` is called, ensuring
    // that the buffer starts at the element specified by `start`.
    start: usize,
}

impl BufferPool<BufferFromSystemMemory> {
    /// Creates a new [`BufferPool`] with the specified memory capacity, in bytes.
    ///
    /// Initializes the buffer pool with pre-allocated memory (`inner_memory`) and sets the pool
    /// mode to the back. The buffer pool uses [`BufferFromSystemMemory`] as a fallback when the
    /// buffer sections are full.
    pub fn new(capacity: usize) -> Self {
        Self {
            pool_back: PoolBack::new(),
            mode: PoolMode::Back,
            shared_state: SharedState::new(),
            inner_memory: InnerMemory::new(capacity),
            system_memory: BufferFromSystemMemory::default(),
            start: 0,
        }
    }
}

#[cfg(test)]
impl BufferPool<TestBufferFromMemory> {
    #[cfg(test)]
    pub fn new_fail_system_memory(capacity: usize) -> Self {
        Self {
            pool_back: PoolBack::new(),
            mode: PoolMode::Back,
            shared_state: SharedState::new(),
            inner_memory: InnerMemory::new(capacity),
            system_memory: TestBufferFromMemory(Vec::new()),
            start: 0,
        }
    }
}

// Defines methods specific to internal behavior and management of the `BufferPool`.
impl<T: Buffer> BufferPool<T> {
    /// Checks if the buffer pool is operating in the front mode.
    ///
    /// This mode indicates that the back of the buffer pool has been filled and the system is now
    /// using the front section for memory allocation. Returns `true` if the pool is in front mode,
    /// otherwise `false`.
    pub fn is_front_mode(&self) -> bool {
        match self.mode {
            PoolMode::Back => false,
            PoolMode::Front(_) => true,
            PoolMode::Alloc => false,
        }
    }

    /// Checks if the buffer pool is operating in the back mode.
    ///
    /// The back mode is the default state, where the buffer pool first tries to allocate memory.
    /// Returns `true` if the pool is in back mode, otherwise `false`.
    pub fn is_back_mode(&self) -> bool {
        match self.mode {
            PoolMode::Back => true,
            PoolMode::Front(_) => false,
            PoolMode::Alloc => false,
        }
    }

    /// Checks if the buffer pool is operating in the system memory allocation mode.
    ///
    /// This mode is used when both the back and front sections of the buffer pool are full,
    /// leading the system to allocate memory from the heap, which has performance trade-offs.
    /// Returns `true` if the pool is in alloc mode, otherwise `false`.
    pub fn is_alloc_mode(&self) -> bool {
        match self.mode {
            PoolMode::Back => false,
            PoolMode::Front(_) => false,
            PoolMode::Alloc => true,
        }
    }

    // Resets the buffer pool based on its current mode when the shared state indicates all slices
    // have been dropped, preparing it for reuse.
    //
    // - In back or front mode, the internal memory is moved to the front, and the back is reset.
    // - In alloc mode, system memory is checked and, if smaller than pool capacity, transferred
    //   back into the pool, switching the mode to `Back`.
    #[inline(always)]
    fn reset(&mut self) {
        #[cfg(feature = "debug")]
        println!("RESET");

        self.inner_memory.len = 0;

        match self.mode {
            PoolMode::Back => {
                self.inner_memory.move_raw_at_front();
                self.pool_back.reset();
            }
            PoolMode::Front(_) => {
                self.inner_memory.move_raw_at_front();
                self.mode = PoolMode::Back;
                self.pool_back.reset();
            }
            PoolMode::Alloc => {
                if self.system_memory.len() < self.inner_memory.pool.capacity() {
                    let raw_len = self.system_memory.len();
                    if raw_len > 0 {
                        self.inner_memory
                            .prepend_raw_data(self.system_memory.get_data_by_ref(raw_len));
                        self.system_memory.get_data_owned();
                    } else {
                        self.inner_memory.reset();
                    }
                    self.mode = PoolMode::Back;
                    self.pool_back.reset();
                }
            }
        }
    }

    // Allocates writable memory of the specified `len` from the heap when the buffer pool cannot
    // fulfill the request.
    //
    // Determines whether to allocate memory from system memory or try to clear memory from the
    // back section of the buffer pool. If clearing is unsuccessful, it may switch to alloc mode or
    // remain in back or front pool modes. When `without_check` is `true`, the function bypasses
    // memory checks and allocates directly from the heap.
    #[inline(never)]
    fn get_writable_from_system_memory(
        &mut self,
        len: usize,
        shared_state: u8,
        without_check: bool,
    ) -> &mut [u8] {
        if self.system_memory.len() > 0
            || without_check
            || self.pool_back.len() == 0
            || !self.pool_back.tail_is_clearable(shared_state)
        {
            self.system_memory.get_writable(len)
        } else {
            #[cfg(feature = "fuzz")]
            assert!(self.inner_memory.raw_len == 0 && self.inner_memory.raw_offset == 0);

            match self
                .pool_back
                .clear_unchecked(&mut self.inner_memory, shared_state, len)
            {
                Ok(_) => {
                    self.change_mode(PoolMode::Back, len, shared_state);
                    self.get_writable_(len, shared_state, false)
                }
                Err(PoolMode::Front(f)) => {
                    self.change_mode(PoolMode::Front(f), len, shared_state);
                    self.get_writable_(len, shared_state, false)
                }
                Err(PoolMode::Alloc) => {
                    self.inner_memory.reset_raw();
                    self.system_memory.get_writable(len)
                }
                Err(_) => panic!(),
            }
        }
    }

    // Returns ownership of the heap-allocated buffer data by converting it into a `Slice` for
    // further use or processing.
    #[inline(never)]
    fn get_data_owned_from_sytem_memory(&mut self) -> Slice {
        self.system_memory.get_data_owned().into()
    }

    // Switches the buffer pool to a different mode of operation, adjusting based on the required
    // memory size (`len`).
    //
    // Depending on the current and target modes (back, front, or alloc), this method adjusts the
    // internal buffer pool's state and memory to ensure a smooth transition while allocating the
    // necessary buffer space (`len`), ensuring no data is lost.
    #[inline(always)]
    fn change_mode(&mut self, mode: PoolMode, len: usize, shared_state: u8) {
        match (&mut self.mode, &mode) {
            (PoolMode::Back, PoolMode::Alloc) => {
                #[cfg(feature = "debug")]
                println!(
                    "BACK => ALLOc {} {:?}",
                    self.pool_back.len(),
                    SystemTime::now()
                );

                self.inner_memory.copy_into_buffer(&mut self.system_memory);
                self.inner_memory.reset_raw();
                self.mode = mode;
            }
            (PoolMode::Back, PoolMode::Front(_)) => {
                #[cfg(feature = "fuzz")]
                assert!(shared_state.leading_zeros() > 0);
                #[cfg(feature = "debug")]
                println!("BACK => FRONT");

                self.inner_memory.len = 0;
                self.inner_memory.move_raw_at_front();
                self.mode = mode;
            }
            (PoolMode::Front(_), PoolMode::Back) => {
                #[cfg(feature = "debug")]
                println!("FRONT +> BACL");

                if !self.pool_back.tail_is_clearable(shared_state) {
                    self.inner_memory.copy_into_buffer(&mut self.system_memory);
                    self.inner_memory.reset_raw();
                    self.mode = PoolMode::Alloc;
                } else if self.pool_back.try_clear_tail_unchecked(
                    &mut self.inner_memory,
                    shared_state,
                    len,
                ) {
                    self.mode = mode;
                } else {
                    #[cfg(feature = "debug")]
                    println!("ALLOC 2");

                    self.inner_memory.copy_into_buffer(&mut self.system_memory);
                    self.inner_memory.reset_raw();
                    self.mode = PoolMode::Alloc;
                }
            }
            (PoolMode::Alloc, PoolMode::Back) => {
                #[cfg(feature = "debug")]
                println!("ALLOC => BACK {:?}", SystemTime::now());

                self.inner_memory.len = self.pool_back.len() + self.pool_back.back_start();
                self.mode = mode;
            }
            (PoolMode::Alloc, PoolMode::Front(_)) => {
                #[cfg(feature = "fuzz")]
                assert!(shared_state.leading_zeros() > 0);
                #[cfg(feature = "debug")]
                println!("ALLOC +> FORNT {:?} {:b}", SystemTime::now(), shared_state);

                self.inner_memory.reset_raw();
                self.inner_memory.len = 0;
                self.mode = mode;
            }
            (PoolMode::Front(_), PoolMode::Alloc) => {
                panic!();
            }
            (PoolMode::Back, PoolMode::Back) => {
                panic!();
            }
            (PoolMode::Front(_), PoolMode::Front(_)) => {
                panic!();
            }
            (PoolMode::Alloc, PoolMode::Alloc) => {
                panic!();
            }
        }
    }

    // Recursively attempts to allocate writable memory of the specified `len`, switching modes if
    // necessary.
    //
    // First tries to allocate memory from the current mode (back, front, or alloc), and if
    // unsuccessful, switches modes and retries, starting with the memory pool before resorting to
    // system memory.
    #[inline(always)]
    fn get_writable_(&mut self, len: usize, shared_state: u8, without_check: bool) -> &mut [u8] {
        let writable = match &mut self.mode {
            PoolMode::Back => {
                self.pool_back
                    .get_writable(len, &mut self.inner_memory, shared_state)
            }
            PoolMode::Front(front) => front.get_writable(len, &mut self.inner_memory, shared_state),
            PoolMode::Alloc => {
                return self.get_writable_from_system_memory(len, shared_state, without_check)
            }
        };

        match writable {
            Ok(offset) => {
                let writable: &mut [u8];
                unsafe {
                    writable = core::slice::from_raw_parts_mut(offset, len);
                }
                writable
            }
            Err(mode) => {
                self.change_mode(mode, len, shared_state);
                let without_check = self.is_alloc_mode();
                self.get_writable_(len, shared_state, without_check)
            }
        }
    }
}

impl<T: Buffer> Buffer for BufferPool<T> {
    type Slice = Slice;

    // Provides a mutable slice of length `len` for writing data into the buffer pool.
    //
    // Checks the current `shared_state` to determine if the buffer pool can be reset. If all
    // slices have been dropped and there are allocated slices in `pool_back`, it resets the buffer
    // pool to free up memory. It then attempts to allocate writable memory, which may switch
    // between different modes as needed.
    #[inline(always)]
    fn get_writable(&mut self, len: usize) -> &mut [u8] {
        let shared_state = self.shared_state.load(Ordering::Relaxed);

        // If all the slices have been dropped, reset the pool to free up memory
        if shared_state == 0 && self.pool_back.len() != 0 {
            self.reset();
        }

        // Attempt to allocate writable memory, potentially switching pool modes
        self.get_writable_(len, shared_state, false)
    }

    // Transfers ownership of the written data as a `Slice`, handling different pool modes.
    //
    // Depending on the current mode (back, front, or alloc), it retrieves the data from the
    // appropriate memory source. In `Back` or `Front` modes, it updates internal state
    // accordingly. In alloc mode, it retrieves data from the heap-allocated system memory.
    #[inline(always)]
    fn get_data_owned(&mut self) -> Self::Slice {
        let shared_state = &mut self.shared_state;

        #[cfg(feature = "debug")]
        let mode: u8 = match self.mode {
            PoolMode::Back => 0,
            PoolMode::Front(_) => 1,
            PoolMode::Alloc => 8,
        };

        #[cfg(feature = "debug")]
        match &mut self.mode {
            PoolMode::Back => {
                println!(
                    "{} {} {}",
                    self.inner_memory.raw_offset, self.inner_memory.raw_len, self.inner_memory.len
                );
                let res = self.inner_memory.get_data_owned(shared_state, mode);
                self.pool_back
                    .set_len_from_inner_memory(self.inner_memory.len);
                println!(
                    "{} {} {}",
                    self.inner_memory.raw_offset, self.inner_memory.raw_len, self.inner_memory.len
                );
                println!("GET DATA BACK {:?}", self.inner_memory.slots);
                res
            }
            PoolMode::Front(f) => {
                let res = self.inner_memory.get_data_owned(shared_state, mode);
                f.len = self.inner_memory.len;
                println!("GET DATA FRONT {:?}", self.inner_memory.slots);
                res
            }
            PoolMode::Alloc => self.get_data_owned_from_sytem_memory(),
        }

        #[cfg(not(feature = "debug"))]
        match &mut self.mode {
            PoolMode::Back => {
                // Retrieve data and update state in Back mode
                let res = self.inner_memory.get_data_owned(shared_state);
                self.pool_back
                    .set_len_from_inner_memory(self.inner_memory.len);
                res
            }
            PoolMode::Front(f) => {
                // Retrieve data and update state in Front mode
                let res = self.inner_memory.get_data_owned(shared_state);
                f.len = self.inner_memory.len;
                res
            }
            PoolMode::Alloc => self.get_data_owned_from_sytem_memory(),
        }
    }

    // Retrieves data differently based on the current buffer pool mode:
    // - In alloc mode, it delegates to the system memory buffer.
    // - In back or front modes, it returns a mutable slice of the internal memory buffer.
    fn get_data_by_ref(&mut self, len: usize) -> &mut [u8] {
        match self.mode {
            PoolMode::Alloc => self.system_memory.get_data_by_ref(len),
            _ => {
                &mut self.inner_memory.pool[self.inner_memory.raw_offset
                    ..self.inner_memory.raw_offset + self.inner_memory.raw_len]
            }
        }
    }

    // Retrieves data differently based on the current pool mode:
    // - In back or front modes, it returns an immutable slice of the internal memory buffer.
    // - In alloc mode, it delegates to the system memory buffer.
    fn get_data_by_ref_(&self, len: usize) -> &[u8] {
        match self.mode {
            PoolMode::Alloc => self.system_memory.get_data_by_ref_(len),
            _ => {
                &self.inner_memory.pool[self.inner_memory.raw_offset
                    ..self.inner_memory.raw_offset + self.inner_memory.raw_len]
            }
        }
    }

    // Returns the length of the written data in the buffer.
    //
    // The implementation checks the current pool mode to determine where to retrieve the length
    // from:
    // - In back or front modes, it returns the length from `inner_memory.raw_len`.
    // - In alloc mode, it returns the length from the system memory buffer.
    fn len(&self) -> usize {
        match self.mode {
            PoolMode::Back => self.inner_memory.raw_len,
            PoolMode::Front(_) => self.inner_memory.raw_len,
            PoolMode::Alloc => self.system_memory.len(),
        }
    }

    // Sets the start index for the buffer, adjusting where reads and writes begin. Used to discard
    // part of the buffer by adjusting the starting point for future operations.
    fn danger_set_start(&mut self, index: usize) {
        self.start = index;
    }

    // Returns `true` if all memory slices have been released (`shared_state` is zero), indicating
    // that no other threads or components are using the pool's memory.
    #[inline(always)]
    fn is_droppable(&self) -> bool {
        self.shared_state.load(Ordering::Relaxed) == 0
    }
}

#[cfg(not(test))]
impl<T: Buffer> Drop for BufferPool<T> {
    // Waits until all slices are released before dropping the `BufferPool`. Will not drop the
    // buffer pool while slices are still in use.
    fn drop(&mut self) {
        while self.shared_state.load(Ordering::Relaxed) != 0 {
            core::hint::spin_loop();
        }
    }
}

// Allows `BufferPool` to be treated as a buffer.
impl<T: Buffer> BufferPool<T> {
    /// Determines if the [`BufferPool`] can be safely dropped.
    ///
    /// Returns `true` if all memory slices managed by the buffer pool have been released (i.e.,
    /// the `shared_state` is zero), indicating that all the slices are dropped. This check helps
    /// prevent dropping the buffer pool while it's still in use.
    pub fn droppable(&self) -> bool {
        self.shared_state.load(Ordering::Relaxed) == 0
    }
}

impl<T: Buffer> AsRef<[u8]> for BufferPool<T> {
    fn as_ref(&self) -> &[u8] {
        &self.get_data_by_ref_(Buffer::len(self))[self.start..]
    }
}

impl<T: Buffer> AsMut<[u8]> for BufferPool<T> {
    fn as_mut(&mut self) -> &mut [u8] {
        let start = self.start;
        self.get_data_by_ref(Buffer::len(self))[start..].as_mut()
    }
}

impl<T: Buffer + AeadBuffer> AeadBuffer for BufferPool<T> {
    fn extend_from_slice(&mut self, other: &[u8]) -> aes_gcm::aead::Result<()> {
        self.get_writable(other.len()).copy_from_slice(other);
        Ok(())
    }

    fn truncate(&mut self, len: usize) {
        let len = len + self.start;
        match self.mode {
            PoolMode::Back => self.inner_memory.raw_len = len,
            PoolMode::Front(_) => self.inner_memory.raw_len = len,
            PoolMode::Alloc => self.system_memory.truncate(len),
        }
    }
}
</file>

<file path="stratum-1.4.0/utils/buffer/src/buffer_pool/pool_back.rs">
// # Back of Buffer Pool
//
// Manages the "back" section of the buffer pool (`BufferPool`).
//
// The `PoolBack` struct is responsible for allocating, clearing, and managing memory slices from
// the back of the pool. It tracks the number of allocated slices and attempts to free up memory
// that is no longer in use, preventing unnecessary memory growth.
//
// Key functions in this module handle:
// - Clearing unused slices from the back of the pool to reclaim memory.
// - Managing slice allocation and ensuring enough capacity for new operations.
// - Switching between different pool modes, such as front or back, depending on memory state.
//
// By default, memory is always first allocated from the back of the `BufferPool`. If the all of
// the initially allocated buffer memory is completely filled and a new memory request comes in,
// `BufferPool` checks whether any memory has been freed at the back or the front using
// `SharedState`. If, for example, a slice has been freed that corresponds to the head of
// `SharedState`, `BufferPool` will switch to front mode and start allocating incoming memory
// requests from there. However, if the entire `SharedState` is full, it will switch to alloc mode
// and begin allocating system memory to fulfill incoming requests at a performance reduction. For
// subsequent memory requests, it will continue to check whether the prefix or suffix of
// `SharedState` has been freed. If it has, `BufferPool` will switch modes and start consuming
// pre-allocated `BufferPool` memory; if not, it will remain in alloc mode.

use crate::buffer_pool::{InnerMemory, PoolFront, PoolMode, POOL_CAPACITY};

// Manages the "back" section of the `BufferPool`.
//
// Handles the allocation of memory slices at the back of the buffer pool. It tracks the number of
// slices in use and attempts to free unused slices when necessary to maximize available memory.
// The back of the buffer pool is used first, if it fills up, the front of the buffer pool is used.
#[derive(Debug, Clone)]
pub struct PoolBack {
    // Starting index of the back section of the buffer pool.
    back_start: usize,

    // Number of allocated slices in the back section of the buffer pool.
    len: usize,
}

impl PoolBack {
    // Initializes a new `PoolBack` with no allocated slices.
    pub fn new() -> Self {
        Self {
            back_start: 0,
            len: 0,
        }
    }

    // Returns the number of allocated slices in the back section of the buffer pool.
    #[inline(always)]
    pub fn len(&self) -> usize {
        self.len
    }

    // Updates the length of the back section based on the state of the inner memory.
    #[inline(always)]
    pub fn set_len_from_inner_memory(&mut self, len: usize) {
        let len = len - self.back_start;

        #[cfg(feature = "fuzz")]
        assert!(len + self.back_start <= POOL_CAPACITY);

        self.len = len;
    }

    // Returns the starting index of the back section in the buffer pool.
    #[inline(always)]
    pub fn back_start(&self) -> usize {
        self.back_start
    }

    // Resets the back section of the pool by clearing its start index and length.
    #[inline(always)]
    pub fn reset(&mut self) {
        self.back_start = 0;
        self.len = 0;
    }

    // From the back section, checks if there are any unset bits and adjusts the length if there
    // are.
    //
    // Assumes the caller has already ensured the safety of the operation (such as slice bounds and
    // memory validity). It skips internal safety checks, relying on the caller to manage the
    // state, making it faster but potentially unsafe if misused.
    //
    // Returns `true` if the tail was cleared successfully, otherwise `false`.
    #[inline(always)]
    pub fn try_clear_tail_unchecked(
        &mut self,
        memory: &mut InnerMemory,
        shared_state: u8,
        len: usize,
    ) -> bool {
        let (leading_0, trailing_0) = (shared_state.leading_zeros(), shared_state.trailing_zeros());
        let element_in_back = POOL_CAPACITY - self.back_start;

        // 0b11000000   6 trailing zeros
        //       |
        //       v      but back start at position 5
        //   back start
        //
        // Pool must drop only 4 elements
        //
        let element_to_drop = usize::min(element_in_back, shared_state.trailing_zeros() as usize);

        // The actual element to drop are element_to_drop - already_dropped
        let already_dropped = POOL_CAPACITY - (self.back_start + self.len);

        match (
            leading_0,
            trailing_0,
            element_in_back,
            element_to_drop < already_dropped,
        ) {
            (0, 0, _, _) => false,
            (0, _, 0, _) => false,
            (_, _, _, true) => false,
            (0, _, _, _) => {
                let element_to_drop = element_to_drop - already_dropped;

                self.len -= element_to_drop;

                #[cfg(feature = "fuzz")]
                assert!(
                    self.len + self.back_start <= POOL_CAPACITY
                        && self.len + element_to_drop + already_dropped + self.back_start
                            == POOL_CAPACITY
                        && self.len + self.back_start <= POOL_CAPACITY
                );

                memory.try_change_len(self.len + self.back_start, len)
            }
            // If leading_0 is > than 0 return and clear the head
            (_, _, _, _) => false,
        }
    }

    // Checks if the tail of the back section can be cleared.
    //
    // Should always be called before attempting to clear the tail. Returns `true` if the tail can
    // be cleared, otherwise `false`.
    #[inline(always)]
    pub fn tail_is_clearable(&self, shared_state: u8) -> bool {
        let element_in_back = POOL_CAPACITY - self.back_start;
        let element_to_drop = usize::min(element_in_back, shared_state.trailing_zeros() as usize);

        element_to_drop <= self.len && self.back_start + self.len >= POOL_CAPACITY
    }

    // Attempts to clear the head of the back section, transitioning to the buffer pool front
    // section if not possible.
    //
    // Returns `Ok` if the head was cleared successfully. Otherwise an `Err(PoolMode::Front)` if
    // the buffer pool should switch to front mode, or an `Err(PoolMode::Alloc)` if the buffer pool
    // should switch to allocation mode.
    #[inline(always)]
    fn try_clear_head(
        &mut self,
        shared_state: u8,
        memory: &mut InnerMemory,
    ) -> Result<(), PoolMode> {
        // 0b00111110  2 leading zeros
        //
        // The first 2 elements have been dropped so back start at 2 and `BufferPool` can go in
        // front mode
        self.back_start = shared_state.leading_zeros() as usize;

        if self.back_start >= 1 && memory.raw_len < memory.slots[self.back_start].0 {
            if self.back_start >= self.len {
                self.len = 0;
            } else {
                self.len -= self.back_start;
            }
            let pool_front =
                PoolFront::new(memory.get_front_capacity(self.back_start), self.back_start);
            Err(PoolMode::Front(pool_front))
        } else {
            Err(PoolMode::Alloc)
        }
    }

    // Clears the tail or head of the back section, transitioning pool modes if necessary.
    //
    // Called when the state of the `BufferPool`, along with both `raw_offset` and `raw_len`,
    // reaches its maximum limits. It is used to clear any available space remaining in the
    // buffer's suffix or prefix. Based on that, the `BufferPool`'s state is updated by changing
    // its pool mode.
    //
    // Returns `Ok` if the tail or head was cleared successfully. Otherwise an
    // `Err(PoolMode::Front)` if the buffer pool should switch to front mode, or an
    // `Err(PoolMode::Alloc)` if the buffer pool should switch to allocation mode.
    #[inline(always)]
    pub fn clear_unchecked(
        &mut self,
        memory: &mut InnerMemory,
        shared_state: u8,
        len: usize,
    ) -> Result<(), PoolMode> {
        let tail_cleared = self.try_clear_tail_unchecked(memory, shared_state, len);

        if tail_cleared {
            return Ok(());
        };

        self.try_clear_head(shared_state, memory)
    }

    // Returns a writable slice of memory from the back section or transitions to a new pool mode
    // if necessary.
    //
    // Returns `Ok(*mut u8)` if writable memory is available, otherwise an `Err(PoolMode)` if a
    // mode change is required.
    #[inline(always)]
    pub fn get_writable(
        &mut self,
        len: usize,
        memory: &mut InnerMemory,
        shared_state: u8,
    ) -> Result<*mut u8, PoolMode> {
        let pool_has_byte_capacity = memory.has_tail_capacity(len);
        let pool_has_slice_capacity = (self.len + self.back_start) < POOL_CAPACITY;

        if pool_has_byte_capacity && pool_has_slice_capacity {
            #[cfg(feature = "fuzz")]
            assert!(self.len + self.back_start < POOL_CAPACITY);
            return Ok(memory.get_writable_raw_unchecked(len));
        }

        if !self.tail_is_clearable(shared_state) {
            return Err(PoolMode::Alloc);
        }

        match self.clear_unchecked(memory, shared_state, len) {
            Ok(_) => {
                let pool_has_byte_capacity = memory.has_tail_capacity(len);
                let pool_has_slice_capacity = self.len < POOL_CAPACITY;

                if pool_has_byte_capacity && pool_has_slice_capacity {
                    #[cfg(feature = "fuzz")]
                    assert!(self.len + self.back_start < POOL_CAPACITY);
                    Ok(memory.get_writable_raw_unchecked(len))
                } else {
                    Err(PoolMode::Alloc)
                }
            }
            Err(pool_mode) => Err(pool_mode),
        }
    }
}
</file>

<file path="stratum-1.4.0/utils/buffer/src/buffer.rs">
// # Buffer from System Memory
//
// Provides memory management for encoding and transmitting message frames between Sv2 roles when
// buffer pools have been exhausted.
//
// `BufferFromSystemMemory` serves as a fallback when a `BufferPool` is full or unable to allocate
// memory fast enough. Instead of relying on pre-allocated memory, it dynamically allocates memory
// on the heap using a `Vec<u8>`, ensuring that message frames can still be processed.
//
// This fallback mechanism allows the buffer to resize dynamically based on data needs, making it
// suitable for scenarios where message sizes vary. However, it introduces performance trade-offs
// such as slower allocation, increased memory fragmentation, and higher system overhead compared
// to using pre-allocated buffers.

use crate::Buffer;
use aes_gcm::aead::Buffer as AeadBuffer;
use alloc::vec::Vec;

/// Manages a dynamically growing buffer in system memory using an internal [`Vec<u8>`].
///
/// Operates on a dynamically sized buffer and provides utilities for writing, reading, and
/// manipulating data. It tracks the current position where data is written, and resizes the buffer
/// as needed.
#[derive(Debug)]
pub struct BufferFromSystemMemory {
    // Underlying buffer storing the data.
    inner: Vec<u8>,

    // Current cursor indicating where the next byte should be written.
    cursor: usize,

    // Starting index for the buffer. Useful for scenarios where part of the buffer is skipped or
    // invalid.
    start: usize,
}

impl BufferFromSystemMemory {
    /// Creates a new buffer with no initial data.
    pub fn new(_: usize) -> Self {
        Self {
            inner: Vec::new(),
            cursor: 0,
            start: 0,
        }
    }
}

impl Default for BufferFromSystemMemory {
    // Creates a new buffer with no initial data.
    fn default() -> Self {
        Self::new(0)
    }
}

impl Buffer for BufferFromSystemMemory {
    type Slice = Vec<u8>;

    // Dynamically allocates or resizes the internal `Vec<u8>` to ensure there is enough space for
    // writing.
    #[inline]
    fn get_writable(&mut self, len: usize) -> &mut [u8] {
        let cursor = self.cursor;

        // Reserve space in the buffer for writing based on the requested `len`
        let len = self.cursor + len;

        // If the internal buffer is not large enough to hold the new data, resize it
        if len > self.inner.len() {
            self.inner.resize(len, 0)
        };

        self.cursor = len;

        // Portion of the buffer where data can be written
        &mut self.inner[cursor..len]
    }

    // Splits off the written portion of the buffer, returning it as a new `Vec<u8>`. Swaps the
    // internal buffer with a newly allocated empty one, effectively returning ownership of the
    // written data while resetting the internal buffer for future use.
    #[inline]
    fn get_data_owned(&mut self) -> Vec<u8> {
        // Split the internal buffer at the cursor position
        let mut tail = self.inner.split_off(self.cursor);

        // Swap the data after the cursor (tail) with the remaining buffer
        core::mem::swap(&mut tail, &mut self.inner);

        // Move ownership of the buffer content up to the cursor, resetting the internal buffer
        // state for future writes
        let head = tail;
        self.cursor = 0;
        head
    }

    // Returns a mutable reference to the written portion of the internal buffer that has been
    // filled up with data, up to the specified length (`len`).
    #[inline]
    fn get_data_by_ref(&mut self, len: usize) -> &mut [u8] {
        &mut self.inner[..usize::min(len, self.cursor)]
    }

    // Returns an immutable reference to the written portion of the internal buffer that has been
    // filled up with data, up to the specified length (`len`).
    #[inline]
    fn get_data_by_ref_(&self, len: usize) -> &[u8] {
        &self.inner[..usize::min(len, self.cursor)]
    }

    // Returns the current write position (cursor) in the buffer, representing how much of the
    // internal buffer has been filled with data.
    #[inline]
    fn len(&self) -> usize {
        self.cursor
    }

    // Sets the start index for the buffer, adjusting where reads and writes begin. Used to discard
    // part of the buffer by adjusting the starting point for future operations.
    #[inline]
    fn danger_set_start(&mut self, index: usize) {
        self.start = index;
    }

    // Indicates that the buffer is always safe to drop, as `Vec<u8>` manages memory internally.
    #[inline]
    fn is_droppable(&self) -> bool {
        true
    }
}

// Used to test if `BufferPool` tries to allocate from system memory.
#[cfg(test)]
pub struct TestBufferFromMemory(pub Vec<u8>);

#[cfg(test)]
impl Buffer for TestBufferFromMemory {
    type Slice = Vec<u8>;

    fn get_writable(&mut self, _len: usize) -> &mut [u8] {
        panic!()
    }

    fn get_data_owned(&mut self) -> Self::Slice {
        panic!()
    }

    fn get_data_by_ref(&mut self, _len: usize) -> &mut [u8] {
        &mut self.0[0..0]
    }

    fn get_data_by_ref_(&self, _len: usize) -> &[u8] {
        &self.0[0..0]
    }

    fn len(&self) -> usize {
        0
    }

    fn danger_set_start(&mut self, _index: usize) {
        todo!()
    }

    fn is_droppable(&self) -> bool {
        true
    }
}

impl AsRef<[u8]> for BufferFromSystemMemory {
    /// Returns a reference to the internal buffer as a byte slice, starting from the specified
    /// `start` index. Provides an immutable view into the buffer's contents, allowing it to be
    /// used as a regular slice for reading.
    fn as_ref(&self) -> &[u8] {
        let start = self.start;
        &self.get_data_by_ref_(Buffer::len(self))[start..]
    }
}

impl AsMut<[u8]> for BufferFromSystemMemory {
    /// Returns a mutable reference to the internal buffer as a byte slice, starting from the
    /// specified `start` index. Allows direct modification of the buffer's contents, while
    /// restricting access to the data after the `start` index.
    fn as_mut(&mut self) -> &mut [u8] {
        let start = self.start;
        self.get_data_by_ref(Buffer::len(self))[start..].as_mut()
    }
}

impl AeadBuffer for BufferFromSystemMemory {
    /// Extends the internal buffer by appending the given byte slice. Dynamically resizes the
    /// internal buffer to accommodate the new data and copies the contents of `other` into it.
    fn extend_from_slice(&mut self, other: &[u8]) -> aes_gcm::aead::Result<()> {
        self.get_writable(other.len()).copy_from_slice(other);
        Ok(())
    }

    /// Truncates the internal buffer to the specified length, adjusting for the `start` index.
    /// Resets the buffer cursor to reflect the new size, effectively discarding any data beyond
    /// the truncated length.
    fn truncate(&mut self, len: usize) {
        let len = len + self.start;
        self.cursor = len;
    }
}
</file>

<file path="stratum-1.4.0/utils/buffer/src/lib.rs">
//! # `buffer_sv2`
//!
//! Handles memory management for Stratum V2 (Sv2) roles.
//!
//! Provides a memory-efficient buffer pool ([`BufferPool`]) that minimizes allocations and
//! deallocations for high-throughput message frame processing in Sv2 roles. [`Slice`] helps
//! minimize memory allocation overhead by reusing large buffers, improving performance and
//! reducing latency. The [`BufferPool`] tracks the usage of memory slices, using atomic operations
//! and shared state tracking to safely manage memory across multiple threads.
//!
//! ## Memory Structure
//!
//! The [`BufferPool`] manages a contiguous block of memory allocated on the heap, divided into
//! fixed-size slots. Memory allocation within this pool operates in three distinct modes:
//!
//! 1. **Back Mode**: By default, memory is allocated sequentially from the back (end) of the buffer
//!    pool. This mode continues until the back slots are fully occupied.
//! 2. **Front Mode**: Once the back slots are exhausted, the [`BufferPool`] checks if any slots at
//!    the front (beginning) have been freed. If available, it switches to front mode, allocating
//!    memory from the front slots.
//! 3. **Alloc Mode**: If both back and front slots are occupied, the [`BufferPool`] enters alloc
//!    mode, where it allocates additional memory directly from the system heap. This mode may
//!    introduce performance overhead due to dynamic memory allocation.
//!
//! [`BufferPool`] dynamically transitions between these modes based on slot availability,
//! optimizing memory usage and performance.
//!
//! ## Usage
//!
//! When an incoming Sv2 message is received, it is buffered for processing. The [`BufferPool`]
//! attempts to allocate memory from its internal slots, starting in back mode. If the back slots
//! are full, it checks for available front slots to switch to front mode. If no internal slots are
//! free, it resorts to alloc mode, allocating memory from the system heap.
//!
//! For operations requiring dedicated buffers, the [`Slice`] type manages its own memory using
//! [`Vec<u8>`]. In high-performance scenarios, [`Slice`] can reference externally managed memory
//! from the [`BufferPool`], reducing dynamic memory allocations and increasing performance.
//!
//! ### Debug Mode
//! Provides additional tracking for debugging memory management issues.

#![cfg_attr(not(feature = "debug"), no_std)]
//#![feature(backtrace)]

mod buffer;
mod buffer_pool;
mod slice;
#[cfg(test)]
mod test;

extern crate alloc;
use alloc::vec::Vec;

pub use crate::buffer::BufferFromSystemMemory;
pub use aes_gcm::aead::Buffer as AeadBuffer;
pub use buffer_pool::BufferPool;
pub use slice::Slice;

/// Represents errors that can occur while writing data into a buffer.
pub enum WriteError {
    /// No data could be written.
    WriteZero,
}

/// Interface for writing data into a buffer.
///
/// An abstraction over different buffer types ([`Vec<u8>`] or [`BufferPool`]), it provides methods
/// for writing data from a byte slice into the buffer, with the option to either write a portion
/// of the data or attempt to write the entire byte slice at once.
pub trait Write {
    /// Writes data from a byte slice (`buf`) into the buffer, returning the number of bytes that
    /// were successfully written.
    fn write(&mut self, buf: &[u8]) -> Result<usize, WriteError>;

    /// Attempts to write the entire byte slice (`buf`) into the buffer. If the buffer cannot
    /// accept the full length of the data, an error is returned.
    fn write_all(&mut self, buf: &[u8]) -> Result<(), WriteError>;
}

impl Write for Vec<u8> {
    /// Writes data from a byte slice into a [`Vec<u8>`] buffer by extending the vector with the
    /// contents of the provided slice.
    #[inline]
    fn write(&mut self, buf: &[u8]) -> Result<usize, WriteError> {
        self.extend_from_slice(buf);
        Ok(buf.len())
    }

    /// Attempts to write all the data from a byte slice into a [`Vec<u8>`] buffer by extending the
    /// vector. Since [`Vec<u8>`] can dynamically resize, this method will always succeed as long
    /// as there is available memory.
    #[inline]
    fn write_all(&mut self, buf: &[u8]) -> Result<(), WriteError> {
        self.extend_from_slice(buf);
        Ok(())
    }
}

impl Write for &mut [u8] {
    /// Writes data from a byte slice into a mutable byte array (`&mut [u8]`), up to the length of
    /// the provided buffer.
    #[inline]
    fn write(&mut self, data: &[u8]) -> Result<usize, WriteError> {
        let amt = core::cmp::min(data.len(), self.len());
        let res = core::mem::take(self);
        let (a, b) = res.split_at_mut(amt);
        a.copy_from_slice(&data[..amt]);
        *self = b;
        Ok(amt)
    }

    /// Attempts to write all the data from a byte slice into a mutable byte array (`&mut [u8]`).
    /// If the buffer is not large enough to contain all the data, an error is returned.
    #[inline]
    fn write_all(&mut self, data: &[u8]) -> Result<(), WriteError> {
        if self.write(data)? == data.len() {
            Ok(())
        } else {
            Err(WriteError::WriteZero)
        }
    }
}

/// Interface for working with memory buffers.
///
/// An abstraction for buffer management, allowing implementors to handle either owned memory
/// ([`Slice`] with [`Vec<u8>`]). Utilities are provided to borrow writable memory, retrieve data
/// from the buffer, and manage memory slices.
///
/// This trait is used during the serialization and deserialization
/// of message types in the [`binary_sv2` crate](https://crates.io/crates/binary_sv2).
pub trait Buffer {
    /// The type of slice that the buffer uses.
    type Slice: AsMut<[u8]> + AsRef<[u8]> + Into<Slice>;

    /// Borrows a mutable slice of the buffer, allowing the caller to write data into it. The
    /// caller specifies the length of the data they need to write.
    fn get_writable(&mut self, len: usize) -> &mut [u8];

    /// Provides ownership of a slice in the buffer pool to the caller and updates the buffer
    /// pool's state by modifying the position in `shared_state` that the slice occupies. The pool
    /// now points to the next set of uninitialized space.
    fn get_data_owned(&mut self) -> Self::Slice;

    /// Provides a mutable reference to the written portion of the buffer, up to the specified
    /// length, without transferring ownership of the buffer. This allows the caller to modify the
    /// buffers contents directly without taking ownership.
    fn get_data_by_ref(&mut self, len: usize) -> &mut [u8];

    /// Provides an immutable reference to the written portion of the buffer, up to the specified
    /// length, without transferring ownership of the buffer. This allows the caller to inspect the
    /// buffers contents without modifying or taking ownership.
    fn get_data_by_ref_(&self, len: usize) -> &[u8];

    /// Returns the size of the written portion of the buffer. This is useful for tracking how much
    /// of the buffer has been filled with data. The number of bytes currently written in the
    /// buffer is returned.
    fn len(&self) -> usize;

    /// Modifies the starting point of the buffer, effectively discarding data up to the given
    /// `index`. This can be useful for performance optimizations in situations where older data
    /// is no longer needed, but its use can be unsafe unless you understand its implications.
    fn danger_set_start(&mut self, index: usize);

    /// Returns `true` if the buffer is empty, `false` otherwise.
    fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Determines if the buffer is safe to drop. This typically checks if the buffer contains
    /// essential data that still needs to be processed.
    ///
    /// Returns `true` if the buffer can be safely dropped, `false` otherwise.
    fn is_droppable(&self) -> bool;
}
</file>

<file path="stratum-1.4.0/utils/buffer/src/slice.rs">
// # Slice
//
// Provides efficient memory management for the Sv2 protocol by allowing memory reuse, either
// through owned memory (`Vec<u8>`) or externally managed memory in a buffer pool (`BufferPool`).
//
// `Slice` helps minimize memory allocation overhead by reusing large buffers, improving
// performance and reducing latency in high-throughput environments. Tracks the usage of memory
// slices, ensuring safe reuse across multiple threads via `SharedState`.
//
// ## Key Features
// - **Memory Reuse**: Divides large buffers into smaller slices, reducing the need for frequent
//   allocations.
// - **Shared Access**: Allows safe concurrent access using atomic state tracking (`Arc<AtomicU8>`).
// - **Flexible Management**: Supports both owned memory and externally managed memory.
//
// ## Usage
// 1. **Owned Memory**: For isolated operations, `Slice` manages its own memory (`Vec<u8>`).
// 2. **Buffer Pool**: In high-performance systems, `Slice` references externally managed memory
//    from a buffer pool (`BufferPool`), reducing dynamic memory allocation.
//
// ### Debug Mode
// Provides additional tracking for debugging memory management issues.

use alloc::{sync::Arc, vec::Vec};
use core::sync::atomic::{AtomicU8, Ordering};
#[cfg(feature = "debug")]
use std::time::SystemTime;

// A special index value used to mark `Slice` as ignored in certain operations, such as memory pool
// tracking or state management.
//
// It can be used as a sentinel value for slices that should not be processed or tracked, helping
// differentiate valid slices from those that need to be skipped. When a `Slice`'s `index` is set
// to `INGORE_INDEX`, it is flagged to be ignored and by any logic that processes or tracks slice
// indices.
pub const INGORE_INDEX: u8 = 59;

/// Allows [`Slice`] to be safely transferred between threads.
///
/// [`Slice`] contains a raw pointer (`*mut u8`), so Rust cannot automatically implement [`Send`].
/// The `unsafe` block asserts that memory access is thread-safe, relaying on `SharedState` and
/// atomic operations to prevent data races.
unsafe impl Send for Slice {}

/// A contiguous block of memory, either preallocated or dynamically allocated.
///
/// It serves as a lightweight handle to a memory buffer, allowing for direct manipulation and
/// shared access. It can either hold a reference to a preallocated memory block or own a
/// dynamically allocated buffer (via [`Vec<u8>`]).
#[derive(Debug, Clone)]
pub struct Slice {
    // Raw pointer to the start of the memory block.
    //
    // Allows for efficient access to the underlying memory. Care should be taken when working with
    // raw pointers to avoid memory safety issues. The pointer must be valid and must point to a
    // properly allocated and initialized memory region.
    pub(crate) offset: *mut u8,

    // Length of the memory block in bytes.
    //
    // Represents how much memory is being used. This is critical for safe memory access, as it
    // prevents reading or writing outside the bounds of the buffer.
    pub(crate) len: usize,

    /// Unique identifier (index) of the slice in the shared memory pool.
    ///
    /// When in back or front mode, tracks the slice within the pool and manages memory reuse. It
    /// allows for quick identification of slices when freeing or reassigning memory. If in alloc
    /// mode, it is set to `IGNORE_INDEX`.
    pub index: u8,

    /// Shared state of the memory pool.
    ///
    /// When in back or front mode, tracks how many slices are currently in use and ensures proper
    /// synchronization of memory access across multiple contexts.
    pub shared_state: SharedState,

    /// Optional dynamically allocated buffer.
    ///
    /// If present, the slice owns the memory and is responsible for managing its lifecycle. If
    /// [`None`], the buffer pool is in back or front mode and the slice points to memory managed
    /// by the memory pool. Is `Some(Vec<u8>)` when in alloc mode.
    pub owned: Option<Vec<u8>>,

    // Mode flag to track the state of the slice during development.
    //
    // Useful for identifying whether the slice is being used correctly in different modes (e.g.,
    // whether is is currently being written to or read from). Typically used for logging and
    // debugging.
    #[cfg(feature = "debug")]
    pub mode: u8,

    /// Timestamp to track when the slice was created.
    ///
    /// Useful for diagnosing time-related issues and tracking the lifespan of memory slices during
    /// development and debugging.
    #[cfg(feature = "debug")]
    pub time: SystemTime,
}

impl Slice {
    /// Returns the length of the slice in bytes.
    ///
    /// If the slice owns its memory (`owned`), it returns the length of the owned buffer. If the
    /// slice does not own the memory, it returns `0`.
    pub fn len(&self) -> usize {
        if let Some(owned) = &self.owned {
            owned.len()
        } else {
            0
        }
    }

    /// Checks if the slice is empty.
    ///
    /// Returns `true` if the slice is empty, i.e., it has no data. Otherwise, returns `false`.
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }
}

impl core::ops::Index<usize> for Slice {
    type Output = u8;

    /// Provides immutable indexing access to the [`Slice`] at the specified position.
    ///
    /// Uses `as_ref` to get a reference to the underlying buffer and returns the byte at the
    /// `index`.
    fn index(&self, index: usize) -> &Self::Output {
        self.as_ref().index(index)
    }
}

impl core::ops::IndexMut<usize> for Slice {
    /// Provides mutable indexing access to the [`Slice`] at the specified position.
    ///
    /// Uses `as_mut` to get a mutable reference to the underlying buffer and returns the byte at
    /// the `index`.
    fn index_mut(&mut self, index: usize) -> &mut Self::Output {
        self.as_mut().index_mut(index)
    }
}

impl core::ops::Index<core::ops::RangeFrom<usize>> for Slice {
    type Output = [u8];

    /// Provides immutable slicing access to a range starting from the given `index`.
    ///
    /// Uses `as_ref` to get a reference to the underlying buffer and returns the range.
    fn index(&self, index: core::ops::RangeFrom<usize>) -> &Self::Output {
        self.as_ref().index(index)
    }
}

impl core::ops::IndexMut<core::ops::RangeFrom<usize>> for Slice {
    /// Provides mutable slicing access to a range starting from the given `index`.
    ///
    /// Uses `as_mut` to get a mutable reference to the underlying buffer and returns the range.
    fn index_mut(&mut self, index: core::ops::RangeFrom<usize>) -> &mut Self::Output {
        self.as_mut().index_mut(index)
    }
}

impl core::ops::Index<core::ops::Range<usize>> for Slice {
    type Output = [u8];

    /// Provides immutable slicing access to the specified range within the `Slice`.
    ///
    /// Uses `as_ref` to get a reference to the underlying buffer and returns the specified range.
    fn index(&self, index: core::ops::Range<usize>) -> &Self::Output {
        self.as_ref().index(index)
    }
}

impl core::ops::IndexMut<core::ops::Range<usize>> for Slice {
    /// Provides mutable slicing access to the specified range within the `Slice`.
    ///
    /// Uses `as_mut` to get a mutable reference to the underlying buffer and returns the specified
    /// range.
    fn index_mut(&mut self, index: core::ops::Range<usize>) -> &mut Self::Output {
        self.as_mut().index_mut(index)
    }
}

impl core::ops::Index<core::ops::RangeFull> for Slice {
    type Output = [u8];

    /// Provides immutable access to the entire range of the [`Slice`].
    ///
    /// Uses `as_ref` to get a reference to the entire underlying buffer.
    fn index(&self, index: core::ops::RangeFull) -> &Self::Output {
        self.as_ref().index(index)
    }
}

impl AsMut<[u8]> for Slice {
    /// Converts the [`Slice`] into a mutable slice of bytes (`&mut [u8]`).
    ///
    /// Returns the owned buffer if present, otherwise converts the raw pointer and length into a
    /// mutable slice.
    #[inline(always)]
    fn as_mut(&mut self) -> &mut [u8] {
        match self.owned.as_mut() {
            None => unsafe { core::slice::from_raw_parts_mut(self.offset, self.len) },
            Some(x) => x,
        }
    }
}

impl AsRef<[u8]> for Slice {
    /// Converts the [`Slice`] into an immutable slice of bytes (`&[u8]`).
    ///
    /// Returns the owned buffer if present, otherwise converts the raw pointer and length into an
    /// immutable slice.
    #[inline(always)]
    fn as_ref(&self) -> &[u8] {
        match self.owned.as_ref() {
            None => unsafe { core::slice::from_raw_parts_mut(self.offset, self.len) },
            Some(x) => x,
        }
    }
}

impl Drop for Slice {
    /// Toggles the shared state when the slice is dropped, allowing the memory to be reused.
    ///
    /// In debug mode, it also tracks the `mode` of the slice when it is dropped.
    fn drop(&mut self) {
        #[cfg(feature = "debug")]
        self.shared_state.toogle(self.index, self.mode);

        #[cfg(not(feature = "debug"))]
        self.shared_state.toogle(self.index);
    }
}

impl From<Vec<u8>> for Slice {
    /// Creates a [`Slice`] from a [`Vec<u8>`], taking ownership of the vector.
    ///
    /// Initializes the [`Slice`] with the vector's pointer and sets the length to `0`.
    fn from(mut v: Vec<u8>) -> Self {
        let offset = v[0..].as_mut_ptr();
        Slice {
            offset,
            len: 0,
            // The slice's memory is owned by a `Vec<u8>`, so the slice does not need an `index` in
            // the pool to manage the memory
            index: crate::slice::INGORE_INDEX,
            shared_state: SharedState::new(),
            owned: Some(v),
            #[cfg(feature = "debug")]
            mode: 2,
            #[cfg(feature = "debug")]
            time: SystemTime::now(),
        }
    }
}

// The shared state of the buffer pool.
//
// Encapsulates an atomic 8-bit value (`AtomicU8`) to track the shared state of memory slices in a
// thread-safe manner. It uses atomic operations to ensure that memory tracking can be done
// concurrently without locks.
//
// Each bit in the `AtomicU8` represents the state of a memory slot (e.g., whether it is allocated
// or free) in the buffer pool, allowing the system to manage and synchronize memory usage across
// multiple slices.
//
// `SharedState` acts like a reference counter, helping the buffer pool know when a buffer slice is
// safe to clear. Each time a memory slice is used or released, the corresponding bit in the shared
// state is toggled. When no slices are in use (all bits are zero), the buffer pool can safely
// reclaim or reuse the memory.
//
// This system ensures that no memory is prematurely cleared while it is still being referenced.
// The buffer pool checks whether any slice is still in use before clearing, and only when the
// shared state indicates that all references have been dropped (i.e., no unprocessed messages
// remain) can the buffer pool safely clear or reuse the memory.
#[derive(Clone, Debug)]
pub struct SharedState(Arc<AtomicU8>);

impl Default for SharedState {
    // Creates a new `SharedState` with an internal `AtomicU8` initialized to `0`, indicating no
    // memory slots are in use.
    fn default() -> Self {
        Self::new()
    }
}

impl SharedState {
    // Creates a new `SharedState` with an internal `AtomicU8` initialized to `0`, indicating no
    // memory slots are in use.
    pub fn new() -> Self {
        Self(Arc::new(AtomicU8::new(0)))
    }

    // Atomically loads and returns the current value of the `SharedState` using the specified
    // memory ordering.
    //
    // Returns the current state of the memory slots as an 8-bit value.
    #[inline(always)]
    pub fn load(&self, ordering: Ordering) -> u8 {
        self.0.load(ordering)
    }

    // Toggles the bit at the specified `position` in the `SharedState`, including logs regarding
    // the shared state of the memory after toggling. The `mode` parameter is used to differentiate
    // between different states or operations (e.g., reading or writing) for debugging purposes.
    //
    // After a message held by a buffer slice has been processed, the corresponding bit in the
    // shared state is toggled (flipped). When the shared state for a given region reaches zero
    // (i.e., all bits are cleared), the buffer pool knows it can safely reclaim or reuse that
    // memory slice.
    //
    // Uses atomic bitwise operations to ensure thread-safe toggling without locks. It manipulates
    // the shared state in-place using the `AtomicU8::fetch_update` method, which atomically
    // applies a bitwise XOR (`^`) to toggle the bit at the specified `position`.
    //
    // Panics if the `position` is outside the range of 1-8, as this refers to an invalid bit.
    #[cfg(feature = "debug")]
    pub fn toogle(&self, position: u8, mode: u8) {
        let mask: u8 = match position {
            1 => 0b10000000,
            2 => 0b01000000,
            3 => 0b00100000,
            4 => 0b00010000,
            5 => 0b00001000,
            6 => 0b00000100,
            7 => 0b00000010,
            8 => 0b00000001,
            INGORE_INDEX => return,
            _ => panic!("{}", position),
        };
        //if position == 2 {
        //    let bt = Backtrace::force_capture();
        //    println!("{:#?}", bt);
        //};
        self.0
            .fetch_update(Ordering::Relaxed, Ordering::Relaxed, |mut shared_state| {
                let pre = shared_state;
                shared_state ^= mask;
                println!("TOOGLE:: {} {:b} {:b}", mode, pre, shared_state);
                Some(shared_state)
            })
            .unwrap();
    }

    // Toggles the bit at the specified `position` in the `SharedState`.
    //
    // After a message held by a buffer slice has been processed, the corresponding bit in the
    // shared state is toggled (flipped). When the shared state for a given region reaches zero
    // (i.e., all bits are cleared), the buffer pool knows it can safely reclaim or reuse that
    // memory slice.
    //
    // Uses atomic bitwise operations to ensure thread-safe toggling without locks. It manipulates
    // the shared state in-place using the `AtomicU8::fetch_update` method, which atomically
    // applies a bitwise XOR (`^`) to toggle the bit at the specified `position`.
    //
    // Panics if the `position` is outside the range of 1-8, as this refers to an invalid bit.
    #[cfg(not(feature = "debug"))]
    pub fn toogle(&self, position: u8) {
        let mask: u8 = match position {
            1 => 0b10000000,
            2 => 0b01000000,
            3 => 0b00100000,
            4 => 0b00010000,
            5 => 0b00001000,
            6 => 0b00000100,
            7 => 0b00000010,
            8 => 0b00000001,
            INGORE_INDEX => return,
            _ => panic!("{}", position),
        };
        self.0
            .fetch_update(Ordering::Relaxed, Ordering::Relaxed, |mut shared_state| {
                shared_state ^= mask;
                Some(shared_state)
            })
            .unwrap();
    }
}
</file>

<file path="stratum-1.4.0/utils/buffer/src/test.rs">
use alloc::vec::Vec;

use crate::{buffer_pool::BufferPool as Pool, slice::Slice, Buffer};
use rand::Rng;

#[test]
fn test() {
    assert!(true)
}

#[test]
fn pool_capicity_without_alloc() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new_fail_system_memory(8 * 5);

    let mut slices: Vec<Slice> = Vec::new();

    for _ in 0..8 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push(owned);
    }
}

#[test]
fn it_drop() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new_fail_system_memory(8 * 5);

    for _ in 0..100 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());
    }
}

#[test]
fn alloc_more_than_pool_capacity() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new(8 * 5);

    let mut slices: Vec<Slice> = Vec::new();

    for _ in 0..18 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push(owned);
    }
}

#[test]
#[should_panic]
fn alloc_more_than_pool_capacity_2() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new_fail_system_memory(8 * 5);

    let mut slices: Vec<Slice> = Vec::new();

    for _ in 0..9 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push(owned);
    }
}

#[test]
fn alloc_more_than_byte_capacity() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new(1);

    let mut slices: Vec<Slice> = Vec::new();

    for _ in 0..18 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push(owned);
    }
}

#[test]
#[should_panic]
fn alloc_more_than_byte_capacity_2() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new_fail_system_memory(1);

    let mut slices: Vec<Slice> = Vec::new();

    for _ in 0..18 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push(owned);
    }
}

#[test]
fn back_front_back() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new_fail_system_memory(8 * 5);

    let mut slices: alloc::collections::VecDeque<Slice> = alloc::collections::VecDeque::new();

    for _ in 0..8 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        assert!(pool.is_back_mode());
    }
    // tail 8 front 0

    // Free the first 3 slices
    slices.pop_front();
    slices.pop_front();
    slices.pop_front();

    // tail 5 front 0

    // Reallocate in front
    for _ in 0..3 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_front(owned);
        assert!(pool.is_front_mode());
    }

    // tail 5 front 3

    // Free the first 3 slices in the back
    slices.pop_back();
    slices.pop_back();
    slices.pop_back();

    // tail 2 front 3

    // Reallocate in back
    for _ in 0..3 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
    }
}
#[test]
fn back_front_alloc() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new(8 * 5);

    let mut slices: alloc::collections::VecDeque<Slice> = alloc::collections::VecDeque::new();

    for _ in 0..8 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
    }

    // Free the first 3 slices
    slices.pop_front();
    slices.pop_front();
    slices.pop_front();

    // Reallocate in front
    for _ in 0..3 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_front(owned);
    }

    assert!(pool.is_front_mode());

    // Allocare in alloc mode
    for _ in 0..30 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
    }
    assert!(!pool.is_front_mode());
}

#[test]
fn back_alloc_back() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new(8 * 5);

    let mut slices: alloc::collections::VecDeque<Slice> = alloc::collections::VecDeque::new();

    let mut control_slices: alloc::collections::VecDeque<[u8; 5]> =
        alloc::collections::VecDeque::new();

    // Allocate 8 slices in back mode
    for _ in 0..8 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        control_slices.push_back(src);
    }

    // Allocate 30 with alloc
    for _ in 0..30 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_front(owned);
        control_slices.push_front(src);
    }

    // Free the last 3 slices
    slices.pop_back();
    slices.pop_back();
    slices.pop_back();

    control_slices.pop_back();
    control_slices.pop_back();
    control_slices.pop_back();

    // Allocate in back mode
    for _ in 0..3 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        control_slices.push_back(src);
        assert!(pool.is_back_mode());
    }

    for i in 0..slices.len() {
        assert!(slices[i].as_mut() == &mut control_slices[i][..]);
    }
}

#[test]
fn back_alloc_front() {
    let mut rng = rand::thread_rng();

    // Allocate a pool of 8 * 5 bytes
    let mut pool = Pool::new(8 * 5);

    let mut slices: alloc::collections::VecDeque<Slice> = alloc::collections::VecDeque::new();

    let mut control_slices: alloc::collections::VecDeque<[u8; 5]> =
        alloc::collections::VecDeque::new();

    for _ in 0..8 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        control_slices.push_back(src);
    }

    // Allocate with alloc
    for _ in 0..30 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        control_slices.push_back(src);
    }

    // Free the first 3 slices
    slices.pop_front().unwrap();
    slices.pop_front().unwrap();
    slices.pop_front().unwrap();

    control_slices.pop_front().unwrap();
    control_slices.pop_front().unwrap();
    control_slices.pop_front().unwrap();

    // Allocare in back
    for _ in 0..3 {
        // Allocate a slice of 5 bytes in the pool
        let n1: u8 = rng.gen();
        let n2: u8 = rng.gen();
        let n3: u8 = rng.gen();
        let n4: u8 = rng.gen();
        let n5: u8 = rng.gen();
        let mut src = [n1, n2, n3, n4, n5];

        let writable = pool.get_writable(5);
        writable.copy_from_slice(&src[..]);

        let mut owned = pool.get_data_owned();
        assert_eq!(&mut src[..], owned.as_mut());

        slices.push_back(owned);
        control_slices.push_back(src);
    }

    assert!(pool.is_front_mode());

    for i in 0..slices.len() {
        assert!(slices[i].as_mut() == &mut control_slices[i][..]);
    }
}
</file>

<file path="stratum-1.4.0/utils/Cargo.toml">
[workspace]
resolver="2"

members = [
    "buffer",
    "error-handling",
    "key-utils",
    "bip32-key-derivation",
]

exclude = [
    "stratum-message-generator"
]
</file>

<file path="stratum-1.4.0/utils/error-handling/Cargo.toml">
[package]
name = "error_handling"
version = "1.0.0"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "Macro used to clean and centralize error handling within async processes"
documentation = "https://docs.rs/error_handling"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]
</file>

<file path="stratum-1.4.0/utils/error-handling/src/lib.rs">
#![allow(clippy::crate_in_macro_def)]
/// # Description
/// This macro handles errors inserting error handling logic for a given `Result<T,
/// crate::error::Error<'a>>` it is used by passing in a `Sender<crate::status::Status>` as the
/// first parameter and a `Result<T, crate::error::Error<'a>>` as the second parameter.
///
/// NOTE: There are 3 caveats to using this macro:
/// 1. can only be used within async functions since status needs to be send over async channel
/// 2. The macro must be used within a loop since it calls `continue` on error. If `unwraps/expects`
///    are used within a function without a loop, you should make the function return a result and
///    handle the result within a main loop
/// 3. The macro must be able to reference the user defined function `crate::status::handle_error(T,
///    U) -> ErrorBranch;` where U is the output of `e.into()`
///
/// # Example
/// ``` ignore
/// let (tx_status: Sender<Status>, rx_status) = async_channel::unbounded();
/// let variable = handle_result!(
///     tx_status,
///     type_.try_into()
/// );
/// ```
#[macro_export]
macro_rules! handle_result {
    ($sender:expr, $res:expr) => {
        match $res {
            Ok(val) => val,
            Err(e) => {
                // handle error
                let res = crate::status::handle_error(&$sender, e.into()).await;
                match res {
                    error_handling::ErrorBranch::Break => break,
                    error_handling::ErrorBranch::Continue => continue,
                }
            }
        }
    };
}

pub enum ErrorBranch {
    Break,
    Continue,
}
</file>

<file path="stratum-1.4.0/utils/key-utils/Cargo.toml">
[package]
name = "key-utils"
version = "1.2.0"
authors = ["The Stratum V2 Developers"]
edition = "2021"
description = "Key utils"
documentation = "https://docs.rs/key-utils"
readme = "README.md"
homepage = "https://stratumprotocol.org"
repository = "https://github.com/stratum-mining/stratum"
license = "MIT OR Apache-2.0"
keywords = ["stratum", "mining", "bitcoin", "protocol"]


# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
name = "key_utils"
path = "src/lib.rs"

[[bin]]
name = "key-utils-bin"
path = "src/main.rs"

[dependencies]
bs58 = { version ="0.4.0", default-features = false, features = ["check", "alloc"] }
secp256k1 = { version = "0.28.2", default-features = false, features =["alloc","rand"] }
serde = { version = "1.0.89", features = ["derive","alloc"], default-features = false }
rand = {version = "0.8.5", default-features = false }
rustversion = "1.0"

[dev-dependencies]
toml = { version = "0.5.6", git = "https://github.com/diondokter/toml-rs", default-features = false, rev = "c4161aa" }

[features]
default = ["std"]
std = ["bs58/std","secp256k1/rand-std", "rand/std", "rand/std_rng"]
</file>

<file path="stratum-1.4.0/utils/key-utils/README.md">
# Key utils

## Binary
It can be used as binary to a random pair of base58 encoded secp256k1 keys: `cargo run`

## Library
It can be imported by other applications that need to serialize and deserialize secp256k1 keys.
</file>

<file path="stratum-1.4.0/utils/key-utils/src/lib.rs">
#![cfg_attr(not(feature = "std"), no_std)]

extern crate alloc;

use alloc::{
    string::{String, ToString},
    vec::Vec,
};
use bs58::{decode, decode::Error as Bs58DecodeError};
use core::{convert::TryFrom, fmt::Display, str::FromStr};
use secp256k1::{
    schnorr::Signature, Keypair, Message as SecpMessage, Secp256k1, SecretKey, SignOnly,
    VerifyOnly, XOnlyPublicKey,
};
use serde::{Deserialize, Serialize};

#[derive(Debug)]
pub enum Error {
    Bs58Decode(Bs58DecodeError),
    Secp256k1(secp256k1::Error),
    KeyVersion(u16),
    KeyLength,
    Custom(String),
}

impl Display for Error {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        match self {
            Self::Bs58Decode(error) => write!(f, "Base58 code error: {error}"),
            Self::Secp256k1(error) => write!(f, "Secp256k1 error: {error}"),
            Self::KeyVersion(obtained) => {
                write!(f, "Unknown public key version. version found: {obtained}")
            }
            Self::KeyLength => write!(f, "Bad key length"),
            Self::Custom(error) => write!(f, "Custom error: {error}"),
        }
    }
}

#[cfg(feature = "std")]
impl std::error::Error for Error {}
#[cfg(not(feature = "std"))]
#[rustversion::since(1.81)]
impl core::error::Error for Error {}

impl From<Bs58DecodeError> for Error {
    fn from(e: Bs58DecodeError) -> Self {
        Error::Bs58Decode(e)
    }
}

impl From<secp256k1::Error> for Error {
    fn from(e: secp256k1::Error) -> Self {
        Error::Secp256k1(e)
    }
}

#[derive(Debug, Copy, Clone, Serialize, Deserialize)]
#[serde(into = "String", try_from = "String")]
pub struct Secp256k1SecretKey(pub SecretKey);

impl TryFrom<String> for Secp256k1SecretKey {
    type Error = Error;

    fn try_from(value: String) -> Result<Self, Self::Error> {
        value.parse()
    }
}

impl FromStr for Secp256k1SecretKey {
    type Err = Error;

    fn from_str(value: &str) -> Result<Self, Self::Err> {
        let decoded = decode(value).with_check(None).into_vec()?;
        let secret = SecretKey::from_slice(&decoded)?;
        Ok(Secp256k1SecretKey(secret))
    }
}

impl From<Secp256k1SecretKey> for String {
    fn from(secret: Secp256k1SecretKey) -> Self {
        secret.to_string()
    }
}

impl Display for Secp256k1SecretKey {
    fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
        let bytes = self.0.secret_bytes();
        f.write_str(&bs58::encode(bytes).with_check().into_string())
    }
}

#[derive(Debug, Copy, Clone, Serialize, Deserialize)]
#[serde(into = "String", try_from = "String")]
pub struct Secp256k1PublicKey(pub XOnlyPublicKey);

impl TryFrom<String> for Secp256k1PublicKey {
    type Error = Error;

    fn try_from(value: String) -> Result<Self, Self::Error> {
        value.parse()
    }
}

impl FromStr for Secp256k1PublicKey {
    type Err = Error;

    fn from_str(value: &str) -> Result<Self, Self::Err> {
        let decoded = decode(value).with_check(None).into_vec()?;
        if decoded.len() < 34 {
            return Err(Error::KeyLength);
        }
        let key_version =
            u16::from_le_bytes(decoded[..2].try_into().expect("Invalid array length"));
        if key_version != 1 {
            return Err(Error::KeyVersion(key_version));
        }
        let public = XOnlyPublicKey::from_slice(&decoded[2..]).map_err(Error::Secp256k1)?;
        Ok(Secp256k1PublicKey(public))
    }
}

impl From<Secp256k1PublicKey> for String {
    fn from(public: Secp256k1PublicKey) -> Self {
        public.to_string()
    }
}

impl Display for Secp256k1PublicKey {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        let mut output = [0_u8; 34];
        output[0] = 1;
        let bytes = self.0.serialize();
        output[2..].copy_from_slice(&bytes);
        f.write_str(&bs58::encode(&output).with_check().into_string())
    }
}

impl Secp256k1PublicKey {
    pub fn into_bytes(self) -> [u8; 32] {
        self.0.serialize()
    }
}
impl Secp256k1SecretKey {
    pub fn into_bytes(self) -> [u8; 32] {
        self.0.secret_bytes()
    }
}

impl From<Secp256k1SecretKey> for Secp256k1PublicKey {
    fn from(value: Secp256k1SecretKey) -> Self {
        let context = secp256k1::Secp256k1::new();
        let (x_coordinate, _) = value.0.public_key(&context).x_only_public_key();
        Self(x_coordinate)
    }
}

pub struct SignatureService {
    secp_sign: Secp256k1<SignOnly>,
    secp_verify: Secp256k1<VerifyOnly>,
}

impl SignatureService {
    pub fn new() -> Self {
        SignatureService {
            secp_sign: Secp256k1::signing_only(),
            secp_verify: Secp256k1::verification_only(),
        }
    }

    #[cfg(feature = "std")]
    pub fn sign(&self, message: Vec<u8>, private_key: SecretKey) -> Signature {
        self.sign_with_rng(message, private_key, &mut rand::thread_rng())
    }

    #[inline]
    pub fn sign_with_rng<R: rand::Rng + rand::CryptoRng>(
        &self,
        message: Vec<u8>,
        private_key: SecretKey,
        rng: &mut R,
    ) -> Signature {
        let secret_key = private_key;
        let kp = Keypair::from_secret_key(&self.secp_sign, &secret_key);

        self.secp_sign.sign_schnorr_with_rng(
            &SecpMessage::from_digest_slice(&message).unwrap(),
            &kp,
            rng,
        )
    }

    pub fn verify(
        &self,
        message: Vec<u8>,
        signature: secp256k1::schnorr::Signature,
        public_key: XOnlyPublicKey,
    ) -> Result<(), secp256k1::Error> {
        let x_only_public_key = public_key;

        // Verify signature
        self.secp_verify.verify_schnorr(
            &signature,
            &secp256k1::Message::from_digest_slice(&message)?,
            &x_only_public_key,
        )
    }
}

impl Default for SignatureService {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod test {
    use super::*;

    #[test]
    fn key_conversions() {
        let secret_key = "zmBEmPhqo3A92FkiLVvyCz6htc3e53ph3ZbD4ASqGaLjwnFLi";
        let public_key = "9bDuixKmZqAJnrmP746n8zU1wyAQRrus7th9dxnkPg6RzQvCnan";
        let bad_public_key1 = "9bDuixKmZqAJnrmP746n8zU1wyAQRrus7th9dxnkPg6RzQvCnam"; // invalid checksum (swapped char)
        let bad_public_key2 = "2myPhc5vkPzuC5FXNK5tee79WmP7uoLh55SxezoF8iqwF3E3rnPY"; // invalid version (version 12)
        let bad_public_key3 = "2wmHTKZkLg2QzXyEXGMBXzKP7JXDUt8yy9SA5hoQwERc92qR6c"; // invalid length (1 B missing)

        let error = bad_public_key1
            .parse::<Secp256k1PublicKey>()
            .expect_err("Bad bud public key failed to raise error");
        assert!(
            matches!(error, Error::Bs58Decode(_)),
            "expected failed checksum error, got {}",
            error
        );
        let error = bad_public_key2
            .parse::<Secp256k1PublicKey>()
            .expect_err("Bad bud public key failed to raise error");
        assert!(
            matches!(error, Error::KeyVersion(_)),
            "expected invalid key version error, got {}",
            error
        );
        let error = bad_public_key3
            .parse::<Secp256k1PublicKey>()
            .expect_err("Bad bud public key failed to raise error");
        assert!(
            matches!(error, Error::KeyLength),
            "expected invalid key length error, got {}",
            error
        );

        let parsed_key = secret_key
            .parse::<Secp256k1SecretKey>()
            .expect("Invalid test key");

        let calculated_public_key = Secp256k1PublicKey::from(parsed_key);
        assert_eq!(calculated_public_key.to_string(), public_key);

        let parsed_public_key = public_key
            .parse::<Secp256k1PublicKey>()
            .expect("Invalid test pubkey");
        assert_eq!(calculated_public_key.0, parsed_public_key.0);
    }
}
</file>

<file path="stratum-1.4.0/utils/key-utils/src/main.rs">
#[cfg(feature = "std")]
use ::key_utils::{Secp256k1PublicKey, Secp256k1SecretKey};
#[cfg(feature = "std")]
use secp256k1::{rand, Keypair, Secp256k1};

#[cfg(feature = "std")]
fn generate_key() -> (Secp256k1SecretKey, Secp256k1PublicKey) {
    let secp = Secp256k1::new();
    let (secret_key, _) = secp.generate_keypair(&mut rand::thread_rng());
    let kp = Keypair::from_secret_key(&secp, &secret_key);
    if kp.x_only_public_key().1 == secp256k1::Parity::Even {
        (
            Secp256k1SecretKey(kp.secret_key()),
            Secp256k1PublicKey(kp.x_only_public_key().0),
        )
    } else {
        generate_key()
    }
}

#[cfg(feature = "std")]
fn main() {
    let (secret, public) = generate_key();
    let secret: String = secret.into();
    let public: String = public.into();
    println!("Secret Key: {secret}");
    println!("Public Key: {public}");
}

#[cfg(not(feature = "std"))]
fn main() {}
</file>

<file path="stratum-1.4.0/utils/tarpaulin.toml">
[default]
run-types = [ "Lib" ]
timeout = "120s"
fail-under = 0

[report]
out = ["Xml"]
</file>

</files>
